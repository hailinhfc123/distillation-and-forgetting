{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b000096c-df2b-4e57-a2c3-aea36de1f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = \"/scratches/dialfs/alta/hln35/.cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratches/dialfs/alta/hln35/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f907f041-3f90-497e-ae3e-94f87407111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b72a828-17e0-4250-975a-54818ea1b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = \"google/flan-t5-small\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cae8529b-f7cc-4257-ac87-63306f38e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_points = load_dataset(\"ai2_arc\", \"ARC-Easy\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5e774c2c-e10f-45fe-9a46-646d70ddb6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey'],\n",
       "        num_rows: 2251\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey'],\n",
       "        num_rows: 2376\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey'],\n",
       "        num_rows: 570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c67a9537-626d-413a-8dbf-6edb9c2240c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_7220990',\n",
       " 'question': 'Which factor will most likely cause a person to develop a fever?',\n",
       " 'choices': {'text': ['a leg muscle relaxing after exercise',\n",
       "   'a bacterial population in the bloodstream',\n",
       "   'several viral particles on the skin',\n",
       "   'carbohydrates being digested in the stomach'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'B'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f7c4d793-4e57-45a5-87b4-164f8df3522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = data_points.filter(lambda x: len(x['choices']['label']) == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1f1e444b-252c-426e-b936-03a502c90c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_ans = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "ans_to_index = {\"A\" : \"0\", \"B\" : \"1\", \"C\" : \"2\", \"D\": \"3\"}\n",
    "ans_id_dict = {71: \"A\", 272: \"B\", 205: \"C\", 309: \"D\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "643fd143-bed0-4dc2-8e0e-4dcbdb7203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"context: for each questions there are a few choices of answer. answer this question by choosing the best choice either A, B, C, or D: \"\n",
    "\n",
    "\n",
    "def preprocess_function(data_points):\n",
    "    inputs = []\n",
    "    for i in range(len(data_points[\"question\"])):\n",
    "        if len(data_points[\"choices\"][i][\"label\"]) != 4:\n",
    "            continue\n",
    "        labels = [index_to_ans[int(t)-1] if t.isdigit() else t for t in data_points[\"choices\"][i][\"label\"]]\n",
    "        q = data_points[\"question\"][i]\n",
    "        choices = \"\"\n",
    "        choice = \"\"\n",
    "        for t in range(len(labels)):\n",
    "            choices += labels[t] + \" \" + data_points[\"choices\"][i][\"text\"][t] + \". \"\n",
    "            \n",
    "        text = prefix + q + \"Choices: \" + choices\n",
    "        inputs.append(text)\n",
    "    model_inputs = tokenizer(inputs, truncation=True)\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e3bccc6f-6a68-425e-9a8d-8946eda42e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3550aadd-fef1-4ba0-aa3c-387d4535966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_small)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_small)\n",
    "model1 = AutoModel.from_pretrained(model_small)\n",
    "model2 = AutoModelForQuestionAnswering.from_pretrained(model_small)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a182b7a5-4ee8-41d8-b373-7c1c9c09b93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d4f614b1-f1a6-43b0-a008-76fae7e71ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "786d67c9-a693-4a12-b553-51c5a287583e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForQuestionAnswering(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "72f0e44f-4187-4437-9425-d5d5c641fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "037a19e0-4699-432e-bde8-d930d0e35afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "question = data_points['train'][0][\"question\"]\n",
    "choices = \"\"\n",
    "choice = \"\"\n",
    "ans = \"\"\n",
    "for t in range(len(data_points['train'][0][\"choices\"][\"label\"])):\n",
    "    choices += data_points['train'][0][\"choices\"][\"label\"][t] + \" \" + data_points['train'][0][\"choices\"][\"text\"][t] + \". \"\n",
    "    \n",
    "text = prefix + question + \"Choices: \" + choices\n",
    "inputs = tokenizer(text, truncation=True, return_tensors=\"pt\").input_ids\n",
    "preds_tokenized = model(inputs.to(\"cuda\"), decoder_input_ids=torch.tensor([[model.config.decoder_start_token_id,]]).to(\"cuda\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "de40338e-5a88-4040-80c1-6a5f19057ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 71, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['This answer is A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c0c2c44f-8f52-4bad-ad98-7191d8772084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 272, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['This answer is B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f0d69579-1ee8-44e7-9a9a-9c3c5dccd031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 205, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['This answer is C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c7cbb13-ea34-4f63-b206-7fc3d5acff7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 309, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['This answer is D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8a420e7-dbad-4aa2-88d0-c79f0a19d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_id_dict = {71: \"A\", 272: \"B\", 205: \"C\", 309: \"D\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8ecbc507-a62d-4908-a532-2cd8b80e4442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32128])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_softmax = torch.nn.functional.softmax(preds_tokenized.logits, dim=-1)\n",
    "preds_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06c32f30-0c68-4583-a90b-eff959ef9356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2568]], device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor([[0.3037]], device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor([[0.3006]], device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor([[0.1347]], device='cuda:0', grad_fn=<SelectBackward0>)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_prob = []\n",
    "for t in ans_id_dict.keys():\n",
    "    answers_prob.append(preds_softmax[...,t])\n",
    "answers_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b167285a-1373-4869-af41-a733d0820000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = data_points['train'][0][\"answerKey\"]\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cebc337a-e765-42b7-aeca-a0f71fbe710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_large = \"google/flan-t5-large\"\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(model_large)\n",
    "model_large = AutoModelForSeq2SeqLM.from_pretrained(model_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dd87f90e-5a53-4e77-acfb-a0b54750ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 71, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large(['This answer is A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "75304ea0-d155-401b-becd-7604cc815590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 272, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large(['This answer is B'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "78ea629c-aadd-433f-9743-d17a93657400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 205, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large(['This answer is C'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "312357b3-da0d-4d88-8018-3243d2055a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 1525, 19, 309, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large(['This answer is D'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5c97524c-3c9e-4b52-8e52-717d415d92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data_points.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c48dd860-8d1e-4b24-a634-0ef8f0d69f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2241\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2365\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'choices', 'answerKey', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 567\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bfa3dec0-b695-4a27-b7d3-0b4cfdede797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_SC_401169',\n",
       " 'question': 'When a switch is used in an electrical circuit, the switch can',\n",
       " 'choices': {'text': ['cause the charge to build.',\n",
       "   'increase and decrease the voltage.',\n",
       "   'cause the current to change direction.',\n",
       "   'stop and start the flow of current.'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'D',\n",
       " 'input_ids': [2625,\n",
       "  10,\n",
       "  21,\n",
       "  284,\n",
       "  746,\n",
       "  132,\n",
       "  33,\n",
       "  3,\n",
       "  9,\n",
       "  360,\n",
       "  3703,\n",
       "  13,\n",
       "  1525,\n",
       "  5,\n",
       "  1525,\n",
       "  48,\n",
       "  822,\n",
       "  57,\n",
       "  4622,\n",
       "  8,\n",
       "  200,\n",
       "  1160,\n",
       "  893,\n",
       "  71,\n",
       "  6,\n",
       "  272,\n",
       "  6,\n",
       "  205,\n",
       "  6,\n",
       "  42,\n",
       "  309,\n",
       "  10,\n",
       "  366,\n",
       "  3,\n",
       "  9,\n",
       "  3615,\n",
       "  19,\n",
       "  261,\n",
       "  16,\n",
       "  46,\n",
       "  4850,\n",
       "  4558,\n",
       "  6,\n",
       "  8,\n",
       "  3615,\n",
       "  54,\n",
       "  3541,\n",
       "  32,\n",
       "  867,\n",
       "  7,\n",
       "  10,\n",
       "  71,\n",
       "  1137,\n",
       "  8,\n",
       "  1567,\n",
       "  12,\n",
       "  918,\n",
       "  5,\n",
       "  5,\n",
       "  272,\n",
       "  993,\n",
       "  11,\n",
       "  6313,\n",
       "  8,\n",
       "  10594,\n",
       "  5,\n",
       "  5,\n",
       "  205,\n",
       "  1137,\n",
       "  8,\n",
       "  750,\n",
       "  12,\n",
       "  483,\n",
       "  2212,\n",
       "  5,\n",
       "  5,\n",
       "  309,\n",
       "  1190,\n",
       "  11,\n",
       "  456,\n",
       "  8,\n",
       "  2537,\n",
       "  13,\n",
       "  750,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d17d8b0b-a6db-4978-9455-89734e1c6ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context: for each questions there are a few choices of answer. answer this question by choosing the best choice either A, B, C, or D: Which factor will most likely cause a person to develop a fever?Choices: A a leg muscle relaxing after exercise. B a bacterial population in the bloodstream. C several viral particles on the skin. D carbohydrates being digested in the stomach. </s>'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large.decode(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7906cb81-7fd7-4d6f-9efd-42cc42c7aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = tokenized_datasets[\"train\"][\"input_ids\"]\n",
    "large_model_outputs = []\n",
    "results = {}\n",
    "group_len = 20\n",
    "# for i in range(0, len(train_input_ids)):\n",
    "#         test_tensor = torch.tensor([train_input_ids[i]])\n",
    "#         preds = model_large(input_ids=test_tensor, decoder_input_ids=torch.tensor([[model_large.config.decoder_start_token_id,]]))      \n",
    "#         preds_prob = []\n",
    "#         for t in ans_id_dict.keys():\n",
    "#             preds_prob.append(preds.logits[...,t][0][0].item())\n",
    "            \n",
    "#         large_model_outputs.append(preds_prob)            \n",
    "\n",
    "# with open(\"QA_large_model_probability_output.txt\", \"w\") as fp:\n",
    "#     json.dump(large_model_outputs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "16106d31-1583-48b6-8770-710fe763fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('QA_large_model_probability_output.txt') as f:\n",
    "    large_model_outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7acf480c-cc2d-4670-8d7b-c17eba4dbe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.8682332038879395,\n",
       "  0.728309154510498,\n",
       "  -0.7202052474021912,\n",
       "  -3.2089343070983887],\n",
       " [-2.0898027420043945,\n",
       "  0.8677738308906555,\n",
       "  -1.2735341787338257,\n",
       "  -0.14586102962493896],\n",
       " [-2.828411340713501,\n",
       "  -1.4099788665771484,\n",
       "  -0.4949261248111725,\n",
       "  0.8294223546981812],\n",
       " [0.7920562624931335,\n",
       "  -0.9160768985748291,\n",
       "  -1.4811376333236694,\n",
       "  -0.6669895648956299],\n",
       " [-3.5180013179779053,\n",
       "  -3.3451547622680664,\n",
       "  0.844027578830719,\n",
       "  -2.4157066345214844]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5b527ed2-41fa-437b-ae7e-12b480bbd781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.8682332038879395,\n",
       "  0.728309154510498,\n",
       "  -0.7202052474021912,\n",
       "  -3.2089343070983887],\n",
       " [-2.0898027420043945,\n",
       "  0.8677738308906555,\n",
       "  -1.2735341787338257,\n",
       "  -0.14586102962493896],\n",
       " [-2.828411340713501,\n",
       "  -1.4099788665771484,\n",
       "  -0.4949261248111725,\n",
       "  0.8294223546981812],\n",
       " [0.7920562624931335,\n",
       "  -0.9160768985748291,\n",
       "  -1.4811376333236694,\n",
       "  -0.6669895648956299],\n",
       " [-3.5180013179779053,\n",
       "  -3.3451547622680664,\n",
       "  0.844027578830719,\n",
       "  -2.4157066345214844]]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8ccfccb3-afb6-492c-af2b-f2bae806292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].add_column(\"labels\", large_model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "78561165-4bae-4331-9936-439aa1da3fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a8a9234c-f16a-4f86-b4bf-16084d491f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4e297175-eae6-4915-85f8-a3b7cff14367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    result = metric.compute(predictions=predictions, references=labels)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5241ff1b-3673-40e4-a923-f46c6172731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers import Trainer, TrainingArguments\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # print(inputs)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels = labels[0,...]\n",
    "        # print(labels)\n",
    "        # forward pass\n",
    "        # print(inputs['input_ids'].shape)\n",
    "        \n",
    "        # print(model)\n",
    "        outputs = model(**inputs, decoder_input_ids=torch.tensor([[0]]))\n",
    "        # print(outputs.logits)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # print(logits.shape)\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        preds_prod = []\n",
    "        for i in range(1):\n",
    "            pred_prob = []\n",
    "            for t in ans_id_dict.keys():\n",
    "                pred_prob.append(torch.nn.functional.softmax(logits, dim=-1)[..., t][0][0].item())\n",
    "                # print(pred_prob)\n",
    "            preds_prod.append(pred_prob)\n",
    "            # print(preds_prod)\n",
    "        # print(labels)\n",
    "        # print(preds_prod)\n",
    "        preds = torch.tensor(preds_prod[0]).to(\"cuda\")\n",
    "        # preds.requires_grad_()\n",
    "        # print(preds)\n",
    "        loss = loss_fct(preds, labels)\n",
    "        loss.requires_grad_()\n",
    "        # print(loss)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "09505f94-791f-4554-a104-4af429069513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"model/flant5_small_lr_10-5_wd_10-2_qa\",\n",
    "#     evaluation_strategy=\"no\",\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     num_train_epochs=3,\n",
    "#     # load_best_model_at_end=True,\n",
    "    \n",
    "# )\n",
    "# # training_args = training_args.set_dataloader(train_batch_size=1, eval_batch_size=1)\n",
    "# trainer = CustomTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     # eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     # compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d9e48f2f-5e3f-4c93-91f1-d206c1e76e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainning by Native Pytorch\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_small).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3c17f68a-db97-41dc-85fd-9a12600a7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"].set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "75cab367-8302-4b73-bf82-91784e24b9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2241\n",
       "})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "800dc2c7-90b9-4f9e-bcc8-737d0bc3d73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choices', 'answerKey', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2241\n",
       "})"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e78ff592-75ec-4d3a-acd8-71f7ff1ed6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "da04805f-ba6d-428d-9192-52d9e711e2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f453086b730>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "549da196-33d2-4d46-a320-6c6656072c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aec93f36-c877-4238-a49a-2e45517c60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0a4628cc-3819-4db0-9aab-997005768400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_ans = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4 : \"E\"}\n",
    "# ans_to_index = {\"A\" : \"0\", \"B\" : \"1\", \"C\" : \"2\", \"D\": \"3\", \"E\":\"4\"}\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "30ba75fd-3144-4393-be82-af4552b67448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 3.0904e+00,  1.9374e-01, -2.7579e-01,  ...,  5.0314e-01,\n",
      "         -5.0757e-01,  4.1380e+00],\n",
      "        [-9.0601e+00,  9.5172e+00, -5.6732e-01,  ...,  1.2307e+01,\n",
      "          6.3581e+00,  9.9587e+01],\n",
      "        [ 7.4962e+00,  3.0095e+00, -4.2048e+00,  ..., -6.2926e+00,\n",
      "          1.1030e+01,  1.8525e+01],\n",
      "        ...,\n",
      "        [-4.5117e-01, -3.3594e-01, -3.8867e-01,  ..., -2.0996e-01,\n",
      "         -2.0000e+00, -9.1406e-01],\n",
      "        [-1.0234e+00, -8.0859e-01,  4.3555e-01,  ..., -5.9326e-02,\n",
      "         -9.2188e-01, -9.2969e-01],\n",
      "        [ 1.0078e+00,  1.5234e-01, -2.4902e-01,  ..., -1.8555e-01,\n",
      "         -2.7148e-01,  1.7969e+00]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0017,  0.0662, -0.0992,  ..., -0.0960, -0.0587,  0.0472],\n",
      "        [ 0.0954,  0.0277,  0.0717,  ..., -0.0119,  0.0083, -0.1101],\n",
      "        [-0.1003, -0.0383, -0.0627,  ...,  0.1585,  0.0452,  0.0452],\n",
      "        ...,\n",
      "        [-0.0425, -0.0286, -0.1124,  ...,  0.0397,  0.0609, -0.0235],\n",
      "        [ 0.0970, -0.0715, -0.0284,  ...,  0.0520,  0.0248,  0.0282],\n",
      "        [ 0.0883,  0.0064, -0.0641,  ..., -0.0483, -0.0634, -0.0836]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1386,  0.3936,  0.5354,  ..., -0.6233,  0.4404,  0.0644],\n",
      "        [-0.4395, -1.0995, -0.0359,  ..., -1.7545, -0.1455, -1.1948],\n",
      "        [-0.1229, -0.8463, -0.2082,  ...,  0.3620, -0.2425,  0.0878],\n",
      "        ...,\n",
      "        [-0.5658, -0.2689, -0.1340,  ..., -0.5183, -0.5142, -0.1255],\n",
      "        [-0.1954,  0.3145, -0.5219,  ...,  0.0818, -0.2979,  0.2962],\n",
      "        [ 0.3979, -0.2175, -0.1955,  ..., -0.0470, -0.2258, -0.0205]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6780,  0.3239, -0.0489,  ...,  0.2835,  0.3801, -0.0395],\n",
      "        [ 0.0408, -0.0548,  0.4788,  ...,  1.0203,  0.3225, -0.3413],\n",
      "        [-0.1966, -0.4428,  0.2334,  ...,  0.0324,  0.2386,  0.0528],\n",
      "        ...,\n",
      "        [ 0.6419, -0.6487,  0.0997,  ...,  0.2750,  0.3143,  0.1082],\n",
      "        [ 0.5315,  0.3504, -0.1884,  ..., -0.1671,  0.0519,  0.2920],\n",
      "        [-0.0590, -0.3860, -0.2752,  ..., -0.0090, -0.0984, -0.2885]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6332, -0.2145,  0.4338,  ..., -0.2122,  0.2672,  0.5865],\n",
      "        [-0.6217, -0.0785,  0.5115,  ..., -0.2065, -0.6410,  0.2802],\n",
      "        [ 0.1339, -0.1462, -1.1158,  ..., -0.3351, -0.7585, -0.1223],\n",
      "        ...,\n",
      "        [ 0.3464, -0.8370, -0.0063,  ...,  0.0423, -0.1785, -0.0991],\n",
      "        [-0.5138,  1.1588, -0.2167,  ..., -0.1830,  0.2729, -0.4084],\n",
      "        [ 0.3407,  0.0822,  0.1198,  ...,  0.1242, -0.4215,  0.8750]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.0800e+00, -1.1272e+01,  5.6890e-01, -1.0343e+01,  4.0852e+00,\n",
      "         -1.6820e+01],\n",
      "        [ 7.9028e+00,  8.4806e+00, -1.6718e+00,  3.0894e+00,  7.4876e+00,\n",
      "          5.5033e+00],\n",
      "        [ 6.1033e+00,  6.7160e+00, -1.9046e+00,  3.5621e+00,  7.4460e+00,\n",
      "          5.6118e+00],\n",
      "        [ 5.2019e+00,  5.6943e+00, -2.1096e+00,  3.5899e+00,  7.1419e+00,\n",
      "          5.6344e+00],\n",
      "        [ 4.4357e+00,  5.0223e+00, -2.2562e+00,  3.7894e+00,  6.8106e+00,\n",
      "          5.5380e+00],\n",
      "        [ 4.2370e+00,  4.5829e+00, -2.6967e+00,  3.8641e+00,  6.6244e+00,\n",
      "          5.5531e+00],\n",
      "        [ 3.7864e+00,  4.0219e+00, -2.7186e+00,  3.9158e+00,  6.3682e+00,\n",
      "          5.4489e+00],\n",
      "        [ 3.6524e+00,  3.7327e+00, -2.5787e+00,  3.9429e+00,  6.2844e+00,\n",
      "          5.4910e+00],\n",
      "        [ 3.0507e+00,  2.8459e+00, -3.5551e+00,  3.8642e+00,  5.8237e+00,\n",
      "          5.3102e+00],\n",
      "        [ 2.4581e+00,  1.8245e+00, -4.0382e+00,  3.8897e+00,  5.2748e+00,\n",
      "          5.1246e+00],\n",
      "        [ 1.8474e+00,  9.1636e-01, -4.4315e+00,  3.8780e+00,  4.7089e+00,\n",
      "          4.8641e+00],\n",
      "        [ 1.5498e+00,  2.0451e-02, -4.8816e+00,  3.8879e+00,  4.0626e+00,\n",
      "          4.6030e+00],\n",
      "        [ 1.0550e+00, -1.3329e+00, -5.2096e+00,  3.7932e+00,  3.3501e+00,\n",
      "          4.2976e+00],\n",
      "        [ 4.9159e-01, -2.3886e+00, -5.8323e+00,  3.7342e+00,  2.4976e+00,\n",
      "          3.9768e+00],\n",
      "        [ 5.9046e-01, -3.6024e+00, -5.7906e+00,  3.6469e+00,  1.6179e+00,\n",
      "          3.6566e+00],\n",
      "        [-2.5486e+00, -5.8941e+00, -8.0223e+00,  3.5217e+00, -2.9039e+00,\n",
      "          2.2533e+00],\n",
      "        [ 8.0078e-02, -8.0078e-02,  3.3398e-01, -3.5938e-01,  3.1445e-01,\n",
      "         -2.7930e-01],\n",
      "        [ 3.7221e+00, -2.5530e-01,  8.8340e+00,  4.0506e+00,  1.0060e+00,\n",
      "         -8.1852e+00],\n",
      "        [ 3.1820e+00,  1.4363e-02,  7.6714e+00,  4.3801e+00,  1.5101e+00,\n",
      "          2.4159e+00],\n",
      "        [ 3.1437e+00,  2.3993e-01,  7.0482e+00,  4.5539e+00,  1.2029e+00,\n",
      "          2.5632e+00],\n",
      "        [ 3.1398e+00,  2.1495e-01,  6.5895e+00,  4.6462e+00,  1.2074e+00,\n",
      "          2.5481e+00],\n",
      "        [ 2.9654e+00,  9.1759e-02,  6.2481e+00,  4.6499e+00,  1.2537e+00,\n",
      "          2.5670e+00],\n",
      "        [ 2.9926e+00,  1.9452e-01,  5.9057e+00,  4.7709e+00,  1.1536e+00,\n",
      "          2.7035e+00],\n",
      "        [ 2.7876e+00,  1.7422e-01,  5.7058e+00,  4.7882e+00,  1.1648e+00,\n",
      "          2.5449e+00],\n",
      "        [ 2.7039e+00,  1.8550e-01,  5.2422e+00,  4.8437e+00,  1.0867e+00,\n",
      "          2.4879e+00],\n",
      "        [ 2.5397e+00,  2.6178e-01,  4.6040e+00,  4.8349e+00,  1.0194e+00,\n",
      "          2.3790e+00],\n",
      "        [ 2.3024e+00,  1.4119e-01,  4.0709e+00,  4.8672e+00,  8.6267e-01,\n",
      "          2.2478e+00],\n",
      "        [ 1.9303e+00,  1.7766e-01,  3.5511e+00,  4.8861e+00,  6.5951e-01,\n",
      "          2.1517e+00],\n",
      "        [ 1.4743e+00,  1.2121e-01,  2.9095e+00,  4.8856e+00,  5.6813e-01,\n",
      "          1.9575e+00],\n",
      "        [ 8.4529e-01,  4.9173e-02,  2.2749e+00,  4.8931e+00,  3.4481e-01,\n",
      "          1.8309e+00],\n",
      "        [-1.5969e-01, -1.3980e-01,  1.8543e+00,  4.8834e+00,  1.2533e-01,\n",
      "          1.6618e+00],\n",
      "        [-6.7430e+00, -1.1300e-01,  4.2587e-01,  4.9117e+00, -7.2816e-01,\n",
      "          1.1048e+00]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0611, 0.0871, 0.0655, 0.0800, 0.0768, 0.0786, 0.1037, 0.0888, 0.0544,\n",
      "        0.1032, 0.0821, 0.0729, 0.0773, 0.0714, 0.0673, 0.0881, 0.0747, 0.0634,\n",
      "        0.1045, 0.0556, 0.0836, 0.0609, 0.0980, 0.0899, 0.0600, 0.0634, 0.0591,\n",
      "        0.0905, 0.0749, 0.0815, 0.0857, 0.0996, 0.1172, 0.0776, 0.0606, 0.0765,\n",
      "        0.0931, 0.0738, 0.0758, 0.0575, 0.0849, 0.0702, 0.0862, 0.0520, 0.0709,\n",
      "        0.1347, 0.1068, 0.0703, 0.0649, 0.0629, 0.0594, 0.0798, 0.0758, 0.0690,\n",
      "        0.1014, 0.0742, 0.0629, 0.0925, 0.0769, 0.0757, 0.0709, 0.0822, 0.0783,\n",
      "        0.0753, 0.0863, 0.0803, 0.0634, 0.0628, 0.1155, 0.0870, 0.0993, 0.0586,\n",
      "        0.0745, 0.0798, 0.1011, 0.0792, 0.0658, 0.0691, 0.0537, 0.0511, 0.0920,\n",
      "        0.0702, 0.0811, 0.0879, 0.0650, 0.0793, 0.0821, 0.0755, 0.0755, 0.1055,\n",
      "        0.0632, 0.0556, 0.0881, 0.0798, 0.0677, 0.0707, 0.0686, 0.0772, 0.0796,\n",
      "        0.0736, 0.0813, 0.0756, 0.0653, 0.0606, 0.0690, 0.0737, 0.0546, 0.0762,\n",
      "        0.0567, 0.0826, 0.0752, 0.0836, 0.1134, 0.0578, 0.0719, 0.0694, 0.0504,\n",
      "        0.0697, 0.0834, 0.0828, 0.0727, 0.1129, 0.0863, 0.0845, 0.0714, 0.0783,\n",
      "        0.0906, 0.0825, 0.0567, 0.0666, 0.0824, 0.0830, 0.0679, 0.0695, 0.0805,\n",
      "        0.0775, 0.1442, 0.0634, 0.0672, 0.0968, 0.0694, 0.1095, 0.0801, 0.0841,\n",
      "        0.0836, 0.0921, 0.0736, 0.0784, 0.0866, 0.0637, 0.0917, 0.1115, 0.0720,\n",
      "        0.0794, 0.0669, 0.0928, 0.0829, 0.1040, 0.0920, 0.0814, 0.0665, 0.0884,\n",
      "        0.0866, 0.0390, 0.0730, 0.0873, 0.0711, 0.0785, 0.0722, 0.1025, 0.0711,\n",
      "        0.0804, 0.0753, 0.0764, 0.0727, 0.0728, 0.0833, 0.0465, 0.0738, 0.0756,\n",
      "        0.0790, 0.0679, 0.0935, 0.0668, 0.0798, 0.0827, 0.0900, 0.1090, 0.0838,\n",
      "        0.0789, 0.0621, 0.1027, 0.0670, 0.1048, 0.0811, 0.0519, 0.0846, 0.0883,\n",
      "        0.0834, 0.0697, 0.0694, 0.0797, 0.0672, 0.0857, 0.0422, 0.0706, 0.0799,\n",
      "        0.0688, 0.0673, 0.0893, 0.1157, 0.0655, 0.0897, 0.0638, 0.0766, 0.0865,\n",
      "        0.0862, 0.0735, 0.0738, 0.1134, 0.0693, 0.0515, 0.0852, 0.0862, 0.0816,\n",
      "        0.0510, 0.0718, 0.0768, 0.0792, 0.0859, 0.0609, 0.0620, 0.0580, 0.0670,\n",
      "        0.0818, 0.0648, 0.0782, 0.0838, 0.0934, 0.0882, 0.0768, 0.0763, 0.0806,\n",
      "        0.0655, 0.0840, 0.0813, 0.0870, 0.0858, 0.0672, 0.0785, 0.0726, 0.0803,\n",
      "        0.0684, 0.0696, 0.0647, 0.0607, 0.0795, 0.0860, 0.0788, 0.0851, 0.0823,\n",
      "        0.0685, 0.0677, 0.0727, 0.1072, 0.0758, 0.0754, 0.0857, 0.0638, 0.0618,\n",
      "        0.0716, 0.0660, 0.0775, 0.0725, 0.0914, 0.0728, 0.0773, 0.0892, 0.0698,\n",
      "        0.0712, 0.0683, 0.0748, 0.0791, 0.1089, 0.0747, 0.0716, 0.0891, 0.0587,\n",
      "        0.1110, 0.0713, 0.0700, 0.0576, 0.0772, 0.0697, 0.0819, 0.0720, 0.0620,\n",
      "        0.0538, 0.0870, 0.0612, 0.1089, 0.0792, 0.0674, 0.1024, 0.0596, 0.1054,\n",
      "        0.0927, 0.0597, 0.0473, 0.0710, 0.0708, 0.0883, 0.0883, 0.0823, 0.0854,\n",
      "        0.1066, 0.0760, 0.0744, 0.0965, 0.0707, 0.1063, 0.0660, 0.0665, 0.1090,\n",
      "        0.0618, 0.0583, 0.0796, 0.0844, 0.0795, 0.0872, 0.0856, 0.0834, 0.0872,\n",
      "        0.0386, 0.0705, 0.0632, 0.0775, 0.0874, 0.0738, 0.0600, 0.0635, 0.0712,\n",
      "        0.0744, 0.0707, 0.0667, 0.0804, 0.0975, 0.0835, 0.0661, 0.0680, 0.0754,\n",
      "        0.0808, 0.0566, 0.0870, 0.0848, 0.0719, 0.0827, 0.0784, 0.0960, 0.0750,\n",
      "        0.0690, 0.0787, 0.0808, 0.0757, 0.0612, 0.0817, 0.0786, 0.0159, 0.0843,\n",
      "        0.0833, 0.0958, 0.0866, 0.1147, 0.0854, 0.0643, 0.0751, 0.0929, 0.0634,\n",
      "        0.0660, 0.0763, 0.0883, 0.0672, 0.0737, 0.0647, 0.0651, 0.0855, 0.1312,\n",
      "        0.1003, 0.0713, 0.0546, 0.0738, 0.0746, 0.0753, 0.0719, 0.1024, 0.0748,\n",
      "        0.0970, 0.0733, 0.0720, 0.0700, 0.0567, 0.0701, 0.0856, 0.0689, 0.0658,\n",
      "        0.0910, 0.0861, 0.0868, 0.1000, 0.0728, 0.0710, 0.0742, 0.0518, 0.0615,\n",
      "        0.0296, 0.0907, 0.0698, 0.0715, 0.0559, 0.0883, 0.0741, 0.0867, 0.0725,\n",
      "        0.0573, 0.0501, 0.0804, 0.0848, 0.0802, 0.0673, 0.0679, 0.0556, 0.0869,\n",
      "        0.0823, 0.0779, 0.0861, 0.0891, 0.0707, 0.0754, 0.0691, 0.0904, 0.0676,\n",
      "        0.0602, 0.0633, 0.0677, 0.0767, 0.0802, 0.0576, 0.1102, 0.0992, 0.0622,\n",
      "        0.0637, 0.0857, 0.0668, 0.0700, 0.1113, 0.0682, 0.0590, 0.0632, 0.0720,\n",
      "        0.0619, 0.0975, 0.0757, 0.0767, 0.0522, 0.0649, 0.0876, 0.0747, 0.0602,\n",
      "        0.0868, 0.0841, 0.0675, 0.0809, 0.0869, 0.0530, 0.0731, 0.1109, 0.0870,\n",
      "        0.0669, 0.0467, 0.0658, 0.0851, 0.0765, 0.0820, 0.0984, 0.0770, 0.0731,\n",
      "        0.1133, 0.0847, 0.1152, 0.0759, 0.0709, 0.0605, 0.0658, 0.1001, 0.0875,\n",
      "        0.0583, 0.0791, 0.0959, 0.0857, 0.1022, 0.0856, 0.1072, 0.0851, 0.0990,\n",
      "        0.0834, 0.0634, 0.0781, 0.0785, 0.0629, 0.0823, 0.0686, 0.0843],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1634,  0.3122,  0.4440,  ..., -0.6359, -0.2465,  0.9794],\n",
      "        [-0.0298, -0.0501, -0.0259,  ..., -0.9398, -0.5520, -0.0512],\n",
      "        [-0.5057,  0.6698, -0.7845,  ..., -0.5691, -0.2595, -0.3453],\n",
      "        ...,\n",
      "        [-0.8434, -0.9217, -0.7415,  ...,  0.3116, -0.0797,  0.0135],\n",
      "        [-0.0514, -0.0590,  0.0734,  ..., -0.9569,  0.0672, -0.3753],\n",
      "        [-0.5097,  0.0685, -0.0382,  ..., -0.1649,  0.1266, -0.5073]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2932, -0.5519,  0.1244,  ..., -0.1178, -0.3886, -0.2186],\n",
      "        [ 0.1785, -0.6853,  1.0910,  ..., -0.7157,  0.7385,  0.2821],\n",
      "        [-0.0865,  1.0434,  0.4403,  ...,  0.6825,  0.1510, -0.2179],\n",
      "        ...,\n",
      "        [ 0.0424, -0.0146,  0.3380,  ...,  0.3027,  0.4833,  0.0569],\n",
      "        [-1.0865, -0.4503, -0.4118,  ...,  0.7681, -1.1219,  0.1770],\n",
      "        [ 0.3509,  0.4915, -0.0080,  ...,  0.9046,  0.1987, -0.1967]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2874e-02, -1.6004e-01,  1.9277e-01,  ...,  4.2484e-01,\n",
      "          2.3705e-01, -1.1111e-01],\n",
      "        [-1.1692e-01,  3.3915e-01, -2.9694e-02,  ..., -2.0241e-01,\n",
      "          3.3858e-01,  2.6584e-01],\n",
      "        [ 3.5917e-01, -3.4098e-01, -3.6591e-01,  ..., -1.2691e-01,\n",
      "         -2.2127e-02, -5.1824e-02],\n",
      "        ...,\n",
      "        [ 5.2229e-02, -2.9446e-01, -1.6881e-01,  ..., -8.0677e-02,\n",
      "         -2.5395e-01, -8.2794e-04],\n",
      "        [ 3.0539e-01, -3.6335e-03,  5.0860e-01,  ...,  7.5305e-02,\n",
      "          3.4325e-01,  3.1474e-01],\n",
      "        [ 1.2160e+00,  5.9106e-02, -3.2575e-01,  ..., -3.1322e-01,\n",
      "          2.7032e-01,  1.9162e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1011, 0.0966, 0.0490, 0.0941, 0.0736, 0.0700, 0.1027, 0.0799, 0.0569,\n",
      "        0.0885, 0.0867, 0.0477, 0.0747, 0.0792, 0.0603, 0.0928, 0.0845, 0.0603,\n",
      "        0.1055, 0.0590, 0.0946, 0.0678, 0.1012, 0.0877, 0.0636, 0.0533, 0.0576,\n",
      "        0.0807, 0.0731, 0.0678, 0.0790, 0.1030, 0.1398, 0.0764, 0.0608, 0.0737,\n",
      "        0.0882, 0.0594, 0.0791, 0.0522, 0.0830, 0.0725, 0.0898, 0.0516, 0.0704,\n",
      "        0.1219, 0.0980, 0.0645, 0.0594, 0.0617, 0.0583, 0.0802, 0.0624, 0.0683,\n",
      "        0.0884, 0.0709, 0.0680, 0.1019, 0.0680, 0.0727, 0.0884, 0.0776, 0.0782,\n",
      "        0.0801, 0.1299, 0.0737, 0.0573, 0.0622, 0.1101, 0.0694, 0.1047, 0.0593,\n",
      "        0.0922, 0.0715, 0.0933, 0.0763, 0.0626, 0.0679, 0.0363, 0.0554, 0.0789,\n",
      "        0.0733, 0.0915, 0.0824, 0.0617, 0.0786, 0.0795, 0.0733, 0.0628, 0.1068,\n",
      "        0.0638, 0.0589, 0.0866, 0.0771, 0.0859, 0.0592, 0.0713, 0.0775, 0.0592,\n",
      "        0.0666, 0.0764, 0.0727, 0.0591, 0.0647, 0.0791, 0.0710, 0.0719, 0.0694,\n",
      "        0.0529, 0.0741, 0.0759, 0.0764, 0.1173, 0.0565, 0.0640, 0.0638, 0.0439,\n",
      "        0.0654, 0.0642, 0.0672, 0.0760, 0.0929, 0.0901, 0.0818, 0.0668, 0.0700,\n",
      "        0.0754, 0.0705, 0.0536, 0.0631, 0.0830, 0.0855, 0.0730, 0.0750, 0.0637,\n",
      "        0.0802, 0.1877, 0.0604, 0.0738, 0.1006, 0.0567, 0.1023, 0.0730, 0.0733,\n",
      "        0.0747, 0.0905, 0.0667, 0.0668, 0.0670, 0.0557, 0.0848, 0.0982, 0.0706,\n",
      "        0.0891, 0.0583, 0.0781, 0.0807, 0.1035, 0.0860, 0.0893, 0.0719, 0.0906,\n",
      "        0.0955, 0.0283, 0.0642, 0.0880, 0.0605, 0.0933, 0.0774, 0.1042, 0.0752,\n",
      "        0.0698, 0.0800, 0.0709, 0.0949, 0.0714, 0.0759, 0.0458, 0.0773, 0.0886,\n",
      "        0.0768, 0.0503, 0.0981, 0.0642, 0.0629, 0.0835, 0.1011, 0.0950, 0.0734,\n",
      "        0.0736, 0.0593, 0.1073, 0.0704, 0.1115, 0.0790, 0.0501, 0.0794, 0.0802,\n",
      "        0.0794, 0.0742, 0.0618, 0.0845, 0.0734, 0.0785, 0.0711, 0.0688, 0.0733,\n",
      "        0.0600, 0.0826, 0.0885, 0.1080, 0.0792, 0.0946, 0.0623, 0.0771, 0.0788,\n",
      "        0.0726, 0.0742, 0.0704, 0.0958, 0.0666, 0.0543, 0.0806, 0.0789, 0.0681,\n",
      "        0.0613, 0.0698, 0.0810, 0.0705, 0.0905, 0.0513, 0.0630, 0.0635, 0.0729,\n",
      "        0.0944, 0.0716, 0.0729, 0.1026, 0.0948, 0.0732, 0.0653, 0.0836, 0.0693,\n",
      "        0.0544, 0.0783, 0.0766, 0.0795, 0.0754, 0.0783, 0.0731, 0.0925, 0.0777,\n",
      "        0.0641, 0.0620, 0.0620, 0.0600, 0.0689, 0.0774, 0.0837, 0.0712, 0.0872,\n",
      "        0.0576, 0.0647, 0.0826, 0.1038, 0.0822, 0.0747, 0.0861, 0.0773, 0.0611,\n",
      "        0.0689, 0.0655, 0.0662, 0.0769, 0.0864, 0.0581, 0.0742, 0.0843, 0.0719,\n",
      "        0.0625, 0.0628, 0.0602, 0.0718, 0.1095, 0.0673, 0.0693, 0.0883, 0.0559,\n",
      "        0.0938, 0.0727, 0.0674, 0.0501, 0.0883, 0.0660, 0.0802, 0.0651, 0.0679,\n",
      "        0.0455, 0.0990, 0.0600, 0.1091, 0.0668, 0.0674, 0.0919, 0.0644, 0.1020,\n",
      "        0.0998, 0.0546, 0.0521, 0.0625, 0.0654, 0.0819, 0.0915, 0.0844, 0.0799,\n",
      "        0.0949, 0.0759, 0.0682, 0.1038, 0.0765, 0.0999, 0.0651, 0.0625, 0.1105,\n",
      "        0.0639, 0.0696, 0.0738, 0.0689, 0.0733, 0.0815, 0.0809, 0.0825, 0.1002,\n",
      "        0.0419, 0.0822, 0.0638, 0.0756, 0.0730, 0.0712, 0.1107, 0.0721, 0.0652,\n",
      "        0.0720, 0.0613, 0.0621, 0.0834, 0.0839, 0.0670, 0.0632, 0.0663, 0.0782,\n",
      "        0.0784, 0.0861, 0.0715, 0.0691, 0.0668, 0.0692, 0.0733, 0.0915, 0.0586,\n",
      "        0.0760, 0.0807, 0.0855, 0.0653, 0.0559, 0.0831, 0.0696, 0.0325, 0.0959,\n",
      "        0.0799, 0.1070, 0.0691, 0.1207, 0.0900, 0.0681, 0.0760, 0.0848, 0.0686,\n",
      "        0.0659, 0.0788, 0.0840, 0.0783, 0.0773, 0.0653, 0.0626, 0.0775, 0.1095,\n",
      "        0.0994, 0.0645, 0.0509, 0.0672, 0.0678, 0.0840, 0.0830, 0.0989, 0.0771,\n",
      "        0.0970, 0.0677, 0.0671, 0.0755, 0.0578, 0.0632, 0.0756, 0.0651, 0.0734,\n",
      "        0.0806, 0.0745, 0.0764, 0.0987, 0.0784, 0.0639, 0.0670, 0.0557, 0.0564,\n",
      "        0.0266, 0.0833, 0.0668, 0.0825, 0.0616, 0.1011, 0.0609, 0.0862, 0.0720,\n",
      "        0.0483, 0.0375, 0.0813, 0.0757, 0.0775, 0.0700, 0.0698, 0.0558, 0.0861,\n",
      "        0.0657, 0.0740, 0.0866, 0.0891, 0.0754, 0.0788, 0.0726, 0.0726, 0.0617,\n",
      "        0.0551, 0.0622, 0.0705, 0.0756, 0.0815, 0.0565, 0.1053, 0.0746, 0.0567,\n",
      "        0.0676, 0.0769, 0.0651, 0.0659, 0.0979, 0.0591, 0.0785, 0.0643, 0.0636,\n",
      "        0.0597, 0.1051, 0.0876, 0.0670, 0.0492, 0.0737, 0.0863, 0.0671, 0.0449,\n",
      "        0.0816, 0.0992, 0.0714, 0.0781, 0.0747, 0.0673, 0.0638, 0.1054, 0.0851,\n",
      "        0.0672, 0.0407, 0.0742, 0.0777, 0.0761, 0.0737, 0.0823, 0.0751, 0.0762,\n",
      "        0.1100, 0.0835, 0.1031, 0.0870, 0.0684, 0.0614, 0.0586, 0.0829, 0.0836,\n",
      "        0.0548, 0.0731, 0.0919, 0.0824, 0.0925, 0.0760, 0.0997, 0.0665, 0.0944,\n",
      "        0.0782, 0.0625, 0.0762, 0.0809, 0.0606, 0.0753, 0.0701, 0.1400],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0113,  0.0086, -0.0656,  ...,  0.0689, -0.0676, -0.1461],\n",
      "        [ 0.0573, -0.0217, -0.0447,  ...,  0.0453,  0.0107, -0.0905],\n",
      "        [-0.0499,  0.1006, -0.0606,  ...,  0.0883, -0.0426, -0.0353],\n",
      "        ...,\n",
      "        [-0.0234, -0.1341, -0.0580,  ...,  0.0219, -0.0481,  0.0422],\n",
      "        [ 0.0307,  0.0780, -0.0407,  ..., -0.0284,  0.0631,  0.0428],\n",
      "        [ 0.0698,  0.0023,  0.0740,  ...,  0.0435,  0.0264,  0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8258, -0.3387,  0.0070,  ..., -0.4886,  0.4375, -0.0390],\n",
      "        [ 1.0499, -0.3686, -0.0839,  ..., -0.9019,  0.0576, -0.6098],\n",
      "        [ 0.2290, -0.3746,  0.4121,  ...,  0.6192, -0.4246,  0.1016],\n",
      "        ...,\n",
      "        [ 0.2723, -0.0880, -0.5017,  ...,  0.3312,  0.0143,  0.0744],\n",
      "        [ 0.2335,  0.0262, -0.3411,  ...,  0.1300, -0.1588,  0.5094],\n",
      "        [-0.2740,  0.2925,  0.4504,  ...,  0.6156,  0.5102, -0.1348]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.7127, -0.2637,  0.1804,  ..., -0.0710,  0.0999, -0.0146],\n",
      "        [-0.3492, -0.1433,  0.8779,  ...,  0.8411, -0.3596,  0.0294],\n",
      "        [-0.5369, -0.0662,  0.2037,  ...,  0.5286,  0.6623,  0.1181],\n",
      "        ...,\n",
      "        [ 0.2475,  0.0828,  0.2078,  ...,  0.1599,  0.2670, -0.1751],\n",
      "        [-0.1206, -0.5360,  0.4772,  ...,  0.2213, -0.1009, -0.0985],\n",
      "        [-0.0793, -0.4375, -0.4026,  ..., -0.2893, -0.2317,  0.0540]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3731, -0.0410,  0.3333,  ..., -0.3581,  0.8027,  0.1312],\n",
      "        [ 0.6812,  0.2422, -0.1939,  ..., -0.2747, -0.8562,  0.4621],\n",
      "        [ 0.3844, -0.2716,  0.0384,  ...,  0.0625,  0.1559, -0.0771],\n",
      "        ...,\n",
      "        [-0.3523,  0.2971,  0.4401,  ...,  0.9476, -0.8132, -0.0980],\n",
      "        [ 0.9898,  1.0853,  0.6211,  ..., -0.1507,  0.3323,  0.1647],\n",
      "        [ 0.4457, -0.0832, -0.1273,  ..., -0.1262, -0.2178,  0.3460]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0984, 0.1115, 0.0913, 0.1298, 0.1049, 0.1089, 0.0551, 0.1272, 0.0787,\n",
      "        0.1437, 0.1223, 0.0457, 0.1102, 0.1142, 0.0916, 0.1337, 0.1060, 0.0962,\n",
      "        0.1436, 0.0861, 0.1027, 0.0775, 0.1263, 0.1113, 0.1056, 0.0817, 0.0814,\n",
      "        0.1147, 0.1225, 0.0995, 0.1294, 0.1402, 0.1020, 0.1146, 0.0924, 0.1040,\n",
      "        0.1373, 0.0964, 0.1108, 0.0929, 0.1105, 0.0971, 0.1443, 0.0748, 0.1078,\n",
      "        0.1717, 0.1161, 0.0996, 0.1057, 0.1008, 0.0935, 0.1259, 0.1113, 0.1115,\n",
      "        0.1361, 0.1065, 0.1018, 0.1346, 0.1035, 0.1020, 0.1130, 0.1442, 0.1168,\n",
      "        0.1152, 0.1516, 0.1302, 0.0853, 0.1096, 0.1421, 0.1115, 0.1346, 0.0950,\n",
      "        0.1204, 0.1077, 0.1166, 0.1129, 0.0956, 0.1214, 0.0312, 0.0953, 0.1314,\n",
      "        0.1041, 0.1221, 0.1324, 0.1064, 0.1317, 0.1154, 0.1209, 0.1101, 0.1565,\n",
      "        0.0970, 0.0961, 0.1170, 0.1235, 0.1201, 0.1029, 0.1078, 0.1150, 0.1025,\n",
      "        0.0883, 0.1079, 0.1046, 0.1105, 0.1120, 0.0926, 0.1111, 0.0929, 0.1145,\n",
      "        0.0776, 0.1158, 0.1026, 0.1105, 0.1304, 0.0775, 0.0938, 0.1037, 0.0647,\n",
      "        0.0962, 0.1065, 0.1125, 0.1189, 0.1388, 0.1221, 0.1376, 0.0970, 0.1125,\n",
      "        0.1189, 0.1218, 0.0855, 0.0866, 0.1122, 0.1118, 0.0995, 0.0852, 0.1051,\n",
      "        0.1069, 0.0908, 0.0899, 0.0914, 0.1395, 0.0877, 0.1512, 0.1118, 0.1262,\n",
      "        0.1177, 0.1313, 0.1047, 0.1109, 0.1044, 0.0910, 0.1257, 0.1356, 0.1106,\n",
      "        0.1331, 0.0966, 0.1136, 0.1162, 0.1603, 0.1271, 0.1076, 0.1117, 0.1134,\n",
      "        0.1347, 0.0314, 0.0946, 0.1246, 0.1008, 0.1330, 0.1011, 0.1446, 0.1129,\n",
      "        0.1161, 0.1150, 0.1214, 0.1152, 0.1048, 0.1166, 0.0785, 0.1178, 0.1137,\n",
      "        0.1025, 0.0865, 0.1336, 0.0992, 0.1125, 0.1112, 0.1450, 0.1437, 0.1079,\n",
      "        0.1068, 0.1056, 0.1468, 0.1094, 0.1407, 0.1322, 0.0882, 0.1143, 0.1207,\n",
      "        0.1139, 0.1127, 0.0984, 0.1098, 0.1163, 0.1078, 0.0680, 0.1031, 0.1060,\n",
      "        0.0858, 0.1266, 0.1291, 0.1569, 0.1019, 0.1151, 0.0993, 0.1065, 0.1162,\n",
      "        0.1177, 0.1133, 0.1065, 0.1519, 0.0886, 0.0923, 0.1205, 0.1141, 0.1074,\n",
      "        0.0804, 0.1255, 0.1050, 0.1094, 0.1249, 0.0904, 0.0967, 0.0961, 0.1081,\n",
      "        0.1450, 0.1071, 0.1344, 0.1303, 0.1177, 0.1099, 0.1131, 0.1071, 0.1206,\n",
      "        0.0955, 0.1168, 0.1048, 0.1221, 0.1235, 0.0806, 0.1277, 0.0763, 0.1254,\n",
      "        0.1119, 0.0913, 0.1008, 0.0940, 0.0947, 0.1109, 0.1139, 0.1225, 0.1221,\n",
      "        0.1061, 0.1049, 0.1166, 0.1318, 0.1411, 0.1045, 0.1238, 0.0816, 0.0885,\n",
      "        0.0995, 0.1091, 0.1092, 0.1165, 0.1282, 0.1026, 0.1013, 0.1163, 0.1056,\n",
      "        0.1170, 0.1026, 0.1256, 0.1069, 0.1391, 0.1180, 0.0960, 0.1212, 0.0929,\n",
      "        0.1425, 0.1009, 0.1189, 0.0979, 0.1263, 0.0856, 0.0954, 0.0638, 0.1036,\n",
      "        0.0764, 0.1223, 0.1084, 0.1562, 0.1161, 0.1260, 0.1305, 0.0838, 0.1528,\n",
      "        0.1259, 0.0977, 0.0679, 0.1022, 0.1132, 0.1103, 0.1117, 0.1093, 0.1390,\n",
      "        0.1333, 0.1263, 0.1231, 0.1259, 0.1143, 0.1475, 0.1002, 0.1068, 0.1429,\n",
      "        0.0988, 0.0999, 0.1251, 0.1039, 0.1053, 0.1219, 0.1224, 0.1119, 0.1243,\n",
      "        0.0440, 0.1083, 0.0883, 0.1040, 0.1218, 0.0794, 0.1004, 0.0989, 0.1106,\n",
      "        0.1080, 0.1095, 0.1042, 0.1124, 0.1177, 0.0990, 0.1160, 0.0890, 0.1134,\n",
      "        0.1221, 0.0910, 0.1261, 0.1087, 0.1128, 0.1378, 0.1172, 0.1301, 0.0937,\n",
      "        0.1196, 0.1268, 0.1228, 0.0945, 0.0848, 0.1331, 0.1063, 0.0176, 0.1390,\n",
      "        0.1232, 0.1361, 0.0986, 0.1328, 0.1320, 0.0999, 0.1087, 0.1172, 0.0884,\n",
      "        0.1136, 0.1263, 0.1361, 0.1128, 0.1135, 0.1051, 0.1010, 0.1200, 0.1672,\n",
      "        0.1307, 0.1075, 0.0980, 0.1099, 0.1028, 0.1110, 0.1103, 0.1524, 0.1139,\n",
      "        0.1095, 0.1060, 0.0942, 0.1138, 0.0898, 0.0960, 0.1244, 0.1205, 0.1156,\n",
      "        0.1077, 0.1357, 0.1193, 0.1264, 0.1227, 0.1112, 0.0987, 0.0482, 0.0941,\n",
      "        0.0253, 0.1150, 0.1000, 0.1119, 0.1006, 0.1112, 0.1145, 0.1401, 0.1114,\n",
      "        0.0841, 0.0776, 0.1003, 0.1182, 0.1083, 0.0800, 0.1030, 0.0848, 0.1349,\n",
      "        0.1087, 0.1165, 0.1147, 0.1039, 0.1096, 0.1119, 0.1203, 0.0987, 0.1139,\n",
      "        0.0912, 0.0973, 0.1113, 0.1220, 0.1334, 0.0783, 0.1576, 0.1159, 0.0929,\n",
      "        0.0898, 0.1213, 0.0988, 0.1035, 0.1521, 0.0925, 0.0932, 0.1110, 0.0704,\n",
      "        0.0994, 0.1428, 0.1082, 0.1081, 0.0736, 0.0894, 0.1123, 0.0948, 0.0928,\n",
      "        0.1273, 0.1317, 0.1126, 0.1056, 0.1000, 0.0988, 0.1057, 0.1414, 0.1122,\n",
      "        0.1094, 0.0685, 0.1049, 0.1187, 0.1078, 0.1232, 0.1179, 0.0972, 0.1228,\n",
      "        0.1463, 0.1013, 0.1318, 0.1290, 0.1208, 0.0903, 0.1016, 0.1471, 0.1224,\n",
      "        0.1034, 0.1229, 0.1286, 0.1322, 0.1270, 0.1316, 0.1215, 0.1111, 0.1506,\n",
      "        0.1237, 0.1085, 0.1361, 0.1100, 0.0864, 0.1233, 0.1193, 0.1134],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1732,  0.1201,  0.6661,  ...,  0.1568, -0.0786, -1.1199],\n",
      "        [ 0.1121,  0.1395,  0.3005,  ..., -0.2471,  1.2044, -0.5895],\n",
      "        [-0.6327, -0.5446, -0.4331,  ...,  0.0756, -0.5538,  0.1490],\n",
      "        ...,\n",
      "        [-0.4400, -0.2224, -0.7754,  ..., -0.5557, -0.5655,  0.2100],\n",
      "        [ 0.6112,  0.5875,  0.8958,  ...,  0.8848, -0.2998,  0.8385],\n",
      "        [ 0.0503, -0.1207, -0.0785,  ..., -0.0287, -0.5396, -0.0675]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2866,  0.8613,  0.2584,  ...,  0.1124, -0.0260,  0.9024],\n",
      "        [-0.2744, -0.1607, -0.7050,  ..., -0.2759, -0.9153, -0.3643],\n",
      "        [-0.0535, -0.6238,  0.0900,  ..., -0.0035,  1.2651,  0.9328],\n",
      "        ...,\n",
      "        [-0.7513, -0.5273, -0.2564,  ...,  0.8542, -0.4356,  0.3844],\n",
      "        [ 0.0081, -0.1176,  0.6646,  ..., -0.6520,  1.0841,  0.6814],\n",
      "        [-0.1384, -0.3923, -0.0303,  ..., -0.1491,  0.4723, -0.1250]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0599, -0.5605,  0.3726,  ...,  0.4802, -0.6640,  0.3189],\n",
      "        [-0.2713, -0.1414,  0.3343,  ...,  0.0309, -0.1154, -0.0112],\n",
      "        [ 0.3559, -0.0724, -0.1606,  ...,  0.2460,  0.1105, -0.0469],\n",
      "        ...,\n",
      "        [ 0.2780, -0.4014,  0.3377,  ..., -0.0078,  0.1749, -0.4753],\n",
      "        [ 0.0145, -0.3840,  0.4798,  ..., -0.1658, -0.5030, -0.4281],\n",
      "        [ 0.1425,  0.1035, -0.0601,  ..., -0.1051, -0.1975, -0.0228]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1248, 0.1265, 0.0775, 0.1179, 0.1105, 0.1060, 0.1317, 0.1094, 0.0733,\n",
      "        0.1322, 0.0998, 0.0360, 0.0919, 0.0861, 0.0809, 0.1183, 0.1021, 0.0822,\n",
      "        0.1150, 0.0884, 0.0961, 0.0694, 0.1107, 0.1037, 0.0823, 0.0693, 0.0595,\n",
      "        0.0936, 0.1099, 0.0910, 0.1008, 0.1315, 0.2052, 0.0973, 0.0776, 0.0969,\n",
      "        0.1128, 0.0880, 0.1020, 0.0752, 0.0976, 0.1008, 0.1161, 0.0722, 0.0979,\n",
      "        0.1365, 0.1107, 0.1071, 0.0776, 0.0924, 0.0799, 0.1069, 0.0835, 0.0994,\n",
      "        0.1167, 0.1081, 0.0816, 0.1358, 0.0953, 0.0832, 0.1051, 0.0906, 0.1070,\n",
      "        0.1043, 0.1412, 0.1030, 0.0693, 0.0834, 0.1183, 0.0922, 0.1197, 0.0826,\n",
      "        0.0984, 0.1008, 0.1081, 0.0934, 0.0863, 0.1098, 0.0392, 0.0885, 0.1162,\n",
      "        0.0940, 0.0959, 0.0874, 0.0906, 0.1150, 0.1054, 0.0945, 0.1009, 0.1284,\n",
      "        0.0842, 0.0766, 0.0972, 0.0965, 0.1028, 0.0838, 0.1013, 0.1059, 0.0857,\n",
      "        0.0868, 0.0936, 0.0974, 0.0951, 0.0922, 0.0821, 0.0892, 0.0924, 0.1014,\n",
      "        0.0693, 0.0979, 0.0900, 0.0918, 0.1473, 0.0748, 0.0821, 0.0921, 0.0567,\n",
      "        0.0903, 0.0839, 0.1012, 0.1042, 0.1066, 0.1236, 0.1114, 0.0986, 0.1060,\n",
      "        0.0939, 0.0868, 0.0803, 0.0738, 0.1110, 0.0943, 0.0947, 0.0797, 0.0805,\n",
      "        0.0953, 0.3510, 0.0814, 0.0883, 0.1175, 0.0877, 0.1429, 0.0863, 0.1170,\n",
      "        0.0844, 0.1162, 0.0884, 0.1001, 0.0919, 0.0859, 0.1062, 0.1182, 0.0858,\n",
      "        0.1277, 0.0881, 0.0896, 0.1099, 0.1265, 0.0986, 0.0950, 0.0882, 0.1241,\n",
      "        0.0978, 0.0298, 0.0903, 0.1005, 0.0774, 0.1167, 0.0878, 0.1090, 0.1034,\n",
      "        0.0941, 0.0915, 0.0957, 0.1141, 0.0916, 0.0947, 0.0647, 0.0963, 0.1032,\n",
      "        0.0953, 0.0693, 0.1234, 0.0717, 0.0886, 0.1056, 0.1250, 0.1133, 0.1011,\n",
      "        0.0797, 0.0821, 0.1371, 0.0920, 0.1263, 0.1131, 0.0679, 0.0993, 0.0947,\n",
      "        0.0935, 0.0975, 0.0889, 0.0912, 0.0920, 0.0900, 0.0885, 0.1007, 0.0906,\n",
      "        0.0812, 0.0989, 0.1119, 0.1315, 0.0892, 0.1020, 0.0802, 0.1004, 0.1078,\n",
      "        0.0990, 0.1053, 0.0833, 0.1221, 0.0736, 0.0748, 0.0966, 0.0979, 0.0829,\n",
      "        0.0741, 0.0909, 0.0892, 0.0849, 0.1183, 0.0708, 0.0925, 0.0820, 0.0982,\n",
      "        0.1202, 0.0924, 0.0965, 0.1364, 0.1040, 0.1009, 0.0956, 0.0958, 0.0874,\n",
      "        0.0763, 0.0966, 0.0930, 0.1027, 0.0908, 0.0812, 0.1019, 0.0761, 0.0988,\n",
      "        0.0852, 0.0844, 0.0768, 0.0785, 0.0882, 0.1060, 0.0894, 0.1099, 0.0987,\n",
      "        0.0711, 0.0817, 0.1197, 0.1068, 0.1154, 0.0996, 0.1220, 0.0762, 0.0704,\n",
      "        0.1027, 0.0851, 0.0908, 0.0906, 0.1116, 0.0815, 0.0890, 0.1047, 0.0913,\n",
      "        0.0979, 0.0839, 0.0955, 0.0808, 0.1388, 0.1086, 0.0961, 0.1031, 0.0818,\n",
      "        0.1210, 0.0967, 0.1018, 0.0736, 0.1117, 0.0790, 0.0918, 0.0521, 0.0905,\n",
      "        0.0591, 0.1059, 0.0933, 0.1268, 0.0990, 0.0976, 0.1200, 0.0888, 0.1173,\n",
      "        0.1137, 0.0746, 0.0623, 0.0869, 0.0913, 0.0948, 0.1048, 0.1032, 0.1076,\n",
      "        0.1369, 0.0966, 0.0971, 0.1129, 0.0903, 0.1384, 0.0838, 0.0908, 0.1334,\n",
      "        0.1025, 0.0805, 0.1085, 0.0858, 0.0895, 0.1072, 0.1023, 0.1034, 0.1233,\n",
      "        0.0468, 0.0966, 0.0749, 0.0948, 0.0958, 0.0801, 0.1162, 0.0948, 0.0948,\n",
      "        0.0872, 0.0873, 0.0885, 0.0975, 0.1098, 0.0953, 0.0905, 0.0846, 0.0943,\n",
      "        0.0970, 0.1059, 0.0927, 0.1014, 0.0839, 0.1185, 0.0867, 0.1086, 0.0750,\n",
      "        0.0855, 0.0963, 0.1079, 0.0830, 0.0733, 0.1049, 0.0809, 0.0316, 0.1169,\n",
      "        0.0898, 0.1366, 0.0971, 0.1355, 0.1161, 0.0819, 0.1005, 0.1030, 0.0926,\n",
      "        0.0889, 0.1046, 0.1073, 0.1087, 0.0897, 0.0820, 0.0829, 0.1058, 0.1471,\n",
      "        0.1117, 0.0855, 0.0770, 0.1148, 0.0727, 0.1007, 0.1010, 0.1266, 0.0964,\n",
      "        0.1091, 0.1045, 0.1028, 0.1069, 0.0773, 0.0783, 0.1222, 0.0863, 0.0907,\n",
      "        0.1017, 0.0997, 0.0913, 0.1044, 0.0997, 0.0951, 0.0889, 0.0348, 0.0871,\n",
      "        0.0265, 0.0995, 0.0866, 0.0947, 0.0713, 0.1127, 0.0971, 0.1220, 0.1014,\n",
      "        0.0708, 0.0639, 0.0926, 0.1067, 0.0903, 0.0753, 0.0817, 0.0732, 0.1149,\n",
      "        0.0855, 0.1019, 0.1082, 0.0888, 0.0968, 0.0998, 0.0989, 0.0750, 0.0773,\n",
      "        0.0818, 0.0766, 0.1105, 0.1035, 0.1132, 0.0728, 0.1082, 0.0935, 0.0755,\n",
      "        0.0717, 0.1105, 0.0930, 0.0890, 0.1332, 0.0826, 0.0850, 0.0898, 0.0715,\n",
      "        0.0899, 0.1254, 0.1056, 0.0798, 0.0622, 0.0746, 0.1085, 0.0851, 0.0571,\n",
      "        0.0988, 0.1210, 0.0927, 0.0960, 0.0822, 0.0875, 0.0896, 0.1192, 0.0975,\n",
      "        0.0909, 0.0594, 0.0981, 0.0990, 0.0932, 0.1134, 0.1066, 0.0876, 0.1065,\n",
      "        0.1328, 0.1124, 0.1214, 0.0955, 0.0983, 0.0843, 0.0817, 0.1075, 0.1035,\n",
      "        0.0792, 0.0979, 0.1150, 0.1011, 0.1088, 0.1053, 0.1461, 0.0974, 0.1183,\n",
      "        0.0994, 0.0864, 0.1212, 0.1041, 0.0734, 0.1099, 0.1008, 0.1498],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0745,  0.0223,  0.0265,  ...,  0.0307, -0.0353, -0.0519],\n",
      "        [ 0.0729,  0.0444,  0.0635,  ...,  0.0632, -0.0046, -0.0139],\n",
      "        [-0.0872,  0.0148,  0.0786,  ...,  0.0080,  0.0509,  0.0519],\n",
      "        ...,\n",
      "        [ 0.0626, -0.0059, -0.0011,  ...,  0.0329, -0.0646, -0.0127],\n",
      "        [ 0.0138,  0.1072, -0.0175,  ..., -0.0078, -0.0994, -0.0447],\n",
      "        [ 0.0922, -0.0339,  0.0442,  ..., -0.0009, -0.0019,  0.0947]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2662, -0.3834,  0.6403,  ...,  0.5434,  0.5324, -0.3720],\n",
      "        [-0.2078,  0.1350, -0.3572,  ...,  0.1558, -0.8970, -0.6226],\n",
      "        [-0.2263,  0.0685, -0.2952,  ...,  0.0395, -0.5273,  0.2269],\n",
      "        ...,\n",
      "        [ 0.2148, -0.4939,  0.0305,  ..., -0.1274, -0.4088, -0.2811],\n",
      "        [ 0.2473,  0.6169, -0.7833,  ...,  0.1593, -0.4956, -0.0970],\n",
      "        [ 0.5093, -0.5479,  0.4562,  ...,  0.0380, -0.0768,  0.1970]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0301,  0.7370,  0.8816,  ...,  0.6435,  0.8678, -0.2088],\n",
      "        [-0.1601, -0.7685, -0.2402,  ..., -0.1422,  0.1240, -0.0075],\n",
      "        [-0.3768,  0.4632,  1.0868,  ...,  0.8704,  0.2969,  0.0132],\n",
      "        ...,\n",
      "        [-0.1606,  1.0316,  0.5101,  ..., -0.4808, -0.3975,  0.2700],\n",
      "        [-0.4039,  0.3879,  0.3615,  ...,  1.0892,  0.4115,  0.2618],\n",
      "        [-0.2253, -0.4128, -0.1644,  ...,  0.2119,  0.2683, -0.1515]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0279, -0.2259,  0.2403,  ..., -0.3565, -0.4675,  0.8589],\n",
      "        [ 0.0276,  0.3014,  0.0200,  ..., -0.0892,  0.1015,  0.2612],\n",
      "        [ 0.3238,  0.3943, -0.7845,  ..., -0.0882,  0.0750, -0.5931],\n",
      "        ...,\n",
      "        [-0.1015, -0.4977,  0.1749,  ...,  0.2885, -0.6432, -1.3906],\n",
      "        [ 0.5942, -0.3729, -1.1704,  ..., -0.0139, -0.0364, -0.3947],\n",
      "        [ 0.7370,  0.4606, -0.5354,  ..., -0.0171,  0.5712,  0.0204]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1361, 0.1287, 0.0837, 0.1184, 0.1088, 0.1046, 0.0411, 0.1116, 0.0900,\n",
      "        0.1624, 0.1386, 0.0563, 0.0923, 0.1202, 0.1045, 0.1416, 0.1099, 0.1061,\n",
      "        0.1416, 0.0755, 0.1051, 0.0932, 0.1229, 0.1304, 0.0958, 0.0883, 0.0743,\n",
      "        0.1221, 0.1226, 0.1232, 0.1185, 0.1274, 0.0914, 0.1114, 0.1007, 0.1003,\n",
      "        0.1242, 0.1044, 0.1090, 0.0917, 0.1109, 0.1141, 0.1405, 0.0926, 0.1175,\n",
      "        0.1655, 0.1105, 0.1215, 0.0943, 0.1034, 0.1035, 0.1236, 0.1080, 0.1246,\n",
      "        0.1335, 0.1169, 0.1064, 0.1205, 0.1168, 0.0978, 0.1192, 0.1108, 0.1220,\n",
      "        0.1187, 0.1347, 0.1442, 0.0944, 0.1140, 0.1502, 0.0998, 0.1381, 0.0994,\n",
      "        0.1210, 0.1091, 0.1083, 0.1028, 0.1059, 0.1390, 0.0402, 0.1022, 0.1360,\n",
      "        0.1213, 0.1206, 0.1114, 0.1109, 0.1184, 0.1175, 0.1207, 0.1169, 0.1398,\n",
      "        0.0808, 0.0979, 0.1301, 0.1219, 0.1195, 0.1058, 0.1107, 0.1227, 0.0967,\n",
      "        0.0936, 0.0977, 0.0840, 0.1115, 0.1126, 0.1044, 0.0990, 0.1234, 0.1242,\n",
      "        0.0795, 0.1114, 0.1026, 0.1145, 0.1673, 0.0805, 0.0981, 0.1081, 0.0655,\n",
      "        0.0975, 0.1141, 0.1308, 0.0985, 0.1353, 0.1377, 0.1525, 0.1106, 0.1105,\n",
      "        0.1131, 0.0996, 0.0846, 0.0767, 0.1193, 0.1119, 0.0945, 0.0774, 0.0894,\n",
      "        0.1119, 0.0640, 0.1030, 0.0890, 0.1432, 0.1035, 0.1535, 0.1048, 0.1207,\n",
      "        0.1091, 0.1325, 0.1130, 0.1076, 0.1118, 0.0884, 0.1363, 0.1320, 0.1124,\n",
      "        0.1549, 0.1063, 0.1228, 0.1200, 0.1307, 0.1284, 0.1110, 0.1237, 0.1214,\n",
      "        0.1246, 0.0434, 0.1200, 0.1087, 0.0979, 0.1534, 0.1061, 0.1364, 0.1301,\n",
      "        0.1084, 0.1144, 0.1180, 0.1140, 0.1090, 0.1130, 0.0840, 0.0993, 0.1201,\n",
      "        0.1167, 0.0904, 0.1405, 0.1066, 0.1037, 0.1175, 0.1439, 0.1268, 0.1162,\n",
      "        0.0936, 0.1112, 0.1210, 0.0996, 0.1384, 0.1280, 0.0866, 0.1227, 0.1206,\n",
      "        0.1257, 0.1081, 0.0982, 0.1100, 0.1190, 0.1104, 0.0921, 0.0992, 0.0920,\n",
      "        0.0923, 0.1225, 0.1168, 0.1552, 0.1042, 0.1196, 0.0956, 0.1125, 0.1150,\n",
      "        0.1150, 0.1366, 0.1151, 0.1277, 0.0977, 0.1023, 0.1191, 0.1276, 0.1106,\n",
      "        0.0848, 0.1342, 0.1026, 0.1188, 0.1435, 0.0903, 0.1032, 0.0908, 0.1071,\n",
      "        0.1437, 0.1199, 0.1307, 0.1361, 0.1304, 0.1163, 0.1242, 0.1193, 0.1326,\n",
      "        0.0965, 0.1041, 0.1278, 0.1186, 0.1176, 0.0913, 0.1122, 0.0876, 0.1249,\n",
      "        0.1084, 0.1158, 0.1074, 0.0993, 0.1101, 0.1132, 0.1358, 0.1248, 0.1239,\n",
      "        0.0980, 0.1023, 0.1169, 0.1374, 0.1288, 0.1155, 0.1217, 0.0870, 0.0885,\n",
      "        0.1066, 0.1107, 0.1103, 0.1089, 0.1243, 0.0948, 0.0924, 0.1275, 0.0989,\n",
      "        0.1308, 0.1135, 0.1056, 0.0874, 0.1139, 0.1239, 0.1067, 0.1365, 0.0948,\n",
      "        0.1475, 0.0999, 0.1200, 0.0880, 0.1212, 0.1011, 0.1187, 0.0772, 0.1008,\n",
      "        0.0856, 0.1284, 0.0876, 0.1394, 0.1233, 0.1272, 0.1442, 0.0969, 0.1428,\n",
      "        0.1183, 0.1054, 0.0786, 0.1042, 0.1016, 0.1071, 0.0933, 0.1249, 0.1112,\n",
      "        0.1359, 0.1191, 0.1235, 0.0938, 0.0995, 0.1615, 0.1017, 0.1035, 0.1552,\n",
      "        0.1138, 0.1032, 0.1379, 0.1068, 0.0996, 0.1141, 0.1162, 0.1189, 0.1195,\n",
      "        0.0600, 0.1183, 0.1140, 0.1233, 0.1387, 0.0973, 0.1195, 0.1222, 0.1025,\n",
      "        0.1165, 0.1164, 0.1187, 0.0998, 0.1155, 0.0954, 0.1065, 0.1034, 0.1024,\n",
      "        0.1032, 0.0999, 0.1158, 0.1168, 0.1071, 0.1188, 0.1055, 0.1215, 0.0938,\n",
      "        0.1059, 0.1021, 0.1312, 0.0887, 0.0837, 0.1222, 0.1009, 0.0349, 0.1229,\n",
      "        0.1438, 0.1463, 0.1051, 0.1552, 0.1296, 0.1004, 0.1299, 0.1159, 0.1011,\n",
      "        0.1024, 0.1125, 0.1316, 0.1283, 0.1190, 0.1111, 0.1026, 0.1346, 0.1765,\n",
      "        0.1238, 0.1022, 0.0936, 0.1326, 0.0992, 0.0991, 0.1212, 0.1384, 0.1236,\n",
      "        0.1244, 0.1161, 0.1109, 0.1228, 0.1009, 0.0938, 0.1299, 0.1048, 0.1045,\n",
      "        0.1276, 0.1267, 0.1356, 0.1103, 0.1142, 0.1168, 0.1063, 0.0522, 0.1257,\n",
      "        0.0391, 0.1110, 0.1046, 0.1216, 0.0842, 0.1314, 0.1020, 0.1321, 0.1201,\n",
      "        0.0820, 0.0782, 0.1044, 0.1093, 0.1016, 0.1051, 0.1096, 0.1043, 0.1537,\n",
      "        0.1096, 0.1063, 0.1227, 0.0965, 0.1204, 0.1231, 0.1200, 0.0933, 0.1206,\n",
      "        0.0903, 0.1200, 0.1369, 0.1178, 0.1170, 0.0759, 0.1748, 0.1056, 0.1015,\n",
      "        0.0870, 0.1039, 0.0994, 0.1023, 0.1467, 0.0910, 0.1021, 0.1080, 0.0876,\n",
      "        0.1021, 0.1338, 0.0989, 0.0936, 0.0771, 0.0719, 0.1378, 0.0919, 0.0840,\n",
      "        0.1223, 0.1394, 0.0961, 0.1159, 0.1030, 0.1080, 0.1234, 0.1420, 0.1055,\n",
      "        0.1099, 0.0902, 0.1182, 0.1105, 0.1156, 0.1261, 0.1265, 0.0939, 0.1247,\n",
      "        0.1374, 0.0999, 0.1225, 0.1211, 0.1352, 0.1006, 0.1138, 0.1144, 0.1300,\n",
      "        0.0914, 0.1146, 0.1494, 0.1181, 0.1118, 0.1387, 0.1169, 0.1097, 0.1372,\n",
      "        0.1156, 0.1005, 0.1321, 0.1245, 0.0917, 0.0981, 0.1151, 0.1244],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5764, -0.6907, -0.1164,  ..., -0.0552, -0.1520, -0.2152],\n",
      "        [-0.0028, -0.1654,  0.7398,  ..., -0.0127, -0.0690,  0.3066],\n",
      "        [ 0.1031,  0.0963, -0.0035,  ..., -0.0066, -0.0349, -0.1599],\n",
      "        ...,\n",
      "        [-0.5514, -0.8232, -0.4210,  ..., -0.1874, -0.4013,  1.8694],\n",
      "        [-0.5341, -0.0695,  0.2604,  ..., -0.0395,  1.0871, -0.4732],\n",
      "        [ 0.3193, -0.1315, -0.4837,  ..., -0.4224, -0.7315, -0.2486]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.1197e-01, -3.2231e-01, -1.0820e+00,  ...,  1.0588e+00,\n",
      "          1.2140e-01, -9.3498e-02],\n",
      "        [-7.0015e-03, -8.4197e-01, -1.2037e-03,  ...,  2.9591e-01,\n",
      "          9.3593e-02,  4.9594e-01],\n",
      "        [ 2.7191e-01, -7.0421e-01,  4.9042e-01,  ...,  2.0461e-01,\n",
      "         -1.9053e-01,  1.8624e-01],\n",
      "        ...,\n",
      "        [ 6.7888e-01,  2.5797e-01, -3.0448e-01,  ..., -8.7517e-01,\n",
      "          9.0893e-01,  2.5103e+00],\n",
      "        [ 7.7797e-01,  7.3016e-01, -9.1517e-01,  ...,  1.2566e-01,\n",
      "         -3.5927e-01,  2.7557e-01],\n",
      "        [-2.8164e-02, -1.5937e+00,  4.4951e-01,  ...,  6.1534e-01,\n",
      "         -1.9103e-02,  1.6407e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0630,  0.1240,  0.4607,  ..., -0.2272, -0.7350,  0.5835],\n",
      "        [ 0.0516,  0.4856, -0.1211,  ...,  0.1567,  0.7067,  0.2490],\n",
      "        [-0.7808, -0.0020,  0.1402,  ...,  0.0442,  0.3771,  0.2805],\n",
      "        ...,\n",
      "        [ 0.2655, -0.2443, -0.0593,  ...,  0.4281,  0.7446,  0.6660],\n",
      "        [ 0.3975, -0.1252, -0.1671,  ..., -0.0261, -0.3513, -0.3017],\n",
      "        [ 0.4927, -0.4903, -0.0654,  ..., -0.1018, -0.7617,  0.5727]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1233, 0.1198, 0.0806, 0.1215, 0.1128, 0.1030, 0.1321, 0.1010, 0.0827,\n",
      "        0.1321, 0.1048, 0.0364, 0.0949, 0.0915, 0.0804, 0.1201, 0.0885, 0.0822,\n",
      "        0.1008, 0.0793, 0.0911, 0.0637, 0.0975, 0.0998, 0.0792, 0.0678, 0.0662,\n",
      "        0.1001, 0.1014, 0.0840, 0.1001, 0.1136, 0.1824, 0.0900, 0.0853, 0.0945,\n",
      "        0.1103, 0.0873, 0.0935, 0.0731, 0.0948, 0.0969, 0.1087, 0.0741, 0.0908,\n",
      "        0.1234, 0.1028, 0.1188, 0.0852, 0.0976, 0.0804, 0.1084, 0.0941, 0.1129,\n",
      "        0.1213, 0.1089, 0.0879, 0.1241, 0.0908, 0.0901, 0.1057, 0.0877, 0.0940,\n",
      "        0.1026, 0.1349, 0.1171, 0.0675, 0.0968, 0.1071, 0.0819, 0.1178, 0.0809,\n",
      "        0.0899, 0.0902, 0.0985, 0.0892, 0.1000, 0.1206, 0.0372, 0.0846, 0.1179,\n",
      "        0.0917, 0.0898, 0.0879, 0.0964, 0.1006, 0.1045, 0.0968, 0.1057, 0.1269,\n",
      "        0.0948, 0.0717, 0.0843, 0.0946, 0.1068, 0.0994, 0.0990, 0.1063, 0.0796,\n",
      "        0.0866, 0.0824, 0.0924, 0.0990, 0.0966, 0.0827, 0.0849, 0.0967, 0.0988,\n",
      "        0.0700, 0.0949, 0.0857, 0.0896, 0.1415, 0.0729, 0.0875, 0.0933, 0.0592,\n",
      "        0.0865, 0.0830, 0.1142, 0.1089, 0.0977, 0.1026, 0.1104, 0.0953, 0.1003,\n",
      "        0.0946, 0.0872, 0.0838, 0.0719, 0.1024, 0.1027, 0.0948, 0.0702, 0.0807,\n",
      "        0.0961, 0.6550, 0.0881, 0.0940, 0.1052, 0.0822, 0.1404, 0.0882, 0.1066,\n",
      "        0.0897, 0.1389, 0.0876, 0.0978, 0.0906, 0.0890, 0.1160, 0.1144, 0.0947,\n",
      "        0.1162, 0.0849, 0.0975, 0.1000, 0.1141, 0.0989, 0.1010, 0.0910, 0.1243,\n",
      "        0.1133, 0.0307, 0.1018, 0.0954, 0.0860, 0.1193, 0.0983, 0.1027, 0.1142,\n",
      "        0.0926, 0.1024, 0.0879, 0.1065, 0.0911, 0.0932, 0.0684, 0.1017, 0.0994,\n",
      "        0.0843, 0.0801, 0.1131, 0.0771, 0.0961, 0.1060, 0.1304, 0.1084, 0.0969,\n",
      "        0.0806, 0.0811, 0.1225, 0.0835, 0.1021, 0.1110, 0.0712, 0.0957, 0.1026,\n",
      "        0.0976, 0.1009, 0.0818, 0.0836, 0.0886, 0.0903, 0.0942, 0.0909, 0.0817,\n",
      "        0.0783, 0.1072, 0.1081, 0.1138, 0.0917, 0.1054, 0.0830, 0.0996, 0.1053,\n",
      "        0.1006, 0.1128, 0.0934, 0.1339, 0.0806, 0.0787, 0.0945, 0.0958, 0.0790,\n",
      "        0.0727, 0.1018, 0.0902, 0.0903, 0.1238, 0.0757, 0.0984, 0.0823, 0.0940,\n",
      "        0.1206, 0.0951, 0.0996, 0.1251, 0.1133, 0.1001, 0.1015, 0.0987, 0.1007,\n",
      "        0.0872, 0.0879, 0.1044, 0.1026, 0.0881, 0.0739, 0.1001, 0.0756, 0.1035,\n",
      "        0.0966, 0.0948, 0.0819, 0.0820, 0.0905, 0.0983, 0.0933, 0.1159, 0.0992,\n",
      "        0.0709, 0.0808, 0.1111, 0.1062, 0.1166, 0.0886, 0.1175, 0.0781, 0.0785,\n",
      "        0.1095, 0.0879, 0.0895, 0.0906, 0.1131, 0.0965, 0.0930, 0.1029, 0.0825,\n",
      "        0.1090, 0.0910, 0.0839, 0.0950, 0.1224, 0.1035, 0.0797, 0.1059, 0.0931,\n",
      "        0.1165, 0.1063, 0.0926, 0.0797, 0.1118, 0.0851, 0.0939, 0.0590, 0.0860,\n",
      "        0.0673, 0.0974, 0.0914, 0.1275, 0.1072, 0.0961, 0.1284, 0.1011, 0.1114,\n",
      "        0.1087, 0.0760, 0.0684, 0.0899, 0.0926, 0.0959, 0.0875, 0.1027, 0.1041,\n",
      "        0.1199, 0.1033, 0.1128, 0.1008, 0.0898, 0.1259, 0.0779, 0.0934, 0.1184,\n",
      "        0.0999, 0.0815, 0.1233, 0.0918, 0.0923, 0.1073, 0.0956, 0.0963, 0.1073,\n",
      "        0.0482, 0.1080, 0.0886, 0.0812, 0.1032, 0.0816, 0.1149, 0.1026, 0.0994,\n",
      "        0.0981, 0.0926, 0.0869, 0.0907, 0.1066, 0.1005, 0.0929, 0.0934, 0.0900,\n",
      "        0.0988, 0.0998, 0.0915, 0.1019, 0.0822, 0.1065, 0.0972, 0.1000, 0.0719,\n",
      "        0.0845, 0.1022, 0.0991, 0.0787, 0.0715, 0.1068, 0.0808, 0.0342, 0.1018,\n",
      "        0.1012, 0.1225, 0.0933, 0.1145, 0.1146, 0.0841, 0.1156, 0.0969, 0.0981,\n",
      "        0.0956, 0.1019, 0.1136, 0.1001, 0.0914, 0.0856, 0.0837, 0.1121, 0.1387,\n",
      "        0.1072, 0.0939, 0.0893, 0.1113, 0.0813, 0.0949, 0.0940, 0.1313, 0.1005,\n",
      "        0.1068, 0.1022, 0.1032, 0.1015, 0.0848, 0.0809, 0.1245, 0.1038, 0.0931,\n",
      "        0.0979, 0.1019, 0.0956, 0.0989, 0.1023, 0.0904, 0.0917, 0.0431, 0.0928,\n",
      "        0.0290, 0.0980, 0.0977, 0.0983, 0.0827, 0.1117, 0.1016, 0.1224, 0.0968,\n",
      "        0.0718, 0.0716, 0.0980, 0.0984, 0.0929, 0.0826, 0.0808, 0.0921, 0.1193,\n",
      "        0.0917, 0.1030, 0.1069, 0.0878, 0.0868, 0.0882, 0.0929, 0.0801, 0.0850,\n",
      "        0.0917, 0.0761, 0.1104, 0.0955, 0.1104, 0.0848, 0.0828, 0.0855, 0.0837,\n",
      "        0.0732, 0.0924, 0.0917, 0.0782, 0.1106, 0.0835, 0.0918, 0.1057, 0.0713,\n",
      "        0.0969, 0.1035, 0.0936, 0.0770, 0.0676, 0.0810, 0.1086, 0.0884, 0.0633,\n",
      "        0.1070, 0.1281, 0.0932, 0.1041, 0.0686, 0.0846, 0.1040, 0.1273, 0.0920,\n",
      "        0.0991, 0.0647, 0.1106, 0.1053, 0.0964, 0.1131, 0.1061, 0.0943, 0.1062,\n",
      "        0.1233, 0.1078, 0.1107, 0.1036, 0.1188, 0.0796, 0.0964, 0.0871, 0.1022,\n",
      "        0.0922, 0.1016, 0.1175, 0.1161, 0.1149, 0.1173, 0.1220, 0.0924, 0.1168,\n",
      "        0.1078, 0.0962, 0.1053, 0.1060, 0.0748, 0.1026, 0.0982, 0.2016],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0239, -0.0055,  0.0627,  ...,  0.0729, -0.0271, -0.0107],\n",
      "        [ 0.0270,  0.1325,  0.0151,  ..., -0.1226, -0.1678,  0.0484],\n",
      "        [-0.0329,  0.0161,  0.0323,  ..., -0.0380, -0.0776,  0.0632],\n",
      "        ...,\n",
      "        [-0.0466,  0.0270, -0.0329,  ..., -0.0717,  0.0284,  0.0224],\n",
      "        [ 0.0969,  0.0160,  0.0574,  ...,  0.0613, -0.0176,  0.0806],\n",
      "        [-0.0843,  0.0039, -0.0187,  ..., -0.0756,  0.0027, -0.0260]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0049, -0.5827, -0.3817,  ..., -0.1075,  0.4317, -0.3986],\n",
      "        [ 0.1861, -0.1106, -0.1915,  ...,  0.5132,  0.1166, -0.4113],\n",
      "        [-0.2911,  0.4970, -0.2502,  ..., -0.3959, -0.4138,  0.6135],\n",
      "        ...,\n",
      "        [-0.1116,  0.5606,  0.3265,  ..., -0.1877, -0.1712, -0.0567],\n",
      "        [ 0.2301, -0.1811,  0.2248,  ...,  0.1271, -0.0455,  0.1938],\n",
      "        [-0.4139,  0.5283, -0.3620,  ..., -0.2091, -0.1015, -0.0773]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2574,  0.0176,  0.2289,  ..., -0.6718,  0.2796,  0.7307],\n",
      "        [ 0.1987, -0.5202, -0.3534,  ..., -0.0064,  0.2998, -0.1017],\n",
      "        [-0.1085, -0.4562, -0.1552,  ...,  0.8092,  0.0983,  0.0695],\n",
      "        ...,\n",
      "        [-0.7986, -0.1620,  0.6178,  ..., -0.0224, -0.4539, -0.2611],\n",
      "        [ 0.9160, -0.1003,  1.1535,  ..., -0.9756,  0.8806,  0.4865],\n",
      "        [ 0.0355,  0.0411,  0.3013,  ...,  1.0121,  0.4384,  0.1861]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5257, -0.4898, -0.2240,  ..., -1.2169, -0.2378, -0.5484],\n",
      "        [-0.1479,  0.1722, -0.0118,  ..., -0.9079, -0.5655,  0.5364],\n",
      "        [ 0.3661, -0.1505, -0.9504,  ...,  0.3240,  1.3391,  0.2544],\n",
      "        ...,\n",
      "        [-0.6842,  0.2323,  0.4583,  ...,  0.6674, -1.0715,  0.6289],\n",
      "        [ 1.0503,  0.4429, -0.0392,  ..., -0.3717, -0.0037, -0.1065],\n",
      "        [-0.3432,  0.2082,  0.2649,  ..., -0.2264,  0.7595, -0.2391]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1823, 0.1739, 0.1211, 0.1481, 0.1461, 0.1474, 0.0481, 0.1619, 0.1269,\n",
      "        0.2082, 0.1633, 0.0721, 0.1475, 0.1412, 0.1499, 0.2098, 0.1379, 0.1332,\n",
      "        0.1819, 0.1132, 0.1307, 0.1023, 0.1494, 0.1734, 0.1493, 0.1181, 0.1187,\n",
      "        0.1625, 0.1485, 0.1612, 0.1397, 0.1622, 0.1751, 0.1479, 0.1332, 0.1441,\n",
      "        0.1609, 0.1431, 0.1582, 0.1216, 0.1466, 0.1483, 0.1538, 0.1221, 0.1353,\n",
      "        0.1780, 0.1515, 0.1604, 0.1347, 0.1364, 0.1324, 0.1386, 0.1414, 0.1628,\n",
      "        0.1451, 0.1703, 0.1373, 0.1596, 0.1482, 0.1283, 0.1339, 0.1375, 0.1715,\n",
      "        0.1458, 0.1592, 0.1638, 0.1321, 0.1602, 0.1797, 0.1439, 0.1767, 0.1191,\n",
      "        0.1469, 0.1419, 0.1292, 0.1360, 0.1498, 0.1660, 0.0516, 0.1444, 0.1598,\n",
      "        0.1405, 0.1445, 0.1380, 0.1562, 0.1571, 0.1508, 0.1568, 0.1505, 0.1458,\n",
      "        0.1345, 0.1159, 0.1305, 0.1654, 0.1639, 0.1383, 0.1497, 0.1493, 0.1171,\n",
      "        0.1312, 0.1253, 0.1305, 0.1453, 0.1392, 0.1377, 0.1458, 0.1576, 0.1337,\n",
      "        0.1131, 0.1432, 0.1298, 0.1395, 0.2053, 0.1167, 0.1473, 0.1491, 0.1137,\n",
      "        0.1324, 0.1259, 0.1639, 0.1463, 0.1573, 0.1581, 0.1588, 0.1252, 0.1446,\n",
      "        0.1420, 0.1396, 0.1200, 0.1368, 0.1593, 0.1494, 0.1268, 0.1129, 0.1198,\n",
      "        0.1580, 0.0430, 0.1464, 0.1311, 0.1425, 0.1392, 0.1721, 0.1564, 0.1585,\n",
      "        0.1534, 0.1553, 0.1502, 0.1440, 0.1399, 0.1137, 0.1656, 0.1565, 0.1605,\n",
      "        0.1596, 0.1453, 0.1683, 0.1509, 0.1576, 0.1428, 0.1521, 0.1481, 0.1749,\n",
      "        0.1651, 0.0669, 0.1383, 0.1454, 0.1417, 0.1764, 0.1303, 0.1619, 0.1642,\n",
      "        0.1214, 0.1459, 0.1435, 0.1573, 0.1492, 0.1320, 0.1258, 0.1436, 0.1634,\n",
      "        0.1338, 0.1111, 0.1956, 0.1279, 0.1540, 0.1497, 0.1631, 0.1808, 0.1532,\n",
      "        0.1328, 0.1414, 0.1479, 0.1351, 0.1499, 0.1428, 0.1270, 0.1626, 0.1617,\n",
      "        0.1277, 0.1515, 0.1478, 0.1343, 0.1392, 0.1408, 0.1219, 0.1178, 0.1224,\n",
      "        0.1367, 0.1629, 0.1506, 0.1764, 0.1583, 0.1645, 0.1393, 0.1562, 0.1406,\n",
      "        0.1430, 0.1817, 0.1391, 0.1715, 0.1329, 0.1374, 0.1448, 0.1459, 0.1541,\n",
      "        0.1198, 0.1516, 0.1257, 0.1482, 0.1479, 0.1338, 0.1281, 0.1303, 0.1317,\n",
      "        0.1957, 0.1517, 0.1460, 0.1578, 0.1624, 0.1656, 0.1493, 0.1444, 0.1527,\n",
      "        0.1347, 0.1251, 0.1574, 0.1555, 0.1617, 0.1213, 0.1525, 0.1148, 0.1460,\n",
      "        0.1554, 0.1342, 0.1197, 0.1320, 0.1401, 0.1645, 0.1547, 0.1598, 0.1422,\n",
      "        0.1191, 0.1502, 0.1686, 0.1826, 0.1427, 0.1649, 0.1709, 0.1513, 0.1189,\n",
      "        0.1442, 0.1471, 0.1420, 0.1291, 0.1800, 0.1301, 0.1395, 0.1625, 0.1334,\n",
      "        0.1654, 0.1317, 0.1611, 0.1258, 0.1537, 0.1495, 0.1134, 0.1678, 0.1421,\n",
      "        0.1762, 0.1422, 0.1284, 0.1203, 0.1681, 0.1410, 0.1378, 0.1063, 0.1367,\n",
      "        0.1040, 0.1547, 0.1288, 0.1738, 0.1370, 0.1526, 0.1727, 0.1455, 0.1665,\n",
      "        0.1383, 0.1243, 0.1149, 0.1341, 0.1333, 0.1576, 0.1195, 0.1489, 0.1425,\n",
      "        0.1610, 0.1421, 0.1683, 0.1576, 0.1508, 0.1761, 0.1339, 0.1408, 0.1883,\n",
      "        0.1431, 0.1437, 0.1572, 0.1448, 0.1434, 0.1599, 0.1318, 0.1348, 0.1457,\n",
      "        0.0879, 0.1412, 0.1438, 0.1553, 0.1592, 0.1340, 0.1551, 0.1342, 0.1534,\n",
      "        0.1460, 0.1287, 0.1549, 0.1356, 0.1716, 0.1498, 0.1492, 0.1143, 0.1382,\n",
      "        0.1614, 0.1455, 0.1573, 0.1590, 0.1491, 0.1424, 0.1407, 0.1377, 0.1154,\n",
      "        0.1435, 0.1529, 0.1426, 0.1232, 0.1111, 0.1383, 0.1324, 0.0405, 0.1577,\n",
      "        0.1553, 0.1552, 0.1519, 0.1701, 0.1483, 0.1211, 0.1693, 0.1326, 0.1571,\n",
      "        0.1301, 0.1612, 0.1647, 0.1505, 0.1452, 0.1400, 0.1289, 0.1504, 0.2044,\n",
      "        0.1617, 0.1564, 0.1330, 0.1495, 0.1375, 0.1442, 0.1668, 0.1771, 0.1381,\n",
      "        0.1580, 0.1584, 0.1452, 0.1355, 0.1453, 0.1188, 0.1502, 0.1538, 0.1342,\n",
      "        0.1586, 0.1460, 0.1473, 0.1507, 0.1433, 0.1447, 0.1486, 0.0969, 0.1564,\n",
      "        0.0453, 0.1313, 0.1442, 0.1547, 0.1585, 0.1590, 0.1474, 0.1725, 0.1394,\n",
      "        0.1250, 0.1396, 0.1348, 0.1561, 0.1283, 0.1232, 0.1501, 0.1425, 0.1702,\n",
      "        0.1309, 0.1590, 0.1574, 0.1233, 0.1393, 0.1349, 0.1476, 0.1237, 0.1497,\n",
      "        0.1268, 0.1341, 0.1881, 0.1199, 0.1494, 0.1274, 0.2070, 0.1399, 0.1427,\n",
      "        0.1314, 0.1356, 0.1457, 0.1444, 0.1599, 0.1185, 0.1446, 0.1559, 0.1146,\n",
      "        0.1375, 0.1430, 0.1546, 0.1273, 0.1110, 0.1106, 0.1730, 0.1225, 0.1134,\n",
      "        0.1645, 0.1596, 0.1345, 0.1555, 0.1374, 0.1390, 0.1805, 0.1931, 0.1529,\n",
      "        0.1387, 0.1160, 0.1555, 0.1383, 0.1545, 0.1791, 0.1720, 0.1345, 0.1631,\n",
      "        0.1751, 0.1431, 0.1560, 0.1630, 0.1673, 0.1293, 0.1504, 0.1324, 0.1585,\n",
      "        0.1259, 0.1566, 0.1873, 0.1472, 0.1635, 0.1548, 0.1805, 0.1480, 0.1531,\n",
      "        0.1578, 0.1498, 0.1534, 0.1822, 0.1122, 0.1446, 0.1374, 0.1439],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5324, -0.2199, -0.3466,  ...,  0.2002,  0.0159, -0.0971],\n",
      "        [-0.2038, -0.2549,  0.3540,  ..., -0.3686,  0.3369,  0.0433],\n",
      "        [ 0.0325,  0.3552,  0.4295,  ...,  0.2879, -0.0353,  0.2573],\n",
      "        ...,\n",
      "        [ 0.4492,  0.3763,  0.5273,  ...,  0.5485, -0.4637,  0.0102],\n",
      "        [-0.2863, -0.4924,  0.3571,  ...,  0.3174, -0.1572, -0.4237],\n",
      "        [-0.7374, -0.2185, -0.3652,  ...,  0.5424,  0.5085, -0.4262]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2270, -1.1919,  0.5062,  ...,  0.1616,  0.7152, -0.0882],\n",
      "        [ 0.0385, -0.1144,  0.7901,  ..., -0.5040,  1.4944,  0.1219],\n",
      "        [ 0.0565, -0.6815,  0.7621,  ...,  0.4182, -0.1431, -1.1848],\n",
      "        ...,\n",
      "        [-0.6486,  1.2245,  0.6097,  ...,  0.0932,  0.1038,  0.7086],\n",
      "        [-2.7130,  2.2363,  2.3685,  ...,  0.8221,  1.0880,  1.0162],\n",
      "        [ 0.1497, -1.3501,  0.1528,  ...,  0.4016,  0.5999,  0.3577]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.9636, -0.0324,  0.2129,  ...,  1.0126, -0.5269, -0.1217],\n",
      "        [ 0.1152, -0.0181,  0.5691,  ..., -0.8296,  0.1988, -0.2636],\n",
      "        [ 0.1415,  0.2413, -0.1047,  ...,  1.1972,  1.1826,  0.3129],\n",
      "        ...,\n",
      "        [-0.2464,  0.5526, -0.7380,  ...,  0.2966, -0.0365,  0.2343],\n",
      "        [-0.1302,  1.0191, -0.2459,  ...,  0.5248, -0.5794, -0.0230],\n",
      "        [ 0.0744,  0.0485, -0.0559,  ..., -0.3950, -0.2776,  0.4095]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1295, 0.1353, 0.0839, 0.1125, 0.1114, 0.0995, 0.0486, 0.1145, 0.0800,\n",
      "        0.1226, 0.1084, 0.0423, 0.1017, 0.0889, 0.0883, 0.1310, 0.1012, 0.0821,\n",
      "        0.1046, 0.0801, 0.0940, 0.0740, 0.1009, 0.0920, 0.0927, 0.0744, 0.0672,\n",
      "        0.1051, 0.1039, 0.0844, 0.1037, 0.1157, 0.1447, 0.1014, 0.0912, 0.1045,\n",
      "        0.1048, 0.0868, 0.0869, 0.0731, 0.1010, 0.1012, 0.1144, 0.0798, 0.0990,\n",
      "        0.1062, 0.0957, 0.1161, 0.0878, 0.1056, 0.0801, 0.1082, 0.0996, 0.1059,\n",
      "        0.1141, 0.1187, 0.0947, 0.1131, 0.0947, 0.0843, 0.1064, 0.0921, 0.0968,\n",
      "        0.1119, 0.1303, 0.1137, 0.0727, 0.1045, 0.1196, 0.0921, 0.1159, 0.0778,\n",
      "        0.0959, 0.1000, 0.0910, 0.1013, 0.0931, 0.1187, 0.0436, 0.0980, 0.1095,\n",
      "        0.0981, 0.0900, 0.0963, 0.1064, 0.1097, 0.1065, 0.1042, 0.1107, 0.1074,\n",
      "        0.0878, 0.0815, 0.0961, 0.1081, 0.1224, 0.1039, 0.1070, 0.1191, 0.0839,\n",
      "        0.0813, 0.0753, 0.0941, 0.1059, 0.1063, 0.0886, 0.0905, 0.1101, 0.0985,\n",
      "        0.0799, 0.0952, 0.0906, 0.1007, 0.1340, 0.0759, 0.0921, 0.0993, 0.0677,\n",
      "        0.0958, 0.0889, 0.1178, 0.0985, 0.1021, 0.0961, 0.1118, 0.1015, 0.1061,\n",
      "        0.0958, 0.0927, 0.0897, 0.0811, 0.1061, 0.0950, 0.0919, 0.0784, 0.0829,\n",
      "        0.0936, 0.2225, 0.0918, 0.0914, 0.1099, 0.0780, 0.1165, 0.0882, 0.1128,\n",
      "        0.1017, 0.1279, 0.0911, 0.1009, 0.0917, 0.0910, 0.1018, 0.1084, 0.0893,\n",
      "        0.1173, 0.0784, 0.0912, 0.0956, 0.0997, 0.0949, 0.1023, 0.0885, 0.1261,\n",
      "        0.1063, 0.0305, 0.1000, 0.0980, 0.0954, 0.1115, 0.0931, 0.0946, 0.1166,\n",
      "        0.0921, 0.1075, 0.0909, 0.1249, 0.0879, 0.0970, 0.0757, 0.1093, 0.1146,\n",
      "        0.0804, 0.0850, 0.1112, 0.0872, 0.0924, 0.1083, 0.1165, 0.1160, 0.1074,\n",
      "        0.0847, 0.0912, 0.1069, 0.0981, 0.1018, 0.1061, 0.0817, 0.0866, 0.1042,\n",
      "        0.1008, 0.1069, 0.0897, 0.0907, 0.0938, 0.0908, 0.0983, 0.0828, 0.0863,\n",
      "        0.0873, 0.1165, 0.1097, 0.1181, 0.0944, 0.0976, 0.0842, 0.1157, 0.0985,\n",
      "        0.1009, 0.1033, 0.0959, 0.1177, 0.0729, 0.0830, 0.0921, 0.0993, 0.0867,\n",
      "        0.0806, 0.1049, 0.0920, 0.0945, 0.1186, 0.0842, 0.0931, 0.0863, 0.1012,\n",
      "        0.1211, 0.1033, 0.1000, 0.1204, 0.1121, 0.0975, 0.1088, 0.0969, 0.0926,\n",
      "        0.0925, 0.0881, 0.1152, 0.0930, 0.0926, 0.0786, 0.1022, 0.0780, 0.1113,\n",
      "        0.1042, 0.0961, 0.0833, 0.0945, 0.0911, 0.0980, 0.0955, 0.1164, 0.1022,\n",
      "        0.0811, 0.0886, 0.1040, 0.1182, 0.1118, 0.1038, 0.1132, 0.0887, 0.0916,\n",
      "        0.0978, 0.0979, 0.0944, 0.0887, 0.1146, 0.0982, 0.0907, 0.1116, 0.0845,\n",
      "        0.1001, 0.0957, 0.0961, 0.0866, 0.1252, 0.1089, 0.0869, 0.1015, 0.0914,\n",
      "        0.1220, 0.0922, 0.0989, 0.0909, 0.1057, 0.0868, 0.1058, 0.0682, 0.0969,\n",
      "        0.0758, 0.1083, 0.0936, 0.1302, 0.1086, 0.0942, 0.1163, 0.1026, 0.1090,\n",
      "        0.1139, 0.0824, 0.0790, 0.0959, 0.0925, 0.0969, 0.0880, 0.1032, 0.1008,\n",
      "        0.1158, 0.1099, 0.1139, 0.1024, 0.0906, 0.1240, 0.0906, 0.0911, 0.1118,\n",
      "        0.1046, 0.0851, 0.1141, 0.0990, 0.0961, 0.1205, 0.1011, 0.0939, 0.1044,\n",
      "        0.0571, 0.1150, 0.0846, 0.0870, 0.1046, 0.0767, 0.1190, 0.1059, 0.1095,\n",
      "        0.0977, 0.1019, 0.0971, 0.0944, 0.0937, 0.0900, 0.0893, 0.0884, 0.0908,\n",
      "        0.1043, 0.0965, 0.0961, 0.0985, 0.0975, 0.1012, 0.0912, 0.1020, 0.0827,\n",
      "        0.0993, 0.0907, 0.0993, 0.0776, 0.0734, 0.1042, 0.0874, 0.0368, 0.1011,\n",
      "        0.1064, 0.1264, 0.0970, 0.1119, 0.1157, 0.0847, 0.0990, 0.0896, 0.0952,\n",
      "        0.0989, 0.0980, 0.1005, 0.1003, 0.0952, 0.0987, 0.0837, 0.1090, 0.1224,\n",
      "        0.1085, 0.0951, 0.0952, 0.1085, 0.0835, 0.0979, 0.0994, 0.1242, 0.0979,\n",
      "        0.0979, 0.1121, 0.0987, 0.1037, 0.0856, 0.0839, 0.1155, 0.1009, 0.0996,\n",
      "        0.1145, 0.1006, 0.1003, 0.1082, 0.0952, 0.0980, 0.0988, 0.0424, 0.0874,\n",
      "        0.0342, 0.1013, 0.1058, 0.0914, 0.0851, 0.1051, 0.1011, 0.1127, 0.0944,\n",
      "        0.0786, 0.0738, 0.0995, 0.1027, 0.0991, 0.0952, 0.0864, 0.0989, 0.1232,\n",
      "        0.0936, 0.1078, 0.1056, 0.0803, 0.0981, 0.1010, 0.0927, 0.0788, 0.0929,\n",
      "        0.0888, 0.0830, 0.1118, 0.0987, 0.1045, 0.0825, 0.1109, 0.0843, 0.0930,\n",
      "        0.0771, 0.1031, 0.0954, 0.0848, 0.1045, 0.0840, 0.0960, 0.1167, 0.0811,\n",
      "        0.1009, 0.1051, 0.1025, 0.0841, 0.0747, 0.0829, 0.1034, 0.0920, 0.0830,\n",
      "        0.1073, 0.1128, 0.0867, 0.1003, 0.0795, 0.0986, 0.0972, 0.1117, 0.0899,\n",
      "        0.0978, 0.0730, 0.1102, 0.1078, 0.0960, 0.1219, 0.1037, 0.0932, 0.1140,\n",
      "        0.1178, 0.1050, 0.1178, 0.1078, 0.1194, 0.0817, 0.0886, 0.1007, 0.1155,\n",
      "        0.0941, 0.1015, 0.1177, 0.1110, 0.1108, 0.1105, 0.1142, 0.0935, 0.1100,\n",
      "        0.1114, 0.0874, 0.0995, 0.1088, 0.0763, 0.0971, 0.1061, 0.1489],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0524, -0.0170, -0.0049,  ...,  0.0252,  0.0064, -0.0110],\n",
      "        [-0.0378,  0.0223,  0.0166,  ...,  0.0115,  0.0147, -0.0168],\n",
      "        [-0.0070,  0.0737,  0.0470,  ...,  0.0947, -0.0101, -0.0095],\n",
      "        ...,\n",
      "        [-0.0118,  0.0058,  0.0302,  ...,  0.0007,  0.0236, -0.0914],\n",
      "        [ 0.0212, -0.0165,  0.0065,  ...,  0.0944,  0.0165,  0.0230],\n",
      "        [-0.0143, -0.0062, -0.0461,  ...,  0.0088,  0.0344, -0.0868]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.9032, -0.9306,  0.1984,  ...,  0.6458, -0.1134, -0.3245],\n",
      "        [-0.4905,  0.0393,  0.0375,  ...,  0.6092, -0.2959, -0.7929],\n",
      "        [ 0.5582,  0.1637,  0.3791,  ...,  0.0424,  0.3029,  0.1186],\n",
      "        ...,\n",
      "        [ 0.0599, -0.5119,  0.1578,  ..., -0.2532,  0.1740, -0.4425],\n",
      "        [ 0.2182,  0.4073,  0.0528,  ...,  0.9446, -0.3834, -0.1314],\n",
      "        [-0.0708,  0.0389,  0.2468,  ..., -0.4309,  0.3768, -0.3553]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4216,  1.2399, -0.1807,  ..., -0.3565,  0.4232, -0.4918],\n",
      "        [ 0.0630, -0.4756, -0.3600,  ..., -0.6368,  0.1005, -0.5631],\n",
      "        [ 0.5775,  1.5225,  0.4753,  ...,  0.0502,  0.7771,  0.3100],\n",
      "        ...,\n",
      "        [ 0.3501,  0.0828,  0.0247,  ..., -0.2247,  0.9479,  0.5335],\n",
      "        [ 1.3424,  0.7536,  0.0332,  ..., -0.7650,  0.0620, -0.1074],\n",
      "        [ 0.8985, -0.3043,  0.6077,  ...,  0.9066,  0.4911, -0.0308]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.5680e-01,  5.4790e-01, -4.1386e-01,  ...,  3.5035e-01,\n",
      "         -4.1554e-04, -9.8481e-01],\n",
      "        [ 4.6125e-01,  4.2619e-01,  1.5103e-01,  ..., -5.8927e-02,\n",
      "         -2.8190e-01,  1.7048e-01],\n",
      "        [ 3.2161e-01,  3.3604e-01,  6.4115e-01,  ...,  1.2450e-01,\n",
      "         -9.5999e-01,  2.0126e-02],\n",
      "        ...,\n",
      "        [-1.8460e-01,  3.6821e-02,  2.1357e-01,  ..., -9.7585e-01,\n",
      "          1.8238e-01, -1.2258e+00],\n",
      "        [ 1.6978e-01,  6.1166e-02, -4.1415e-01,  ..., -3.3305e-01,\n",
      "         -2.2883e-01, -4.5280e-01],\n",
      "        [-2.3363e-01,  2.7870e-01, -9.4648e-02,  ...,  3.0592e-01,\n",
      "         -2.0375e-01,  3.1511e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1886, 0.1991, 0.1351, 0.2061, 0.1643, 0.1537, 0.0367, 0.1889, 0.1366,\n",
      "        0.2244, 0.2075, 0.0711, 0.1653, 0.1526, 0.1417, 0.1897, 0.1527, 0.1500,\n",
      "        0.1462, 0.1214, 0.1341, 0.1183, 0.1785, 0.1816, 0.1348, 0.1301, 0.1167,\n",
      "        0.1781, 0.1559, 0.1544, 0.1636, 0.1968, 0.1489, 0.1391, 0.1555, 0.1763,\n",
      "        0.1778, 0.1605, 0.1541, 0.1234, 0.1564, 0.1625, 0.1674, 0.1251, 0.1462,\n",
      "        0.1784, 0.1625, 0.2011, 0.1560, 0.1766, 0.1347, 0.1992, 0.1539, 0.2141,\n",
      "        0.1729, 0.1897, 0.1555, 0.1805, 0.1573, 0.1322, 0.1909, 0.1648, 0.1724,\n",
      "        0.1695, 0.2020, 0.2117, 0.1260, 0.1771, 0.1623, 0.1363, 0.1937, 0.1313,\n",
      "        0.1540, 0.1491, 0.1360, 0.1502, 0.1704, 0.2130, 0.0655, 0.1618, 0.1767,\n",
      "        0.1308, 0.1594, 0.1601, 0.1608, 0.1895, 0.1665, 0.1489, 0.1677, 0.1704,\n",
      "        0.1308, 0.1419, 0.1525, 0.1613, 0.1959, 0.1701, 0.1607, 0.1613, 0.1281,\n",
      "        0.1393, 0.1344, 0.1624, 0.1628, 0.1576, 0.1443, 0.1399, 0.1806, 0.1590,\n",
      "        0.1160, 0.1600, 0.1284, 0.1670, 0.1937, 0.1351, 0.1473, 0.1449, 0.1113,\n",
      "        0.1603, 0.1502, 0.1883, 0.1612, 0.1499, 0.1609, 0.1850, 0.1461, 0.1646,\n",
      "        0.1737, 0.1514, 0.1480, 0.1349, 0.1457, 0.1601, 0.1508, 0.1224, 0.1446,\n",
      "        0.1678, 0.0338, 0.1591, 0.1524, 0.1570, 0.1477, 0.1823, 0.1699, 0.1854,\n",
      "        0.1760, 0.2255, 0.1467, 0.1563, 0.1508, 0.1564, 0.1678, 0.1932, 0.1615,\n",
      "        0.2153, 0.1586, 0.1727, 0.1586, 0.1739, 0.1641, 0.1471, 0.1679, 0.1903,\n",
      "        0.1608, 0.0484, 0.1640, 0.1433, 0.1445, 0.2125, 0.1730, 0.1971, 0.1931,\n",
      "        0.1203, 0.1741, 0.1697, 0.1675, 0.1569, 0.1606, 0.1468, 0.1676, 0.1650,\n",
      "        0.1352, 0.1358, 0.1528, 0.1486, 0.1445, 0.1654, 0.1749, 0.1527, 0.1756,\n",
      "        0.1279, 0.1583, 0.1577, 0.1485, 0.1725, 0.1828, 0.1284, 0.1567, 0.1900,\n",
      "        0.1616, 0.1393, 0.1402, 0.1556, 0.1644, 0.1465, 0.1342, 0.1258, 0.1411,\n",
      "        0.1294, 0.1772, 0.1473, 0.1880, 0.1570, 0.1777, 0.1536, 0.1998, 0.1725,\n",
      "        0.1682, 0.1651, 0.1386, 0.1929, 0.1273, 0.1337, 0.1743, 0.1537, 0.1532,\n",
      "        0.1292, 0.1753, 0.1433, 0.1488, 0.1814, 0.1333, 0.1741, 0.1445, 0.1371,\n",
      "        0.1992, 0.1517, 0.1898, 0.1650, 0.1608, 0.1783, 0.1553, 0.1635, 0.1645,\n",
      "        0.1456, 0.1440, 0.1737, 0.1580, 0.1417, 0.1206, 0.1792, 0.1043, 0.1550,\n",
      "        0.1558, 0.1540, 0.1384, 0.1484, 0.1535, 0.1641, 0.1844, 0.1755, 0.1549,\n",
      "        0.1361, 0.1364, 0.1609, 0.1994, 0.1800, 0.1729, 0.1959, 0.1345, 0.1357,\n",
      "        0.1629, 0.1572, 0.1555, 0.1296, 0.1625, 0.1530, 0.1567, 0.1693, 0.1457,\n",
      "        0.1625, 0.1466, 0.1613, 0.1505, 0.1434, 0.1716, 0.1433, 0.1552, 0.1450,\n",
      "        0.1946, 0.1535, 0.1923, 0.1599, 0.1733, 0.1484, 0.1596, 0.1046, 0.1505,\n",
      "        0.1241, 0.1592, 0.1577, 0.1980, 0.1799, 0.1513, 0.1938, 0.1588, 0.1786,\n",
      "        0.1679, 0.1329, 0.1256, 0.1667, 0.1640, 0.1678, 0.1491, 0.1672, 0.1820,\n",
      "        0.1691, 0.1719, 0.1766, 0.1570, 0.1549, 0.2053, 0.1503, 0.1529, 0.1876,\n",
      "        0.1641, 0.1300, 0.2131, 0.1514, 0.1524, 0.1717, 0.1592, 0.1388, 0.1606,\n",
      "        0.0890, 0.1534, 0.1445, 0.1592, 0.1703, 0.1132, 0.1728, 0.1617, 0.1895,\n",
      "        0.1734, 0.1548, 0.1828, 0.1622, 0.1768, 0.1560, 0.1700, 0.1399, 0.1416,\n",
      "        0.1657, 0.1490, 0.1578, 0.1721, 0.1672, 0.1770, 0.1723, 0.1672, 0.1165,\n",
      "        0.1591, 0.1713, 0.1743, 0.1282, 0.1189, 0.1672, 0.1515, 0.0398, 0.1801,\n",
      "        0.1895, 0.1741, 0.1533, 0.1992, 0.1792, 0.1335, 0.1730, 0.1411, 0.1433,\n",
      "        0.1603, 0.1773, 0.1711, 0.1750, 0.1651, 0.1678, 0.1434, 0.1900, 0.2016,\n",
      "        0.1729, 0.1690, 0.1463, 0.1726, 0.1474, 0.1517, 0.1651, 0.1857, 0.1600,\n",
      "        0.1851, 0.1812, 0.1556, 0.1861, 0.1385, 0.1514, 0.1913, 0.1781, 0.1534,\n",
      "        0.1667, 0.1491, 0.1522, 0.1417, 0.1555, 0.1649, 0.1781, 0.0734, 0.1583,\n",
      "        0.0553, 0.1745, 0.1813, 0.1557, 0.1389, 0.1864, 0.1598, 0.1797, 0.1561,\n",
      "        0.1381, 0.1276, 0.1635, 0.1507, 0.1508, 0.1271, 0.1294, 0.1589, 0.2165,\n",
      "        0.1435, 0.1672, 0.1718, 0.1290, 0.1566, 0.1633, 0.1495, 0.1113, 0.1636,\n",
      "        0.1325, 0.1351, 0.1839, 0.1610, 0.1580, 0.1395, 0.2164, 0.1569, 0.1676,\n",
      "        0.1300, 0.1611, 0.1555, 0.1355, 0.1855, 0.1389, 0.1598, 0.1774, 0.1290,\n",
      "        0.1462, 0.1684, 0.1561, 0.1291, 0.1065, 0.1184, 0.1896, 0.1488, 0.1268,\n",
      "        0.1634, 0.2160, 0.1517, 0.1714, 0.1194, 0.1674, 0.1826, 0.1813, 0.1641,\n",
      "        0.1591, 0.1069, 0.1814, 0.1715, 0.1606, 0.1880, 0.1620, 0.1495, 0.2060,\n",
      "        0.1957, 0.1602, 0.1733, 0.1670, 0.2020, 0.1552, 0.1714, 0.1561, 0.1705,\n",
      "        0.1758, 0.1518, 0.2011, 0.1711, 0.1832, 0.1621, 0.1866, 0.1653, 0.1599,\n",
      "        0.1815, 0.1420, 0.1772, 0.1774, 0.1236, 0.1404, 0.1398, 0.1418],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.8306e-02, -2.1392e-01,  3.0510e-01,  ...,  5.2802e-01,\n",
      "         -2.5478e-01,  3.1105e-01],\n",
      "        [ 6.0395e-01, -1.9938e-01, -7.4305e-02,  ...,  1.9084e-02,\n",
      "         -5.1699e-01,  3.2412e-01],\n",
      "        [ 3.3134e-01,  4.3623e-01, -7.4615e-02,  ..., -1.0883e-01,\n",
      "         -7.8590e-02, -4.3395e-01],\n",
      "        ...,\n",
      "        [ 3.9666e-01, -5.1755e-01, -4.9945e-01,  ..., -6.8742e-02,\n",
      "         -2.1013e-01, -1.2761e-01],\n",
      "        [-2.4238e-01, -1.6196e-01, -7.2590e-01,  ...,  3.7884e-01,\n",
      "          7.1642e-01, -3.4611e-01],\n",
      "        [-1.6989e-01,  3.5121e-01,  4.0855e-01,  ..., -2.6805e-01,\n",
      "          7.0943e-04,  7.9996e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2523,  1.6755, -0.5070,  ...,  0.3138, -1.5032,  0.0714],\n",
      "        [-1.5813,  0.1862,  1.3464,  ...,  0.8775, -2.6296,  0.0338],\n",
      "        [ 0.7520,  0.6026,  0.2470,  ...,  1.6493,  1.5977,  1.2486],\n",
      "        ...,\n",
      "        [-2.1363,  2.0477, -0.7108,  ...,  0.7253, -0.2299,  0.6001],\n",
      "        [-1.0708,  0.8701,  0.0677,  ...,  0.3016,  0.0052,  0.6530],\n",
      "        [ 0.5807,  0.4996, -0.2767,  ..., -0.3523,  0.3567,  0.6908]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.0094e-01, -9.8847e-02, -5.1034e-01,  ..., -1.0058e+00,\n",
      "          4.1063e-02,  6.7269e-01],\n",
      "        [ 6.3107e-01, -2.2128e-01, -3.6303e-01,  ...,  4.8177e-01,\n",
      "         -4.5934e-05,  2.4221e-01],\n",
      "        [ 1.3443e-01, -1.2406e-01,  1.5987e-01,  ..., -3.2290e-01,\n",
      "          3.1617e-01,  2.2348e-01],\n",
      "        ...,\n",
      "        [ 3.1583e-01, -6.1177e-01,  1.3636e-01,  ...,  5.2468e-01,\n",
      "         -3.3175e-01,  7.3769e-01],\n",
      "        [ 5.2550e-01,  4.9946e-01, -4.8975e-01,  ...,  4.3534e-01,\n",
      "         -1.0068e+00,  5.4005e-01],\n",
      "        [-5.0514e-01, -2.1353e-01, -8.9403e-02,  ...,  5.9895e-02,\n",
      "         -2.9452e-02, -3.9046e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1266, 0.1255, 0.0862, 0.1293, 0.1053, 0.1073, 0.0457, 0.1053, 0.0897,\n",
      "        0.1236, 0.1195, 0.0492, 0.1062, 0.0873, 0.0940, 0.1371, 0.0979, 0.0899,\n",
      "        0.1059, 0.1055, 0.0977, 0.0843, 0.1018, 0.1035, 0.1023, 0.0879, 0.0874,\n",
      "        0.1008, 0.1089, 0.0943, 0.0990, 0.1061, 0.1457, 0.0953, 0.1075, 0.1060,\n",
      "        0.1015, 0.0965, 0.1096, 0.0922, 0.1047, 0.1093, 0.1207, 0.0899, 0.1028,\n",
      "        0.1084, 0.0938, 0.1223, 0.0890, 0.1161, 0.0932, 0.1156, 0.1065, 0.1186,\n",
      "        0.1070, 0.1218, 0.0939, 0.1010, 0.1013, 0.0954, 0.1180, 0.0969, 0.1116,\n",
      "        0.1128, 0.1406, 0.1073, 0.0774, 0.1139, 0.1164, 0.0888, 0.1341, 0.0953,\n",
      "        0.0965, 0.0996, 0.0916, 0.0989, 0.0993, 0.1163, 0.0440, 0.1041, 0.1048,\n",
      "        0.1024, 0.0915, 0.0942, 0.1103, 0.1112, 0.1037, 0.1005, 0.1175, 0.1160,\n",
      "        0.0952, 0.0932, 0.0957, 0.1007, 0.1228, 0.1072, 0.1152, 0.1046, 0.0894,\n",
      "        0.0899, 0.0922, 0.1047, 0.1082, 0.1089, 0.0949, 0.0899, 0.1170, 0.0996,\n",
      "        0.0766, 0.1228, 0.0942, 0.1021, 0.1187, 0.0863, 0.0992, 0.1030, 0.0728,\n",
      "        0.0971, 0.0931, 0.1159, 0.0974, 0.0971, 0.0973, 0.1186, 0.0961, 0.0974,\n",
      "        0.1067, 0.1012, 0.0952, 0.0987, 0.1051, 0.1071, 0.1010, 0.0932, 0.0900,\n",
      "        0.0921, 0.2434, 0.0998, 0.1037, 0.1082, 0.0928, 0.1151, 0.0894, 0.1160,\n",
      "        0.1077, 0.1334, 0.0959, 0.0919, 0.0983, 0.0991, 0.1005, 0.1161, 0.0991,\n",
      "        0.1150, 0.0917, 0.1040, 0.1027, 0.1054, 0.1002, 0.1000, 0.0943, 0.1225,\n",
      "        0.1071, 0.0346, 0.1094, 0.0982, 0.0915, 0.1149, 0.0978, 0.1099, 0.1062,\n",
      "        0.0936, 0.0997, 0.0982, 0.1134, 0.1030, 0.1014, 0.0873, 0.1045, 0.1161,\n",
      "        0.0847, 0.0862, 0.1166, 0.0852, 0.1002, 0.1056, 0.1050, 0.1132, 0.1058,\n",
      "        0.0944, 0.0969, 0.0977, 0.1064, 0.1057, 0.1066, 0.0899, 0.1070, 0.1114,\n",
      "        0.0978, 0.1095, 0.0935, 0.0972, 0.1045, 0.0935, 0.0932, 0.0923, 0.0927,\n",
      "        0.0928, 0.1126, 0.1073, 0.1177, 0.1014, 0.0962, 0.0906, 0.1100, 0.0988,\n",
      "        0.1078, 0.1140, 0.1016, 0.1110, 0.0843, 0.0949, 0.1000, 0.0859, 0.0924,\n",
      "        0.0950, 0.1003, 0.0947, 0.1012, 0.1154, 0.0946, 0.1081, 0.0947, 0.1005,\n",
      "        0.1052, 0.1003, 0.1121, 0.1129, 0.1170, 0.1136, 0.1175, 0.1010, 0.0918,\n",
      "        0.0903, 0.0967, 0.1108, 0.1046, 0.0871, 0.0942, 0.1110, 0.0937, 0.1089,\n",
      "        0.1018, 0.1058, 0.0863, 0.1009, 0.0931, 0.0973, 0.1034, 0.1247, 0.0924,\n",
      "        0.0950, 0.0930, 0.1114, 0.1078, 0.1081, 0.1111, 0.1162, 0.1002, 0.0925,\n",
      "        0.1083, 0.1053, 0.0998, 0.0908, 0.1087, 0.1027, 0.0935, 0.1185, 0.0867,\n",
      "        0.1127, 0.0942, 0.1108, 0.1010, 0.1007, 0.1141, 0.0945, 0.1094, 0.1037,\n",
      "        0.1111, 0.1016, 0.1140, 0.0925, 0.1086, 0.0971, 0.1016, 0.0842, 0.1028,\n",
      "        0.0839, 0.1025, 0.1080, 0.1262, 0.1030, 0.0971, 0.1080, 0.1064, 0.1196,\n",
      "        0.1024, 0.0860, 0.0794, 0.1055, 0.0980, 0.1009, 0.0893, 0.1005, 0.1143,\n",
      "        0.1155, 0.1100, 0.1211, 0.1042, 0.1010, 0.1139, 0.0970, 0.0982, 0.1173,\n",
      "        0.1104, 0.0908, 0.1208, 0.0945, 0.1041, 0.1164, 0.0966, 0.0875, 0.1129,\n",
      "        0.0624, 0.1173, 0.0958, 0.1002, 0.1175, 0.0876, 0.1165, 0.1024, 0.1036,\n",
      "        0.1091, 0.0987, 0.1015, 0.0996, 0.1007, 0.1106, 0.1023, 0.0946, 0.0995,\n",
      "        0.1105, 0.0960, 0.1015, 0.1093, 0.0924, 0.1118, 0.1043, 0.1032, 0.0941,\n",
      "        0.0983, 0.1059, 0.0881, 0.0825, 0.0910, 0.1106, 0.0956, 0.0372, 0.1058,\n",
      "        0.1113, 0.1235, 0.1016, 0.1137, 0.1160, 0.0883, 0.0968, 0.0997, 0.0914,\n",
      "        0.1020, 0.0922, 0.1104, 0.1118, 0.0934, 0.1038, 0.0952, 0.1250, 0.1307,\n",
      "        0.1250, 0.1012, 0.0953, 0.1153, 0.0953, 0.0985, 0.0963, 0.1129, 0.0936,\n",
      "        0.1145, 0.1067, 0.1017, 0.1072, 0.0953, 0.0925, 0.1110, 0.0961, 0.0971,\n",
      "        0.0992, 0.0996, 0.1107, 0.1061, 0.0925, 0.1076, 0.1046, 0.0481, 0.1125,\n",
      "        0.0416, 0.1072, 0.1043, 0.0929, 0.0998, 0.1095, 0.1043, 0.1201, 0.0911,\n",
      "        0.0891, 0.0889, 0.0982, 0.0979, 0.0934, 0.0915, 0.0918, 0.1001, 0.1221,\n",
      "        0.0996, 0.1127, 0.1050, 0.0885, 0.1028, 0.1036, 0.0862, 0.0854, 0.0957,\n",
      "        0.0937, 0.0887, 0.1153, 0.1073, 0.1142, 0.0915, 0.1168, 0.0890, 0.1018,\n",
      "        0.0848, 0.1113, 0.1046, 0.0986, 0.1169, 0.0986, 0.1080, 0.1194, 0.0864,\n",
      "        0.1027, 0.1012, 0.1094, 0.0933, 0.0905, 0.0879, 0.1105, 0.0924, 0.0922,\n",
      "        0.1026, 0.1191, 0.0901, 0.1089, 0.0822, 0.1047, 0.1054, 0.1055, 0.1010,\n",
      "        0.1047, 0.0953, 0.1196, 0.1132, 0.1121, 0.1131, 0.1086, 0.0947, 0.1228,\n",
      "        0.1187, 0.1014, 0.1122, 0.1154, 0.1219, 0.0975, 0.1030, 0.1096, 0.1000,\n",
      "        0.1127, 0.1032, 0.1113, 0.1092, 0.1125, 0.1085, 0.1134, 0.1016, 0.1038,\n",
      "        0.1108, 0.0911, 0.1027, 0.1218, 0.0830, 0.0912, 0.1078, 0.1572],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-4.1852e-02, -2.0532e-02,  2.7851e-03,  ..., -1.5832e-02,\n",
      "         -3.9493e-02, -1.0337e-01],\n",
      "        [ 2.3198e-02,  5.0777e-02, -4.0717e-02,  ..., -5.4594e-02,\n",
      "          6.9468e-03,  6.4093e-02],\n",
      "        [ 3.4308e-02,  1.2735e-02,  2.4542e-02,  ..., -3.4477e-02,\n",
      "          5.6480e-03,  2.5853e-02],\n",
      "        ...,\n",
      "        [ 3.2445e-02, -2.3860e-03, -8.0874e-02,  ...,  5.4072e-03,\n",
      "          2.0231e-02, -6.5058e-03],\n",
      "        [-6.1900e-03,  1.2210e-02, -5.4205e-02,  ...,  7.4131e-02,\n",
      "         -6.6258e-02,  1.8173e-06],\n",
      "        [ 3.4276e-02,  1.7903e-02,  1.9092e-03,  ...,  1.5184e-02,\n",
      "          8.6654e-02, -2.3223e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1500, -0.4275, -0.2060,  ...,  0.2596, -0.1302, -0.8314],\n",
      "        [-0.2846, -0.1862, -0.3288,  ..., -0.1684,  0.0127, -0.2688],\n",
      "        [ 0.0495,  0.2998, -0.1144,  ...,  0.3886,  0.2661,  0.4879],\n",
      "        ...,\n",
      "        [ 0.4062, -0.1597, -0.3188,  ..., -0.4424, -0.1467, -0.0312],\n",
      "        [-0.4175,  0.2271, -0.1118,  ...,  0.4221, -0.5313, -0.1366],\n",
      "        [ 0.0553, -0.0668, -0.4560,  ...,  0.1302,  0.3892, -0.7839]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.0836, -0.4365, -0.9384,  ..., -0.1316,  1.0983,  0.4190],\n",
      "        [-0.4437,  0.1409,  0.3421,  ..., -0.0816,  1.0803,  0.4259],\n",
      "        [ 0.3335,  0.1158,  0.7095,  ...,  0.4256,  0.9768, -0.0081],\n",
      "        ...,\n",
      "        [ 0.9582,  0.1011, -0.7687,  ...,  0.6717,  0.5746, -0.1313],\n",
      "        [-0.0203,  0.5256,  0.7002,  ..., -0.7458, -0.2460, -0.6294],\n",
      "        [ 0.1596, -0.0934,  0.0880,  ...,  0.2482,  0.4702,  0.5942]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0429,  1.0518,  0.7046,  ...,  0.6400,  1.7001,  0.1651],\n",
      "        [ 0.4787, -0.4426,  0.9439,  ...,  0.0757,  0.8821,  1.3136],\n",
      "        [-1.0146, -0.2474, -0.2580,  ...,  0.7978, -0.5641, -0.6054],\n",
      "        ...,\n",
      "        [-0.8995, -0.3418, -0.5958,  ...,  0.5245,  0.6095, -0.1485],\n",
      "        [ 0.7586, -0.3791,  0.7764,  ..., -0.3009,  0.5566,  0.6124],\n",
      "        [-0.4533, -0.5715, -0.1028,  ...,  0.4382,  0.5162,  0.0270]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1814, 0.1800, 0.1519, 0.1945, 0.1526, 0.1902, 0.0388, 0.1876, 0.1648,\n",
      "        0.1964, 0.1772, 0.0765, 0.1569, 0.1458, 0.1495, 0.2142, 0.1768, 0.1552,\n",
      "        0.1892, 0.1409, 0.1432, 0.1331, 0.1631, 0.1875, 0.1590, 0.1393, 0.1388,\n",
      "        0.1784, 0.1764, 0.1475, 0.1744, 0.1829, 0.2048, 0.1592, 0.1776, 0.1603,\n",
      "        0.1777, 0.1721, 0.1762, 0.1525, 0.1707, 0.1627, 0.1836, 0.1320, 0.1603,\n",
      "        0.1725, 0.1575, 0.2077, 0.1598, 0.1855, 0.1609, 0.1572, 0.1550, 0.2023,\n",
      "        0.1620, 0.1987, 0.1544, 0.1741, 0.1626, 0.1399, 0.1523, 0.1662, 0.2034,\n",
      "        0.1743, 0.1731, 0.1848, 0.1404, 0.1848, 0.1950, 0.1586, 0.1966, 0.1512,\n",
      "        0.1527, 0.1603, 0.1251, 0.1600, 0.1804, 0.1843, 0.0692, 0.1808, 0.1937,\n",
      "        0.1539, 0.1736, 0.1552, 0.1922, 0.1872, 0.1637, 0.1651, 0.1692, 0.1739,\n",
      "        0.1547, 0.1407, 0.1510, 0.1687, 0.1980, 0.1839, 0.1456, 0.1915, 0.1508,\n",
      "        0.1341, 0.1463, 0.1667, 0.1583, 0.1907, 0.1605, 0.1670, 0.2009, 0.1841,\n",
      "        0.1451, 0.1635, 0.1417, 0.1539, 0.1668, 0.1480, 0.1435, 0.1740, 0.1472,\n",
      "        0.1378, 0.1722, 0.1757, 0.1842, 0.1757, 0.1552, 0.1647, 0.1665, 0.1703,\n",
      "        0.1734, 0.1734, 0.1559, 0.1471, 0.1447, 0.1756, 0.1415, 0.1505, 0.1370,\n",
      "        0.1585, 0.0215, 0.1725, 0.1613, 0.1414, 0.1561, 0.2068, 0.1664, 0.1676,\n",
      "        0.1621, 0.2138, 0.1528, 0.1558, 0.1574, 0.1712, 0.1615, 0.1877, 0.1579,\n",
      "        0.1701, 0.1565, 0.1719, 0.1766, 0.1607, 0.1632, 0.1590, 0.1783, 0.1666,\n",
      "        0.1483, 0.0511, 0.1519, 0.1480, 0.1615, 0.1813, 0.1444, 0.1701, 0.2025,\n",
      "        0.1531, 0.1743, 0.1663, 0.1768, 0.1682, 0.1690, 0.1378, 0.1528, 0.1926,\n",
      "        0.1419, 0.1506, 0.1787, 0.1724, 0.1664, 0.1605, 0.1630, 0.1688, 0.1846,\n",
      "        0.1409, 0.1752, 0.1499, 0.1519, 0.1835, 0.1756, 0.1593, 0.1542, 0.1901,\n",
      "        0.1649, 0.1675, 0.1522, 0.1609, 0.1657, 0.1547, 0.1473, 0.1534, 0.1463,\n",
      "        0.1486, 0.1541, 0.1550, 0.1844, 0.1757, 0.1555, 0.1442, 0.1900, 0.1590,\n",
      "        0.1715, 0.1753, 0.1652, 0.1814, 0.1544, 0.1475, 0.1587, 0.1559, 0.1865,\n",
      "        0.1626, 0.1811, 0.1525, 0.1580, 0.1536, 0.1446, 0.1626, 0.1541, 0.1447,\n",
      "        0.1830, 0.1433, 0.1872, 0.1666, 0.1800, 0.1787, 0.1720, 0.1651, 0.1632,\n",
      "        0.1428, 0.1503, 0.1753, 0.1841, 0.1568, 0.1528, 0.1758, 0.1312, 0.1636,\n",
      "        0.1690, 0.1720, 0.1490, 0.1721, 0.1590, 0.1766, 0.1695, 0.1936, 0.1732,\n",
      "        0.1560, 0.1506, 0.1644, 0.1945, 0.1843, 0.1924, 0.1950, 0.1560, 0.1453,\n",
      "        0.1474, 0.1811, 0.1596, 0.1530, 0.1734, 0.1608, 0.1629, 0.1776, 0.1561,\n",
      "        0.1817, 0.1503, 0.1549, 0.1554, 0.1395, 0.1583, 0.1577, 0.1613, 0.1416,\n",
      "        0.2042, 0.1417, 0.1862, 0.1533, 0.1681, 0.1453, 0.1531, 0.1432, 0.1730,\n",
      "        0.1380, 0.1617, 0.1664, 0.1791, 0.1725, 0.1744, 0.1775, 0.1561, 0.1537,\n",
      "        0.1644, 0.1422, 0.1441, 0.1611, 0.1527, 0.1636, 0.1347, 0.1709, 0.2062,\n",
      "        0.1757, 0.1794, 0.1869, 0.1726, 0.1534, 0.1765, 0.1430, 0.1550, 0.1918,\n",
      "        0.1747, 0.1526, 0.1926, 0.1441, 0.1700, 0.1633, 0.1571, 0.1483, 0.1737,\n",
      "        0.0940, 0.1663, 0.1680, 0.1861, 0.1721, 0.1421, 0.1905, 0.1684, 0.1543,\n",
      "        0.1819, 0.1382, 0.1841, 0.1527, 0.1802, 0.1837, 0.1633, 0.1474, 0.1530,\n",
      "        0.1809, 0.1449, 0.1598, 0.1616, 0.1717, 0.1573, 0.1539, 0.1578, 0.1531,\n",
      "        0.1673, 0.1655, 0.1574, 0.1537, 0.1499, 0.1776, 0.1602, 0.0474, 0.1753,\n",
      "        0.1939, 0.1825, 0.1566, 0.1837, 0.1731, 0.1487, 0.1587, 0.1515, 0.1735,\n",
      "        0.1673, 0.1687, 0.2093, 0.1612, 0.1323, 0.1826, 0.1339, 0.1738, 0.1718,\n",
      "        0.1701, 0.1648, 0.1660, 0.1674, 0.1513, 0.1570, 0.1703, 0.1799, 0.1618,\n",
      "        0.1472, 0.1644, 0.1724, 0.1614, 0.1510, 0.1646, 0.2021, 0.1724, 0.1629,\n",
      "        0.1643, 0.1708, 0.1726, 0.1620, 0.1538, 0.1587, 0.1708, 0.0890, 0.1852,\n",
      "        0.0608, 0.1355, 0.1740, 0.1762, 0.1536, 0.1568, 0.1834, 0.1442, 0.1563,\n",
      "        0.1343, 0.1449, 0.1510, 0.1735, 0.1611, 0.1603, 0.1728, 0.1676, 0.1969,\n",
      "        0.1573, 0.1641, 0.1575, 0.1500, 0.1505, 0.1712, 0.1607, 0.1330, 0.1601,\n",
      "        0.1536, 0.1482, 0.1685, 0.1628, 0.1668, 0.1546, 0.2092, 0.1536, 0.1729,\n",
      "        0.1397, 0.1731, 0.1567, 0.1688, 0.1685, 0.1429, 0.1660, 0.1708, 0.1647,\n",
      "        0.1459, 0.1659, 0.1644, 0.1511, 0.1382, 0.1213, 0.1698, 0.1603, 0.1447,\n",
      "        0.1706, 0.1851, 0.1754, 0.1727, 0.1560, 0.1619, 0.1781, 0.1984, 0.1514,\n",
      "        0.1781, 0.1573, 0.1716, 0.1762, 0.1656, 0.1620, 0.1612, 0.1499, 0.1704,\n",
      "        0.1799, 0.1664, 0.1841, 0.1706, 0.1697, 0.1516, 0.1537, 0.1598, 0.1637,\n",
      "        0.1715, 0.1532, 0.1802, 0.1635, 0.1813, 0.1742, 0.1907, 0.1654, 0.1747,\n",
      "        0.1634, 0.1695, 0.1627, 0.1674, 0.1542, 0.1512, 0.1420, 0.1143],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0658,  0.2747, -0.5239,  ..., -0.0572, -0.5691, -0.3575],\n",
      "        [ 0.1482, -0.0663, -0.0133,  ...,  0.5509, -0.0494, -0.1081],\n",
      "        [-0.3499, -0.4114, -0.0854,  ...,  0.1630,  0.3397, -0.2192],\n",
      "        ...,\n",
      "        [-0.1816, -0.3492,  0.0183,  ..., -0.4569, -0.5532,  0.0393],\n",
      "        [-0.4246,  0.0967, -0.5535,  ..., -0.1039,  0.0363, -0.4638],\n",
      "        [-0.3432, -0.1792,  0.1087,  ...,  0.1766,  0.4325, -0.6046]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4593,  0.7258, -0.8507,  ...,  1.0279, -1.3175, -1.5984],\n",
      "        [ 0.3090, -0.3169,  0.0036,  ..., -0.5553,  1.0446,  0.5714],\n",
      "        [ 0.7103,  0.2515, -0.3509,  ...,  0.0320, -0.2313,  0.5732],\n",
      "        ...,\n",
      "        [ 1.5400, -1.2530, -1.8278,  ..., -1.9265,  1.7133,  0.8839],\n",
      "        [ 0.1599,  0.8449,  0.3880,  ..., -0.4320,  0.1011, -1.6214],\n",
      "        [ 0.3523,  0.6671,  0.2554,  ..., -1.1911,  0.3694, -0.3970]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6166,  0.2410,  0.7320,  ...,  0.4552, -0.0797,  0.1767],\n",
      "        [-0.4890, -0.1329,  0.1032,  ..., -0.5635, -0.7168,  0.0035],\n",
      "        [ 1.1350,  0.2759, -0.0743,  ...,  0.0223,  0.6699,  0.6026],\n",
      "        ...,\n",
      "        [ 0.0054,  1.0262, -1.2433,  ..., -0.1967,  0.0244,  0.4089],\n",
      "        [ 0.1329, -0.2006, -0.9980,  ..., -0.0551, -0.4712, -0.6677],\n",
      "        [ 0.0826,  0.0680,  0.1503,  ...,  0.0096, -0.2680, -0.0459]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1292, 0.1332, 0.1043, 0.1262, 0.1095, 0.1204, 0.0536, 0.1163, 0.1102,\n",
      "        0.1310, 0.1236, 0.0648, 0.1172, 0.0911, 0.1065, 0.1384, 0.1110, 0.0995,\n",
      "        0.1058, 0.1118, 0.1023, 0.0890, 0.1126, 0.0988, 0.0947, 0.0990, 0.1000,\n",
      "        0.1103, 0.1169, 0.1005, 0.1040, 0.1139, 0.1867, 0.1062, 0.1172, 0.1177,\n",
      "        0.1063, 0.1135, 0.1261, 0.0884, 0.1046, 0.1086, 0.1244, 0.1004, 0.1066,\n",
      "        0.1076, 0.1076, 0.1249, 0.1107, 0.1197, 0.1009, 0.1279, 0.1018, 0.1316,\n",
      "        0.1168, 0.1157, 0.1011, 0.1165, 0.1105, 0.1001, 0.1099, 0.1105, 0.1109,\n",
      "        0.1070, 0.1213, 0.0980, 0.0893, 0.1214, 0.1110, 0.1050, 0.1385, 0.0991,\n",
      "        0.1133, 0.1024, 0.1028, 0.1100, 0.1016, 0.1206, 0.0508, 0.1159, 0.1137,\n",
      "        0.1100, 0.1021, 0.1091, 0.1206, 0.1112, 0.1143, 0.1242, 0.1199, 0.1270,\n",
      "        0.1017, 0.1004, 0.1123, 0.1095, 0.1265, 0.1167, 0.1155, 0.1214, 0.1016,\n",
      "        0.1034, 0.0946, 0.1091, 0.1114, 0.1072, 0.1100, 0.1052, 0.1169, 0.1001,\n",
      "        0.0960, 0.1065, 0.0907, 0.1079, 0.1212, 0.0980, 0.1068, 0.1157, 0.0921,\n",
      "        0.1159, 0.1042, 0.1251, 0.1054, 0.1060, 0.1090, 0.1220, 0.1067, 0.1130,\n",
      "        0.1044, 0.1190, 0.1119, 0.1053, 0.1081, 0.1102, 0.1117, 0.0954, 0.0939,\n",
      "        0.0972, 0.2220, 0.1201, 0.1035, 0.1188, 0.1111, 0.1187, 0.0918, 0.1271,\n",
      "        0.1134, 0.1401, 0.1052, 0.1052, 0.1127, 0.1125, 0.1082, 0.1237, 0.1084,\n",
      "        0.1214, 0.1011, 0.1093, 0.1150, 0.1052, 0.1104, 0.1166, 0.1201, 0.1134,\n",
      "        0.1132, 0.0416, 0.1168, 0.0999, 0.1050, 0.1188, 0.1010, 0.1094, 0.1159,\n",
      "        0.1024, 0.1142, 0.0981, 0.1233, 0.1049, 0.1129, 0.0917, 0.1173, 0.1183,\n",
      "        0.1062, 0.0940, 0.1098, 0.0954, 0.1136, 0.1295, 0.0976, 0.1226, 0.1068,\n",
      "        0.0922, 0.1058, 0.0986, 0.1047, 0.1066, 0.1001, 0.1009, 0.1222, 0.1204,\n",
      "        0.1055, 0.1154, 0.1062, 0.1107, 0.1026, 0.1068, 0.1103, 0.0995, 0.0969,\n",
      "        0.1073, 0.1093, 0.1162, 0.1252, 0.1053, 0.1060, 0.0942, 0.1241, 0.1164,\n",
      "        0.1099, 0.1260, 0.1106, 0.1040, 0.0909, 0.0982, 0.1053, 0.1015, 0.1010,\n",
      "        0.1048, 0.1007, 0.0982, 0.1036, 0.1161, 0.0972, 0.0974, 0.0962, 0.1030,\n",
      "        0.1092, 0.1046, 0.1126, 0.1171, 0.1050, 0.1041, 0.1212, 0.1146, 0.1041,\n",
      "        0.0973, 0.1066, 0.1146, 0.0985, 0.1045, 0.0982, 0.1095, 0.0976, 0.1097,\n",
      "        0.1035, 0.1073, 0.0975, 0.1062, 0.1120, 0.1051, 0.1068, 0.1178, 0.0963,\n",
      "        0.0989, 0.1043, 0.1158, 0.1116, 0.1190, 0.1109, 0.1132, 0.1136, 0.1022,\n",
      "        0.1062, 0.1193, 0.0990, 0.1017, 0.1051, 0.1216, 0.1055, 0.1262, 0.0965,\n",
      "        0.1171, 0.0925, 0.1107, 0.0946, 0.1136, 0.1223, 0.1101, 0.1186, 0.1159,\n",
      "        0.1129, 0.1032, 0.1181, 0.0963, 0.1063, 0.1008, 0.1192, 0.0836, 0.1104,\n",
      "        0.0938, 0.1138, 0.1088, 0.1214, 0.1085, 0.1088, 0.1298, 0.1128, 0.1161,\n",
      "        0.0978, 0.1085, 0.0940, 0.1040, 0.1109, 0.1182, 0.0860, 0.1060, 0.1138,\n",
      "        0.1142, 0.1157, 0.1405, 0.1196, 0.1159, 0.1165, 0.1095, 0.1039, 0.1156,\n",
      "        0.1118, 0.1061, 0.1232, 0.1052, 0.0970, 0.1177, 0.1085, 0.0944, 0.1183,\n",
      "        0.0812, 0.1100, 0.1017, 0.1113, 0.1164, 0.0946, 0.1290, 0.1070, 0.1060,\n",
      "        0.1115, 0.1024, 0.1157, 0.1044, 0.1135, 0.1074, 0.1042, 0.1076, 0.1084,\n",
      "        0.1087, 0.1118, 0.0999, 0.1170, 0.1087, 0.1173, 0.0987, 0.1077, 0.1224,\n",
      "        0.1079, 0.1101, 0.1062, 0.0983, 0.0953, 0.1094, 0.1043, 0.0480, 0.1179,\n",
      "        0.1139, 0.1409, 0.1139, 0.1200, 0.1113, 0.0920, 0.1148, 0.0909, 0.0904,\n",
      "        0.1014, 0.1115, 0.1264, 0.1043, 0.0962, 0.1076, 0.0989, 0.1236, 0.1201,\n",
      "        0.1287, 0.1186, 0.1031, 0.1264, 0.1003, 0.1155, 0.1003, 0.1222, 0.1010,\n",
      "        0.1124, 0.1149, 0.1045, 0.1176, 0.0959, 0.0968, 0.1251, 0.1040, 0.1036,\n",
      "        0.1110, 0.1132, 0.1164, 0.1050, 0.1178, 0.1207, 0.1094, 0.0577, 0.1121,\n",
      "        0.0460, 0.1002, 0.1125, 0.1112, 0.1024, 0.1206, 0.1133, 0.1289, 0.1019,\n",
      "        0.1008, 0.1025, 0.1038, 0.1040, 0.1066, 0.1022, 0.1008, 0.1138, 0.1221,\n",
      "        0.1027, 0.1129, 0.1274, 0.1036, 0.1164, 0.1090, 0.1227, 0.0935, 0.0966,\n",
      "        0.1059, 0.0990, 0.1279, 0.1141, 0.1136, 0.0921, 0.1099, 0.1046, 0.1104,\n",
      "        0.0973, 0.1082, 0.1053, 0.1034, 0.1098, 0.1022, 0.1096, 0.1205, 0.1079,\n",
      "        0.1006, 0.1041, 0.1190, 0.0978, 0.0886, 0.0896, 0.1114, 0.1044, 0.0922,\n",
      "        0.1186, 0.1161, 0.1029, 0.1068, 0.1019, 0.1065, 0.1054, 0.1152, 0.1031,\n",
      "        0.1097, 0.0968, 0.1159, 0.1058, 0.1064, 0.1216, 0.1284, 0.1078, 0.1180,\n",
      "        0.1172, 0.1104, 0.1050, 0.1142, 0.1213, 0.0981, 0.1047, 0.0963, 0.1228,\n",
      "        0.1231, 0.1127, 0.1144, 0.1081, 0.1167, 0.1049, 0.1355, 0.0976, 0.1185,\n",
      "        0.1168, 0.1065, 0.1145, 0.1291, 0.1001, 0.0995, 0.0958, 0.1948],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0049,  0.0087, -0.0041,  ..., -0.0242, -0.0342,  0.0770],\n",
      "        [ 0.0545,  0.0334, -0.0561,  ...,  0.0011,  0.0350, -0.0955],\n",
      "        [ 0.0357, -0.0106, -0.0284,  ..., -0.0236,  0.0647,  0.0009],\n",
      "        ...,\n",
      "        [ 0.0703, -0.0166, -0.0148,  ..., -0.0457,  0.0481, -0.0338],\n",
      "        [ 0.0461,  0.0232, -0.0202,  ...,  0.0111,  0.0274, -0.0263],\n",
      "        [ 0.0101, -0.0083, -0.0280,  ..., -0.0309,  0.0317,  0.0438]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-3.0279e-01,  6.0085e-02, -3.4483e-02,  ..., -2.8402e-01,\n",
      "          4.9480e-02,  1.0642e-01],\n",
      "        [-3.5169e-01,  2.1580e-01, -9.0171e-02,  ..., -2.6317e-01,\n",
      "         -7.8465e-02, -3.7453e-01],\n",
      "        [-1.7473e-01, -1.4353e-01,  4.7580e-01,  ...,  4.1745e-01,\n",
      "         -1.1848e-01, -8.2767e-02],\n",
      "        ...,\n",
      "        [-2.6841e-01,  9.1301e-04,  2.8529e-01,  ..., -3.5194e-01,\n",
      "          8.4356e-02, -3.2949e-01],\n",
      "        [-3.3616e-01, -5.3959e-01, -1.1655e-01,  ...,  2.1385e-01,\n",
      "         -2.4752e-01, -3.0728e-01],\n",
      "        [ 1.1062e-01, -2.4903e-01,  1.3186e-01,  ...,  6.5892e-01,\n",
      "          9.5682e-01, -4.1661e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0130,  0.4973,  1.2035,  ...,  0.4577, -0.1311, -0.8631],\n",
      "        [ 0.1128,  0.2781,  0.4529,  ..., -0.1668, -1.2140, -0.5774],\n",
      "        [ 0.4635,  0.0100,  0.4912,  ..., -0.2777,  1.2846, -0.2404],\n",
      "        ...,\n",
      "        [ 0.4075,  0.5908, -0.5305,  ...,  0.2156,  0.4163,  0.6773],\n",
      "        [-0.4537,  0.7421,  0.9626,  ...,  0.2887,  0.2537,  0.0634],\n",
      "        [ 0.8138, -1.1314, -0.0873,  ...,  0.8088, -0.0824, -0.2539]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6650, -0.5507, -1.0337,  ..., -0.7747,  0.0824,  0.0286],\n",
      "        [-0.9211, -0.3249,  0.1350,  ...,  0.3314, -1.4190,  0.8166],\n",
      "        [ 0.2097, -0.4607, -0.4002,  ..., -0.9366, -0.1995, -0.2764],\n",
      "        ...,\n",
      "        [-0.3657,  0.7765, -0.4240,  ...,  0.2135, -0.4693,  0.1394],\n",
      "        [ 0.9114,  0.2567,  0.9936,  ..., -0.5692,  0.0952, -0.0533],\n",
      "        [ 1.2122,  0.2904, -0.1790,  ..., -0.4104,  0.2396,  0.0470]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1866, 0.1603, 0.1574, 0.1721, 0.1699, 0.1634, 0.0367, 0.1774, 0.1445,\n",
      "        0.1676, 0.1862, 0.0950, 0.1704, 0.1347, 0.1666, 0.2157, 0.1587, 0.1520,\n",
      "        0.1533, 0.1541, 0.1377, 0.1250, 0.1654, 0.1861, 0.1735, 0.1577, 0.1561,\n",
      "        0.1683, 0.1607, 0.1532, 0.1634, 0.1475, 0.2044, 0.1575, 0.1645, 0.1809,\n",
      "        0.1669, 0.1625, 0.1898, 0.1467, 0.1531, 0.1477, 0.1590, 0.1543, 0.1701,\n",
      "        0.1670, 0.1686, 0.1900, 0.1508, 0.1823, 0.1591, 0.1694, 0.1681, 0.1887,\n",
      "        0.1660, 0.1698, 0.1583, 0.1653, 0.1545, 0.1402, 0.1505, 0.1490, 0.1673,\n",
      "        0.1663, 0.1880, 0.1843, 0.1479, 0.1451, 0.1801, 0.1563, 0.1711, 0.1635,\n",
      "        0.1841, 0.1765, 0.1296, 0.1595, 0.1711, 0.1753, 0.0698, 0.1673, 0.1743,\n",
      "        0.1566, 0.1889, 0.1944, 0.1629, 0.1734, 0.1548, 0.1712, 0.1639, 0.1464,\n",
      "        0.1466, 0.1461, 0.1717, 0.1865, 0.1956, 0.1659, 0.1611, 0.1575, 0.1591,\n",
      "        0.1608, 0.1619, 0.1609, 0.1511, 0.1679, 0.1778, 0.1502, 0.1701, 0.1599,\n",
      "        0.1329, 0.1519, 0.1485, 0.1553, 0.1652, 0.1501, 0.1516, 0.1606, 0.1308,\n",
      "        0.1492, 0.1746, 0.1919, 0.1708, 0.1813, 0.1396, 0.1644, 0.1557, 0.1459,\n",
      "        0.1467, 0.1909, 0.1596, 0.1456, 0.1339, 0.1539, 0.1475, 0.1642, 0.1590,\n",
      "        0.1723, 0.0077, 0.1786, 0.1603, 0.1870, 0.1537, 0.1637, 0.1585, 0.1651,\n",
      "        0.1805, 0.1813, 0.1511, 0.1435, 0.1554, 0.1599, 0.1648, 0.1977, 0.1620,\n",
      "        0.1335, 0.1545, 0.1572, 0.1404, 0.1660, 0.1597, 0.1584, 0.1781, 0.1395,\n",
      "        0.1560, 0.0451, 0.1559, 0.1678, 0.1794, 0.1818, 0.1486, 0.1661, 0.1687,\n",
      "        0.1431, 0.1510, 0.1641, 0.1705, 0.1563, 0.1807, 0.1503, 0.1748, 0.1711,\n",
      "        0.1294, 0.1434, 0.1600, 0.1670, 0.1620, 0.1585, 0.1427, 0.1654, 0.1965,\n",
      "        0.1470, 0.1815, 0.0926, 0.1613, 0.1905, 0.1416, 0.1553, 0.1902, 0.1828,\n",
      "        0.1693, 0.1372, 0.1538, 0.1517, 0.1586, 0.1475, 0.1762, 0.1631, 0.1681,\n",
      "        0.1333, 0.1662, 0.1429, 0.1712, 0.1524, 0.1558, 0.1658, 0.1661, 0.1738,\n",
      "        0.1873, 0.1702, 0.1588, 0.1837, 0.1451, 0.1595, 0.1713, 0.1513, 0.1733,\n",
      "        0.1670, 0.1919, 0.1766, 0.1455, 0.1711, 0.1263, 0.1755, 0.1359, 0.1431,\n",
      "        0.1573, 0.1475, 0.1877, 0.1606, 0.1821, 0.1649, 0.1688, 0.1503, 0.1686,\n",
      "        0.1477, 0.1358, 0.1755, 0.1614, 0.1591, 0.1553, 0.1723, 0.1673, 0.1618,\n",
      "        0.1574, 0.1738, 0.1473, 0.1669, 0.1675, 0.1548, 0.1719, 0.1835, 0.1803,\n",
      "        0.1444, 0.1504, 0.1756, 0.1796, 0.1834, 0.1712, 0.1703, 0.1529, 0.1355,\n",
      "        0.1559, 0.1620, 0.1643, 0.1551, 0.1620, 0.1664, 0.1700, 0.1874, 0.1431,\n",
      "        0.1682, 0.1396, 0.1711, 0.1553, 0.1673, 0.1555, 0.1805, 0.1593, 0.1702,\n",
      "        0.1867, 0.1340, 0.1853, 0.1569, 0.1659, 0.1559, 0.1459, 0.1796, 0.1694,\n",
      "        0.1475, 0.1447, 0.1529, 0.1588, 0.1682, 0.1590, 0.1698, 0.1608, 0.1726,\n",
      "        0.1662, 0.1394, 0.1576, 0.1486, 0.1491, 0.1846, 0.1416, 0.1527, 0.1823,\n",
      "        0.1741, 0.1742, 0.1794, 0.1510, 0.1616, 0.1589, 0.1632, 0.1612, 0.1916,\n",
      "        0.1496, 0.1756, 0.1777, 0.1584, 0.1652, 0.1497, 0.1407, 0.1730, 0.1577,\n",
      "        0.0941, 0.1545, 0.1851, 0.1705, 0.1896, 0.1437, 0.1821, 0.1444, 0.1846,\n",
      "        0.1530, 0.1383, 0.1840, 0.1623, 0.1662, 0.1880, 0.1721, 0.1626, 0.1473,\n",
      "        0.1728, 0.1569, 0.1616, 0.1601, 0.1723, 0.1496, 0.1400, 0.1671, 0.1613,\n",
      "        0.1693, 0.1741, 0.1479, 0.1531, 0.1593, 0.1585, 0.1409, 0.0488, 0.1822,\n",
      "        0.1934, 0.0991, 0.1728, 0.1852, 0.1460, 0.1254, 0.1648, 0.1443, 0.1376,\n",
      "        0.1559, 0.1464, 0.1770, 0.1648, 0.1681, 0.1760, 0.1432, 0.1759, 0.1812,\n",
      "        0.1536, 0.1744, 0.1475, 0.1818, 0.1364, 0.1867, 0.1738, 0.1871, 0.1473,\n",
      "        0.1663, 0.1668, 0.1581, 0.1757, 0.1412, 0.1507, 0.1667, 0.1535, 0.1534,\n",
      "        0.1668, 0.1588, 0.1708, 0.1574, 0.1733, 0.1626, 0.1583, 0.0726, 0.1676,\n",
      "        0.0690, 0.1565, 0.1732, 0.1770, 0.1418, 0.1524, 0.1868, 0.1584, 0.1786,\n",
      "        0.1545, 0.1640, 0.1518, 0.1785, 0.1829, 0.1507, 0.1520, 0.1600, 0.1917,\n",
      "        0.1348, 0.1615, 0.1459, 0.1575, 0.1438, 0.1499, 0.1751, 0.1579, 0.1711,\n",
      "        0.1613, 0.1618, 0.1605, 0.1634, 0.1535, 0.1293, 0.2191, 0.1440, 0.1696,\n",
      "        0.1474, 0.1605, 0.1543, 0.1678, 0.1645, 0.1437, 0.1782, 0.1773, 0.1874,\n",
      "        0.1408, 0.1709, 0.1822, 0.1403, 0.1625, 0.1468, 0.1770, 0.1550, 0.1345,\n",
      "        0.1720, 0.1969, 0.1542, 0.1635, 0.1502, 0.1457, 0.1946, 0.1760, 0.1564,\n",
      "        0.1535, 0.1696, 0.1620, 0.1402, 0.1561, 0.1553, 0.1647, 0.1497, 0.1596,\n",
      "        0.1616, 0.1665, 0.1551, 0.1832, 0.1766, 0.1509, 0.1660, 0.1827, 0.1641,\n",
      "        0.1766, 0.1729, 0.2137, 0.1760, 0.1676, 0.1681, 0.1745, 0.1571, 0.1769,\n",
      "        0.1815, 0.1599, 0.1531, 0.1429, 0.1496, 0.1530, 0.1410, 0.1370],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.6199e-01, -2.4788e-02,  7.9427e-02,  ...,  2.8843e-01,\n",
      "         -2.0320e-01, -4.8141e-05],\n",
      "        [ 9.3034e-02,  5.7421e-02,  3.1462e-01,  ...,  2.5522e-01,\n",
      "          5.1278e-02,  4.9030e-02],\n",
      "        [-4.4552e-01, -1.2324e-01, -4.0993e-02,  ..., -3.4341e-01,\n",
      "         -2.9817e-01, -1.6750e-01],\n",
      "        ...,\n",
      "        [-1.3493e-02, -2.2289e-01,  3.0347e-01,  ...,  7.2535e-02,\n",
      "          2.6183e-01, -1.6205e-01],\n",
      "        [-2.4894e-01, -2.9148e-01, -4.2892e-01,  ..., -6.2269e-02,\n",
      "         -1.9237e-01, -1.0971e-01],\n",
      "        [-1.9738e-01, -1.9546e-01,  2.7093e-01,  ..., -2.7708e-01,\n",
      "         -3.1998e-01, -1.7467e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.5490e-01,  6.3929e-01,  2.0389e+00,  ..., -6.3123e-01,\n",
      "         -6.9190e-01, -7.4458e-01],\n",
      "        [-8.9375e-01, -5.7365e-01, -1.0546e+00,  ..., -2.2328e-01,\n",
      "          4.7877e-01,  2.3748e-01],\n",
      "        [ 2.3359e-03,  3.5057e-01,  1.2767e+00,  ..., -2.1825e-01,\n",
      "          1.2230e+00, -5.7473e-01],\n",
      "        ...,\n",
      "        [-1.5117e+00,  3.1503e-01, -3.5046e-01,  ...,  7.1486e-02,\n",
      "         -1.5744e-01, -9.4374e-01],\n",
      "        [-1.0602e+00,  3.8536e-01,  4.8130e-01,  ..., -2.1769e-01,\n",
      "          5.9772e-01, -1.8067e+00],\n",
      "        [-4.4174e-01,  7.0441e-02, -2.1564e-01,  ..., -7.1597e-01,\n",
      "         -2.4843e+00,  6.4347e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5516, -0.4732,  0.2147,  ..., -0.3065,  0.1894, -0.6158],\n",
      "        [-0.2069,  0.8611, -0.3323,  ..., -0.3117, -0.9707, -0.5221],\n",
      "        [-0.1830,  0.1445, -0.0765,  ..., -1.2056,  0.0187, -0.0017],\n",
      "        ...,\n",
      "        [-0.7539,  0.9871,  0.5350,  ...,  0.1706, -0.2534, -0.4277],\n",
      "        [ 0.1725, -0.2457,  0.3422,  ..., -1.3792,  0.6631, -0.0642],\n",
      "        [ 0.0474, -0.2721, -0.0282,  ...,  0.9109,  0.9452,  0.2519]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1361, 0.1363, 0.1236, 0.1199, 0.1254, 0.1070, 0.0627, 0.1196, 0.1179,\n",
      "        0.1281, 0.1313, 0.0695, 0.1195, 0.1066, 0.1271, 0.1485, 0.1156, 0.1204,\n",
      "        0.1228, 0.1135, 0.1037, 0.1041, 0.1160, 0.1114, 0.1263, 0.1086, 0.1192,\n",
      "        0.1128, 0.1136, 0.1140, 0.1186, 0.1226, 0.2443, 0.1314, 0.1101, 0.1180,\n",
      "        0.1046, 0.1096, 0.1269, 0.1067, 0.1254, 0.1155, 0.1221, 0.1124, 0.1170,\n",
      "        0.1231, 0.1219, 0.1280, 0.1153, 0.1152, 0.1195, 0.1299, 0.1171, 0.1277,\n",
      "        0.1059, 0.1194, 0.1050, 0.1209, 0.1152, 0.1063, 0.1163, 0.1226, 0.1284,\n",
      "        0.1106, 0.1331, 0.1068, 0.1111, 0.1142, 0.1157, 0.1098, 0.1329, 0.0969,\n",
      "        0.1049, 0.1143, 0.1129, 0.1170, 0.1122, 0.1161, 0.0654, 0.1309, 0.1125,\n",
      "        0.1155, 0.1209, 0.1205, 0.1362, 0.1193, 0.1426, 0.1219, 0.1453, 0.1274,\n",
      "        0.1098, 0.1113, 0.1162, 0.1166, 0.1239, 0.1274, 0.1206, 0.1236, 0.1097,\n",
      "        0.1158, 0.1076, 0.1208, 0.1222, 0.1245, 0.1281, 0.1173, 0.1264, 0.1091,\n",
      "        0.1158, 0.1142, 0.1137, 0.1123, 0.1150, 0.1080, 0.1126, 0.1308, 0.1067,\n",
      "        0.1158, 0.1101, 0.1070, 0.1105, 0.1223, 0.1264, 0.1193, 0.1153, 0.1211,\n",
      "        0.1155, 0.1295, 0.1188, 0.1311, 0.1237, 0.1204, 0.1152, 0.1123, 0.1186,\n",
      "        0.1167, 0.5971, 0.1360, 0.1122, 0.1183, 0.1171, 0.1212, 0.1088, 0.1233,\n",
      "        0.1163, 0.1291, 0.1144, 0.1076, 0.1132, 0.1277, 0.1117, 0.1193, 0.1166,\n",
      "        0.1106, 0.1146, 0.1173, 0.1138, 0.0982, 0.1321, 0.1237, 0.1398, 0.1338,\n",
      "        0.1178, 0.0342, 0.1114, 0.1221, 0.1125, 0.1230, 0.1103, 0.1249, 0.1265,\n",
      "        0.1090, 0.1297, 0.0997, 0.1288, 0.1093, 0.1134, 0.1175, 0.1204, 0.1250,\n",
      "        0.1136, 0.1172, 0.1124, 0.1041, 0.1118, 0.1199, 0.0966, 0.1231, 0.1135,\n",
      "        0.1035, 0.1182, 0.0892, 0.1054, 0.1221, 0.1199, 0.1126, 0.1194, 0.1153,\n",
      "        0.1179, 0.1134, 0.1138, 0.1195, 0.1151, 0.1269, 0.1371, 0.1303, 0.1130,\n",
      "        0.1129, 0.1269, 0.1099, 0.1176, 0.1106, 0.1232, 0.1128, 0.1309, 0.1150,\n",
      "        0.1273, 0.1371, 0.1191, 0.1232, 0.1051, 0.1117, 0.1132, 0.1193, 0.1110,\n",
      "        0.1141, 0.1086, 0.1156, 0.1136, 0.1232, 0.1122, 0.1044, 0.1085, 0.0983,\n",
      "        0.1131, 0.1150, 0.1225, 0.1330, 0.1105, 0.1017, 0.1231, 0.1162, 0.1150,\n",
      "        0.1060, 0.1229, 0.1297, 0.1095, 0.1141, 0.1307, 0.1333, 0.1322, 0.1139,\n",
      "        0.1122, 0.1175, 0.1017, 0.1150, 0.1161, 0.1111, 0.1149, 0.1165, 0.1121,\n",
      "        0.1057, 0.1328, 0.1203, 0.1145, 0.1265, 0.1217, 0.1146, 0.1303, 0.1131,\n",
      "        0.1174, 0.1152, 0.1127, 0.1135, 0.1090, 0.1253, 0.1099, 0.1257, 0.1033,\n",
      "        0.1270, 0.1041, 0.1181, 0.1107, 0.1168, 0.1225, 0.1142, 0.1081, 0.1112,\n",
      "        0.1189, 0.1213, 0.1236, 0.1164, 0.1107, 0.1193, 0.1193, 0.1080, 0.1230,\n",
      "        0.1206, 0.1126, 0.1160, 0.1164, 0.1213, 0.1257, 0.1266, 0.1101, 0.1280,\n",
      "        0.1163, 0.1182, 0.1184, 0.1153, 0.1105, 0.1288, 0.0955, 0.1209, 0.1295,\n",
      "        0.1141, 0.1001, 0.1440, 0.1320, 0.1273, 0.1202, 0.1147, 0.1065, 0.1138,\n",
      "        0.1370, 0.1122, 0.1264, 0.1101, 0.1275, 0.1215, 0.1126, 0.1178, 0.1163,\n",
      "        0.0981, 0.1124, 0.1124, 0.1159, 0.1296, 0.1042, 0.1531, 0.1123, 0.1326,\n",
      "        0.1199, 0.1212, 0.1117, 0.1088, 0.1258, 0.1145, 0.1207, 0.1134, 0.1115,\n",
      "        0.1170, 0.1247, 0.1073, 0.1215, 0.1226, 0.1204, 0.1145, 0.1170, 0.1330,\n",
      "        0.1209, 0.1269, 0.1150, 0.1141, 0.1183, 0.1127, 0.1160, 0.0424, 0.1128,\n",
      "        0.1460, 0.1634, 0.1210, 0.1085, 0.1152, 0.1156, 0.1207, 0.1093, 0.1073,\n",
      "        0.1148, 0.1133, 0.1135, 0.1018, 0.1064, 0.1213, 0.1156, 0.1246, 0.1227,\n",
      "        0.1532, 0.1232, 0.1238, 0.1306, 0.1222, 0.1080, 0.1145, 0.1206, 0.1132,\n",
      "        0.1259, 0.1160, 0.1061, 0.1015, 0.1109, 0.1183, 0.1161, 0.1245, 0.1159,\n",
      "        0.1217, 0.1182, 0.1167, 0.1188, 0.1268, 0.1382, 0.1141, 0.0620, 0.1127,\n",
      "        0.0625, 0.1187, 0.1114, 0.1249, 0.1219, 0.1159, 0.1254, 0.1338, 0.1078,\n",
      "        0.1083, 0.1184, 0.1199, 0.1251, 0.1110, 0.1144, 0.1107, 0.1235, 0.1236,\n",
      "        0.1105, 0.1122, 0.1284, 0.1121, 0.1225, 0.1121, 0.1234, 0.1080, 0.1201,\n",
      "        0.1182, 0.1205, 0.1141, 0.1177, 0.1302, 0.1078, 0.1458, 0.1151, 0.1317,\n",
      "        0.1009, 0.1023, 0.1160, 0.1233, 0.1135, 0.1131, 0.1275, 0.1278, 0.1279,\n",
      "        0.1150, 0.1125, 0.1210, 0.1027, 0.1226, 0.0970, 0.1210, 0.1092, 0.1271,\n",
      "        0.1133, 0.1428, 0.1076, 0.1163, 0.1220, 0.1189, 0.1239, 0.1231, 0.1256,\n",
      "        0.1277, 0.1220, 0.1095, 0.1203, 0.1276, 0.1144, 0.1201, 0.1196, 0.1175,\n",
      "        0.1350, 0.1206, 0.1167, 0.1200, 0.1266, 0.1184, 0.1133, 0.1264, 0.1172,\n",
      "        0.1289, 0.1162, 0.1212, 0.1212, 0.1123, 0.1109, 0.1476, 0.1111, 0.1149,\n",
      "        0.1449, 0.1207, 0.1169, 0.1503, 0.1028, 0.0959, 0.1183, 0.3636],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0147,  0.0063, -0.0076,  ...,  0.0221, -0.0301,  0.0536],\n",
      "        [ 0.0235,  0.0435,  0.0039,  ..., -0.0066,  0.0100, -0.0131],\n",
      "        [ 0.0355,  0.0218, -0.0073,  ..., -0.0163, -0.0500, -0.0168],\n",
      "        ...,\n",
      "        [-0.0111, -0.0088,  0.0152,  ..., -0.0160,  0.0179,  0.0248],\n",
      "        [ 0.0058,  0.0118,  0.0118,  ...,  0.0123,  0.0319,  0.0142],\n",
      "        [ 0.0145,  0.0351, -0.0017,  ...,  0.0090, -0.0382,  0.0309]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0060,  0.1042,  0.2290,  ..., -0.1013,  0.2601,  0.0531],\n",
      "        [-0.1423, -0.0520, -0.1110,  ...,  0.0932,  0.0061, -0.1830],\n",
      "        [ 0.3170,  0.2963, -0.0757,  ..., -0.0896,  0.1259,  0.3659],\n",
      "        ...,\n",
      "        [ 0.2676,  0.0602, -0.0082,  ...,  0.1001,  0.0848, -0.0525],\n",
      "        [-0.1272, -0.0155, -0.2829,  ..., -0.1382,  0.1047,  0.0834],\n",
      "        [ 0.1582, -0.0777, -0.1367,  ...,  0.0633, -0.2769,  0.3326]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.8286, -0.2260,  0.7398,  ...,  0.8707, -0.9612,  0.6046],\n",
      "        [-0.5551, -0.5316, -1.1855,  ..., -1.0516, -0.7987, -0.9745],\n",
      "        [ 1.1342, -0.5737,  0.8622,  ..., -0.3761, -0.0087,  1.2091],\n",
      "        ...,\n",
      "        [ 0.0077,  0.0348, -1.0372,  ...,  0.6166,  0.0098,  1.4793],\n",
      "        [ 0.9325, -0.2971, -1.8002,  ...,  0.4762, -0.9235,  0.7087],\n",
      "        [ 0.9640,  0.6282,  0.0745,  ...,  0.3297,  1.2280,  0.3207]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.7593,  0.5410, -0.7859,  ..., -0.0351, -0.5292,  0.2911],\n",
      "        [ 0.2381, -0.7668,  0.5307,  ..., -2.3040, -1.7652, -2.1704],\n",
      "        [ 0.5989,  1.2186, -0.2126,  ...,  1.1823, -0.3383,  0.7158],\n",
      "        ...,\n",
      "        [ 1.1379, -1.1632,  0.1885,  ..., -0.8280, -0.0322, -0.3071],\n",
      "        [ 0.2789,  0.3011, -0.9351,  ...,  0.0045, -0.6360,  1.3496],\n",
      "        [ 0.5196, -0.0714,  0.2448,  ...,  0.4923, -0.9994, -0.3376]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2206, 0.1845, 0.1888, 0.1662, 0.1895, 0.1967, 0.0197, 0.1959, 0.1980,\n",
      "        0.1889, 0.2053, 0.1016, 0.2066, 0.1822, 0.1850, 0.2302, 0.1984, 0.2051,\n",
      "        0.1851, 0.1869, 0.1832, 0.1538, 0.1996, 0.2067, 0.1851, 0.1917, 0.1770,\n",
      "        0.1965, 0.1948, 0.1887, 0.1782, 0.1520, 0.1822, 0.1890, 0.1889, 0.2049,\n",
      "        0.1841, 0.1909, 0.2093, 0.1816, 0.1997, 0.1803, 0.1996, 0.1745, 0.1895,\n",
      "        0.1858, 0.1862, 0.2018, 0.1883, 0.1943, 0.1987, 0.1919, 0.2034, 0.2321,\n",
      "        0.1889, 0.2036, 0.1771, 0.1849, 0.1860, 0.1601, 0.1951, 0.1999, 0.2135,\n",
      "        0.1813, 0.1684, 0.2131, 0.1950, 0.1697, 0.1533, 0.1817, 0.1783, 0.1646,\n",
      "        0.1999, 0.1944, 0.1565, 0.1939, 0.2111, 0.1985, 0.0302, 0.2068, 0.2231,\n",
      "        0.1844, 0.1994, 0.2418, 0.2037, 0.2025, 0.1871, 0.2158, 0.2256, 0.1575,\n",
      "        0.1605, 0.1951, 0.1958, 0.2029, 0.2062, 0.2065, 0.1957, 0.1928, 0.1744,\n",
      "        0.2142, 0.1984, 0.1939, 0.1700, 0.1926, 0.2157, 0.1913, 0.2109, 0.1789,\n",
      "        0.1844, 0.1800, 0.1770, 0.1874, 0.1631, 0.1858, 0.1870, 0.1964, 0.1688,\n",
      "        0.1971, 0.2016, 0.2098, 0.1900, 0.2124, 0.1869, 0.1865, 0.1753, 0.1769,\n",
      "        0.1903, 0.2417, 0.1982, 0.2017, 0.1729, 0.1639, 0.1761, 0.1832, 0.1961,\n",
      "        0.2047, 0.0293, 0.2227, 0.2040, 0.2019, 0.1927, 0.2206, 0.1932, 0.1848,\n",
      "        0.2217, 0.2129, 0.1830, 0.1841, 0.2089, 0.1751, 0.1714, 0.2537, 0.2179,\n",
      "        0.1560, 0.1960, 0.1886, 0.1592, 0.2002, 0.1815, 0.1776, 0.1967, 0.1628,\n",
      "        0.1805, 0.0631, 0.1632, 0.1790, 0.2002, 0.2303, 0.1587, 0.2001, 0.2209,\n",
      "        0.1927, 0.1969, 0.1847, 0.1856, 0.1846, 0.1904, 0.1842, 0.2192, 0.1716,\n",
      "        0.1624, 0.1959, 0.1931, 0.2014, 0.1942, 0.1956, 0.1215, 0.1995, 0.2007,\n",
      "        0.1762, 0.2066, 0.0739, 0.1862, 0.2107, 0.1670, 0.2008, 0.1998, 0.2073,\n",
      "        0.2167, 0.1600, 0.1925, 0.1839, 0.2091, 0.1921, 0.1970, 0.1917, 0.2052,\n",
      "        0.1817, 0.1924, 0.1667, 0.2125, 0.1955, 0.1864, 0.1863, 0.2051, 0.1695,\n",
      "        0.2132, 0.1988, 0.2030, 0.1973, 0.1699, 0.1843, 0.1970, 0.1930, 0.2364,\n",
      "        0.1942, 0.2058, 0.1952, 0.1977, 0.1858, 0.1778, 0.1884, 0.1720, 0.1769,\n",
      "        0.1782, 0.1755, 0.2065, 0.1816, 0.1705, 0.1840, 0.2058, 0.1977, 0.2106,\n",
      "        0.1803, 0.1672, 0.1791, 0.1819, 0.1916, 0.1899, 0.1779, 0.1901, 0.1744,\n",
      "        0.1773, 0.2031, 0.1684, 0.2067, 0.1837, 0.1919, 0.1869, 0.2095, 0.2131,\n",
      "        0.1865, 0.1863, 0.1780, 0.1950, 0.2138, 0.2087, 0.1904, 0.2084, 0.1796,\n",
      "        0.1885, 0.1965, 0.2083, 0.1723, 0.1846, 0.2015, 0.1972, 0.2002, 0.1628,\n",
      "        0.1922, 0.1800, 0.2060, 0.1833, 0.1577, 0.1986, 0.2119, 0.1796, 0.1878,\n",
      "        0.2019, 0.1654, 0.1772, 0.1983, 0.1774, 0.2020, 0.1900, 0.2141, 0.2029,\n",
      "        0.1875, 0.1959, 0.1879, 0.2031, 0.1989, 0.1971, 0.1989, 0.1830, 0.1777,\n",
      "        0.1910, 0.1867, 0.2087, 0.1686, 0.1888, 0.2177, 0.1771, 0.1907, 0.2071,\n",
      "        0.2151, 0.1957, 0.2213, 0.1700, 0.2155, 0.1828, 0.1941, 0.1846, 0.2064,\n",
      "        0.2073, 0.2014, 0.1985, 0.2098, 0.1853, 0.1928, 0.1646, 0.2060, 0.2029,\n",
      "        0.1283, 0.1698, 0.2142, 0.2315, 0.2114, 0.1716, 0.1967, 0.1896, 0.2083,\n",
      "        0.2079, 0.1853, 0.2074, 0.1832, 0.2173, 0.2050, 0.1749, 0.1777, 0.2029,\n",
      "        0.2094, 0.1958, 0.1811, 0.1995, 0.2013, 0.1897, 0.2015, 0.1890, 0.1754,\n",
      "        0.2090, 0.1978, 0.1846, 0.1661, 0.1896, 0.1912, 0.1898, 0.0486, 0.2426,\n",
      "        0.1927, 0.0203, 0.2161, 0.2084, 0.1877, 0.1868, 0.1813, 0.1863, 0.1590,\n",
      "        0.1691, 0.1790, 0.1982, 0.1834, 0.1977, 0.2140, 0.1695, 0.1873, 0.2174,\n",
      "        0.1880, 0.2177, 0.1757, 0.2052, 0.2018, 0.2111, 0.1928, 0.1975, 0.1792,\n",
      "        0.1652, 0.1897, 0.1822, 0.1915, 0.1711, 0.1938, 0.1745, 0.1857, 0.1732,\n",
      "        0.2002, 0.1684, 0.2035, 0.1789, 0.1910, 0.1953, 0.1978, 0.0750, 0.1960,\n",
      "        0.0884, 0.1769, 0.1818, 0.2144, 0.1959, 0.1953, 0.2068, 0.1186, 0.2115,\n",
      "        0.1938, 0.2122, 0.1843, 0.1883, 0.1910, 0.1760, 0.1872, 0.1884, 0.2004,\n",
      "        0.1397, 0.1915, 0.1528, 0.1823, 0.1551, 0.1999, 0.1818, 0.1667, 0.1935,\n",
      "        0.1825, 0.2128, 0.1561, 0.1935, 0.2028, 0.1750, 0.2320, 0.1942, 0.2171,\n",
      "        0.1921, 0.1677, 0.2194, 0.2009, 0.1569, 0.1699, 0.2199, 0.2187, 0.2266,\n",
      "        0.1782, 0.1861, 0.2145, 0.1658, 0.2123, 0.1674, 0.1904, 0.1928, 0.2009,\n",
      "        0.2004, 0.2270, 0.1964, 0.1699, 0.1960, 0.1889, 0.2105, 0.1953, 0.1959,\n",
      "        0.1984, 0.2046, 0.1784, 0.1843, 0.2011, 0.1654, 0.1880, 0.1919, 0.1651,\n",
      "        0.1809, 0.1978, 0.1758, 0.1944, 0.1996, 0.1927, 0.1813, 0.2128, 0.1847,\n",
      "        0.2024, 0.1758, 0.2011, 0.1861, 0.2474, 0.1871, 0.2177, 0.1949, 0.1951,\n",
      "        0.1970, 0.1920, 0.1769, 0.1866, 0.1855, 0.1709, 0.1890, 0.0986],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1628, -0.0435,  0.0914,  ..., -0.0063,  0.0470,  0.0556],\n",
      "        [-0.1628, -0.1897,  0.4286,  ...,  0.2982,  0.2783,  0.3390],\n",
      "        [ 0.0901,  0.0050, -0.0741,  ...,  0.0509, -0.2185, -0.0814],\n",
      "        ...,\n",
      "        [-0.0645,  0.0008, -0.1848,  ...,  0.0534, -0.1842,  0.1009],\n",
      "        [-0.0216, -0.2725, -0.3324,  ..., -0.1613,  0.3264,  0.1481],\n",
      "        [ 0.0346, -0.1411,  0.7676,  ..., -0.1862,  0.6459,  0.1197]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5348,  0.8691,  1.3655,  ...,  1.4805, -0.7715, -0.4708],\n",
      "        [-0.4483,  0.1682, -1.3039,  ...,  1.7823,  0.1698, -0.7226],\n",
      "        [-0.1712,  0.4110, -0.6648,  ...,  0.9514,  2.0118,  0.3380],\n",
      "        ...,\n",
      "        [ 0.5689,  1.6558,  1.5241,  ..., -1.5816, -0.0644, -0.4433],\n",
      "        [-0.1465, -0.6735, -1.0231,  ...,  0.5419,  0.7072,  0.8888],\n",
      "        [ 0.8561,  0.3223, -0.2734,  ..., -2.1130,  2.5741, -0.6919]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.9458, -0.0664,  0.7316,  ..., -0.2246,  0.8065,  0.8882],\n",
      "        [-0.5407, -0.2127, -1.6067,  ..., -1.2230,  0.2508, -0.2187],\n",
      "        [-0.5732,  0.1746,  0.2714,  ...,  0.0113,  0.0138, -0.5526],\n",
      "        ...,\n",
      "        [ 1.4575,  0.3723,  0.5974,  ...,  0.1328, -0.6351,  0.4659],\n",
      "        [ 0.3102,  0.0495, -0.9048,  ...,  0.2718,  0.8283, -0.4208],\n",
      "        [-0.2171, -0.0866, -0.0916,  ...,  0.5481, -0.3989, -0.2292]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1163, 0.1061, 0.1072, 0.0860, 0.1208, 0.1067, 0.0727, 0.1137, 0.1122,\n",
      "        0.1019, 0.1037, 0.0760, 0.1163, 0.0968, 0.1102, 0.1489, 0.1107, 0.1105,\n",
      "        0.1159, 0.1134, 0.1068, 0.1179, 0.1058, 0.0897, 0.1387, 0.1277, 0.1149,\n",
      "        0.1156, 0.1038, 0.1030, 0.1056, 0.1013, 0.3542, 0.1173, 0.1063, 0.1198,\n",
      "        0.1014, 0.1240, 0.1142, 0.1061, 0.1214, 0.1087, 0.1181, 0.1152, 0.0992,\n",
      "        0.1137, 0.1117, 0.1121, 0.1004, 0.1064, 0.1129, 0.1242, 0.1089, 0.1043,\n",
      "        0.0986, 0.1032, 0.1103, 0.1110, 0.1157, 0.1139, 0.1079, 0.1016, 0.1228,\n",
      "        0.0927, 0.1144, 0.0981, 0.1244, 0.1063, 0.1089, 0.1215, 0.1087, 0.1015,\n",
      "        0.1109, 0.0923, 0.1150, 0.1147, 0.1168, 0.1124, 0.0810, 0.1136, 0.0969,\n",
      "        0.1281, 0.1068, 0.1142, 0.1141, 0.1047, 0.1165, 0.1068, 0.1362, 0.1122,\n",
      "        0.0966, 0.0892, 0.1089, 0.0965, 0.1058, 0.1291, 0.1134, 0.0941, 0.1055,\n",
      "        0.1087, 0.1112, 0.1135, 0.1143, 0.1056, 0.1236, 0.1084, 0.1117, 0.1133,\n",
      "        0.1153, 0.1023, 0.1100, 0.0977, 0.1135, 0.1084, 0.1050, 0.1032, 0.1095,\n",
      "        0.1039, 0.1073, 0.1024, 0.1069, 0.1258, 0.1170, 0.0933, 0.1146, 0.1006,\n",
      "        0.1076, 0.1170, 0.1091, 0.1303, 0.1056, 0.0979, 0.0989, 0.1180, 0.1071,\n",
      "        0.1082, 0.9791, 0.1218, 0.1139, 0.0973, 0.1175, 0.1134, 0.0995, 0.1044,\n",
      "        0.1078, 0.1229, 0.1083, 0.0987, 0.1031, 0.1112, 0.1073, 0.1178, 0.1025,\n",
      "        0.0979, 0.1166, 0.1031, 0.1088, 0.0969, 0.1185, 0.1101, 0.1288, 0.1088,\n",
      "        0.1014, 0.0598, 0.1052, 0.1070, 0.0992, 0.1095, 0.1063, 0.1148, 0.1066,\n",
      "        0.1029, 0.1202, 0.1061, 0.1232, 0.1042, 0.1163, 0.1081, 0.1141, 0.1138,\n",
      "        0.1139, 0.1357, 0.1150, 0.1144, 0.1235, 0.1307, 0.0753, 0.1246, 0.0984,\n",
      "        0.1063, 0.1203, 0.0601, 0.1144, 0.1135, 0.1083, 0.1051, 0.1157, 0.1036,\n",
      "        0.1333, 0.1170, 0.1114, 0.1035, 0.1129, 0.1148, 0.1248, 0.1109, 0.1226,\n",
      "        0.1076, 0.0908, 0.1018, 0.0998, 0.1055, 0.1157, 0.1053, 0.1216, 0.0944,\n",
      "        0.1350, 0.1303, 0.1320, 0.0982, 0.1139, 0.1043, 0.1044, 0.1139, 0.1131,\n",
      "        0.1028, 0.0986, 0.1142, 0.1103, 0.1152, 0.1102, 0.1120, 0.1060, 0.0983,\n",
      "        0.0971, 0.0985, 0.1175, 0.1042, 0.1076, 0.1005, 0.1263, 0.1034, 0.1074,\n",
      "        0.1187, 0.1067, 0.1127, 0.0939, 0.1060, 0.1290, 0.1079, 0.1357, 0.1025,\n",
      "        0.1137, 0.0974, 0.1094, 0.1106, 0.1181, 0.1036, 0.1103, 0.0918, 0.1058,\n",
      "        0.1148, 0.1174, 0.1128, 0.1009, 0.0980, 0.1076, 0.1112, 0.1279, 0.1057,\n",
      "        0.1003, 0.1181, 0.1130, 0.1069, 0.1018, 0.1132, 0.1139, 0.1100, 0.1017,\n",
      "        0.1170, 0.1117, 0.1249, 0.1171, 0.1140, 0.1105, 0.1123, 0.1004, 0.1060,\n",
      "        0.1202, 0.1073, 0.1039, 0.1088, 0.1099, 0.1070, 0.1146, 0.1406, 0.1191,\n",
      "        0.1079, 0.1105, 0.0937, 0.1071, 0.0913, 0.1010, 0.0981, 0.1178, 0.1201,\n",
      "        0.1067, 0.0983, 0.1312, 0.0939, 0.1030, 0.1083, 0.1122, 0.1031, 0.1100,\n",
      "        0.1023, 0.0917, 0.1269, 0.1157, 0.1128, 0.1043, 0.0987, 0.1016, 0.1116,\n",
      "        0.1047, 0.1078, 0.1083, 0.1035, 0.1131, 0.1218, 0.1067, 0.1039, 0.1015,\n",
      "        0.0977, 0.1079, 0.1170, 0.1257, 0.1215, 0.1049, 0.1600, 0.1165, 0.1084,\n",
      "        0.1132, 0.1139, 0.1000, 0.0927, 0.1110, 0.1173, 0.0949, 0.1167, 0.1109,\n",
      "        0.1045, 0.1266, 0.1110, 0.1101, 0.1150, 0.0997, 0.1053, 0.0942, 0.1550,\n",
      "        0.1170, 0.1203, 0.0924, 0.1056, 0.1185, 0.1102, 0.1137, 0.0576, 0.1147,\n",
      "        0.1239, 0.1868, 0.1114, 0.1073, 0.0991, 0.1010, 0.1049, 0.0984, 0.1110,\n",
      "        0.1003, 0.1019, 0.1018, 0.0960, 0.1155, 0.0975, 0.1121, 0.1218, 0.1001,\n",
      "        0.1153, 0.1210, 0.1108, 0.1152, 0.1113, 0.1127, 0.1111, 0.1098, 0.1190,\n",
      "        0.1045, 0.1024, 0.1108, 0.0987, 0.1196, 0.1148, 0.1091, 0.1009, 0.1013,\n",
      "        0.1037, 0.1093, 0.1106, 0.1206, 0.1163, 0.1052, 0.1080, 0.1042, 0.1084,\n",
      "        0.0655, 0.1026, 0.1098, 0.1044, 0.1170, 0.1059, 0.1114, 0.0956, 0.1170,\n",
      "        0.1156, 0.1222, 0.1102, 0.1168, 0.1046, 0.1262, 0.1128, 0.1096, 0.0982,\n",
      "        0.1018, 0.1092, 0.1076, 0.1213, 0.0991, 0.1241, 0.1183, 0.1133, 0.1114,\n",
      "        0.1053, 0.1177, 0.0845, 0.1011, 0.0997, 0.1123, 0.1303, 0.1016, 0.1128,\n",
      "        0.1031, 0.1062, 0.1161, 0.1112, 0.0970, 0.1131, 0.1144, 0.1257, 0.1536,\n",
      "        0.1085, 0.1035, 0.1184, 0.1099, 0.1245, 0.1116, 0.1104, 0.1069, 0.1242,\n",
      "        0.1115, 0.1088, 0.1069, 0.1026, 0.1221, 0.1115, 0.1099, 0.1082, 0.1225,\n",
      "        0.1135, 0.1206, 0.0982, 0.0941, 0.1022, 0.1050, 0.1358, 0.1075, 0.0915,\n",
      "        0.1118, 0.1041, 0.0954, 0.1064, 0.1069, 0.1173, 0.1093, 0.1303, 0.0977,\n",
      "        0.1051, 0.1059, 0.0946, 0.1076, 0.1175, 0.1191, 0.1288, 0.1143, 0.1062,\n",
      "        0.1071, 0.1230, 0.1167, 0.0892, 0.1176, 0.1121, 0.1034, 0.2590],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1632, 0.1333, 0.1948, 0.0930, 0.1554, 0.1498, 0.0011, 0.1465, 0.1978,\n",
      "        0.1238, 0.1046, 0.0750, 0.1728, 0.1531, 0.1861, 0.1950, 0.1677, 0.1890,\n",
      "        0.1131, 0.1715, 0.1593, 0.1490, 0.1614, 0.0966, 0.2009, 0.2240, 0.1955,\n",
      "        0.1681, 0.1361, 0.1638, 0.1631, 0.1037, 0.2445, 0.1737, 0.1695, 0.1689,\n",
      "        0.1431, 0.1728, 0.1621, 0.1790, 0.1905, 0.1347, 0.1794, 0.1631, 0.1416,\n",
      "        0.1252, 0.1495, 0.1188, 0.1889, 0.1521, 0.1837, 0.1436, 0.1725, 0.1682,\n",
      "        0.1544, 0.1349, 0.1629, 0.1508, 0.1481, 0.1536, 0.1382, 0.1709, 0.1549,\n",
      "        0.1289, 0.1057, 0.1314, 0.1972, 0.1304, 0.1237, 0.1977, 0.1183, 0.1703,\n",
      "        0.1673, 0.1718, 0.1594, 0.1859, 0.1953, 0.1355, 0.0312, 0.1825, 0.1358,\n",
      "        0.1597, 0.1745, 0.1631, 0.1556, 0.1462, 0.1593, 0.1381, 0.1990, 0.1482,\n",
      "        0.1594, 0.1662, 0.1848, 0.1604, 0.1510, 0.1969, 0.1483, 0.1325, 0.1606,\n",
      "        0.1975, 0.1768, 0.1570, 0.1574, 0.1504, 0.1750, 0.1569, 0.1441, 0.1581,\n",
      "        0.1906, 0.1381, 0.1835, 0.1546, 0.0864, 0.1996, 0.1797, 0.1730, 0.2071,\n",
      "        0.1760, 0.1744, 0.1438, 0.1533, 0.1789, 0.1642, 0.1545, 0.1668, 0.1440,\n",
      "        0.1558, 0.2056, 0.2004, 0.2113, 0.1249, 0.1525, 0.1451, 0.1743, 0.1880,\n",
      "        0.1536, 0.0033, 0.1921, 0.2043, 0.1446, 0.1811, 0.1379, 0.1560, 0.1352,\n",
      "        0.1489, 0.1279, 0.1657, 0.1600, 0.1702, 0.1602, 0.1263, 0.1436, 0.1717,\n",
      "        0.0922, 0.1745, 0.1543, 0.1488, 0.1419, 0.1555, 0.1836, 0.2058, 0.1412,\n",
      "        0.1410, 0.0061, 0.1516, 0.1769, 0.1766, 0.1416, 0.1470, 0.1504, 0.1392,\n",
      "        0.1655, 0.1244, 0.1882, 0.1495, 0.1714, 0.1713, 0.1774, 0.1770, 0.1684,\n",
      "        0.1765, 0.1965, 0.1540, 0.1887, 0.1715, 0.1560, 0.0848, 0.1691, 0.1574,\n",
      "        0.1697, 0.1767, 0.0575, 0.1557, 0.1505, 0.1283, 0.1893, 0.1652, 0.1385,\n",
      "        0.1495, 0.1402, 0.1870, 0.1911, 0.1684, 0.1814, 0.1902, 0.1559, 0.1800,\n",
      "        0.1790, 0.1373, 0.1108, 0.1339, 0.1690, 0.1497, 0.1905, 0.1398, 0.1541,\n",
      "        0.1726, 0.1590, 0.2002, 0.1503, 0.1926, 0.1732, 0.1389, 0.1634, 0.2161,\n",
      "        0.1901, 0.1393, 0.1780, 0.1830, 0.1699, 0.1794, 0.1718, 0.1750, 0.1529,\n",
      "        0.1040, 0.1509, 0.1685, 0.1446, 0.1392, 0.1610, 0.1799, 0.1489, 0.1588,\n",
      "        0.1673, 0.1587, 0.1448, 0.1541, 0.1985, 0.1633, 0.1693, 0.1191, 0.1290,\n",
      "        0.1628, 0.1515, 0.1772, 0.1701, 0.1931, 0.1541, 0.1427, 0.1529, 0.1532,\n",
      "        0.1761, 0.1934, 0.1507, 0.1544, 0.1405, 0.1410, 0.1462, 0.2325, 0.1796,\n",
      "        0.1448, 0.1753, 0.1609, 0.1945, 0.1249, 0.1537, 0.1902, 0.1495, 0.1499,\n",
      "        0.1598, 0.1722, 0.2004, 0.1819, 0.1503, 0.1385, 0.1530, 0.1375, 0.1741,\n",
      "        0.1528, 0.1529, 0.1505, 0.1926, 0.1349, 0.1862, 0.1514, 0.1704, 0.1362,\n",
      "        0.1891, 0.1447, 0.1548, 0.1133, 0.1309, 0.1499, 0.1402, 0.1596, 0.1796,\n",
      "        0.1538, 0.1623, 0.2006, 0.1444, 0.1691, 0.1923, 0.1654, 0.1347, 0.1652,\n",
      "        0.1284, 0.1305, 0.1526, 0.1655, 0.1675, 0.1341, 0.1656, 0.1616, 0.1470,\n",
      "        0.1438, 0.1940, 0.1496, 0.1739, 0.1688, 0.1488, 0.1543, 0.1521, 0.1512,\n",
      "        0.1016, 0.1480, 0.1615, 0.1666, 0.1840, 0.1485, 0.1204, 0.1407, 0.1548,\n",
      "        0.1521, 0.1645, 0.1534, 0.1487, 0.1625, 0.1960, 0.1542, 0.1482, 0.1561,\n",
      "        0.1708, 0.1668, 0.1682, 0.1788, 0.1636, 0.1464, 0.1643, 0.1441, 0.1921,\n",
      "        0.1694, 0.1700, 0.1380, 0.1914, 0.2372, 0.1394, 0.1990, 0.0113, 0.1741,\n",
      "        0.1284, 0.0084, 0.1727, 0.1423, 0.1192, 0.1830, 0.1452, 0.1386, 0.1677,\n",
      "        0.1434, 0.1581, 0.1732, 0.1225, 0.1622, 0.1647, 0.1910, 0.1168, 0.1168,\n",
      "        0.1365, 0.1854, 0.1879, 0.1647, 0.1787, 0.1707, 0.1608, 0.1597, 0.1498,\n",
      "        0.1613, 0.1443, 0.1560, 0.1412, 0.1765, 0.1870, 0.1413, 0.1399, 0.1507,\n",
      "        0.1567, 0.1540, 0.1488, 0.1524, 0.1617, 0.1586, 0.1599, 0.0307, 0.1639,\n",
      "        0.0647, 0.1496, 0.1529, 0.1681, 0.1826, 0.1634, 0.1646, 0.0726, 0.1767,\n",
      "        0.1894, 0.2188, 0.1540, 0.1613, 0.1742, 0.1860, 0.1585, 0.1454, 0.1127,\n",
      "        0.1497, 0.1514, 0.1182, 0.1498, 0.1457, 0.1654, 0.1748, 0.1972, 0.1700,\n",
      "        0.1562, 0.1916, 0.0885, 0.1571, 0.1544, 0.2120, 0.0872, 0.1781, 0.1841,\n",
      "        0.1932, 0.1608, 0.1745, 0.1791, 0.1388, 0.1633, 0.1551, 0.1465, 0.2130,\n",
      "        0.1485, 0.1529, 0.1863, 0.1652, 0.1938, 0.1619, 0.1388, 0.1680, 0.2294,\n",
      "        0.1588, 0.1410, 0.1615, 0.1376, 0.2027, 0.1781, 0.1629, 0.1159, 0.1550,\n",
      "        0.1546, 0.2323, 0.1250, 0.1271, 0.1527, 0.1069, 0.1796, 0.1830, 0.0992,\n",
      "        0.1346, 0.1542, 0.1403, 0.1523, 0.1231, 0.1724, 0.1456, 0.1083, 0.1400,\n",
      "        0.1669, 0.1475, 0.0861, 0.1515, 0.1636, 0.1498, 0.1616, 0.1895, 0.1602,\n",
      "        0.1565, 0.1802, 0.1420, 0.0895, 0.1877, 0.1640, 0.1741, 0.1472],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0316,  0.0092, -0.0255,  ...,  0.0726, -0.0743,  0.0248],\n",
      "        [-0.1088,  0.0024,  0.1102,  ...,  0.0702,  0.0320, -0.0037],\n",
      "        [-0.0230, -0.0305,  0.0025,  ...,  0.0196,  0.0333,  0.1948],\n",
      "        ...,\n",
      "        [-0.1560,  0.0096, -0.0963,  ...,  0.0430,  0.1022,  0.0862],\n",
      "        [-0.0767, -0.0164, -0.1109,  ..., -0.0115, -0.0603, -0.0807],\n",
      "        [-0.0468, -0.0603, -0.0261,  ..., -0.0425, -0.0016,  0.0135]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.0710,  0.3545,  0.2558,  ...,  0.1152,  0.0430,  0.1335],\n",
      "        [ 0.4681, -0.0511,  0.0667,  ...,  0.4771,  0.3069,  0.1899],\n",
      "        [ 0.3772,  0.1508,  0.0563,  ...,  0.1763,  0.4988,  0.4928],\n",
      "        ...,\n",
      "        [ 0.6960,  0.3359, -0.4520,  ..., -0.6645, -0.9856, -0.0658],\n",
      "        [-0.6903,  0.0355, -0.0203,  ...,  0.7897,  0.1775,  0.0076],\n",
      "        [ 0.1741, -0.4171, -0.5959,  ...,  0.2152,  0.1388,  0.0876]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2786, -0.2883,  0.0648,  ...,  0.2315, -0.2977, -0.0890],\n",
      "        [-0.1140, -0.0972, -0.2791,  ...,  0.1615, -0.0062, -0.0048],\n",
      "        [-0.1915,  0.1413, -0.0066,  ...,  0.0944, -0.4530,  0.1340],\n",
      "        ...,\n",
      "        [ 0.4569, -0.0240,  0.2770,  ...,  0.1638, -0.7838, -0.1067],\n",
      "        [ 0.0901, -0.1524, -0.3662,  ..., -0.5115, -0.1581, -0.0399],\n",
      "        [-0.1037, -0.4300,  1.0623,  ..., -0.0198,  0.0641,  0.0797]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0309,  0.8300, -0.1528,  ...,  0.6671, -0.0367,  0.0835],\n",
      "        [-0.6095, -0.1118,  0.4716,  ...,  0.0104, -0.2327, -0.1252],\n",
      "        [-0.3762,  0.2548, -0.1232,  ..., -0.0717,  0.0949,  0.0996],\n",
      "        ...,\n",
      "        [ 0.2422, -0.2590, -0.7917,  ...,  0.1248, -0.0694,  0.5615],\n",
      "        [-0.0187, -0.8893, -0.6847,  ...,  0.0726,  0.2611, -0.1610],\n",
      "        [-0.4290,  0.4593, -0.5816,  ..., -0.3810, -0.5659, -0.6963]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[  1.3353, -25.1529,   4.6120,   3.6100,   1.0900,   2.6584],\n",
      "        [  3.2505,   1.2971,   6.7612,   6.7746,   1.7294,   5.8305],\n",
      "        [  3.0387,   1.9773,   5.0789,   4.6303,   1.7700,   4.9016],\n",
      "        [  2.7200,   2.1190,   4.1400,   3.2973,   1.8664,   4.2681],\n",
      "        [  2.4569,   2.1655,   3.4619,   2.5026,   1.8302,   3.7855],\n",
      "        [  2.2515,   2.1332,   2.8842,   1.8353,   1.8698,   3.3443],\n",
      "        [  2.0742,   2.1248,   2.4379,   1.4177,   1.8573,   2.9583],\n",
      "        [  1.8628,   2.1215,   2.0795,   1.0468,   1.8589,   2.6317],\n",
      "        [  1.7060,   2.0391,   1.6730,   0.5993,   1.8779,   2.3421],\n",
      "        [  1.5756,   2.0168,   1.4029,   0.4227,   1.8664,   2.0348],\n",
      "        [  1.4858,   1.9753,   1.1227,   0.1609,   1.8450,   1.8140],\n",
      "        [  1.2769,   1.9431,   0.9581,  -0.1143,   1.7901,   1.5724],\n",
      "        [  1.2177,   1.8941,   0.6852,  -0.2768,   1.7729,   1.2964],\n",
      "        [  1.0857,   1.8530,   0.5036,  -0.4322,   1.7873,   1.0547],\n",
      "        [  1.0224,   1.7929,   0.3737,  -0.6314,   1.7858,   0.9571],\n",
      "        [  0.8988,   1.7757,   0.1034,  -0.7444,   1.7557,   0.7197],\n",
      "        [  0.7587,   1.6904,  -0.0951,  -1.0199,   1.7005,   0.4328],\n",
      "        [  0.5486,   1.6080,  -0.4154,  -1.2641,   1.6820,   0.0803],\n",
      "        [  0.4158,   1.5189,  -0.6349,  -1.4908,   1.6787,  -0.2166],\n",
      "        [  0.2733,   1.4070,  -0.9328,  -1.7340,   1.6087,  -0.5213],\n",
      "        [  0.0460,   1.3348,  -1.1021,  -1.9239,   1.4934,  -0.7981],\n",
      "        [ -0.0762,   1.2081,  -1.3296,  -2.0935,   1.4453,  -1.0604],\n",
      "        [ -0.1566,   1.1033,  -1.5380,  -2.2246,   1.3809,  -1.3582],\n",
      "        [ -0.3635,   0.9814,  -1.7353,  -2.3883,   1.3722,  -1.6029],\n",
      "        [ -0.4079,   0.9054,  -1.9446,  -2.5002,   1.2551,  -1.8028],\n",
      "        [ -0.4915,   0.7691,  -2.0115,  -2.6414,   1.1779,  -1.9208],\n",
      "        [ -0.5099,   0.6166,  -2.1463,  -2.7290,   1.1346,  -2.0560],\n",
      "        [ -0.5999,   0.5326,  -2.2426,  -2.8641,   1.0584,  -2.2070],\n",
      "        [ -0.5502,   0.3819,  -2.3292,  -2.9319,   1.0175,  -2.3021],\n",
      "        [ -0.4620,   0.2823,  -2.3988,  -2.9593,   0.9290,  -2.3801],\n",
      "        [ -0.4499,   0.1642,  -2.5017,  -3.0220,   0.9009,  -2.5273],\n",
      "        [ 25.3303,  -0.3031,  -2.5347,  -2.9779,   0.8144,  -2.5429]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0590, 0.1103, 0.0982, 0.1213, 0.1063, 0.1066, 0.1539, 0.1239, 0.0835,\n",
      "        0.1314, 0.1261, 0.0705, 0.0936, 0.1046, 0.0947, 0.1571, 0.0993, 0.0822,\n",
      "        0.1423, 0.0815, 0.1176, 0.0699, 0.1210, 0.1274, 0.0786, 0.0969, 0.0812,\n",
      "        0.1046, 0.1026, 0.1136, 0.1084, 0.1522, 0.1366, 0.1008, 0.0906, 0.0915,\n",
      "        0.1163, 0.1104, 0.1034, 0.0782, 0.1161, 0.0918, 0.1176, 0.0696, 0.1081,\n",
      "        0.1631, 0.1442, 0.1068, 0.0961, 0.0902, 0.0933, 0.1078, 0.1017, 0.1037,\n",
      "        0.1270, 0.1057, 0.0917, 0.1362, 0.1019, 0.0995, 0.1058, 0.1163, 0.1075,\n",
      "        0.1046, 0.0756, 0.1172, 0.1053, 0.0928, 0.1403, 0.1078, 0.1190, 0.0931,\n",
      "        0.0938, 0.0994, 0.1218, 0.0991, 0.0756, 0.1033, 0.0855, 0.0828, 0.1143,\n",
      "        0.1049, 0.1185, 0.1129, 0.0939, 0.1278, 0.1011, 0.1077, 0.0996, 0.1318,\n",
      "        0.1008, 0.0886, 0.1085, 0.1212, 0.0940, 0.1065, 0.0977, 0.1254, 0.1098,\n",
      "        0.0985, 0.1094, 0.1143, 0.0830, 0.0853, 0.0755, 0.0980, 0.0648, 0.1078,\n",
      "        0.0778, 0.1048, 0.0986, 0.1099, 0.1355, 0.0747, 0.0823, 0.0881, 0.0720,\n",
      "        0.1091, 0.1100, 0.1138, 0.1038, 0.1450, 0.0579, 0.0580, 0.0947, 0.1130,\n",
      "        0.1131, 0.0946, 0.0748, 0.0787, 0.1243, 0.1195, 0.0858, 0.0916, 0.0982,\n",
      "        0.1130, 0.1269, 0.0832, 0.0903, 0.1104, 0.0841, 0.1543, 0.1079, 0.1148,\n",
      "        0.1266, 0.1179, 0.1025, 0.0923, 0.1141, 0.0860, 0.1275, 0.1351, 0.0859,\n",
      "        0.1302, 0.0895, 0.1057, 0.1125, 0.1353, 0.1242, 0.1071, 0.0714, 0.1150,\n",
      "        0.1147, 0.0455, 0.1068, 0.1195, 0.0876, 0.1195, 0.1019, 0.1282, 0.1058,\n",
      "        0.1034, 0.0972, 0.1040, 0.0394, 0.0985, 0.1143, 0.0617, 0.1047, 0.1153,\n",
      "        0.0927, 0.0942, 0.1042, 0.0846, 0.1218, 0.1096, 0.1266, 0.1310, 0.1165,\n",
      "        0.0855, 0.0878, 0.1195, 0.1057, 0.1403, 0.1214, 0.0772, 0.1192, 0.1387,\n",
      "        0.1169, 0.0780, 0.0799, 0.1198, 0.1055, 0.1151, 0.0303, 0.0896, 0.1043,\n",
      "        0.0832, 0.1271, 0.0764, 0.1536, 0.0936, 0.1089, 0.0940, 0.1107, 0.1366,\n",
      "        0.1412, 0.1190, 0.1063, 0.1331, 0.0889, 0.0749, 0.1318, 0.1204, 0.1110,\n",
      "        0.0555, 0.1138, 0.0889, 0.1248, 0.1159, 0.1036, 0.0954, 0.0830, 0.0481,\n",
      "        0.1221, 0.0800, 0.1188, 0.0776, 0.1179, 0.1102, 0.1117, 0.1055, 0.1245,\n",
      "        0.0851, 0.1208, 0.1039, 0.1036, 0.0997, 0.0943, 0.1052, 0.0760, 0.1214,\n",
      "        0.0988, 0.0905, 0.0897, 0.0803, 0.1032, 0.1226, 0.1132, 0.0985, 0.1257,\n",
      "        0.0936, 0.0998, 0.1104, 0.1242, 0.1097, 0.1172, 0.0964, 0.0417, 0.0679,\n",
      "        0.1080, 0.0989, 0.1089, 0.1022, 0.1243, 0.1020, 0.1045, 0.1131, 0.1157,\n",
      "        0.0925, 0.0398, 0.1036, 0.1075, 0.1452, 0.1034, 0.1031, 0.1163, 0.0846,\n",
      "        0.1578, 0.1003, 0.1121, 0.0894, 0.1173, 0.0803, 0.1064, 0.0964, 0.1086,\n",
      "        0.0649, 0.1278, 0.0974, 0.1512, 0.1047, 0.1193, 0.1343, 0.0736, 0.1368,\n",
      "        0.1330, 0.0868, 0.0665, 0.0882, 0.0942, 0.0930, 0.1101, 0.1207, 0.1079,\n",
      "        0.1440, 0.1121, 0.1060, 0.0882, 0.0931, 0.1498, 0.0933, 0.0953, 0.1260,\n",
      "        0.0892, 0.0922, 0.0985, 0.1179, 0.1097, 0.1150, 0.1069, 0.1240, 0.0589,\n",
      "        0.0409, 0.0984, 0.0804, 0.1001, 0.1089, 0.0912, 0.1120, 0.0940, 0.0990,\n",
      "        0.1015, 0.1048, 0.0957, 0.0991, 0.1278, 0.0994, 0.0865, 0.0585, 0.1009,\n",
      "        0.1105, 0.0599, 0.1190, 0.1172, 0.1046, 0.1166, 0.1111, 0.1201, 0.0859,\n",
      "        0.0980, 0.1135, 0.1217, 0.0928, 0.0832, 0.0985, 0.0957, 0.0426, 0.0988,\n",
      "        0.1032, 0.1206, 0.1123, 0.1284, 0.1205, 0.0931, 0.1136, 0.1231, 0.0855,\n",
      "        0.0965, 0.1052, 0.1067, 0.0997, 0.1054, 0.0906, 0.0838, 0.1106, 0.1504,\n",
      "        0.1131, 0.0887, 0.0880, 0.0967, 0.0844, 0.1042, 0.1156, 0.1224, 0.1106,\n",
      "        0.1414, 0.1120, 0.0946, 0.1102, 0.0777, 0.0919, 0.1230, 0.1000, 0.0980,\n",
      "        0.1091, 0.1141, 0.1068, 0.1172, 0.1155, 0.0910, 0.0973, 0.0640, 0.0888,\n",
      "        0.0377, 0.1043, 0.0897, 0.0990, 0.0793, 0.1331, 0.1149, 0.1263, 0.1028,\n",
      "        0.0790, 0.0628, 0.1087, 0.1332, 0.1019, 0.0561, 0.0918, 0.0803, 0.1264,\n",
      "        0.1017, 0.1120, 0.1343, 0.0858, 0.1002, 0.1074, 0.0986, 0.1130, 0.0909,\n",
      "        0.0850, 0.0764, 0.0882, 0.1117, 0.1095, 0.0772, 0.1598, 0.1179, 0.0797,\n",
      "        0.0371, 0.1143, 0.1039, 0.0990, 0.1408, 0.0962, 0.0636, 0.1058, 0.0805,\n",
      "        0.0911, 0.1378, 0.0859, 0.0987, 0.0770, 0.0876, 0.1184, 0.1054, 0.0700,\n",
      "        0.1357, 0.1201, 0.0998, 0.0900, 0.1367, 0.0664, 0.0971, 0.1277, 0.1230,\n",
      "        0.1130, 0.0637, 0.0989, 0.1154, 0.1000, 0.1220, 0.1201, 0.1007, 0.1215,\n",
      "        0.1307, 0.1184, 0.1359, 0.0397, 0.0994, 0.1008, 0.0900, 0.1321, 0.1356,\n",
      "        0.0792, 0.1182, 0.1243, 0.1051, 0.1235, 0.1052, 0.1279, 0.0970, 0.1110,\n",
      "        0.1063, 0.0835, 0.1252, 0.1158, 0.0724, 0.1228, 0.1183, 0.0381],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0303, -0.0011, -0.0677,  ..., -0.0271,  0.1310, -0.0103],\n",
      "        [ 0.1032,  0.1035,  0.1512,  ..., -0.0564,  0.0743, -0.0152],\n",
      "        [ 0.0041, -0.0028, -0.0197,  ...,  0.0889,  0.0206,  0.0350],\n",
      "        ...,\n",
      "        [ 0.0501,  0.0222,  0.0024,  ..., -0.0960, -0.0169,  0.0002],\n",
      "        [ 0.0420, -0.0292,  0.1778,  ...,  0.0465,  0.0072, -0.0136],\n",
      "        [ 0.0512,  0.0508, -0.0212,  ...,  0.1185, -0.0153,  0.0022]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3305, -0.4051,  0.3153,  ..., -0.1913, -0.1598, -0.5219],\n",
      "        [ 0.4407,  0.1110,  0.3796,  ..., -0.7958,  0.2804,  0.0054],\n",
      "        [-0.2410,  0.3975, -0.2628,  ...,  0.1244,  0.3104, -0.2422],\n",
      "        ...,\n",
      "        [-0.2096, -0.4158,  0.2408,  ..., -0.0142, -0.0736, -0.0102],\n",
      "        [ 0.5132,  0.5884,  0.0764,  ...,  0.4965,  0.4364, -0.5824],\n",
      "        [ 0.0657, -0.2672,  0.1800,  ...,  0.1711,  0.3949, -0.2769]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0980, -0.6398, -0.0512,  ...,  0.1690, -0.3511,  0.3978],\n",
      "        [ 0.1823, -0.1971, -0.2320,  ...,  0.1931, -0.2250,  0.2486],\n",
      "        [ 0.2513,  0.2686, -0.5848,  ...,  0.0931, -0.0802, -0.0246],\n",
      "        ...,\n",
      "        [ 0.2036, -0.0891,  0.3283,  ..., -0.3342, -0.3944,  0.0900],\n",
      "        [-0.5571, -0.5678, -0.0911,  ..., -0.1248, -0.2883,  0.4313],\n",
      "        [ 0.0547, -0.1509,  0.0410,  ..., -0.0535, -0.0048, -0.6134]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.5113, -0.9592, -0.3655,  ..., -0.2302,  0.7962, -1.2894],\n",
      "        [-0.4296, -0.0484, -0.1774,  ..., -0.4558,  0.4249, -0.4612],\n",
      "        [ 0.3273,  0.3681,  0.4232,  ...,  0.2776,  0.1431,  0.0487],\n",
      "        ...,\n",
      "        [-0.1028, -0.3369,  0.1093,  ..., -0.0434, -0.0180,  0.3774],\n",
      "        [-0.4246,  0.2163, -0.4479,  ..., -0.5286, -0.0308, -0.1616],\n",
      "        [ 2.4853, -1.9013, -4.9618,  ..., -0.2505,  1.0346, -5.2150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1226, 0.1050, 0.0813, 0.0892, 0.1021, 0.0814, 0.1173, 0.1024, 0.0738,\n",
      "        0.1224, 0.1034, 0.0774, 0.0855, 0.0903, 0.0895, 0.2034, 0.1038, 0.0791,\n",
      "        0.1126, 0.0705, 0.0938, 0.0823, 0.1033, 0.1007, 0.0825, 0.0796, 0.0707,\n",
      "        0.0983, 0.0996, 0.0957, 0.1067, 0.1121, 0.1278, 0.1131, 0.0782, 0.0982,\n",
      "        0.0983, 0.0897, 0.1055, 0.0626, 0.0951, 0.0862, 0.1127, 0.0797, 0.0843,\n",
      "        0.1572, 0.1307, 0.0923, 0.0991, 0.0792, 0.0867, 0.1029, 0.1039, 0.0900,\n",
      "        0.1000, 0.0808, 0.0922, 0.1048, 0.0897, 0.0847, 0.0879, 0.0940, 0.0939,\n",
      "        0.0931, 0.1276, 0.0918, 0.0935, 0.0909, 0.1312, 0.0883, 0.1185, 0.0713,\n",
      "        0.0455, 0.0951, 0.1113, 0.1095, 0.0963, 0.0867, 0.0669, 0.0863, 0.1016,\n",
      "        0.0972, 0.1115, 0.0907, 0.0924, 0.0976, 0.1018, 0.1004, 0.0998, 0.1003,\n",
      "        0.0771, 0.0982, 0.0921, 0.0856, 0.0847, 0.1086, 0.1028, 0.0962, 0.0845,\n",
      "        0.1100, 0.1010, 0.1075, 0.0808, 0.0848, 0.0953, 0.0954, 0.1014, 0.0843,\n",
      "        0.0720, 0.1132, 0.1069, 0.0963, 0.1332, 0.0823, 0.1010, 0.0945, 0.0640,\n",
      "        0.0930, 0.1020, 0.1034, 0.0951, 0.1358, 0.1175, 0.1175, 0.0925, 0.1102,\n",
      "        0.1009, 0.0948, 0.0692, 0.0914, 0.1009, 0.0945, 0.1057, 0.0875, 0.1043,\n",
      "        0.1005, 0.1665, 0.0979, 0.0830, 0.0910, 0.0857, 0.1293, 0.0813, 0.0904,\n",
      "        0.0975, 0.1248, 0.0907, 0.0840, 0.0889, 0.0756, 0.1049, 0.1370, 0.1014,\n",
      "        0.1004, 0.0847, 0.1027, 0.1029, 0.1361, 0.0947, 0.1245, 0.1026, 0.0967,\n",
      "        0.0994, 0.1058, 0.0918, 0.1219, 0.0855, 0.1031, 0.0838, 0.1239, 0.1075,\n",
      "        0.1034, 0.0854, 0.0949, 0.1009, 0.1011, 0.1098, 0.0674, 0.0990, 0.1004,\n",
      "        0.1079, 0.0836, 0.1363, 0.0726, 0.0801, 0.0868, 0.1052, 0.1124, 0.1029,\n",
      "        0.0962, 0.0811, 0.1190, 0.0946, 0.1161, 0.0986, 0.0780, 0.1003, 0.1085,\n",
      "        0.1145, 0.0781, 0.0945, 0.1115, 0.1021, 0.0945, 0.0742, 0.0960, 0.1117,\n",
      "        0.0998, 0.1045, 0.1370, 0.1314, 0.0911, 0.0937, 0.0749, 0.1243, 0.1452,\n",
      "        0.0987, 0.1118, 0.1304, 0.1158, 0.0823, 0.0964, 0.0866, 0.0989, 0.0933,\n",
      "        0.0801, 0.0966, 0.1055, 0.1142, 0.1353, 0.0949, 0.0757, 0.0801, 0.0880,\n",
      "        0.1043, 0.0806, 0.1021, 0.1204, 0.1193, 0.1106, 0.1415, 0.0936, 0.1137,\n",
      "        0.0946, 0.0842, 0.1030, 0.0911, 0.1283, 0.0986, 0.0988, 0.1451, 0.0992,\n",
      "        0.0920, 0.0790, 0.0695, 0.0839, 0.0899, 0.0942, 0.1060, 0.1036, 0.0950,\n",
      "        0.0959, 0.0940, 0.1094, 0.1234, 0.0963, 0.1018, 0.1124, 0.0941, 0.0782,\n",
      "        0.0979, 0.1032, 0.1052, 0.0985, 0.1104, 0.0838, 0.0966, 0.1075, 0.0971,\n",
      "        0.1061, 0.0956, 0.1057, 0.0931, 0.1204, 0.1016, 0.1123, 0.0921, 0.0784,\n",
      "        0.1600, 0.1089, 0.1098, 0.0760, 0.0922, 0.0860, 0.1045, 0.0938, 0.1076,\n",
      "        0.0843, 0.1005, 0.0846, 0.1102, 0.0845, 0.0907, 0.1119, 0.0898, 0.1763,\n",
      "        0.1145, 0.0712, 0.0759, 0.0714, 0.0931, 0.1043, 0.0980, 0.0876, 0.1021,\n",
      "        0.0967, 0.0884, 0.1108, 0.1325, 0.0913, 0.0985, 0.0809, 0.0944, 0.1040,\n",
      "        0.0981, 0.0917, 0.0999, 0.0984, 0.0991, 0.0876, 0.0961, 0.1061, 0.1176,\n",
      "        0.0685, 0.0955, 0.0827, 0.0876, 0.1058, 0.0834, 0.1360, 0.0798, 0.0780,\n",
      "        0.0988, 0.0869, 0.0886, 0.0930, 0.0994, 0.0941, 0.0941, 0.0855, 0.0908,\n",
      "        0.1073, 0.1085, 0.0932, 0.1100, 0.1082, 0.0962, 0.0767, 0.1128, 0.0873,\n",
      "        0.0985, 0.0928, 0.1023, 0.0812, 0.0870, 0.0986, 0.0980, 0.0636, 0.1160,\n",
      "        0.0971, 0.1183, 0.0859, 0.1211, 0.0867, 0.0845, 0.0907, 0.1027, 0.0906,\n",
      "        0.0885, 0.0933, 0.1225, 0.0759, 0.0993, 0.0924, 0.0907, 0.0953, 0.1429,\n",
      "        0.1092, 0.0911, 0.0739, 0.1131, 0.0796, 0.1178, 0.0973, 0.1158, 0.1032,\n",
      "        0.1047, 0.1014, 0.1009, 0.0901, 0.0814, 0.0795, 0.1079, 0.0770, 0.0900,\n",
      "        0.1088, 0.0958, 0.1014, 0.1069, 0.1014, 0.0837, 0.0922, 0.1319, 0.0828,\n",
      "        0.0528, 0.0989, 0.0854, 0.1023, 0.0851, 0.1192, 0.0968, 0.0931, 0.0961,\n",
      "        0.0725, 0.0671, 0.0982, 0.1068, 0.0882, 0.0748, 0.1048, 0.0701, 0.1224,\n",
      "        0.0806, 0.0988, 0.1049, 0.1092, 0.1002, 0.0918, 0.1024, 0.1033, 0.0667,\n",
      "        0.0880, 0.0982, 0.0758, 0.0861, 0.0970, 0.0846, 0.1322, 0.1009, 0.0767,\n",
      "        0.0914, 0.1083, 0.0902, 0.0888, 0.1176, 0.0770, 0.0735, 0.0938, 0.0895,\n",
      "        0.0807, 0.1041, 0.1105, 0.0825, 0.0761, 0.0676, 0.1031, 0.0727, 0.0683,\n",
      "        0.1084, 0.1207, 0.1040, 0.1061, 0.1340, 0.0935, 0.1127, 0.1290, 0.0951,\n",
      "        0.1005, 0.0729, 0.1051, 0.0947, 0.1191, 0.0900, 0.1128, 0.0950, 0.1046,\n",
      "        0.1237, 0.1006, 0.1113, 0.1384, 0.0807, 0.0884, 0.0840, 0.1403, 0.1025,\n",
      "        0.0813, 0.0905, 0.1389, 0.0999, 0.1390, 0.1102, 0.1191, 0.0939, 0.1097,\n",
      "        0.1052, 0.0735, 0.1104, 0.0912, 0.0808, 0.0966, 0.0813, 0.2299],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2193, -0.0672,  0.1917,  ..., -0.1413,  0.0142,  0.4663],\n",
      "        [ 0.0615, -0.1355,  0.2081,  ...,  0.2324,  0.0613, -0.0959],\n",
      "        [ 0.0950, -0.1934, -0.0201,  ..., -0.5323,  0.6101, -0.3897],\n",
      "        ...,\n",
      "        [ 0.1662,  0.2615, -0.2619,  ...,  0.2742, -1.2078, -0.3090],\n",
      "        [ 0.2820,  0.0347, -0.0266,  ...,  1.0036, -0.4664,  0.4465],\n",
      "        [ 0.0818,  0.2163,  1.3763,  ...,  0.4058, -0.2394,  0.1987]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1250, -0.2613, -0.7943,  ..., -0.1954,  0.0832,  0.2226],\n",
      "        [ 0.0143, -0.2182, -0.7057,  ..., -0.5999,  0.0861,  0.0217],\n",
      "        [-0.2821,  0.0606, -0.4338,  ..., -0.4861,  0.1091,  0.2891],\n",
      "        ...,\n",
      "        [-0.4470,  0.8991, -0.1612,  ...,  0.5299,  0.1282,  0.3406],\n",
      "        [ 0.1578, -0.1211, -0.4008,  ..., -0.7233,  0.3516,  0.2179],\n",
      "        [ 0.0830,  0.3142,  0.0099,  ..., -0.2659, -0.6991, -0.6577]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.7517, -0.2223,  0.2775,  ...,  1.0290, -0.4414, -0.6515],\n",
      "        [-0.2138,  0.0146, -0.3127,  ..., -0.5760,  0.5859, -0.0859],\n",
      "        [ 0.0785, -0.1604,  0.4384,  ..., -0.1537,  0.1058, -0.0540],\n",
      "        ...,\n",
      "        [-0.2255, -0.0946, -0.0786,  ..., -0.7619,  0.2058, -0.9050],\n",
      "        [-0.3003, -0.0460, -0.0600,  ...,  0.2604,  0.3018, -0.2083],\n",
      "        [-0.8008, -0.8743,  0.1069,  ...,  1.5059, -0.6674, -0.8852]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0564, 0.1125, 0.0767, 0.0986, 0.0898, 0.0936, 0.1188, 0.0883, 0.0747,\n",
      "        0.1112, 0.1000, 0.0973, 0.0821, 0.0749, 0.0762, 0.0969, 0.0989, 0.0810,\n",
      "        0.1114, 0.0730, 0.0853, 0.0705, 0.1158, 0.1040, 0.0798, 0.0820, 0.0733,\n",
      "        0.0921, 0.0957, 0.0823, 0.1022, 0.1126, 0.1263, 0.0930, 0.0720, 0.0612,\n",
      "        0.0961, 0.0878, 0.0941, 0.0690, 0.0982, 0.0856, 0.1018, 0.0611, 0.0808,\n",
      "        0.1291, 0.1125, 0.0906, 0.0821, 0.0833, 0.0736, 0.0986, 0.0779, 0.0899,\n",
      "        0.0945, 0.0897, 0.0790, 0.1027, 0.0826, 0.0886, 0.0892, 0.0990, 0.0973,\n",
      "        0.0940, 0.1271, 0.0981, 0.0825, 0.0768, 0.1075, 0.0864, 0.1047, 0.0798,\n",
      "        0.0213, 0.0898, 0.0991, 0.0836, 0.0744, 0.0874, 0.0793, 0.0709, 0.0972,\n",
      "        0.0897, 0.0952, 0.0976, 0.0825, 0.1128, 0.0895, 0.1021, 0.0823, 0.1069,\n",
      "        0.0794, 0.0833, 0.1027, 0.0919, 0.0916, 0.0755, 0.0991, 0.0990, 0.0857,\n",
      "        0.0865, 0.0937, 0.0897, 0.0718, 0.0831, 0.0844, 0.0888, 0.0804, 0.0946,\n",
      "        0.0654, 0.1007, 0.0980, 0.0896, 0.1269, 0.0727, 0.0853, 0.0762, 0.0643,\n",
      "        0.0874, 0.0860, 0.0881, 0.0876, 0.1186, 0.1119, 0.1204, 0.0843, 0.0946,\n",
      "        0.0876, 0.0736, 0.0652, 0.0809, 0.0947, 0.0924, 0.0988, 0.0898, 0.0855,\n",
      "        0.0920, 0.1349, 0.0752, 0.0791, 0.1024, 0.0829, 0.1249, 0.0878, 0.0986,\n",
      "        0.0832, 0.1036, 0.0823, 0.0795, 0.0850, 0.0809, 0.1012, 0.1201, 0.0865,\n",
      "        0.0952, 0.0818, 0.0995, 0.0919, 0.1156, 0.1069, 0.0892, 0.0851, 0.0922,\n",
      "        0.0991, 0.0552, 0.0840, 0.0941, 0.0811, 0.1003, 0.0903, 0.0977, 0.0923,\n",
      "        0.1019, 0.0878, 0.0974, 0.0901, 0.0858, 0.0873, 0.0571, 0.0909, 0.0910,\n",
      "        0.0933, 0.0804, 0.1025, 0.0660, 0.0915, 0.0864, 0.1235, 0.1120, 0.0894,\n",
      "        0.0746, 0.0735, 0.1200, 0.0823, 0.1063, 0.0985, 0.0749, 0.0922, 0.0926,\n",
      "        0.0936, 0.0755, 0.0797, 0.1044, 0.0854, 0.0890, 0.0797, 0.0908, 0.0917,\n",
      "        0.0747, 0.1018, 0.1157, 0.1168, 0.0883, 0.0950, 0.0752, 0.0741, 0.1065,\n",
      "        0.0977, 0.0962, 0.0682, 0.0996, 0.0693, 0.0770, 0.1021, 0.0925, 0.0852,\n",
      "        0.0715, 0.1040, 0.0943, 0.0926, 0.1188, 0.0697, 0.0950, 0.0710, 0.0971,\n",
      "        0.1092, 0.0748, 0.0966, 0.1011, 0.1018, 0.0880, 0.0773, 0.0911, 0.0967,\n",
      "        0.0800, 0.0942, 0.0884, 0.0993, 0.1181, 0.0915, 0.0851, 0.1053, 0.1000,\n",
      "        0.0860, 0.0774, 0.0719, 0.0736, 0.0786, 0.0998, 0.0950, 0.0982, 0.1018,\n",
      "        0.0808, 0.0862, 0.0939, 0.1138, 0.0928, 0.0878, 0.0928, 0.0606, 0.0739,\n",
      "        0.0876, 0.0790, 0.0976, 0.0874, 0.0851, 0.0854, 0.0885, 0.1005, 0.0898,\n",
      "        0.0784, 0.0858, 0.0841, 0.0902, 0.1135, 0.0897, 0.0806, 0.0852, 0.0775,\n",
      "        0.1088, 0.0832, 0.0983, 0.0686, 0.1019, 0.0818, 0.0851, 0.0916, 0.0927,\n",
      "        0.0574, 0.0993, 0.0809, 0.1055, 0.0851, 0.1020, 0.1015, 0.0784, 0.1173,\n",
      "        0.1141, 0.0724, 0.0640, 0.0751, 0.0784, 0.0903, 0.0884, 0.0943, 0.0978,\n",
      "        0.1021, 0.0909, 0.0978, 0.1189, 0.0818, 0.1019, 0.0625, 0.0840, 0.1160,\n",
      "        0.0905, 0.0870, 0.0830, 0.1007, 0.1011, 0.1051, 0.0939, 0.0975, 0.1047,\n",
      "        0.0642, 0.0878, 0.0710, 0.0821, 0.0992, 0.0852, 0.1153, 0.0880, 0.0767,\n",
      "        0.0994, 0.0822, 0.0790, 0.0921, 0.1020, 0.0903, 0.0863, 0.0835, 0.0848,\n",
      "        0.0888, 0.0913, 0.0952, 0.0879, 0.0822, 0.0996, 0.0895, 0.1020, 0.0789,\n",
      "        0.0794, 0.0963, 0.0976, 0.0756, 0.0676, 0.0980, 0.0860, 0.0870, 0.1102,\n",
      "        0.0976, 0.1079, 0.0968, 0.1095, 0.0904, 0.0756, 0.0928, 0.1160, 0.0795,\n",
      "        0.0953, 0.0844, 0.1068, 0.0857, 0.0907, 0.0784, 0.0823, 0.0853, 0.1117,\n",
      "        0.1064, 0.0747, 0.0877, 0.0916, 0.0815, 0.0959, 0.0962, 0.1271, 0.0867,\n",
      "        0.1142, 0.0853, 0.0849, 0.0927, 0.0674, 0.0699, 0.1017, 0.0797, 0.0829,\n",
      "        0.0927, 0.0951, 0.0946, 0.1000, 0.0837, 0.0824, 0.0837, 0.0738, 0.0788,\n",
      "        0.0411, 0.0860, 0.0799, 0.0922, 0.0671, 0.1099, 0.0797, 0.1024, 0.0802,\n",
      "        0.0695, 0.0518, 0.0902, 0.0965, 0.0789, 0.0252, 0.0764, 0.0671, 0.1089,\n",
      "        0.0808, 0.1026, 0.0961, 0.1074, 0.0868, 0.0832, 0.0829, 0.0919, 0.0771,\n",
      "        0.0825, 0.0752, 0.0840, 0.0837, 0.1123, 0.0707, 0.1192, 0.0983, 0.0696,\n",
      "        0.0752, 0.1090, 0.0925, 0.1015, 0.1148, 0.0831, 0.0667, 0.0821, 0.0744,\n",
      "        0.0853, 0.1070, 0.0907, 0.0824, 0.0639, 0.0817, 0.1074, 0.0927, 0.0643,\n",
      "        0.0986, 0.1170, 0.0897, 0.0907, 0.0971, 0.0842, 0.0883, 0.0891, 0.0954,\n",
      "        0.0921, 0.0623, 0.0978, 0.1107, 0.0854, 0.0929, 0.1103, 0.0860, 0.0982,\n",
      "        0.1073, 0.1081, 0.1129, 0.1174, 0.0832, 0.0794, 0.0729, 0.1025, 0.0979,\n",
      "        0.0732, 0.0957, 0.0864, 0.0951, 0.0991, 0.0976, 0.0938, 0.0826, 0.1023,\n",
      "        0.0837, 0.0801, 0.0970, 0.0879, 0.0720, 0.0942, 0.0931, 0.0135],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0017,  0.0113,  0.1034,  ..., -0.0795,  0.0791,  0.0364],\n",
      "        [-0.0625, -0.0043,  0.0371,  ...,  0.0961,  0.0084, -0.0181],\n",
      "        [ 0.0061,  0.0565, -0.0144,  ...,  0.0216, -0.0473, -0.0278],\n",
      "        ...,\n",
      "        [ 0.0262, -0.0972, -0.0150,  ..., -0.0940,  0.0379,  0.0408],\n",
      "        [ 0.0680,  0.0891, -0.0146,  ...,  0.0835, -0.0594,  0.0356],\n",
      "        [ 0.0718, -0.0125, -0.1135,  ..., -0.0094, -0.0146,  0.0303]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 4.7681e-02,  5.0142e-01,  4.8732e-01,  ...,  1.0231e-01,\n",
      "          2.2494e-01, -2.0428e-01],\n",
      "        [ 7.0633e-02, -2.3739e-01,  2.1577e-02,  ...,  2.1102e-01,\n",
      "         -2.6967e-02, -3.4505e-01],\n",
      "        [ 1.2923e-03, -2.6287e-01, -6.9095e-04,  ..., -2.8579e-01,\n",
      "         -1.8609e-01,  5.8362e-01],\n",
      "        ...,\n",
      "        [-1.6066e-01, -9.1270e-02,  9.3185e-02,  ..., -1.6331e-01,\n",
      "          8.0777e-01,  4.6045e-02],\n",
      "        [ 1.6985e-01,  5.4158e-01, -4.1995e-02,  ..., -6.0138e-01,\n",
      "          2.5961e-01,  5.1222e-01],\n",
      "        [ 5.2994e-01, -2.5813e-01,  1.6718e-02,  ..., -3.1635e-01,\n",
      "          6.0518e-02, -3.4879e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0630, -0.3510,  0.2188,  ..., -0.6127, -0.2282, -0.1283],\n",
      "        [-0.1809,  0.1538, -0.5460,  ...,  0.2633,  0.6053,  0.0034],\n",
      "        [ 0.1364, -0.4731,  0.1195,  ...,  0.5969, -0.0557, -0.0202],\n",
      "        ...,\n",
      "        [-0.1638, -0.0118, -0.1063,  ..., -0.0814, -0.5303,  0.0232],\n",
      "        [-0.1006,  0.2321, -0.1512,  ...,  0.5546,  0.0657, -0.0114],\n",
      "        [ 0.0172,  0.6742, -0.0529,  ..., -0.4042, -0.4960,  0.0087]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.0058,  1.7116,  3.0737,  ...,  1.4710,  2.6761, -0.2939],\n",
      "        [-0.5086,  0.7223,  0.8214,  ...,  0.4836, -0.2568,  0.6056],\n",
      "        [-0.4870,  0.8123, -0.1156,  ..., -1.6228,  0.1540,  0.3599],\n",
      "        ...,\n",
      "        [ 0.2201,  0.6734,  0.3486,  ..., -0.2520,  0.7012, -0.4914],\n",
      "        [ 0.1165,  0.1860,  0.6178,  ...,  0.1251, -1.0967,  0.6090],\n",
      "        [ 2.6777,  0.5394,  0.3085,  ...,  5.2360,  1.2602,  2.7441]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0469,  0.1174,  0.1357,  0.0963,  0.1065,  0.1234,  0.1143,  0.1458,\n",
      "         0.0879,  0.1150,  0.1247,  0.0809,  0.1006,  0.1000,  0.0908,  0.1377,\n",
      "         0.1032,  0.0819,  0.1155,  0.0841,  0.1237,  0.1046,  0.1100,  0.1194,\n",
      "         0.1031,  0.1026,  0.0909,  0.1146,  0.1296,  0.1151,  0.1061,  0.1191,\n",
      "         0.1283,  0.0826,  0.1032,  0.0990,  0.1136,  0.1254,  0.1090,  0.0885,\n",
      "         0.1317,  0.0980,  0.1125,  0.0779,  0.1080,  0.1128,  0.1308,  0.0935,\n",
      "         0.1254,  0.0961,  0.0957,  0.1186,  0.1155,  0.1045,  0.0981,  0.0960,\n",
      "         0.1193,  0.1045,  0.1034,  0.0903,  0.0992,  0.1062,  0.1156,  0.1157,\n",
      "         0.1136,  0.1281,  0.1250,  0.0868,  0.1256,  0.1158,  0.1202,  0.0938,\n",
      "        -0.0359,  0.0796,  0.1235,  0.0995,  0.0901,  0.1117,  0.0887,  0.1055,\n",
      "         0.1178,  0.1204,  0.1108,  0.0905,  0.0993,  0.0988,  0.0922,  0.1118,\n",
      "         0.1101,  0.1226,  0.1026,  0.1190,  0.1205,  0.0918,  0.1000,  0.1247,\n",
      "         0.0853,  0.0425,  0.1014,  0.0980,  0.1301,  0.1191,  0.0889,  0.0975,\n",
      "         0.1121,  0.1048,  0.0902,  0.1087,  0.0889,  0.1121,  0.0899,  0.0987,\n",
      "         0.1366,  0.0919,  0.1009,  0.1197,  0.0851,  0.1242,  0.1184,  0.1004,\n",
      "         0.1169,  0.1428,  0.0467,  0.1105,  0.1223,  0.1034,  0.1230,  0.1001,\n",
      "         0.0840,  0.0997,  0.1112,  0.1444,  0.0920,  0.1065,  0.1103,  0.0999,\n",
      "         0.1199,  0.1011,  0.0935,  0.1104,  0.0912,  0.1100,  0.1071,  0.1144,\n",
      "         0.1432,  0.0872,  0.1089,  0.1252,  0.1240,  0.0920,  0.1187,  0.1289,\n",
      "         0.1030,  0.1626,  0.1017,  0.1084,  0.0995,  0.1553,  0.1259,  0.1249,\n",
      "         0.0892,  0.1075,  0.0975,  0.0643,  0.1025,  0.1199,  0.1026,  0.1206,\n",
      "         0.1195,  0.1299,  0.1180,  0.0959,  0.0985,  0.1061,  0.0871,  0.0905,\n",
      "         0.1066,  0.0879,  0.1050,  0.1130,  0.0919,  0.0986,  0.1526,  0.0889,\n",
      "         0.1166,  0.1125,  0.1035,  0.1229,  0.1172,  0.0794,  0.0954,  0.1019,\n",
      "         0.1001,  0.0810,  0.1050,  0.0774,  0.1280,  0.1441,  0.1285,  0.0952,\n",
      "         0.0861,  0.1250,  0.1243,  0.1133,  0.0603,  0.0880,  0.1211,  0.1042,\n",
      "         0.1306,  0.1620,  0.1461,  0.1096,  0.1070,  0.1140,  0.1237,  0.1683,\n",
      "         0.1395,  0.1098,  0.1529,  0.1144,  0.1044,  0.0994,  0.1100,  0.1033,\n",
      "         0.0998,  0.0729,  0.1113,  0.1121,  0.1186,  0.1215,  0.1316, -0.0718,\n",
      "         0.0900,  0.0551,  0.1138,  0.0863,  0.0981,  0.0872,  0.1503,  0.1023,\n",
      "         0.1539,  0.1113,  0.0433,  0.1026,  0.1384,  0.1208,  0.1072,  0.0339,\n",
      "         0.1009,  0.1435,  0.1262,  0.1130,  0.1079,  0.0932,  0.0869,  0.0923,\n",
      "         0.1381,  0.1229,  0.1132,  0.1000,  0.1178,  0.0804,  0.1106,  0.0957,\n",
      "         0.1175,  0.0913,  0.1144,  0.1148,  0.0653,  0.1066,  0.0975,  0.1078,\n",
      "         0.1087,  0.1039,  0.0953,  0.0179,  0.1006,  0.1153,  0.1108,  0.1060,\n",
      "         0.0574,  0.1296,  0.0853,  0.1238,  0.1194,  0.1073,  0.1056,  0.1045,\n",
      "         0.1809,  0.0984,  0.1221,  0.0950,  0.1045,  0.0934,  0.1325,  0.1014,\n",
      "         0.1069,  0.0928,  0.1307,  0.0935,  0.1515,  0.1284,  0.1057,  0.1314,\n",
      "         0.0863,  0.1252,  0.1104,  0.1053,  0.0841,  0.1074,  0.0854,  0.1213,\n",
      "         0.1175,  0.1087,  0.1274,  0.1314,  0.0920,  0.1158,  0.1124,  0.1123,\n",
      "         0.1343,  0.0972,  0.0986,  0.1580,  0.1039,  0.0975,  0.0943,  0.1162,\n",
      "         0.1310,  0.0715,  0.0899,  0.1173,  0.1167,  0.0769,  0.1373,  0.0914,\n",
      "         0.0889,  0.1183,  0.1002,  0.1236,  0.1255,  0.1296,  0.0845,  0.1125,\n",
      "         0.0989,  0.1028,  0.1237,  0.0892,  0.1024,  0.1069,  0.1139,  0.0923,\n",
      "         0.1801,  0.1258,  0.1140,  0.1095,  0.0939,  0.1087,  0.1133,  0.0813,\n",
      "         0.1447,  0.1017,  0.1382,  0.0887,  0.1040,  0.1215,  0.0999,  0.0419,\n",
      "         0.1030,  0.1114,  0.1184,  0.0868,  0.1236,  0.1126,  0.0985,  0.1040,\n",
      "         0.1123,  0.0938,  0.1008,  0.1191,  0.1235,  0.1178,  0.1002,  0.0989,\n",
      "         0.0999,  0.1192,  0.1514,  0.0941,  0.1031,  0.0606,  0.1084,  0.0961,\n",
      "         0.1099,  0.1232,  0.1065,  0.1155, -0.0620,  0.1319,  0.1145,  0.0971,\n",
      "         0.1000,  0.1092,  0.1225,  0.0963,  0.1077,  0.1043,  0.1142,  0.1101,\n",
      "         0.1195,  0.1350,  0.1034,  0.0920,  0.0677,  0.0960,  0.0435,  0.0969,\n",
      "         0.0945,  0.0980,  0.0979,  0.1386,  0.0952,  0.0980,  0.1123,  0.0860,\n",
      "         0.0784,  0.1011,  0.1137,  0.1197,  0.0864,  0.1248,  0.0933,  0.1587,\n",
      "         0.0958,  0.1110,  0.1381,  0.1222,  0.0809,  0.0920,  0.0930,  0.1006,\n",
      "         0.0946,  0.1014,  0.0872,  0.0932,  0.1210,  0.1086,  0.0930,  0.1405,\n",
      "         0.1159,  0.0838,  0.0696,  0.1023,  0.1224,  0.1167,  0.1627,  0.0934,\n",
      "         0.0294,  0.1189,  0.0970,  0.1062,  0.1227,  0.0970,  0.1142,  0.0966,\n",
      "         0.0955,  0.1141,  0.0725,  0.0958,  0.1244,  0.1395,  0.0879,  0.0952,\n",
      "         0.1421,  0.0212,  0.0972,  0.1714,  0.1169,  0.1101,  0.0871,  0.0546,\n",
      "         0.1342,  0.1186,  0.1197,  0.0911,  0.0985,  0.1066,  0.1048,  0.1076,\n",
      "         0.1138,  0.0569,  0.1003,  0.1048,  0.0981,  0.1547,  0.0989,  0.0904,\n",
      "         0.1274,  0.1178,  0.1049,  0.1340,  0.0842,  0.1146,  0.0968,  0.0900,\n",
      "         0.1008,  0.1036,  0.1111,  0.0964,  0.0905,  0.1293,  0.1044,  0.0342],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0100, -0.0678, -0.0928,  ..., -0.0664,  0.0614,  0.0062],\n",
      "        [ 0.0580, -0.0942,  0.0353,  ..., -0.0009, -0.0904, -0.0462],\n",
      "        [ 0.0298, -0.0914,  0.0521,  ...,  0.0152,  0.0596, -0.0325],\n",
      "        ...,\n",
      "        [ 0.0280, -0.0388,  0.0770,  ..., -0.0638, -0.0012, -0.0227],\n",
      "        [ 0.0141,  0.0703,  0.0269,  ...,  0.0517,  0.0373,  0.0035],\n",
      "        [-0.0149,  0.0502, -0.0341,  ..., -0.0499,  0.0490,  0.0024]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3194,  0.1498, -0.1164,  ..., -0.1677,  0.0845, -0.3350],\n",
      "        [ 0.1043, -0.1539,  0.3636,  ..., -0.9753,  0.5924,  0.4925],\n",
      "        [-0.3200, -0.3975, -0.2568,  ...,  0.1241, -0.0342, -0.1005],\n",
      "        ...,\n",
      "        [ 0.1862,  0.0144, -0.0083,  ...,  0.3408,  0.0048,  0.0720],\n",
      "        [-0.1973, -0.3941, -0.1839,  ...,  0.7902,  0.2611, -0.5196],\n",
      "        [-0.1059, -0.3034,  0.0534,  ..., -0.1847,  0.8844, -0.0708]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2586, -0.2435, -0.2226,  ...,  0.4320, -0.1773, -0.5191],\n",
      "        [-0.5717, -0.0196, -0.3647,  ...,  0.1738, -0.0963,  0.2850],\n",
      "        [-0.1439, -0.2310, -0.1683,  ...,  0.2003,  0.2258, -0.0205],\n",
      "        ...,\n",
      "        [ 0.2387, -0.3240, -0.3165,  ...,  0.0883, -0.6754, -0.0957],\n",
      "        [-0.1513, -0.1249,  0.2530,  ..., -0.1965,  0.6324,  0.1599],\n",
      "        [ 0.1055, -0.0213, -0.1738,  ...,  0.0186,  0.1196, -0.1789]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3996, -0.1762,  0.3326,  ..., -5.2208, -1.4842,  4.4721],\n",
      "        [ 0.3134, -0.1582,  0.1905,  ...,  0.4176,  0.0807,  0.0962],\n",
      "        [-0.0402,  0.1346, -0.3125,  ...,  0.4709, -0.8726,  0.7352],\n",
      "        ...,\n",
      "        [ 0.0587,  0.1257,  0.3595,  ..., -0.1855,  0.0498,  0.2075],\n",
      "        [ 0.7478, -0.3667,  0.2265,  ...,  0.7337, -0.4550, -0.1102],\n",
      "        [ 2.2795,  0.2870,  1.7499,  ..., -3.7307, -1.8441,  2.2083]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0372,  0.1174,  0.1102,  0.1259,  0.1048,  0.1390,  0.1222,  0.1268,\n",
      "         0.1002,  0.1090,  0.0926,  0.1168,  0.1052,  0.0846,  0.1111,  0.1913,\n",
      "         0.1070,  0.0914,  0.1210,  0.1042,  0.1336,  0.1211,  0.1176,  0.1183,\n",
      "         0.0977,  0.1018,  0.1088,  0.0962,  0.1136,  0.1009,  0.1093,  0.1204,\n",
      "         0.1365,  0.1023,  0.1087,  0.1201,  0.1337,  0.0814,  0.1293,  0.1194,\n",
      "         0.1263,  0.0911,  0.1257,  0.0951,  0.0972,  0.1250,  0.1278,  0.1113,\n",
      "         0.1150,  0.0769,  0.1024,  0.1259,  0.1229,  0.1216,  0.1136,  0.1095,\n",
      "         0.1224,  0.1352,  0.1152,  0.1062,  0.0997,  0.1082,  0.1072,  0.1146,\n",
      "         0.1271,  0.1347,  0.1130,  0.1205,  0.1545,  0.0910,  0.0855,  0.0894,\n",
      "         0.0343,  0.0905,  0.1322,  0.1285,  0.1098,  0.0970,  0.0967,  0.1330,\n",
      "         0.1111,  0.1237,  0.1155,  0.1180,  0.0957,  0.1139,  0.1309,  0.0878,\n",
      "         0.1207,  0.1174,  0.1046,  0.1206,  0.1227,  0.1253,  0.1167,  0.1210,\n",
      "         0.0873,  0.0572,  0.1048,  0.1010,  0.1318,  0.1138,  0.0989,  0.1237,\n",
      "         0.0944,  0.1157,  0.1184,  0.0963,  0.0845,  0.1167,  0.1132,  0.1159,\n",
      "         0.1255,  0.0861,  0.1063,  0.1033,  0.0777,  0.1147,  0.1213,  0.0936,\n",
      "         0.1273,  0.1376,  0.2096,  0.1308,  0.1052,  0.1044,  0.1030,  0.1362,\n",
      "         0.0908,  0.1084,  0.1105,  0.1307,  0.0870,  0.0971,  0.1223,  0.1166,\n",
      "         0.1291,  0.1075,  0.1214,  0.1040,  0.0888,  0.1095,  0.1049,  0.1156,\n",
      "         0.1251,  0.1082,  0.1093,  0.1050,  0.1298,  0.0778,  0.1366,  0.1148,\n",
      "         0.1109,  0.1261,  0.1316,  0.1223,  0.1097,  0.1085,  0.1134,  0.1213,\n",
      "         0.1081,  0.1058,  0.1044,  0.0825,  0.0949,  0.1358,  0.0925,  0.1367,\n",
      "         0.1121,  0.1413,  0.1212,  0.1035,  0.1063,  0.0879,  0.0903,  0.1022,\n",
      "         0.1058,  0.1023,  0.1137,  0.1038,  0.1022,  0.0934,  0.1307,  0.1024,\n",
      "         0.0976,  0.1246,  0.0849,  0.1382,  0.1039,  0.0785,  0.1152,  0.1014,\n",
      "         0.1163,  0.0881,  0.1145,  0.0868,  0.1279,  0.1171,  0.1258,  0.1033,\n",
      "         0.0832,  0.1267,  0.1231,  0.1149,  0.0864,  0.1003,  0.1279,  0.0858,\n",
      "         0.1119,  0.1485,  0.1347,  0.1205,  0.1001,  0.0975,  0.1524,  0.1380,\n",
      "         0.1464,  0.1112,  0.1410,  0.1152,  0.1012,  0.1185,  0.1076,  0.1030,\n",
      "         0.1294,  0.0940,  0.1221,  0.0990,  0.1187,  0.1197,  0.1163,  0.0797,\n",
      "         0.1079,  0.1977,  0.0994,  0.1119,  0.0976,  0.0971,  0.1335,  0.1251,\n",
      "         0.2060,  0.1030, -0.0384,  0.1064,  0.1540,  0.1281,  0.0871,  0.3303,\n",
      "         0.1148,  0.1325,  0.1280,  0.1252,  0.0916,  0.1227,  0.0891,  0.0827,\n",
      "         0.1358,  0.1122,  0.1034,  0.1036,  0.1219,  0.1084,  0.1097,  0.0964,\n",
      "         0.1390,  0.1063,  0.1206,  0.1263,  0.0772,  0.0974,  0.0956,  0.0985,\n",
      "         0.0969,  0.1166,  0.1243,  0.0419,  0.1181,  0.1128,  0.0926,  0.1126,\n",
      "         0.1104,  0.0908,  0.0940,  0.1418,  0.1143,  0.1252,  0.1098,  0.0838,\n",
      "         0.1621,  0.0928,  0.0769,  0.1166,  0.1032,  0.1154,  0.1002,  0.1022,\n",
      "         0.0985,  0.1174,  0.1186,  0.1118,  0.1249,  0.1110,  0.1129,  0.1425,\n",
      "         0.0879,  0.1437,  0.1241,  0.0964,  0.0856,  0.0940,  0.0977,  0.1095,\n",
      "         0.1367,  0.1145,  0.1417,  0.1219,  0.0958,  0.1088,  0.1199,  0.1221,\n",
      "         0.1178,  0.1050,  0.1024,  0.1212,  0.1182,  0.1002,  0.0937,  0.1039,\n",
      "         0.1195,  0.1189,  0.1050,  0.1123,  0.1054,  0.0844,  0.1247,  0.0921,\n",
      "         0.0950,  0.1153,  0.0945,  0.1106,  0.1162,  0.1171,  0.1024,  0.1216,\n",
      "         0.1089,  0.0910,  0.1269,  0.1117,  0.0969,  0.0990,  0.1324,  0.1062,\n",
      "         0.1307,  0.1408,  0.1017,  0.1210,  0.1162,  0.1083,  0.1262,  0.0881,\n",
      "         0.0966,  0.1137,  0.1270,  0.1117,  0.1222,  0.1247,  0.1132,  0.0969,\n",
      "         0.1302,  0.1075,  0.1277,  0.1013,  0.1000,  0.1257,  0.1008,  0.1026,\n",
      "         0.0984,  0.0994,  0.0925,  0.1243,  0.1204,  0.1114,  0.1090,  0.0887,\n",
      "         0.0790,  0.1330,  0.1170,  0.1057,  0.1033,  0.0675,  0.1145,  0.1181,\n",
      "         0.0987,  0.1113,  0.1455,  0.1021,  0.0375,  0.1244,  0.1123,  0.1128,\n",
      "         0.0969,  0.1058,  0.1377,  0.1074,  0.1206,  0.1230,  0.1427,  0.1230,\n",
      "         0.1314,  0.1245,  0.1085,  0.1132,  0.0799,  0.1046,  0.0398,  0.1162,\n",
      "         0.1167,  0.1055,  0.0946,  0.1421,  0.1077,  0.1155,  0.1086,  0.0967,\n",
      "         0.0937,  0.1337,  0.1083,  0.1190,  0.1704,  0.1130,  0.1322,  0.1301,\n",
      "         0.1073,  0.1075,  0.1177,  0.1113,  0.1005,  0.1110,  0.1126,  0.0933,\n",
      "         0.0820,  0.1143,  0.1103,  0.0970,  0.1217,  0.1147,  0.0985,  0.1197,\n",
      "         0.1140,  0.0995,  0.0776,  0.1109,  0.1027,  0.0978,  0.1815,  0.0985,\n",
      "         0.0259,  0.1032,  0.0970,  0.1005,  0.0997,  0.1145,  0.1112,  0.0894,\n",
      "         0.1040,  0.1168,  0.0862,  0.1201,  0.1203,  0.1233,  0.1166,  0.1163,\n",
      "         0.1470,  0.0348,  0.0937,  0.1665,  0.1276,  0.1037,  0.0751,  0.0837,\n",
      "         0.1187,  0.1172,  0.1177,  0.1162,  0.0952,  0.1159,  0.1293,  0.1149,\n",
      "         0.1224,  0.1332,  0.0874,  0.0859,  0.1144,  0.1531,  0.1290,  0.1009,\n",
      "         0.1139,  0.1325,  0.1072,  0.1258,  0.1172,  0.1124,  0.1153,  0.0925,\n",
      "         0.1179,  0.1111,  0.1075,  0.1021,  0.1322,  0.1196,  0.0997, -0.0417],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0320,  0.0524, -0.4241,  ...,  0.1931,  0.6028,  0.2512],\n",
      "        [-0.2055, -0.4373, -0.0464,  ...,  0.6802, -0.1671,  0.2270],\n",
      "        [-0.6603, -0.2868, -0.0412,  ..., -0.2835,  0.1432,  0.6267],\n",
      "        ...,\n",
      "        [-0.0831,  0.0285, -0.2285,  ..., -0.4847, -0.4197,  0.0935],\n",
      "        [-0.3282,  0.2082, -0.4506,  ..., -0.6525, -0.1981,  0.1427],\n",
      "        [-0.1275,  0.5001,  0.5747,  ...,  0.3738,  0.9640,  0.3380]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0248, -0.0747,  0.9070,  ...,  0.5682,  0.0094, -0.2541],\n",
      "        [ 0.0843,  0.2868,  0.6553,  ...,  0.7500,  0.7257, -0.3317],\n",
      "        [ 0.0601,  0.4145,  0.2318,  ..., -0.9981, -0.2426, -0.0580],\n",
      "        ...,\n",
      "        [-0.5342,  0.8626, -1.5218,  ...,  0.3843,  0.3969,  0.4220],\n",
      "        [-0.9320,  0.5345,  0.2681,  ...,  0.3420,  0.8905,  0.4298],\n",
      "        [-0.3126,  0.6676, -0.8485,  ..., -1.1989,  0.7015,  0.4605]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3453,  0.3738,  0.9872,  ..., -0.0118, -2.3717, -1.1488],\n",
      "        [-0.2984, -0.2747, -0.0913,  ..., -0.9439, -0.6280, -0.1459],\n",
      "        [-0.3574, -0.9705, -0.2431,  ..., -0.2306,  0.7049,  0.0826],\n",
      "        ...,\n",
      "        [ 0.0903, -0.0155,  0.6507,  ...,  0.2866,  0.3926, -0.0075],\n",
      "        [-0.2775, -0.5157, -0.0690,  ...,  0.0400,  0.0498, -1.4676],\n",
      "        [-1.6398,  1.1564,  0.8943,  ..., -2.3675, -2.7083, -1.2798]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0181,  0.1241,  0.1061,  0.1167,  0.1222,  0.1186,  0.1097,  0.1147,\n",
      "         0.1014,  0.1277,  0.1207,  0.1068,  0.0911,  0.0882,  0.0927,  0.1128,\n",
      "         0.1012,  0.0904,  0.1038,  0.0883,  0.1046,  0.0945,  0.1036,  0.1133,\n",
      "         0.1062,  0.1044,  0.0999,  0.1139,  0.1152,  0.1058,  0.1039,  0.1275,\n",
      "         0.1158,  0.0948,  0.1027,  0.0911,  0.1085,  0.1173,  0.1020,  0.0879,\n",
      "         0.1202,  0.0906,  0.1104,  0.0815,  0.0884,  0.1357,  0.1280,  0.1067,\n",
      "         0.0916,  0.0975,  0.1010,  0.1361,  0.0973,  0.1162,  0.0956,  0.1074,\n",
      "         0.1224,  0.1244,  0.0909,  0.1019,  0.1082,  0.0957,  0.0940,  0.1154,\n",
      "         0.1152,  0.1110,  0.0936,  0.0764,  0.1104,  0.1179,  0.0410,  0.0937,\n",
      "         0.0239,  0.1012,  0.1259,  0.0892,  0.0930,  0.0999,  0.0876,  0.0931,\n",
      "         0.0932,  0.1049,  0.1061,  0.1190,  0.1068,  0.1091,  0.0929,  0.1160,\n",
      "         0.1022,  0.1124,  0.0987,  0.1011,  0.0887,  0.1154,  0.1114,  0.0945,\n",
      "         0.0973,  0.0543,  0.1112,  0.1031,  0.1273,  0.1080,  0.0903,  0.0845,\n",
      "         0.0913,  0.0968,  0.0948,  0.1193,  0.0848,  0.1030,  0.0944,  0.1169,\n",
      "         0.1216,  0.0909,  0.1074,  0.1033,  0.0839,  0.1065,  0.1200,  0.1079,\n",
      "         0.1054,  0.1166,  0.2471,  0.1230,  0.0994,  0.1033,  0.0930,  0.1084,\n",
      "         0.0817,  0.0869,  0.1137,  0.1173,  0.1075,  0.1066,  0.1096,  0.1075,\n",
      "         0.1121,  0.1034,  0.1073,  0.1135,  0.0873,  0.1166,  0.1038,  0.1154,\n",
      "         0.1103,  0.0965,  0.1121,  0.0971,  0.1090,  0.0985,  0.1086,  0.1235,\n",
      "         0.1047,  0.1282,  0.0897,  0.1207,  0.0822,  0.1213,  0.1127,  0.1189,\n",
      "         0.0926,  0.1003,  0.0898,  0.0705,  0.1105,  0.1143,  0.1127,  0.1184,\n",
      "         0.1190,  0.1159,  0.1057,  0.1031,  0.0912,  0.0988,  0.1012,  0.1208,\n",
      "         0.0946,  0.0717,  0.1128,  0.1005,  0.1125,  0.0935,  0.1256,  0.0899,\n",
      "         0.1022,  0.0817,  0.1104,  0.1495,  0.1237,  0.0756,  0.0995,  0.1106,\n",
      "         0.0866,  0.0748,  0.1082,  0.0910,  0.1259,  0.0998,  0.1224,  0.1009,\n",
      "         0.0995,  0.1275,  0.0967,  0.1149,  0.0726,  0.0869,  0.1157,  0.0903,\n",
      "         0.1201,  0.1288,  0.1101,  0.1074,  0.1068,  0.1084,  0.1078,  0.1363,\n",
      "         0.1037,  0.1101,  0.0847,  0.1116,  0.0963,  0.0832,  0.1233,  0.1116,\n",
      "         0.1111,  0.0938,  0.1192,  0.0879,  0.1163,  0.1289,  0.0970,  0.0722,\n",
      "         0.0946,  0.1871,  0.1060,  0.0855,  0.0988,  0.1221,  0.1080,  0.1186,\n",
      "         0.1037,  0.0965,  0.0370,  0.1032,  0.1090,  0.1096,  0.0936,  0.2642,\n",
      "         0.0952,  0.1180,  0.1323,  0.1038,  0.1055,  0.0953,  0.0906,  0.0825,\n",
      "         0.1127,  0.1225,  0.1001,  0.1018,  0.1071,  0.1059,  0.1087,  0.0920,\n",
      "         0.1258,  0.0904,  0.1032,  0.1171,  0.0730,  0.0953,  0.1024,  0.1050,\n",
      "         0.1105,  0.1113,  0.0993,  0.0207,  0.0881,  0.1172,  0.1130,  0.0942,\n",
      "         0.1063,  0.1029,  0.0992,  0.0948,  0.1225,  0.1031,  0.1071,  0.0865,\n",
      "         0.1316,  0.0972,  0.1117,  0.1004,  0.1171,  0.0977,  0.1114,  0.1175,\n",
      "         0.1004,  0.0916,  0.1171,  0.1040,  0.1083,  0.1080,  0.1214,  0.1215,\n",
      "         0.0834,  0.1431,  0.1150,  0.1009,  0.0862,  0.0889,  0.0793,  0.1107,\n",
      "         0.1109,  0.1103,  0.1098,  0.1238,  0.0960,  0.1071,  0.1229,  0.1022,\n",
      "         0.1145,  0.0995,  0.0917,  0.1307,  0.0909,  0.1066,  0.0938,  0.1040,\n",
      "         0.1257,  0.0818,  0.1026,  0.1136,  0.1167,  0.0727,  0.1069,  0.0823,\n",
      "         0.0840,  0.1223,  0.1058,  0.1248,  0.1277,  0.1028,  0.1039,  0.1142,\n",
      "         0.1009,  0.1097,  0.1112,  0.1194,  0.1142,  0.0910,  0.1117,  0.0999,\n",
      "         0.0780,  0.1128,  0.0955,  0.1014,  0.1214,  0.1057,  0.1210,  0.0986,\n",
      "         0.1176,  0.1213,  0.1187,  0.0895,  0.0841,  0.1099,  0.1154,  0.0426,\n",
      "         0.1089,  0.1009,  0.1331,  0.1054,  0.1098,  0.0941,  0.0916,  0.1188,\n",
      "         0.1069,  0.1082,  0.1016,  0.1091,  0.1273,  0.1106,  0.0978,  0.1009,\n",
      "         0.1087,  0.1040,  0.1174,  0.0895,  0.0835,  0.0571,  0.1025,  0.0913,\n",
      "         0.1177,  0.1147,  0.1323,  0.1043,  0.0123,  0.1036,  0.1143,  0.1010,\n",
      "         0.0859,  0.0955,  0.1299,  0.0894,  0.0951,  0.1029,  0.1123,  0.1230,\n",
      "         0.1207,  0.1051,  0.1001,  0.0991,  0.0674,  0.0938,  0.0491,  0.1034,\n",
      "         0.0949,  0.0883,  0.0867,  0.1361,  0.1025,  0.0989,  0.0969,  0.0831,\n",
      "         0.0730,  0.0971,  0.0905,  0.1027,  0.0349,  0.0937,  0.0772,  0.1296,\n",
      "         0.0890,  0.1081,  0.0992,  0.1082,  0.0957,  0.1067,  0.0984,  0.1026,\n",
      "         0.0960,  0.1028,  0.0856,  0.1008,  0.1127,  0.1167,  0.0929,  0.1402,\n",
      "         0.1127,  0.0963,  0.0882,  0.1164,  0.1171,  0.1049,  0.1358,  0.0946,\n",
      "         0.0268,  0.0885,  0.0953,  0.0859,  0.0979,  0.0921,  0.1071,  0.0823,\n",
      "         0.1021,  0.0968,  0.0794,  0.0868,  0.1118,  0.1307,  0.0874,  0.1058,\n",
      "         0.1136,  0.0201,  0.0881,  0.1309,  0.1233,  0.1002,  0.0812,  0.1151,\n",
      "         0.1136,  0.0910,  0.1158,  0.1012,  0.0914,  0.1157,  0.0981,  0.1131,\n",
      "         0.1284,  0.1167,  0.1024,  0.1042,  0.1008,  0.1331,  0.1038,  0.0974,\n",
      "         0.1162,  0.1025,  0.1007,  0.1149,  0.0876,  0.0982,  0.0985,  0.1018,\n",
      "         0.0826,  0.0973,  0.1080,  0.0981,  0.0908,  0.1204,  0.1013,  0.0193],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1055,  0.0111, -0.0065,  ...,  0.0414,  0.0545, -0.0577],\n",
      "        [-0.0689,  0.0303,  0.0175,  ...,  0.0328, -0.0511, -0.0183],\n",
      "        [-0.0370, -0.0459, -0.0083,  ...,  0.0136,  0.0262, -0.0188],\n",
      "        ...,\n",
      "        [-0.0258, -0.0204, -0.0352,  ...,  0.0165, -0.0188, -0.0459],\n",
      "        [ 0.0957,  0.0193,  0.0667,  ...,  0.0080,  0.0078,  0.0983],\n",
      "        [ 0.0504,  0.0210,  0.0046,  ..., -0.0160, -0.0245,  0.0096]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4741, -0.3493,  0.5883,  ...,  0.0101,  0.2999,  0.4321],\n",
      "        [ 0.3512, -0.0603,  0.1353,  ..., -0.0410, -0.8615,  0.1769],\n",
      "        [-0.0615,  0.0768,  0.1717,  ...,  0.3327, -0.1291, -0.1954],\n",
      "        ...,\n",
      "        [-0.0343, -0.1721,  0.1146,  ...,  0.0593, -0.2444, -0.1935],\n",
      "        [-0.0958, -0.2090, -0.4365,  ...,  0.1233, -0.2226, -0.3958],\n",
      "        [-0.3691, -0.0233,  0.3271,  ..., -0.2713,  0.0457, -0.2753]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0140,  0.8021,  0.2758,  ...,  0.3275, -0.0707, -0.0445],\n",
      "        [-0.2156, -0.6959,  0.2067,  ..., -0.0977,  0.2931,  0.0885],\n",
      "        [ 0.1217, -0.2810,  0.1221,  ...,  0.0472,  0.5414,  0.1574],\n",
      "        ...,\n",
      "        [ 0.2840,  0.0636, -0.0620,  ..., -0.1272,  0.3597, -0.0113],\n",
      "        [ 0.1364, -0.3953, -0.2593,  ...,  0.2956, -0.7651, -0.0422],\n",
      "        [ 0.2846, -0.3120,  0.3476,  ..., -0.1944,  0.2298,  0.0476]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5631,  0.9246,  0.5516,  ...,  0.4426,  1.9550, -1.7098],\n",
      "        [ 0.4946,  0.2098, -0.7010,  ...,  0.4789,  0.1548, -0.2334],\n",
      "        [-0.1940, -0.3869, -0.2380,  ...,  0.9393, -0.4814, -0.0895],\n",
      "        ...,\n",
      "        [ 0.2441,  0.0735, -0.0502,  ...,  0.2511,  1.3505,  0.1931],\n",
      "        [ 0.2820,  0.4709, -0.2533,  ..., -0.0674, -0.3646,  0.0610],\n",
      "        [ 0.1486,  1.8368,  1.7657,  ...,  0.4364,  2.8693, -0.7114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0247,  0.2744,  0.2504,  0.2103,  0.2545,  0.2260,  0.2371,  0.1921,\n",
      "         0.1383,  0.1949,  0.1620,  0.1556,  0.1810,  0.1529,  0.1521,  0.2338,\n",
      "         0.1639,  0.1293,  0.1756,  0.1634,  0.2351,  0.1491,  0.1811,  0.3032,\n",
      "         0.1989,  0.1907,  0.1469,  0.1826,  0.2058,  0.2483,  0.2032,  0.2598,\n",
      "         0.2083,  0.1868,  0.2168,  0.2865,  0.2128,  0.1846,  0.1918,  0.1528,\n",
      "         0.2388,  0.1277,  0.1861,  0.1399,  0.1353,  0.2359,  0.2460,  0.2097,\n",
      "         0.1754,  0.1988,  0.1697,  0.2018,  0.2187,  0.1619,  0.1867,  0.1696,\n",
      "         0.1539,  0.2234,  0.1340,  0.2109,  0.1881,  0.1570,  0.1720,  0.2282,\n",
      "         0.2220,  0.2034,  0.1904,  0.1355,  0.1819,  0.1830,  0.0946,  0.1474,\n",
      "         0.0361,  0.1744,  0.1914,  0.1986,  0.2190,  0.2137,  0.5389,  0.1928,\n",
      "         0.1926,  0.1599,  0.2187,  0.1568,  0.1427,  0.1193,  0.1573,  0.1898,\n",
      "         0.1652,  0.1842,  0.1699,  0.2240,  0.1277,  0.2016,  0.1561,  0.1884,\n",
      "         0.1620,  0.0866,  0.1505,  0.2136,  0.2283,  0.3587,  0.1537,  0.1597,\n",
      "         0.1422,  0.1727,  0.1536,  0.1862,  0.1712,  0.1757,  0.1721,  0.1531,\n",
      "         0.2054,  0.1503,  0.2022,  0.1852,  0.1552,  0.1853,  0.1984,  0.1700,\n",
      "         0.1894,  0.2217,  0.0349,  0.2039,  0.1830,  0.1874,  0.1731,  0.1526,\n",
      "         0.1416,  0.1706,  0.1659,  0.2628,  0.1518,  0.1962,  0.1835,  0.1736,\n",
      "         0.2118,  0.1956,  0.2546,  0.1955,  0.1315,  0.1635,  0.1690,  0.2169,\n",
      "         0.1906,  0.1772,  0.1757,  0.2007,  0.2096,  0.1627,  0.2339,  0.2054,\n",
      "         0.2187,  0.2387,  0.1378,  0.1768,  0.2794,  0.3314,  0.2129,  0.2741,\n",
      "         0.1632,  0.1749,  0.1767,  0.1199,  0.2226,  0.4102,  0.1689,  0.1860,\n",
      "         0.1682,  0.2092,  0.1674,  0.1916,  0.1564,  0.1541,  0.1377,  0.1616,\n",
      "         0.1759,  0.1284,  0.1520,  0.2801,  0.1741,  0.1961,  0.2075,  0.1935,\n",
      "         0.1656,  0.1937,  0.1209,  0.2757,  0.2001,  0.1270,  0.1898,  0.1933,\n",
      "         0.1838,  0.1062,  0.1757,  0.1477,  0.2278,  0.2755,  0.2653,  0.1649,\n",
      "         0.1442,  0.1965,  0.1828,  0.1741,  0.1211,  0.1512,  0.1877,  0.2085,\n",
      "         0.1661,  0.4062,  0.3586,  0.2166,  0.1528,  0.2450,  0.2572,  0.2072,\n",
      "         0.2298,  0.2109,  0.3201,  0.2018,  0.2100,  0.2297,  0.2470,  0.1644,\n",
      "         0.1469,  0.1649,  0.2497,  0.1831,  0.3430,  0.2571,  0.1514,  0.1240,\n",
      "         0.1755,  0.0479,  0.1935,  0.1474,  0.1338,  0.1621,  0.2130,  0.1730,\n",
      "         0.2988,  0.1876,  0.0749,  0.1708,  0.2306,  0.2121,  0.1303, -0.0605,\n",
      "         0.1842,  0.2059,  0.3354,  0.1894,  0.1835,  0.1567,  0.1733,  0.1240,\n",
      "         0.2826,  0.2231,  0.1683,  0.1717,  0.2125,  0.1311,  0.1872,  0.1722,\n",
      "         0.1935,  0.1428,  0.2037,  0.2216,  0.1125,  0.1923,  0.1711,  0.1952,\n",
      "         0.1522,  0.1789,  0.1395,  0.0284,  0.1758,  0.1676,  0.1877,  0.1760,\n",
      "         0.0468,  0.1972,  0.1426,  0.2039,  0.2370,  0.1757,  0.1672,  0.2035,\n",
      "         0.2580,  0.1566,  0.2624,  0.1916,  0.1547,  0.1668,  0.2045,  0.1525,\n",
      "         0.1971,  0.2200,  0.2068,  0.1943,  0.3243,  0.2074,  0.2326,  0.2369,\n",
      "         0.1554,  0.2164,  0.1747,  0.1690,  0.1446,  0.1815,  0.1301,  0.1813,\n",
      "         0.1636,  0.1893,  0.1838,  0.2661,  0.1354,  0.1557,  0.1719,  0.1841,\n",
      "         0.1948,  0.1588,  0.2067,  0.2327,  0.1591,  0.1273,  0.1761,  0.1802,\n",
      "         0.2033,  0.1102,  0.1917,  0.2059,  0.2001,  0.1772,  0.2268,  0.1517,\n",
      "         0.1667,  0.2468,  0.1528,  0.1993,  0.1913,  0.2316,  0.1518,  0.2581,\n",
      "         0.1404,  0.1882,  0.2763,  0.2454,  0.2111,  0.1692,  0.1856,  0.1661,\n",
      "         0.2062,  0.1897,  0.1653,  0.2076,  0.1678,  0.2189,  0.1874,  0.1488,\n",
      "         0.2166,  0.1616,  0.2460,  0.1546,  0.1571,  0.1850,  0.1629,  0.0597,\n",
      "         0.1851,  0.1786,  0.2274,  0.1930,  0.1687,  0.1771,  0.1638,  0.2032,\n",
      "         0.1495,  0.1507,  0.1644,  0.1712,  0.1865,  0.1937,  0.2011,  0.1794,\n",
      "         0.1654,  0.1797,  0.2214,  0.1479,  0.1468, -0.0694,  0.2047,  0.1384,\n",
      "         0.2367,  0.2606,  0.2233,  0.1765, -0.0265,  0.1975,  0.2518,  0.1950,\n",
      "         0.1712,  0.2102,  0.1888,  0.1659,  0.1858,  0.1649,  0.2202,  0.2463,\n",
      "         0.1978,  0.2183,  0.1678,  0.1442,  0.1257,  0.1616,  0.0955,  0.1988,\n",
      "         0.1764,  0.1339,  0.1858,  0.1752,  0.1594,  0.1516,  0.1559,  0.1672,\n",
      "         0.1293,  0.2272,  0.1919,  0.2014,  0.0942,  0.2036,  0.1338,  0.2167,\n",
      "         0.1542,  0.1791,  0.2151,  0.1649,  0.1637,  0.1482,  0.1620,  0.1527,\n",
      "         0.1646,  0.2441,  0.1302,  0.1455,  0.1976,  0.1612,  0.2326,  0.2138,\n",
      "         0.1762,  0.1812,  0.1559,  0.1805,  0.1930,  0.2073,  0.2967,  0.1668,\n",
      "         0.0295,  0.1982,  0.1662,  0.1584,  0.2250,  0.1495,  0.1352,  0.2036,\n",
      "         0.1649,  0.1955,  0.1426,  0.1653,  0.2245,  0.2578,  0.1636,  0.1540,\n",
      "         0.2728, -0.0295,  0.1755,  0.2498,  0.3218,  0.1029,  0.1360,  0.0476,\n",
      "         0.2415,  0.1858,  0.2629,  0.1705,  0.1605,  0.2562,  0.1737,  0.1937,\n",
      "         0.2383,  0.1237,  0.1401,  0.1487,  0.1704,  0.3016,  0.1744,  0.1650,\n",
      "         0.1964,  0.1875,  0.1993,  0.1798,  0.1386,  0.1904,  0.2044,  0.1544,\n",
      "         0.1387,  0.1582,  0.1766,  0.1507,  0.1787,  0.2523,  0.1713,  0.0345],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0103, -0.0478, -0.0255,  ..., -0.1017, -0.0507,  0.0041],\n",
      "        [ 0.0425,  0.0893,  0.0686,  ..., -0.0715,  0.0594, -0.0175],\n",
      "        [ 0.0284,  0.0518,  0.0477,  ..., -0.0254, -0.0071, -0.0160],\n",
      "        ...,\n",
      "        [ 0.0121,  0.0113, -0.0430,  ..., -0.0232, -0.0775,  0.0060],\n",
      "        [ 0.0212,  0.0094,  0.0528,  ...,  0.1028, -0.0834,  0.0215],\n",
      "        [ 0.0054,  0.0589, -0.0036,  ...,  0.0205,  0.0608,  0.0484]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0136,  0.1684, -0.5010,  ...,  0.0672,  0.4551, -0.5367],\n",
      "        [-0.1220,  0.5154, -0.5459,  ...,  0.1926, -0.1955,  1.0111],\n",
      "        [-0.4797, -0.1001,  0.0590,  ...,  0.1136, -0.3485,  0.0761],\n",
      "        ...,\n",
      "        [ 0.0184, -0.6661,  0.1271,  ...,  0.1164,  0.2321,  0.0195],\n",
      "        [-0.0258, -0.2062,  0.2282,  ...,  0.2009,  0.2859, -0.1038],\n",
      "        [ 0.2866,  0.0393, -0.0922,  ...,  0.2750,  0.3324,  0.2960]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.7242, -0.7682,  0.5900,  ...,  0.9890,  0.5210, -0.0980],\n",
      "        [ 0.2600,  0.9223,  0.3029,  ..., -0.0950,  0.3179,  0.3046],\n",
      "        [ 0.6291, -0.7508, -0.2938,  ..., -0.3325, -0.5012,  0.6701],\n",
      "        ...,\n",
      "        [ 0.1479, -0.6524,  0.7594,  ..., -1.1751, -0.0386,  0.1119],\n",
      "        [-0.1139,  0.3870,  0.3619,  ..., -0.1765,  0.2952, -0.0616],\n",
      "        [ 0.5156, -0.0332, -0.8217,  ...,  0.1144,  0.9336, -0.1793]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6857,  1.1051,  0.4852,  ...,  3.6646,  2.1642,  1.0591],\n",
      "        [ 0.7102,  0.2520, -0.4167,  ..., -0.4147, -0.3807, -0.1541],\n",
      "        [ 0.3789,  0.3677,  0.3496,  ..., -0.4846,  0.0140,  0.6507],\n",
      "        ...,\n",
      "        [ 0.3383, -0.5714,  0.1787,  ..., -0.1092, -0.9490,  0.3045],\n",
      "        [-0.0077,  1.1030, -0.3845,  ..., -0.6736, -0.1564,  0.5101],\n",
      "        [-0.5144, -0.3225,  1.4274,  ...,  3.8576,  3.7626,  2.4353]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0672,  0.1607,  0.1423,  0.1395,  0.1672,  0.1619,  0.1510,  0.1228,\n",
      "         0.1267,  0.1680,  0.1430,  0.1467,  0.1076,  0.1131,  0.1373,  0.1907,\n",
      "         0.1563,  0.1120,  0.1680,  0.1582,  0.1455,  0.1044,  0.1309,  0.1412,\n",
      "         0.1205,  0.1183,  0.1378,  0.1193,  0.1735,  0.1489,  0.1253,  0.1791,\n",
      "         0.1572,  0.1290,  0.1604,  0.1521,  0.1336,  0.1420,  0.1502,  0.1293,\n",
      "         0.1339,  0.1135,  0.1577,  0.1272,  0.1279,  0.1464,  0.1261,  0.1464,\n",
      "         0.1278,  0.1281,  0.1162,  0.1655,  0.1534,  0.1383,  0.1326,  0.1364,\n",
      "         0.1587,  0.1687,  0.1280,  0.1399,  0.1367,  0.1111,  0.1186,  0.1275,\n",
      "         0.1867,  0.1525,  0.1112,  0.1629,  0.1873,  0.1347,  0.0987,  0.1174,\n",
      "        -0.0254,  0.1148,  0.1609,  0.1538,  0.1606,  0.1194,  0.1221,  0.1545,\n",
      "         0.1206,  0.1494,  0.1381,  0.1556,  0.1352,  0.1225,  0.1520,  0.1464,\n",
      "         0.1433,  0.1649,  0.1446,  0.1520,  0.1476,  0.1475,  0.1184,  0.1199,\n",
      "         0.1354,  0.0703,  0.1441,  0.1360,  0.1450,  0.1560,  0.0968,  0.1533,\n",
      "         0.1323,  0.1443,  0.1412,  0.1216,  0.1166,  0.1350,  0.1453,  0.1317,\n",
      "         0.1596,  0.1082,  0.1696,  0.1249,  0.1217,  0.1649,  0.1421,  0.1437,\n",
      "         0.1370,  0.1772,  0.3709,  0.1981,  0.1263,  0.1310,  0.1295,  0.1594,\n",
      "         0.1285,  0.1429,  0.1355,  0.1492,  0.1198,  0.1293,  0.1335,  0.1235,\n",
      "         0.1800,  0.1472,  0.1382,  0.1330,  0.1399,  0.1223,  0.1216,  0.1470,\n",
      "         0.1483,  0.1365,  0.1354,  0.1357,  0.1259,  0.1162,  0.1420,  0.1704,\n",
      "         0.1618,  0.1463,  0.1506,  0.1789,  0.1338,  0.0961,  0.1519,  0.1636,\n",
      "         0.1431,  0.1244,  0.1285,  0.1168,  0.1374,  0.1755,  0.1269,  0.1380,\n",
      "         0.1466,  0.1701,  0.1368,  0.1486,  0.1289,  0.1120,  0.1376,  0.1233,\n",
      "         0.1160,  0.1355,  0.1233,  0.1164,  0.1094,  0.1386,  0.1672,  0.1263,\n",
      "         0.1302,  0.1411,  0.1371,  0.1662,  0.1483,  0.1336,  0.1330,  0.1249,\n",
      "         0.1776,  0.1478,  0.1195,  0.1014,  0.1611,  0.1771,  0.1406,  0.1405,\n",
      "         0.1235,  0.1411,  0.1349,  0.1371,  0.1239,  0.1436,  0.1588,  0.1212,\n",
      "         0.1187,  0.1705,  0.1856,  0.1249,  0.1357,  0.1323,  0.1828,  0.1707,\n",
      "         0.1281,  0.1521,  0.1851,  0.1642,  0.1186,  0.1314,  0.1458,  0.1110,\n",
      "         0.1399,  0.1329,  0.1561,  0.1434,  0.1363,  0.1633,  0.1421, -0.0965,\n",
      "         0.1493,  0.2853,  0.1406,  0.1331,  0.1340,  0.1689,  0.1506,  0.1368,\n",
      "         0.2381,  0.1255, -0.0597,  0.1203,  0.1667,  0.1550,  0.1253,  0.3971,\n",
      "         0.1309,  0.1548,  0.1774,  0.1508,  0.1395,  0.1397,  0.1229,  0.1357,\n",
      "         0.1397,  0.1312,  0.1352,  0.1266,  0.1640,  0.1253,  0.1564,  0.1133,\n",
      "         0.1533,  0.1380,  0.1623,  0.1715,  0.1225,  0.1127,  0.1415,  0.1362,\n",
      "         0.1339,  0.1612,  0.1369,  0.0373,  0.1483,  0.1333,  0.1369,  0.1355,\n",
      "         0.1361,  0.1410,  0.1201,  0.1542,  0.1543,  0.1276,  0.1428,  0.1323,\n",
      "         0.1706,  0.1314,  0.1520,  0.1307,  0.1404,  0.1585,  0.1182,  0.1482,\n",
      "         0.1442,  0.1161,  0.1527,  0.1236,  0.1560,  0.1504,  0.1517,  0.1630,\n",
      "         0.1273,  0.1706,  0.1454,  0.1240,  0.1406,  0.1693,  0.0861,  0.1275,\n",
      "         0.1449,  0.1272,  0.1550,  0.1747,  0.1333,  0.1587,  0.1670,  0.1439,\n",
      "         0.1612,  0.1399,  0.1493,  0.1703,  0.1460,  0.1327,  0.1248,  0.1349,\n",
      "         0.1528,  0.1299,  0.1249,  0.1344,  0.1566,  0.1244,  0.1671,  0.1311,\n",
      "         0.1225,  0.1909,  0.1207,  0.1571,  0.1708,  0.1780,  0.1099,  0.1479,\n",
      "         0.1423,  0.1283,  0.1377,  0.1209,  0.1347,  0.1296,  0.1124,  0.1766,\n",
      "         0.1463,  0.1562,  0.1445,  0.1577,  0.1340,  0.1255,  0.1255,  0.1295,\n",
      "         0.1360,  0.1243,  0.1519,  0.0955,  0.1502,  0.1510,  0.1285,  0.1106,\n",
      "         0.1889,  0.1486,  0.1690,  0.1465,  0.1176,  0.1835,  0.1323,  0.1273,\n",
      "         0.1258,  0.1137,  0.1357,  0.1436,  0.1669,  0.1532,  0.1434,  0.1381,\n",
      "         0.1262,  0.1483,  0.1779,  0.1531,  0.1137,  0.0693,  0.1113,  0.1354,\n",
      "         0.1506,  0.1348,  0.1464,  0.1581, -0.0401,  0.1448,  0.1493,  0.1282,\n",
      "         0.1137,  0.1553,  0.1794,  0.1559,  0.1450,  0.1447,  0.1521,  0.1826,\n",
      "         0.1585,  0.1584,  0.1214,  0.1302,  0.0886,  0.1175,  0.0734,  0.1711,\n",
      "         0.1371,  0.1104,  0.1269,  0.1469,  0.1358,  0.1324,  0.1591,  0.1284,\n",
      "         0.1198,  0.1548,  0.1396,  0.1331,  0.2851,  0.1421,  0.1390,  0.1435,\n",
      "         0.1009,  0.1233,  0.1265,  0.1499,  0.1163,  0.1685,  0.1409,  0.1556,\n",
      "         0.1336,  0.1131,  0.1487,  0.1135,  0.1519,  0.1187,  0.1224,  0.1694,\n",
      "         0.1755,  0.1051,  0.1307,  0.1419,  0.1494,  0.1525,  0.1755,  0.1373,\n",
      "        -0.0239,  0.1718,  0.1263,  0.1347,  0.1417,  0.1203,  0.1149,  0.1157,\n",
      "         0.1410,  0.1369,  0.1079,  0.1517,  0.1530,  0.1282,  0.1276,  0.1128,\n",
      "         0.1808,  0.0762,  0.1167,  0.1662,  0.1674,  0.1329,  0.1070,  0.1030,\n",
      "         0.1241,  0.1210,  0.1880,  0.1217,  0.1094,  0.1402,  0.1290,  0.1389,\n",
      "         0.1603,  0.1790,  0.1146,  0.1387,  0.1583,  0.1613,  0.1284,  0.1094,\n",
      "         0.1556,  0.1658,  0.1346,  0.1992,  0.1623,  0.1478,  0.1409,  0.1187,\n",
      "         0.1180,  0.1497,  0.1391,  0.1118,  0.1366,  0.1546,  0.1423, -0.0299],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0011, -0.5595,  0.0584,  ...,  0.2560,  0.0090,  0.0767],\n",
      "        [ 0.1053,  0.1368, -0.5947,  ...,  0.1867,  0.1318,  0.1909],\n",
      "        [ 0.0945, -0.1289, -0.0152,  ...,  0.4174, -0.5025, -0.0697],\n",
      "        ...,\n",
      "        [ 0.0435,  0.1415,  0.4770,  ..., -0.5994, -0.3817,  0.1731],\n",
      "        [ 0.3933, -0.5649,  0.7641,  ..., -0.5423, -0.3654,  0.8877],\n",
      "        [-0.0149,  0.1522, -0.0558,  ...,  0.2796, -0.0881,  0.0787]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.0328e-01,  1.0678e+00,  1.2657e+00,  ..., -1.1769e-01,\n",
      "         -6.7984e-01,  2.1003e-04],\n",
      "        [-3.6706e-01, -4.8762e-01,  1.3656e+00,  ...,  7.6040e-01,\n",
      "         -2.1498e-01, -8.8958e-02],\n",
      "        [ 9.4744e-01, -6.3359e-01,  1.2847e-01,  ..., -1.4081e+00,\n",
      "         -3.8662e-01,  9.3733e-01],\n",
      "        ...,\n",
      "        [ 4.2038e-01,  9.3170e-01,  4.4365e-02,  ..., -3.9618e-01,\n",
      "          1.2292e+00, -5.0436e-02],\n",
      "        [-2.0558e-01, -8.3862e-01, -6.0309e-02,  ..., -2.6039e-01,\n",
      "         -7.0753e-01, -2.4387e-01],\n",
      "        [ 1.1524e-01,  1.7014e-01,  1.4353e+00,  ..., -5.8516e-01,\n",
      "          8.3369e-01,  3.2646e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.0297, -0.4154, -0.7922,  ..., -1.6048,  0.6257, -1.1314],\n",
      "        [ 0.5422, -0.3385,  0.0624,  ...,  0.6909,  0.4371, -0.7233],\n",
      "        [ 0.1233,  0.1399, -0.4730,  ...,  0.0715, -0.0218,  1.6268],\n",
      "        ...,\n",
      "        [ 0.7545,  0.7113, -0.6059,  ..., -0.3921,  0.3945,  0.3259],\n",
      "        [ 0.4692, -0.4477,  0.7820,  ...,  1.0258, -0.4584,  0.3419],\n",
      "        [-1.6563,  2.4570, -5.7564,  ..., -1.5266,  2.2579,  0.1092]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0244,  0.1339,  0.1342,  0.1234,  0.1551,  0.1443,  0.1329,  0.1261,\n",
      "         0.1262,  0.1398,  0.1272,  0.1164,  0.1185,  0.1262,  0.1140,  0.1252,\n",
      "         0.1142,  0.1165,  0.1227,  0.1165,  0.1386,  0.1129,  0.1255,  0.1378,\n",
      "         0.1234,  0.1223,  0.1255,  0.1396,  0.1086,  0.1259,  0.1211,  0.1465,\n",
      "         0.1354,  0.1311,  0.1284,  0.1187,  0.1264,  0.1318,  0.1291,  0.1145,\n",
      "         0.1381,  0.1084,  0.1307,  0.1054,  0.1287,  0.1516,  0.1403,  0.1417,\n",
      "         0.1277,  0.1244,  0.1225,  0.1525,  0.1306,  0.1210,  0.1266,  0.1151,\n",
      "         0.1375,  0.1352,  0.1160,  0.1564,  0.1203,  0.1147,  0.1304,  0.1414,\n",
      "         0.1480,  0.1405,  0.1365,  0.1038,  0.1471,  0.1227,  0.0613,  0.1227,\n",
      "         0.0185,  0.1268,  0.1437,  0.1200,  0.1219,  0.1362,  0.1156,  0.1271,\n",
      "         0.1098,  0.1240,  0.1412,  0.1436,  0.1267,  0.1157,  0.1283,  0.1505,\n",
      "         0.1352,  0.1296,  0.1297,  0.1415,  0.1127,  0.1385,  0.1216,  0.1321,\n",
      "         0.1071,  0.0639,  0.1224,  0.1336,  0.1740,  0.1351,  0.1192,  0.1085,\n",
      "         0.1224,  0.1250,  0.1345,  0.1219,  0.1028,  0.1050,  0.1237,  0.1308,\n",
      "         0.1388,  0.1155,  0.1311,  0.1441,  0.1089,  0.1387,  0.1291,  0.1321,\n",
      "         0.1438,  0.1564,  0.2017,  0.1518,  0.1364,  0.1153,  0.1310,  0.1304,\n",
      "         0.1208,  0.1101,  0.1357,  0.1343,  0.1171,  0.1264,  0.1401,  0.1337,\n",
      "         0.1333,  0.1260,  0.1261,  0.1281,  0.1189,  0.1233,  0.1321,  0.1376,\n",
      "         0.1341,  0.1163,  0.1335,  0.1334,  0.1257,  0.1244,  0.1408,  0.1420,\n",
      "         0.1324,  0.1287,  0.1150,  0.1542,  0.1263,  0.1030,  0.1367,  0.1286,\n",
      "         0.1140,  0.1311,  0.1139,  0.1022,  0.1421,  0.1391,  0.1377,  0.1405,\n",
      "         0.1329,  0.1439,  0.1340,  0.1256,  0.1240,  0.1159,  0.1178,  0.1240,\n",
      "         0.1235,  0.1102,  0.1317,  0.1121,  0.1330,  0.1113,  0.1528,  0.1100,\n",
      "         0.1162,  0.1003,  0.1184,  0.1482,  0.1423,  0.1000,  0.1420,  0.1254,\n",
      "         0.1219,  0.1258,  0.1308,  0.1188,  0.1478,  0.1289,  0.1345,  0.1213,\n",
      "         0.1075,  0.1518,  0.1217,  0.1428,  0.0993,  0.1393,  0.1314,  0.1248,\n",
      "         0.1221,  0.1350,  0.1358,  0.1198,  0.1305,  0.1590,  0.1454,  0.1342,\n",
      "         0.1299,  0.1233,  0.1290,  0.1401,  0.1079,  0.1111,  0.1443,  0.1271,\n",
      "         0.1236,  0.1247,  0.1373,  0.1252,  0.1498,  0.1519,  0.1350,  0.0908,\n",
      "         0.1302,  0.1840,  0.1240,  0.1082,  0.1258,  0.1325,  0.1495,  0.1404,\n",
      "         0.1576,  0.1321,  0.0450,  0.1297,  0.1387,  0.1267,  0.1098,  0.2937,\n",
      "         0.1267,  0.1650,  0.1487,  0.1222,  0.1244,  0.1290,  0.1204,  0.1135,\n",
      "         0.1308,  0.1486,  0.1226,  0.1288,  0.1237,  0.1339,  0.1214,  0.1208,\n",
      "         0.1451,  0.1134,  0.1327,  0.1601,  0.0908,  0.1319,  0.1190,  0.1386,\n",
      "         0.1307,  0.1418,  0.1252,  0.0104,  0.1192,  0.1359,  0.1311,  0.1232,\n",
      "         0.1088,  0.1332,  0.1237,  0.1193,  0.1550,  0.1178,  0.1341,  0.1190,\n",
      "         0.1484,  0.1214,  0.1378,  0.1289,  0.1311,  0.1202,  0.1332,  0.1323,\n",
      "         0.1253,  0.1221,  0.1348,  0.1260,  0.1436,  0.1322,  0.1396,  0.1447,\n",
      "         0.1209,  0.1652,  0.1247,  0.1244,  0.1132,  0.1200,  0.1016,  0.1285,\n",
      "         0.1308,  0.1311,  0.1302,  0.1340,  0.1157,  0.1280,  0.1273,  0.1242,\n",
      "         0.1423,  0.1393,  0.1294,  0.1563,  0.1325,  0.1279,  0.1071,  0.1272,\n",
      "         0.1480,  0.0827,  0.1301,  0.1488,  0.1441,  0.1114,  0.1347,  0.1158,\n",
      "         0.1138,  0.1567,  0.1289,  0.1423,  0.1459,  0.1230,  0.1151,  0.1251,\n",
      "         0.1224,  0.1321,  0.1409,  0.1409,  0.1276,  0.1259,  0.1339,  0.1290,\n",
      "         0.1014,  0.1324,  0.1267,  0.1296,  0.1379,  0.1251,  0.1398,  0.1150,\n",
      "         0.1312,  0.1248,  0.1352,  0.1206,  0.1174,  0.1289,  0.1339,  0.0704,\n",
      "         0.1312,  0.1245,  0.1431,  0.1376,  0.1219,  0.1164,  0.1260,  0.1301,\n",
      "         0.1202,  0.1223,  0.1211,  0.1296,  0.1487,  0.1437,  0.1219,  0.1275,\n",
      "         0.1321,  0.1268,  0.1316,  0.1113,  0.1183,  0.0633,  0.1310,  0.1108,\n",
      "         0.1204,  0.1356,  0.1489,  0.1254, -0.0178,  0.1475,  0.1430,  0.1261,\n",
      "         0.1268,  0.1167,  0.1459,  0.1222,  0.1109,  0.1158,  0.1374,  0.1429,\n",
      "         0.1301,  0.1286,  0.1268,  0.1255,  0.0903,  0.1138,  0.0690,  0.1184,\n",
      "         0.1257,  0.1113,  0.1187,  0.1533,  0.1187,  0.1221,  0.1350,  0.1085,\n",
      "         0.1039,  0.1304,  0.1298,  0.1327,  0.0734,  0.1282,  0.1174,  0.1446,\n",
      "         0.1104,  0.1349,  0.1308,  0.1382,  0.1071,  0.1320,  0.1251,  0.1267,\n",
      "         0.1228,  0.1171,  0.1186,  0.1191,  0.1344,  0.1258,  0.1055,  0.1507,\n",
      "         0.1400,  0.1291,  0.1029,  0.1201,  0.1328,  0.1278,  0.1632,  0.1228,\n",
      "         0.0193,  0.1251,  0.1115,  0.1134,  0.1164,  0.1103,  0.1365,  0.1190,\n",
      "         0.1297,  0.1219,  0.1082,  0.1218,  0.1257,  0.1569,  0.1140,  0.1368,\n",
      "         0.1259, -0.0224,  0.1178,  0.1460,  0.1655,  0.1238,  0.1078,  0.1216,\n",
      "         0.1497,  0.1133,  0.1312,  0.1169,  0.1178,  0.1314,  0.1219,  0.1334,\n",
      "         0.1421,  0.1099,  0.1272,  0.1198,  0.1333,  0.1465,  0.1193,  0.1215,\n",
      "         0.1525,  0.1210,  0.1131,  0.1437,  0.1097,  0.1303,  0.1162,  0.1153,\n",
      "         0.1102,  0.1319,  0.1167,  0.1103,  0.1154,  0.1351,  0.1226,  0.0176],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1003, -0.0554, -0.0045,  ..., -0.0245, -0.0069,  0.1022],\n",
      "        [-0.0760,  0.0381, -0.0541,  ..., -0.1272, -0.0204,  0.0301],\n",
      "        [-0.0168, -0.0262,  0.0310,  ..., -0.0453,  0.0235,  0.0157],\n",
      "        ...,\n",
      "        [-0.0338,  0.1238,  0.0028,  ...,  0.0150,  0.0259,  0.0167],\n",
      "        [-0.0492, -0.0502, -0.0229,  ...,  0.0586,  0.0063,  0.1000],\n",
      "        [ 0.0177, -0.0324, -0.0637,  ...,  0.0914,  0.0708,  0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4581,  0.2074,  0.0759,  ..., -0.3429, -0.1176, -0.2929],\n",
      "        [-0.0481, -0.3391, -0.1562,  ..., -0.3745, -0.3730, -0.3669],\n",
      "        [ 0.0122,  0.0298,  0.0736,  ..., -0.1330, -0.2764,  0.0439],\n",
      "        ...,\n",
      "        [-0.3877, -0.3961, -0.5219,  ...,  0.1181,  0.0986,  0.0988],\n",
      "        [ 0.1852,  0.0059,  0.2679,  ..., -0.3655, -0.0257, -0.4195],\n",
      "        [ 0.3341,  0.3478,  0.0307,  ...,  0.1211, -0.3234, -0.2127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2363,  0.1980,  0.5042,  ...,  0.2566,  1.0859,  0.0255],\n",
      "        [-0.0567,  0.3047,  0.5898,  ..., -0.8193, -0.0710,  0.0457],\n",
      "        [-0.2661,  0.7141, -0.0216,  ...,  0.3894, -0.3871,  0.0538],\n",
      "        ...,\n",
      "        [ 0.1323,  0.1340, -0.1977,  ..., -0.8181,  0.7983,  0.0748],\n",
      "        [-0.0429, -0.9077,  0.1565,  ...,  0.2469,  0.9709, -0.0515],\n",
      "        [ 0.2953,  0.5100,  0.3534,  ...,  0.6395,  0.8578,  0.0978]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0688,  0.4726, -0.1820,  ..., -1.7101,  0.8884,  0.2459],\n",
      "        [-1.0712, -0.5447, -0.4433,  ..., -0.2843, -0.4590,  0.4820],\n",
      "        [ 0.6524, -0.7305, -0.3314,  ...,  0.2871,  0.7373,  0.0890],\n",
      "        ...,\n",
      "        [ 0.1177, -0.8575, -0.5635,  ...,  0.6193,  1.0019,  0.4499],\n",
      "        [ 0.6097, -0.6464, -0.6755,  ..., -0.3927,  0.4659, -0.7938],\n",
      "        [-0.1464,  0.2281,  2.2651,  ..., -1.7242,  0.4707, -4.9698]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0296,  0.2216,  0.2100,  0.2412,  0.2929,  0.2495,  0.2599,  0.2394,\n",
      "         0.2199,  0.2328,  0.2162,  0.1972,  0.2247,  0.2162,  0.1876,  0.1933,\n",
      "         0.1968,  0.2058,  0.2358,  0.2361,  0.2906,  0.2144,  0.2257,  0.2500,\n",
      "         0.2272,  0.2641,  0.1988,  0.2338,  0.2176,  0.2600,  0.2431,  0.2695,\n",
      "         0.2760,  0.2102,  0.2698,  0.2281,  0.2231,  0.2039,  0.2495,  0.1969,\n",
      "         0.2321,  0.2239,  0.2619,  0.2073,  0.2230,  0.2093,  0.2494,  0.2352,\n",
      "         0.2427,  0.2256,  0.2062,  0.2464,  0.2578,  0.2189,  0.2476,  0.2408,\n",
      "         0.2109,  0.2353,  0.2044,  0.2095,  0.2421,  0.2402,  0.2306,  0.2722,\n",
      "         0.2858,  0.2185,  0.2479,  0.2212,  0.2309,  0.2280,  0.2663,  0.2377,\n",
      "         0.0357,  0.1828,  0.2679,  0.2310,  0.2374,  0.2304,  0.2050,  0.2364,\n",
      "         0.2354,  0.2353,  0.3393,  0.2511,  0.2229,  0.2051,  0.2400,  0.2450,\n",
      "         0.2166,  0.2437,  0.2238,  0.2690,  0.1908,  0.2265,  0.2097,  0.2276,\n",
      "         0.1971,  0.1041,  0.2213,  0.2295,  0.3445,  0.2552,  0.1922,  0.2650,\n",
      "         0.2323,  0.2410,  0.2185,  0.2129,  0.2077,  0.2287,  0.2098,  0.2196,\n",
      "         0.2791,  0.2082,  0.2188,  0.3083,  0.1954,  0.2625,  0.2276,  0.2430,\n",
      "         0.2221,  0.2479,  0.0473,  0.3589,  0.2188,  0.2357,  0.2527,  0.2317,\n",
      "         0.2149,  0.2150,  0.2432,  0.2738,  0.1872,  0.2270,  0.2384,  0.2338,\n",
      "         0.2298,  0.2225,  0.2114,  0.1924,  0.2123,  0.2199,  0.2307,  0.2338,\n",
      "         0.2403,  0.1837,  0.2489,  0.2557,  0.2076,  0.2055,  0.2369,  0.2907,\n",
      "         0.2531,  0.2622,  0.2020,  0.2750,  0.2357,  0.3017,  0.2342,  0.2394,\n",
      "         0.1947,  0.2427,  0.2733,  0.1786,  0.2803,  0.2516,  0.2376,  0.2348,\n",
      "         0.2157,  0.2723,  0.2530,  0.2324,  0.2276,  0.1859,  0.2007,  0.2204,\n",
      "         0.2243,  0.1808,  0.2805,  0.1841,  0.1974,  0.2376,  0.2448,  0.2105,\n",
      "         0.2231,  0.2444,  0.1456,  0.2802,  0.2481,  0.1624,  0.2381,  0.1606,\n",
      "         0.2222,  0.1975,  0.2511,  0.2088,  0.2863,  0.3323,  0.2045,  0.2051,\n",
      "         0.2024,  0.2982,  0.1721,  0.2539,  0.1675,  0.2331,  0.2442,  0.1961,\n",
      "         0.2201,  0.2850,  0.2692,  0.2598,  0.2141,  0.2643,  0.1917,  0.2157,\n",
      "         0.2634,  0.2121,  0.2515,  0.2865,  0.2306,  0.2177,  0.2535,  0.2175,\n",
      "         0.2225,  0.2276,  0.2845,  0.1981,  0.2535,  0.2385,  0.2190,  0.1080,\n",
      "         0.2112,  0.0578,  0.2331,  0.1959,  0.2045,  0.2159,  0.3116,  0.2441,\n",
      "         0.1478,  0.2235,  0.1238,  0.2340,  0.2701,  0.2046,  0.1812,  0.0511,\n",
      "         0.2226,  0.2621,  0.2812,  0.2300,  0.2163,  0.2319,  0.2470,  0.1935,\n",
      "         0.2355,  0.2974,  0.2429,  0.2284,  0.2472,  0.2326,  0.2511,  0.1977,\n",
      "         0.2685,  0.1906,  0.2208,  0.3256,  0.1763,  0.2345,  0.2131,  0.2552,\n",
      "         0.2266,  0.2525,  0.1924, -0.0324,  0.2395,  0.2247,  0.1952,  0.2413,\n",
      "         0.0980,  0.2621,  0.2204,  0.2210,  0.4075,  0.1971,  0.2487,  0.1959,\n",
      "         0.2733,  0.2051,  0.1996,  0.2345,  0.2145,  0.2162,  0.2430,  0.2422,\n",
      "         0.1954,  0.2102,  0.2264,  0.2109,  0.2583,  0.2284,  0.2162,  0.2495,\n",
      "         0.2272,  0.2408,  0.2285,  0.2522,  0.2135,  0.2112,  0.1645,  0.2252,\n",
      "         0.2414,  0.2050,  0.2796,  0.2611,  0.1781,  0.2231,  0.2192,  0.2514,\n",
      "         0.2673,  0.2353,  0.2267,  0.2851,  0.2126,  0.1839,  0.2222,  0.3323,\n",
      "         0.2497,  0.1148,  0.2557,  0.2683,  0.2535,  0.2149,  0.2911,  0.1890,\n",
      "         0.1777,  0.2585,  0.2522,  0.1973,  0.2678,  0.2378,  0.2146,  0.2197,\n",
      "         0.2254,  0.2035,  0.2230,  0.2154,  0.2337,  0.2617,  0.2414,  0.2399,\n",
      "         0.3350,  0.2716,  0.2044,  0.2641,  0.2600,  0.2455,  0.2519,  0.2357,\n",
      "         0.2515,  0.2049,  0.2543,  0.2173,  0.2041,  0.2231,  0.2633,  0.1652,\n",
      "         0.2524,  0.2513,  0.2867,  0.2262,  0.1890,  0.2406,  0.2361,  0.2290,\n",
      "         0.1948,  0.1944,  0.2142,  0.2242,  0.2225,  0.2346,  0.2206,  0.2287,\n",
      "         0.1816,  0.2160,  0.2622,  0.2117,  0.2148, -0.0945,  0.2097,  0.2092,\n",
      "         0.2369,  0.2329,  0.2192,  0.2252, -0.0400,  0.2379,  0.2779,  0.2368,\n",
      "         0.2055,  0.2285,  0.2342,  0.2216,  0.2361,  0.2262,  0.2213,  0.2796,\n",
      "         0.2488,  0.2257,  0.2125,  0.2018,  0.1555,  0.2070,  0.1133,  0.2477,\n",
      "         0.2227,  0.2084,  0.2046,  0.2649,  0.2044,  0.1983,  0.2550,  0.2292,\n",
      "         0.1991,  0.2596,  0.2287,  0.2528,  0.2507,  0.2309,  0.2038,  0.2576,\n",
      "         0.2145,  0.2185,  0.2216,  0.2101,  0.1999,  0.2416,  0.2184,  0.2348,\n",
      "         0.2011,  0.2233,  0.2053,  0.2145,  0.2240,  0.2061,  0.1727,  0.2402,\n",
      "         0.2292,  0.2299,  0.1584,  0.2282,  0.2050,  0.2145,  0.3623,  0.2350,\n",
      "         0.0362,  0.2999,  0.2019,  0.2116,  0.2117,  0.2322,  0.2089,  0.1989,\n",
      "         0.2291,  0.2420,  0.1388,  0.2146,  0.2507,  0.2332,  0.2081,  0.2376,\n",
      "         0.2622,  0.0315,  0.1931,  0.2697,  0.2789,  0.2334,  0.1951,  0.0916,\n",
      "         0.2839,  0.2162,  0.2357,  0.1962,  0.2020,  0.2799,  0.1903,  0.2565,\n",
      "         0.2512,  0.1870,  0.2285,  0.1905,  0.2537,  0.2411,  0.2180,  0.2289,\n",
      "         0.3049,  0.1982,  0.2191,  0.2124,  0.1987,  0.2168,  0.1999,  0.2060,\n",
      "         0.2002,  0.2360,  0.2516,  0.2127,  0.2170,  0.2485,  0.2249,  0.0341],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0338, -0.1045,  0.0112,  ..., -0.0063, -0.0144,  0.0048],\n",
      "        [ 0.0175,  0.0808, -0.0867,  ...,  0.0892, -0.0817,  0.0317],\n",
      "        [ 0.0365, -0.0893,  0.0675,  ...,  0.0268, -0.0599,  0.0090],\n",
      "        ...,\n",
      "        [ 0.0189, -0.0607,  0.0233,  ..., -0.0037,  0.1209,  0.0213],\n",
      "        [ 0.0164, -0.0006,  0.1702,  ..., -0.0868, -0.0179, -0.0130],\n",
      "        [-0.0016,  0.0015,  0.0248,  ...,  0.0943,  0.0888, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-7.9879e-02, -7.5674e-01,  1.9172e-01,  ..., -3.3586e-01,\n",
      "         -9.9196e-02,  2.1641e-01],\n",
      "        [ 6.6580e-01, -3.4170e-04,  3.3022e-01,  ..., -3.2322e-01,\n",
      "          5.3017e-01,  1.4362e-01],\n",
      "        [-2.7228e-01,  3.9801e-01,  3.9070e-01,  ..., -9.6995e-02,\n",
      "          4.6725e-01,  1.7887e-01],\n",
      "        ...,\n",
      "        [-1.6946e-01,  2.7358e-01,  1.3819e-01,  ..., -2.3281e-01,\n",
      "          1.2810e-01,  2.3012e-01],\n",
      "        [ 2.2246e-01,  5.2987e-03, -8.8427e-02,  ..., -7.9983e-01,\n",
      "          4.2493e-01, -4.4656e-02],\n",
      "        [ 6.2005e-01,  6.1742e-01,  1.2391e+00,  ...,  3.7342e-01,\n",
      "         -7.2662e-01, -4.4551e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4570,  0.0512,  0.6210,  ...,  0.8005, -0.3576, -0.0644],\n",
      "        [ 0.0795, -0.7409, -0.3487,  ...,  0.4284, -0.2786,  0.1193],\n",
      "        [ 0.0488, -0.8022, -0.1186,  ...,  0.4103, -0.6070,  0.0508],\n",
      "        ...,\n",
      "        [-0.5860,  0.2100,  0.6179,  ..., -0.0816,  0.2593, -0.3114],\n",
      "        [ 0.6059, -0.5531,  0.2569,  ..., -0.4948, -0.0016,  0.0205],\n",
      "        [ 0.7089,  0.6657, -0.5792,  ..., -0.4173, -0.5973,  0.3249]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 9.9080e-01, -5.7690e-01, -3.6501e-03,  ...,  2.2457e-01,\n",
      "         -2.2652e+00, -1.6855e+00],\n",
      "        [-4.6963e-01, -8.3940e-01,  7.2930e-02,  ..., -4.8221e-01,\n",
      "         -5.9900e-01,  8.7155e-02],\n",
      "        [-8.6419e-02,  1.5644e-01, -5.1267e-01,  ..., -6.9450e-01,\n",
      "         -4.7479e-01,  9.2940e-02],\n",
      "        ...,\n",
      "        [-2.8897e-02,  8.3622e-02,  3.3349e-01,  ...,  2.0935e-02,\n",
      "          1.9875e-01, -8.0366e-02],\n",
      "        [ 3.2997e-01,  7.2427e-01,  3.1634e-01,  ..., -3.1425e-01,\n",
      "         -2.8786e-01, -2.4385e-01],\n",
      "        [ 1.3513e+00, -6.9742e-01,  1.8798e+00,  ...,  6.6696e-01,\n",
      "         -2.7917e+00, -3.6569e+00]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0393,  0.1471,  0.1375,  0.1600,  0.1495,  0.1667,  0.1282,  0.1363,\n",
      "         0.1070,  0.1182,  0.1437,  0.1275,  0.1136,  0.1239,  0.1206,  0.1479,\n",
      "         0.1581,  0.1337,  0.1625,  0.1492,  0.2243,  0.1562,  0.1196,  0.1756,\n",
      "         0.1314,  0.1428,  0.1383,  0.1326,  0.1209,  0.1302,  0.1676,  0.1575,\n",
      "         0.1778,  0.1252,  0.1455,  0.1737,  0.1326,  0.1277,  0.1436,  0.1506,\n",
      "         0.1580,  0.1193,  0.1445,  0.1367,  0.1267,  0.1644,  0.1476,  0.1527,\n",
      "         0.1786,  0.1551,  0.1344,  0.1495,  0.1535,  0.1467,  0.1291,  0.1080,\n",
      "         0.1502,  0.1482,  0.1256,  0.1235,  0.1786,  0.1457,  0.1179,  0.1772,\n",
      "         0.1869,  0.1712,  0.1705,  0.1988,  0.1621,  0.1172,  0.1888,  0.1244,\n",
      "        -0.0589,  0.1128,  0.1658,  0.1412,  0.1688,  0.1456,  0.1611,  0.1683,\n",
      "         0.1467,  0.1228,  0.1557,  0.1410,  0.1487,  0.1170,  0.1917,  0.1684,\n",
      "         0.1568,  0.1346,  0.1443,  0.1749,  0.1542,  0.1539,  0.1331,  0.1184,\n",
      "         0.1096,  0.0788,  0.1441,  0.1250,  0.1919,  0.1403,  0.1129,  0.1550,\n",
      "         0.1450,  0.1539,  0.1563,  0.1752,  0.1316,  0.1580,  0.1163,  0.1854,\n",
      "         0.1468,  0.1273,  0.1074,  0.1768,  0.1172,  0.1630,  0.1158,  0.1563,\n",
      "         0.1663,  0.1651,  0.3161,  0.1794,  0.1516,  0.1863,  0.1275,  0.1294,\n",
      "         0.1244,  0.1575,  0.1614,  0.1843,  0.1491,  0.1649,  0.1944,  0.1342,\n",
      "         0.1339,  0.1296,  0.1549,  0.1318,  0.1696,  0.1370,  0.1663,  0.1650,\n",
      "         0.1334,  0.1276,  0.1553,  0.1417,  0.1300,  0.1397,  0.1434,  0.1872,\n",
      "         0.1865,  0.1810,  0.1672,  0.1552,  0.1325,  0.1603,  0.1414,  0.1213,\n",
      "         0.1245,  0.1513,  0.1534,  0.0939,  0.1491,  0.1385,  0.2011,  0.1540,\n",
      "         0.1258,  0.1408,  0.1948,  0.1563,  0.1602, -0.1140,  0.1293,  0.1354,\n",
      "         0.1556,  0.1041,  0.1535,  0.1224,  0.1416,  0.1513,  0.1714,  0.1396,\n",
      "         0.1226,  0.1508,  0.1255,  0.1821,  0.1647,  0.1153,  0.1572,  0.1457,\n",
      "         0.1298, -0.1788,  0.1296,  0.1278,  0.1504,  0.1282,  0.1465,  0.1264,\n",
      "         0.1498,  0.1606,  0.1328,  0.1175,  0.1114,  0.1645,  0.1536,  0.1322,\n",
      "         0.1551,  0.1890,  0.1527,  0.1538,  0.1232,  0.1574,  0.1400,  0.1473,\n",
      "         0.1297,  0.1591,  0.1604,  0.1716,  0.1410,  0.1475,  0.1864,  0.1391,\n",
      "         0.1595,  0.1390,  0.1702,  0.1382,  0.1742,  0.1703,  0.1423, -0.0990,\n",
      "         0.1730,  0.2410,  0.1334,  0.1393,  0.1401,  0.1171,  0.1652,  0.1338,\n",
      "         0.1705,  0.1369,  0.0810,  0.1600,  0.1636,  0.1496,  0.1256,  0.4151,\n",
      "         0.1478,  0.1838,  0.1483,  0.1561,  0.1514,  0.1313,  0.1453,  0.1376,\n",
      "         0.1658,  0.1736,  0.1214,  0.1608,  0.1454,  0.1459,  0.1426,  0.1290,\n",
      "         0.1666,  0.1323,  0.1624,  0.1302,  0.1574,  0.1432,  0.1384,  0.1597,\n",
      "         0.1612,  0.2012,  0.1156,  0.1045,  0.1537,  0.1328,  0.1248,  0.1588,\n",
      "         0.1151,  0.1381,  0.1066,  0.1688,  0.2070,  0.1372,  0.1566,  0.1487,\n",
      "         0.1820,  0.1500,  0.1333,  0.1510,  0.1409,  0.1712,  0.1376,  0.1214,\n",
      "         0.1911,  0.1194,  0.1700,  0.1517,  0.1022,  0.1303,  0.1378,  0.1761,\n",
      "         0.1217,  0.1310,  0.1619,  0.1338,  0.1356,  0.1185,  0.1249,  0.1750,\n",
      "         0.1355,  0.1364,  0.1611,  0.1546,  0.1219,  0.1361,  0.1437,  0.1350,\n",
      "         0.1703,  0.1466,  0.1604,  0.1798,  0.1670,  0.1705,  0.1530,  0.1577,\n",
      "         0.1687,  0.1495,  0.1441,  0.1640,  0.1592,  0.1382,  0.1671,  0.1302,\n",
      "         0.1191,  0.1652,  0.1573,  0.1393,  0.1654,  0.1597,  0.1415,  0.1304,\n",
      "         0.1409,  0.1575,  0.1436,  0.1209,  0.1781,  0.1561,  0.1109,  0.1525,\n",
      "         0.1583,  0.1852,  0.1682,  0.1843,  0.1701,  0.1151,  0.1664,  0.1173,\n",
      "         0.1430,  0.1359,  0.1419,  0.1272,  0.1737,  0.1549,  0.1781,  0.1634,\n",
      "         0.1949,  0.1491,  0.1965,  0.1341,  0.1709,  0.1376,  0.1330,  0.1557,\n",
      "         0.1262,  0.1325,  0.1334,  0.1447,  0.1419,  0.1693,  0.1199,  0.1464,\n",
      "         0.1144,  0.1854,  0.1554,  0.1509,  0.1095, -0.0859,  0.1368,  0.1492,\n",
      "         0.1281,  0.1650,  0.1290,  0.1568, -0.0190,  0.1325,  0.1666,  0.1276,\n",
      "         0.1341,  0.1194,  0.1729,  0.1789,  0.1203,  0.1694,  0.1514,  0.1673,\n",
      "         0.1667,  0.1669,  0.1975,  0.1485,  0.1267,  0.0943,  0.0662,  0.1778,\n",
      "         0.1512,  0.1305,  0.1139,  0.1951,  0.1501,  0.1365,  0.1485,  0.1645,\n",
      "         0.1298,  0.1491,  0.1166,  0.1452,  0.3627,  0.1563,  0.1661,  0.1575,\n",
      "         0.1390,  0.1358,  0.1423,  0.1515,  0.1652,  0.1638,  0.1429,  0.1462,\n",
      "         0.1245,  0.1196,  0.1318,  0.1230,  0.1120,  0.1437,  0.1196,  0.1458,\n",
      "         0.1421,  0.1414,  0.1379,  0.1722,  0.1655,  0.1277,  0.2115,  0.1331,\n",
      "         0.0610,  0.1486,  0.1625,  0.1413,  0.1396,  0.1280,  0.1798,  0.1331,\n",
      "         0.1293,  0.1461,  0.0998,  0.1344,  0.1624,  0.1450,  0.1252,  0.1211,\n",
      "         0.1639, -0.0185,  0.1038,  0.1501,  0.1309,  0.1353,  0.1236,  0.0699,\n",
      "         0.1302,  0.1570,  0.1706,  0.1012,  0.1335,  0.1455,  0.1247,  0.1657,\n",
      "         0.1636,  0.1892,  0.1623,  0.1161,  0.1641,  0.1592,  0.1491,  0.1486,\n",
      "         0.1669,  0.1607,  0.1489,  0.1403,  0.1350,  0.1440,  0.1244,  0.1201,\n",
      "         0.1672,  0.1524,  0.1741,  0.1457,  0.2008,  0.1332,  0.1430, -0.0581],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1235, -0.2294, -0.1435,  ...,  0.0318,  0.0021,  0.2459],\n",
      "        [-0.2883, -0.1948, -0.3869,  ..., -0.0066, -0.1010, -0.0683],\n",
      "        [-0.2496,  0.0371,  0.4483,  ..., -0.4165, -0.0669, -0.0287],\n",
      "        ...,\n",
      "        [-0.0024,  0.0865,  0.1233,  ...,  0.0901,  0.0902,  0.0615],\n",
      "        [ 0.1961, -0.0339, -0.1853,  ..., -0.1644, -0.2676,  0.0546],\n",
      "        [-0.0834, -0.0522,  0.0719,  ...,  0.0630, -0.0416,  0.0799]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0724,  0.8127, -0.1422,  ...,  0.3499,  0.6591, -0.2740],\n",
      "        [ 0.0117, -0.4337, -1.3428,  ..., -0.1668,  0.7229,  0.3074],\n",
      "        [ 0.2655,  1.2280,  0.9155,  ..., -0.8180,  0.6188, -0.1661],\n",
      "        ...,\n",
      "        [ 0.5768,  1.2044,  0.0313,  ...,  0.1313, -1.9596,  0.1728],\n",
      "        [-1.0980, -1.4764, -2.2608,  ...,  0.9217,  0.9319, -0.3809],\n",
      "        [ 0.6698,  0.4141, -0.2126,  ..., -0.0807,  0.3633,  0.1180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 4.3286, -1.4047, -0.4815,  ..., -0.5152,  1.9303, -0.1296],\n",
      "        [ 0.1940,  0.4437, -0.3369,  ..., -0.6165,  0.3483, -0.1873],\n",
      "        [ 0.3186,  0.3830, -0.1017,  ..., -0.2301,  0.8354,  0.2191],\n",
      "        ...,\n",
      "        [-0.4027,  0.1967,  0.0516,  ...,  0.0349, -1.0002,  0.3366],\n",
      "        [ 0.0363, -0.3647, -0.4093,  ..., -0.3978, -1.1740,  0.0934],\n",
      "        [ 2.9547,  0.4251,  0.0376,  ..., -1.1124,  2.3374, -0.5631]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0278,  0.1838,  0.1826,  0.1870,  0.2084,  0.2005,  0.1997,  0.1945,\n",
      "         0.1991,  0.1870,  0.1708,  0.1670,  0.1707,  0.1659,  0.1631,  0.1640,\n",
      "         0.1655,  0.1560,  0.1910,  0.1801,  0.1768,  0.1785,  0.1819,  0.2038,\n",
      "         0.1768,  0.1719,  0.1831,  0.1886,  0.1674,  0.1682,  0.1666,  0.1794,\n",
      "         0.2036,  0.1881,  0.1881,  0.1895,  0.1825,  0.1880,  0.1824,  0.1752,\n",
      "         0.2014,  0.1724,  0.1909,  0.1654,  0.1804,  0.1868,  0.1914,  0.1800,\n",
      "         0.1907,  0.1854,  0.1778,  0.1983,  0.1841,  0.1809,  0.1735,  0.1778,\n",
      "         0.1911,  0.1792,  0.1755,  0.1920,  0.1797,  0.1747,  0.1811,  0.2038,\n",
      "         0.2181,  0.1849,  0.1785,  0.1822,  0.2083,  0.1725,  0.1300,  0.1739,\n",
      "        -0.0321,  0.1736,  0.2061,  0.1718,  0.1846,  0.1852,  0.1721,  0.1798,\n",
      "         0.1642,  0.1780,  0.1860,  0.1941,  0.1932,  0.1637,  0.1993,  0.1892,\n",
      "         0.1788,  0.1844,  0.1919,  0.1773,  0.1642,  0.1875,  0.1663,  0.1853,\n",
      "         0.1764,  0.1079,  0.1796,  0.1765,  0.2155,  0.1794,  0.1714,  0.1804,\n",
      "         0.1946,  0.1897,  0.1878,  0.1693,  0.1790,  0.1659,  0.1705,  0.1802,\n",
      "         0.1767,  0.1745,  0.1806,  0.1771,  0.1679,  0.2016,  0.1822,  0.1820,\n",
      "         0.1913,  0.1944,  0.3081,  0.2075,  0.1968,  0.1860,  0.1779,  0.1792,\n",
      "         0.1739,  0.1786,  0.1997,  0.1889,  0.1696,  0.1874,  0.2137,  0.1870,\n",
      "         0.1737,  0.1631,  0.1870,  0.1751,  0.1744,  0.1673,  0.1753,  0.2066,\n",
      "         0.1751,  0.1632,  0.1896,  0.2020,  0.1780,  0.1757,  0.1711,  0.1963,\n",
      "         0.1770,  0.2005,  0.1774,  0.2095,  0.1751,  0.1672,  0.2051,  0.1795,\n",
      "         0.1770,  0.1868,  0.1691,  0.1353,  0.1893,  0.1739,  0.1885,  0.2072,\n",
      "         0.1964,  0.1964,  0.1993,  0.1893,  0.1880,  0.1713,  0.1769,  0.1761,\n",
      "         0.1696,  0.1506,  0.1812,  0.1554,  0.1796,  0.1555,  0.1878,  0.1742,\n",
      "         0.1626,  0.1759,  0.1655,  0.2122,  0.2060,  0.1633,  0.1796,  0.1602,\n",
      "         0.1677,  0.2302,  0.1907,  0.1681,  0.2002,  0.1766,  0.1860,  0.1850,\n",
      "         0.1617,  0.1944,  0.1512,  0.1892,  0.1369,  0.1889,  0.1868,  0.1677,\n",
      "         0.1725,  0.1927,  0.1759,  0.1865,  0.1779,  0.1878,  0.1732,  0.1828,\n",
      "         0.1883,  0.1950,  0.1742,  0.2045,  0.1743,  0.1728,  0.1757,  0.1749,\n",
      "         0.1857,  0.1774,  0.1919,  0.1796,  0.2064,  0.1929,  0.1710,  0.1087,\n",
      "         0.1820,  0.2628,  0.1707,  0.1697,  0.1803,  0.1960,  0.1980,  0.1847,\n",
      "         0.1888,  0.1630, -0.0115,  0.1801,  0.1835,  0.1830,  0.1510,  0.5361,\n",
      "         0.1591,  0.2062,  0.1905,  0.1776,  0.1839,  0.1849,  0.1704,  0.1660,\n",
      "         0.1791,  0.2007,  0.1897,  0.1766,  0.1883,  0.1917,  0.1697,  0.1523,\n",
      "         0.1923,  0.1776,  0.1866,  0.1909,  0.1656,  0.1820,  0.1883,  0.1885,\n",
      "         0.1651,  0.2017,  0.1741, -0.0294,  0.1777,  0.1787,  0.1652,  0.1803,\n",
      "         0.1396,  0.1714,  0.1668,  0.1960,  0.2102,  0.1684,  0.1795,  0.1916,\n",
      "         0.1910,  0.1686,  0.1870,  0.1861,  0.1829,  0.1869,  0.1850,  0.1952,\n",
      "         0.1797,  0.1747,  0.1828,  0.1654,  0.1788,  0.1827,  0.1907,  0.1930,\n",
      "         0.1797,  0.1667,  0.1915,  0.1745,  0.1780,  0.1893,  0.1497,  0.1972,\n",
      "         0.1858,  0.1742,  0.1897,  0.1980,  0.1611,  0.1834,  0.1913,  0.1934,\n",
      "         0.1965,  0.1884,  0.1784,  0.2009,  0.1905,  0.1760,  0.1614,  0.1863,\n",
      "         0.1865, -0.1710,  0.1886,  0.2001,  0.1917,  0.1644,  0.1940,  0.1591,\n",
      "         0.1605,  0.2038,  0.1987,  0.2010,  0.2110,  0.1912,  0.1785,  0.1812,\n",
      "         0.1737,  0.1805,  0.1917,  0.1808,  0.1861,  0.1823,  0.1928,  0.1662,\n",
      "         0.0984,  0.1754,  0.1793,  0.1893,  0.1843,  0.1790,  0.1866,  0.1681,\n",
      "         0.2011,  0.1970,  0.1856,  0.1790,  0.1726,  0.1869,  0.1929,  0.1175,\n",
      "         0.1853,  0.1683,  0.2019,  0.1908,  0.1660,  0.1904,  0.1792,  0.1663,\n",
      "         0.1729,  0.1611,  0.1936,  0.1908,  0.1777,  0.1969,  0.1711,  0.1810,\n",
      "         0.1764,  0.1749,  0.1867,  0.1811,  0.1598,  0.0953,  0.1893,  0.1834,\n",
      "         0.1646,  0.1750,  0.1721,  0.1875, -0.0301,  0.2004,  0.1998,  0.1658,\n",
      "         0.1734,  0.1642,  0.1887,  0.1800,  0.1761,  0.1751,  0.1837,  0.1898,\n",
      "         0.2034,  0.1907,  0.1680,  0.1713,  0.1527,  0.1779,  0.1098,  0.1811,\n",
      "         0.1761,  0.1547,  0.1644,  0.2092,  0.1713,  0.2000,  0.1890,  0.1669,\n",
      "         0.1648,  0.1860,  0.1574,  0.1860,  0.1689,  0.1993,  0.1765,  0.1857,\n",
      "         0.1671,  0.1839,  0.1722,  0.1922,  0.1818,  0.1822,  0.1844,  0.1923,\n",
      "         0.1612,  0.1787,  0.1863,  0.1697,  0.1799,  0.1602,  0.1697,  0.1852,\n",
      "         0.1703,  0.1788,  0.1490,  0.1690,  0.1587,  0.1794,  0.2297,  0.1854,\n",
      "         0.0414,  0.1792,  0.1825,  0.1690,  0.1845,  0.1691,  0.1851,  0.1626,\n",
      "         0.1841,  0.1797,  0.1414,  0.1837,  0.1842,  0.2008,  0.1785,  0.1956,\n",
      "         0.1933,  0.0161,  0.1681,  0.2014,  0.1794,  0.1519,  0.1640,  0.1297,\n",
      "         0.1999,  0.1490,  0.1768,  0.1862,  0.1534,  0.1864,  0.1646,  0.1892,\n",
      "         0.2050,  0.1475,  0.1798,  0.1632,  0.1961,  0.2078,  0.1826,  0.1712,\n",
      "         0.1990,  0.1976,  0.1617,  0.1796,  0.1784,  0.1976,  0.1614,  0.1790,\n",
      "         0.1696,  0.1850,  0.1825,  0.1674,  0.1777,  0.1939,  0.1694,  0.0380],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0053,  0.0191, -0.0433,  ...,  0.0123,  0.0798,  0.0043],\n",
      "        [ 0.0043, -0.0451, -0.0090,  ..., -0.0198,  0.0097,  0.0136],\n",
      "        [ 0.0252,  0.0022, -0.0084,  ...,  0.0261,  0.0189,  0.0032],\n",
      "        ...,\n",
      "        [ 0.0020, -0.0260, -0.0160,  ...,  0.0945, -0.0130,  0.0035],\n",
      "        [ 0.0073, -0.0218, -0.0444,  ...,  0.0155,  0.0230,  0.0062],\n",
      "        [-0.0198, -0.0604,  0.0379,  ..., -0.0226, -0.0017, -0.0026]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1156,  0.1198, -0.4109,  ...,  0.4316,  0.3292, -0.1182],\n",
      "        [ 0.0242,  0.3920, -0.1949,  ...,  0.9352,  0.4467,  0.0383],\n",
      "        [ 0.0441,  0.6047, -0.9313,  ..., -0.1518, -0.2211, -0.0977],\n",
      "        ...,\n",
      "        [-0.1310,  0.5081, -0.2454,  ...,  0.0531, -0.0295,  0.0123],\n",
      "        [-0.1099,  0.2986, -0.0448,  ..., -0.6248,  0.3782,  0.0736],\n",
      "        [ 0.3320, -0.5717, -0.3360,  ..., -0.1590, -0.3557, -0.0377]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 4.0742e-01,  1.3103e+00, -1.9340e+00,  ..., -9.9626e-01,\n",
      "          1.3944e+00, -2.2432e-02],\n",
      "        [ 1.9077e-01,  5.1656e-02, -9.8356e-01,  ..., -1.9076e-01,\n",
      "          3.5562e-01, -1.3087e-03],\n",
      "        [ 6.6560e-02,  3.9726e-01, -1.3167e+00,  ..., -6.3855e-01,\n",
      "         -1.4136e+00,  4.8224e-02],\n",
      "        ...,\n",
      "        [-2.4255e-01, -3.7881e-01, -9.8390e-02,  ...,  1.2402e+00,\n",
      "          1.3612e-01,  2.9521e-02],\n",
      "        [ 1.1894e-02, -6.1239e-01,  9.2702e-01,  ...,  3.0958e-01,\n",
      "         -3.8830e-02,  1.5131e-02],\n",
      "        [-2.1817e-01, -1.2884e-02, -8.8931e-01,  ..., -4.9337e-01,\n",
      "         -2.3966e-01, -1.1308e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2793, -1.3029,  2.1417,  ...,  0.0513,  0.7199, -2.3275],\n",
      "        [-0.3879, -1.1006, -0.9552,  ...,  0.3108,  0.1941, -1.6823],\n",
      "        [ 0.1328,  0.4339,  0.9366,  ...,  1.0451,  0.5582, -1.1591],\n",
      "        ...,\n",
      "        [-0.1505,  0.8190,  0.9105,  ...,  0.6127, -0.2689, -0.7897],\n",
      "        [ 1.6702, -0.2464, -0.3284,  ..., -0.9547, -0.0197, -0.4736],\n",
      "        [-2.3424, -4.2366,  5.5217,  ..., -1.5460,  3.5880, -2.0883]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0666,  0.2531,  0.2438,  0.2595,  0.2988,  0.2150,  0.2956,  0.2534,\n",
      "         0.2416,  0.2680,  0.1837,  0.2219,  0.2444,  0.2528,  0.2283,  0.2258,\n",
      "         0.2423,  0.2095,  0.2759,  0.2619,  0.2631,  0.2323,  0.2815,  0.2547,\n",
      "         0.2568,  0.2291,  0.2039,  0.2016,  0.2326,  0.2512,  0.2414,  0.2568,\n",
      "         0.3012,  0.2316,  0.3019,  0.3009,  0.2474,  0.2204,  0.2642,  0.2371,\n",
      "         0.2893,  0.2112,  0.2599,  0.2780,  0.2592,  0.2536,  0.2712,  0.2304,\n",
      "         0.2711,  0.2303,  0.2604,  0.2840,  0.2423,  0.2345,  0.2550,  0.2405,\n",
      "         0.2537,  0.2640,  0.2525,  0.2175,  0.2788,  0.2409,  0.2809,  0.2788,\n",
      "         0.2886,  0.2327,  0.2370,  0.3365,  0.2650,  0.2256,  0.4717,  0.2110,\n",
      "         0.0658,  0.1909,  0.3033,  0.2439,  0.2845,  0.2769,  0.2173,  0.3028,\n",
      "         0.2268,  0.3008,  0.2978,  0.2795,  0.2473,  0.2216,  0.2715,  0.2603,\n",
      "         0.2485,  0.2437,  0.2238,  0.2279,  0.2518,  0.2491,  0.2214,  0.2607,\n",
      "         0.2455,  0.1329,  0.2349,  0.2020,  0.3249,  0.2376,  0.2230,  0.2958,\n",
      "         0.2660,  0.2705,  0.2421,  0.2239,  0.2589,  0.2303,  0.2097,  0.2291,\n",
      "         0.2421,  0.2337,  0.2529,  0.2315,  0.2222,  0.2984,  0.2598,  0.2543,\n",
      "         0.2465,  0.2558,  0.0691,  0.2716,  0.2571,  0.2416,  0.2600,  0.2387,\n",
      "         0.2300,  0.2543,  0.2753,  0.2358,  0.1929,  0.2335,  0.2646,  0.2583,\n",
      "         0.2661,  0.2386,  0.2466,  0.2105,  0.2401,  0.2197,  0.2509,  0.2586,\n",
      "         0.2458,  0.2367,  0.2600,  0.2758,  0.2560,  0.2505,  0.2230,  0.2848,\n",
      "         0.2507,  0.2649,  0.2566,  0.2780,  0.2346,  0.3724,  0.2810,  0.2275,\n",
      "         0.2423,  0.2420,  0.2987,  0.1793,  0.2579,  0.2851,  0.2923,  0.2598,\n",
      "         0.2237,  0.2856,  0.2501,  0.2505,  0.2310,  0.1828,  0.2156,  0.2376,\n",
      "         0.2467,  0.2081,  0.2284,  0.2319,  0.2279,  0.2374,  0.2720,  0.2636,\n",
      "         0.2137,  0.2925,  0.1705,  0.2884,  0.2694,  0.2244,  0.2456,  0.2004,\n",
      "         0.2064,  0.2913,  0.2600,  0.2397,  0.3378,  0.3787,  0.2447,  0.2399,\n",
      "         0.2326,  0.2615,  0.2012,  0.2132,  0.1874,  0.2526,  0.2832,  0.2415,\n",
      "         0.1953,  0.2883,  0.3194,  0.3068,  0.2052,  0.2863,  0.2650,  0.2047,\n",
      "         0.2845,  0.2601,  0.2874,  0.3027,  0.2634,  0.2765,  0.2084,  0.2140,\n",
      "         0.2386,  0.2293,  0.2559,  0.2750,  0.2738,  0.2588,  0.2058,  0.1560,\n",
      "         0.2599,  0.0624,  0.2436,  0.2588,  0.2589,  0.2433,  0.3066,  0.2729,\n",
      "         0.2047,  0.2538, -0.0732,  0.2590,  0.2928,  0.2408,  0.2125,  0.0827,\n",
      "         0.2262,  0.2667,  0.2681,  0.2512,  0.2445,  0.2686,  0.2550,  0.2174,\n",
      "         0.2551,  0.2597,  0.2994,  0.2281,  0.2715,  0.2589,  0.2377,  0.2184,\n",
      "         0.2769,  0.2258,  0.2359,  0.2626,  0.2824,  0.3255,  0.2507,  0.2670,\n",
      "         0.2209,  0.2487,  0.2233, -0.0543,  0.2930,  0.2292,  0.2183,  0.2706,\n",
      "         0.1297,  0.2453,  0.2435,  0.2771,  0.2990,  0.2414,  0.2606,  0.1978,\n",
      "         0.2465,  0.2619,  0.2040,  0.2600,  0.2422,  0.2785,  0.2246,  0.2452,\n",
      "         0.2554,  0.2773,  0.2513,  0.2393,  0.2460,  0.2233,  0.2455,  0.2308,\n",
      "         0.2691,  0.2347,  0.2335,  0.2366,  0.2769,  0.2613,  0.1989,  0.2213,\n",
      "         0.2554,  0.2499,  0.2852,  0.2753,  0.2085,  0.2385,  0.2744,  0.2636,\n",
      "         0.2711,  0.2712,  0.2436,  0.3165,  0.2407,  0.2253,  0.2422,  0.2840,\n",
      "         0.2118,  0.2286,  0.2776,  0.3019,  0.2489,  0.2484,  0.2736,  0.1939,\n",
      "         0.2508,  0.2825,  0.2706,  0.1959,  0.2872,  0.2317,  0.2497,  0.2481,\n",
      "         0.2820,  0.2441,  0.2700,  0.2313,  0.2609,  0.2664,  0.2864,  0.2396,\n",
      "         0.3148,  0.2555,  0.2468,  0.2736,  0.2570,  0.2473,  0.2397,  0.2406,\n",
      "         0.2203,  0.2444,  0.2187,  0.2473,  0.2214,  0.2373,  0.2606,  0.1716,\n",
      "         0.2719,  0.2466,  0.2339,  0.2118,  0.2149,  0.3053,  0.2472,  0.2465,\n",
      "         0.2337,  0.2156,  0.2484,  0.2510,  0.2206,  0.2845,  0.2412,  0.2404,\n",
      "         0.1996,  0.2741,  0.2593,  0.2718,  0.2325, -0.1138,  0.2038,  0.2706,\n",
      "         0.2521,  0.2562,  0.2038,  0.2356, -0.0657,  0.2782,  0.2454,  0.2374,\n",
      "         0.2316,  0.2894,  0.2186,  0.2761,  0.2464,  0.2621,  0.2385,  0.2300,\n",
      "         0.2727,  0.2241,  0.2182,  0.2403,  0.2204,  0.2173,  0.1478,  0.2895,\n",
      "         0.2639,  0.2138,  0.2656,  0.2862,  0.2404,  0.2657,  0.2671,  0.2265,\n",
      "         0.2684,  0.2632,  0.2649,  0.2712,  0.4141,  0.2755,  0.2734,  0.2789,\n",
      "         0.2044,  0.2048,  0.2899,  0.2438,  0.2476,  0.2577,  0.2593,  0.2330,\n",
      "         0.2217,  0.2553,  0.2798,  0.2282,  0.2364,  0.1991,  0.2374,  0.2063,\n",
      "         0.2254,  0.2427,  0.1872,  0.2272,  0.2011,  0.2378,  0.2996,  0.2477,\n",
      "         0.0699,  0.3300,  0.3027,  0.2179,  0.2695,  0.2400,  0.2497,  0.2283,\n",
      "         0.2138,  0.2517,  0.1771,  0.2491,  0.2718,  0.2189,  0.2426,  0.2450,\n",
      "         0.2617,  0.0489,  0.2396,  0.2641,  0.2626,  0.2755,  0.2103, -0.1182,\n",
      "         0.2378,  0.2738,  0.2584,  0.2554,  0.2252,  0.2236,  0.2326,  0.2467,\n",
      "         0.2779,  0.2998,  0.2325,  0.1961,  0.2735,  0.2594,  0.2412,  0.2280,\n",
      "         0.2970,  0.2883,  0.2105,  0.2585,  0.2590,  0.2677,  0.2439,  0.2340,\n",
      "         0.2407,  0.2949,  0.2645,  0.2244,  0.2446,  0.2642,  0.3122,  0.1034],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0335,  0.0169,  0.0978,  ...,  0.0049, -0.0314, -0.0059],\n",
      "        [ 0.0023, -0.0320, -0.0202,  ..., -0.1409,  0.0210, -0.0292],\n",
      "        [ 0.0499, -0.0015, -0.0098,  ...,  0.0645,  0.0653,  0.0919],\n",
      "        ...,\n",
      "        [-0.0470,  0.0069, -0.0527,  ..., -0.0337,  0.0377, -0.0289],\n",
      "        [ 0.0410,  0.0246,  0.0737,  ...,  0.0794,  0.0371,  0.0043],\n",
      "        [-0.0430, -0.0031, -0.0177,  ...,  0.0705, -0.0490, -0.0055]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4355,  0.0166, -0.3311,  ...,  0.4468, -0.4952, -1.5052],\n",
      "        [ 0.6841, -0.7152, -0.1442,  ..., -0.1361,  0.8138,  0.8489],\n",
      "        [ 0.8115,  0.6346,  0.0431,  ...,  0.7477, -1.2580,  0.5632],\n",
      "        ...,\n",
      "        [-0.0247,  0.2867, -0.3213,  ...,  0.1791,  0.0494,  0.9371],\n",
      "        [ 0.0344,  0.4031,  0.2654,  ..., -0.4828, -0.5817, -0.7683],\n",
      "        [-0.0668,  0.1704, -0.0601,  ...,  0.2386, -0.1587,  0.1405]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1053, -0.1707, -0.7607,  ..., -0.9217,  0.2588,  0.1648],\n",
      "        [ 1.8195,  1.0612, -0.5323,  ...,  0.4188, -0.7562, -0.1131],\n",
      "        [-0.9606, -1.5768, -0.8472,  ..., -0.8385, -1.0475, -0.2834],\n",
      "        ...,\n",
      "        [-0.9040, -0.2555,  0.7045,  ...,  1.2561, -1.4495,  0.2962],\n",
      "        [-0.2865, -0.3029, -0.3584,  ..., -0.8769,  2.1450, -0.2921],\n",
      "        [-0.0645, -0.5242, -0.6706,  ...,  0.9700,  0.3375, -0.4173]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2337,  0.6166, -1.2629,  ..., -0.1913, -1.3182, -1.1035],\n",
      "        [-0.3395, -0.3087,  0.1698,  ..., -1.3401,  0.8036, -0.7556],\n",
      "        [ 0.3117, -0.9182,  0.1085,  ...,  0.1395,  2.0587,  0.1694],\n",
      "        ...,\n",
      "        [-0.4109, -0.0684, -0.3717,  ...,  1.0969,  0.6829, -1.0320],\n",
      "        [ 0.1780, -0.6832,  0.5838,  ..., -0.5122, -0.6819,  0.2034],\n",
      "        [ 1.5010,  4.0118,  0.3105,  ..., -0.7244,  1.7404, -2.0168]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.3256e-02,  1.0424e-01,  1.1283e-01,  1.2592e-01,  1.4445e-01,\n",
      "         1.4018e-01,  1.1365e-01,  1.3735e-01,  1.2042e-01,  8.4938e-02,\n",
      "         1.2016e-01,  1.2017e-01,  1.5055e-01,  1.3288e-01,  1.1129e-01,\n",
      "         1.4956e-01,  1.4177e-01,  1.0093e-01,  1.3377e-01,  1.2450e-01,\n",
      "         1.4969e-01,  1.3771e-01,  1.2656e-01,  1.3419e-01,  1.2379e-01,\n",
      "         1.1321e-01,  1.0351e-01,  1.0482e-01,  1.2322e-01,  1.4226e-01,\n",
      "         1.3665e-01,  1.5297e-01,  1.2576e-01,  1.2678e-01,  1.6331e-01,\n",
      "         2.0726e-01,  1.4156e-01,  1.3436e-01,  1.2585e-01,  1.4649e-01,\n",
      "         1.5601e-01,  1.1455e-01,  1.3117e-01,  1.2713e-01,  1.2625e-01,\n",
      "         1.4039e-01,  1.3197e-01,  1.2981e-01,  1.4021e-01,  1.2224e-01,\n",
      "         1.3072e-01,  1.4533e-01,  1.4643e-01,  1.2351e-01,  1.1299e-01,\n",
      "         1.1588e-01,  1.4475e-01,  1.3044e-01,  1.4666e-01,  1.0991e-01,\n",
      "         1.1976e-01,  1.2093e-01,  1.3278e-01,  1.6025e-01,  1.4387e-01,\n",
      "         1.1931e-01,  1.1371e-01,  2.8692e-01,  1.6884e-01,  9.4719e-02,\n",
      "         3.8472e-01,  1.4775e-01, -1.9150e-02,  1.1985e-01,  1.3703e-01,\n",
      "         1.4735e-01,  1.3822e-01,  1.2727e-01,  1.1226e-01,  1.6393e-01,\n",
      "         1.4147e-01,  1.4038e-01,  1.6317e-01,  1.0877e-01,  1.3724e-01,\n",
      "         1.3030e-01,  1.6061e-01,  1.2928e-01,  1.3731e-01,  1.4495e-01,\n",
      "         1.3478e-01,  1.5473e-01,  1.3692e-01,  1.1178e-01,  1.0282e-01,\n",
      "         1.1681e-01,  1.3232e-01, -7.7191e-02,  1.2565e-01,  1.1040e-01,\n",
      "         1.4814e-01,  1.3104e-01,  1.0854e-01,  1.5610e-01,  1.2162e-01,\n",
      "         1.6345e-01,  1.1851e-01,  1.1008e-01,  1.0448e-01,  1.3964e-01,\n",
      "         1.0152e-01,  1.3212e-01,  1.1794e-01,  1.1866e-01,  1.3272e-01,\n",
      "         1.3902e-01,  1.1445e-01,  1.3371e-01,  9.3105e-02,  1.3231e-01,\n",
      "         1.1730e-01,  1.5028e-01,  3.2492e-01,  1.6555e-01,  1.1201e-01,\n",
      "         1.3882e-01,  1.2651e-01,  1.0354e-01,  1.1496e-01,  1.3995e-01,\n",
      "         1.6102e-01,  1.3611e-01,  1.1370e-01,  1.4662e-01,  1.2830e-01,\n",
      "         1.2211e-01,  1.3069e-01,  1.2907e-01,  1.2581e-01,  1.1030e-01,\n",
      "         1.6235e-01,  1.3555e-01,  1.1091e-01,  1.3505e-01,  1.5697e-01,\n",
      "         1.2742e-01,  1.1060e-01,  1.4089e-01,  1.2426e-01,  9.5346e-02,\n",
      "         1.2653e-01,  1.4059e-01,  1.4488e-01,  1.3095e-01,  1.2799e-01,\n",
      "         1.3573e-01,  1.3327e-01,  1.9551e-01,  1.4766e-01,  1.1223e-01,\n",
      "         1.3150e-01,  1.2286e-01,  1.4148e-01,  7.5700e-02,  1.0856e-01,\n",
      "         1.3548e-01,  1.7620e-01,  1.5043e-01,  1.1498e-01,  1.6744e-01,\n",
      "         1.5140e-01,  1.3358e-01,  1.3033e-01,  1.1547e-01,  1.4155e-01,\n",
      "         1.0487e-01,  1.1834e-01,  1.2330e-01,  1.1487e-01,  1.5682e-01,\n",
      "         9.9291e-02,  1.4469e-01,  1.1204e-01,  1.3817e-01,  1.1275e-01,\n",
      "         1.4102e-01,  1.0398e-01,  1.1431e-01,  1.1731e-01,  1.4790e-01,\n",
      "         1.2518e-01,  1.1031e-01,  1.5303e-01,  2.5197e-01,  1.0188e-01,\n",
      "         9.9688e-02,  1.2576e-01,  1.4601e-01,  1.2813e-01,  1.0834e-01,\n",
      "         1.0968e-01,  1.3143e-01,  9.8642e-02,  1.1266e-01,  1.0698e-01,\n",
      "         1.1939e-01,  1.3051e-01,  1.1756e-01,  1.3199e-01,  1.5674e-01,\n",
      "         1.7579e-01,  1.5175e-01,  1.3756e-01,  1.5263e-01,  1.4245e-01,\n",
      "         1.2335e-01,  1.3480e-01,  1.2589e-01,  1.6090e-01,  1.3516e-01,\n",
      "         1.4142e-01,  1.4030e-01,  1.3203e-01,  1.2016e-01,  1.2316e-01,\n",
      "         1.1107e-01,  1.2046e-01,  1.2964e-01,  1.5715e-01,  1.4155e-01,\n",
      "         1.3183e-01,  7.2758e-02,  1.2165e-01,  2.2771e-01,  1.3095e-01,\n",
      "         1.5828e-01,  1.2733e-01,  1.1185e-01,  1.4900e-01,  1.3659e-01,\n",
      "         1.6051e-01,  1.3092e-01, -5.7232e-04,  1.4556e-01,  1.3850e-01,\n",
      "         1.0796e-01,  9.7735e-02,  4.7253e-01,  1.4681e-01,  1.4016e-01,\n",
      "         1.4840e-01,  1.5451e-01,  1.0706e-01,  1.3787e-01,  1.2421e-01,\n",
      "         1.6368e-01,  1.3693e-01,  1.1661e-01,  1.4605e-01,  1.0816e-01,\n",
      "         1.2231e-01,  1.4368e-01,  1.3883e-01,  1.1313e-01,  1.3072e-01,\n",
      "         1.0439e-01,  1.2059e-01,  1.3775e-01,  2.0839e-01,  1.2702e-01,\n",
      "         1.2140e-01,  1.5755e-01,  8.6480e-02,  1.3719e-01,  1.0671e-01,\n",
      "         1.2733e-02,  1.6080e-01,  1.2387e-01,  1.0115e-01,  1.4468e-01,\n",
      "        -8.8149e-02,  1.3980e-01,  1.2462e-01,  1.4455e-01,  1.7305e-01,\n",
      "         1.3690e-01,  1.2514e-01,  1.1165e-01,  1.4829e-01,  1.2456e-01,\n",
      "         1.0806e-01,  1.1849e-01,  1.2862e-01,  1.3949e-01,  1.1412e-01,\n",
      "         9.2281e-02,  1.8622e-01,  1.2935e-01,  1.5545e-01,  1.2439e-01,\n",
      "         1.3034e-01,  1.3903e-01,  1.0279e-01,  1.5889e-01,  1.3833e-01,\n",
      "         9.8755e-02,  1.1575e-01,  1.1342e-01,  1.2477e-01,  1.0759e-01,\n",
      "         1.2792e-01,  1.1504e-01,  1.3787e-01,  1.2488e-01,  1.5587e-01,\n",
      "         1.4143e-01,  1.5985e-01,  1.2024e-01,  1.5540e-01,  1.1043e-01,\n",
      "         1.4272e-01,  1.3564e-01,  1.4464e-01,  1.1492e-01,  1.1985e-01,\n",
      "         9.3374e-02,  1.4491e-01,  1.4648e-01,  1.1072e-01,  1.5224e-01,\n",
      "         1.3324e-01,  1.3329e-01,  1.4182e-01,  1.4347e-01,  1.4917e-01,\n",
      "         1.1661e-01,  1.0769e-01,  1.3715e-01,  1.0137e-01,  1.2390e-01,\n",
      "         1.4318e-01,  1.3388e-01,  1.3335e-01,  1.3460e-01,  1.3247e-01,\n",
      "         1.2358e-01,  9.3636e-02,  1.2646e-01,  1.2185e-01,  1.4624e-01,\n",
      "         1.5857e-01,  1.2767e-01,  8.4132e-02,  1.1999e-01,  1.3899e-01,\n",
      "         1.3634e-01,  1.3752e-01,  1.3297e-01,  1.5951e-01,  1.1054e-01,\n",
      "         1.2232e-01,  9.6483e-02,  1.3278e-01,  1.1115e-01,  1.1191e-01,\n",
      "         1.5408e-01,  1.3478e-01,  2.2190e-01,  1.7224e-01,  1.0497e-01,\n",
      "         1.3612e-01,  1.1506e-01,  1.1212e-01,  1.4682e-01,  1.2744e-01,\n",
      "         1.2772e-01,  1.2292e-01,  1.2574e-01,  1.4091e-01,  1.1732e-01,\n",
      "         1.0204e-01,  1.4618e-01,  1.5699e-01,  1.1367e-01,  9.8784e-02,\n",
      "         1.9208e-01,  1.2652e-01,  1.3870e-01,  1.2962e-01,  7.0569e-02,\n",
      "         1.0209e-01,  1.4507e-01,  1.1276e-01, -1.6843e-01,  1.2406e-01,\n",
      "         1.2526e-01,  2.0525e-04,  1.3072e-01,  1.3953e-01,  1.1004e-01,\n",
      "         1.1259e-01,  1.5321e-01,  1.2065e-01,  1.8077e-01,  1.1935e-01,\n",
      "         1.2235e-01,  8.8496e-02,  1.3792e-01,  1.3023e-01,  1.2112e-01,\n",
      "         1.3731e-01,  1.5366e-01,  1.2231e-01,  1.1001e-01,  7.2794e-02,\n",
      "         1.7602e-01,  1.2211e-01,  1.0629e-01,  1.2849e-01,  1.3888e-01,\n",
      "         1.4554e-01,  1.2937e-01,  1.5445e-01,  1.4431e-01,  1.1335e-01,\n",
      "         1.6116e-01,  1.3423e-01,  1.4054e-01,  4.6310e-01,  1.4176e-01,\n",
      "         1.2664e-01,  1.5264e-01,  1.1547e-01,  1.2733e-01,  1.2550e-01,\n",
      "         1.2884e-01,  1.2439e-01,  1.2796e-01,  1.3431e-01,  1.0792e-01,\n",
      "         1.3203e-01,  1.1300e-01,  1.6419e-01,  1.4692e-01,  1.2412e-01,\n",
      "         9.5398e-02,  1.1065e-01,  1.1223e-01,  1.1806e-01,  1.1240e-01,\n",
      "         1.1753e-01,  1.3376e-01,  1.5371e-01,  1.1260e-01,  1.3448e-01,\n",
      "         1.2340e-01,  2.2139e-02,  1.4888e-01,  1.8237e-01,  1.1757e-01,\n",
      "         1.3376e-01,  1.2128e-01,  1.3484e-01,  1.2990e-01,  1.0961e-01,\n",
      "         1.3874e-01,  1.0121e-01,  1.3774e-01,  1.5227e-01,  1.1118e-01,\n",
      "         8.7887e-02,  1.2286e-01,  1.3330e-01,  5.7552e-04,  9.9955e-02,\n",
      "         1.2152e-01,  1.3882e-01,  1.5394e-01,  1.1130e-01,  7.1332e-02,\n",
      "         1.3132e-01,  1.4579e-01,  1.4170e-01,  1.2581e-01,  1.2057e-01,\n",
      "         1.2968e-01,  1.3442e-01,  1.3405e-01,  1.3941e-01,  2.1253e-01,\n",
      "         1.2035e-01,  9.4545e-02,  1.6039e-01,  1.3347e-01,  1.2422e-01,\n",
      "         1.0671e-01,  1.5156e-01,  1.7890e-01,  1.1758e-01,  1.2636e-01,\n",
      "         1.7431e-01,  1.4978e-01,  1.0207e-01,  1.2231e-01,  1.1319e-01,\n",
      "         1.1722e-01,  1.2808e-01,  1.1531e-01,  1.4289e-01,  1.4373e-01,\n",
      "         1.1349e-01, -1.6004e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1386, -0.4540,  0.1621,  ...,  0.2561, -0.0695,  0.0381],\n",
      "        [ 0.0590,  0.5955, -0.0475,  ...,  0.2693, -0.2763,  0.0023],\n",
      "        [ 0.0842,  0.2059, -0.2345,  ..., -0.3564,  0.0864, -0.0007],\n",
      "        ...,\n",
      "        [ 0.2653, -0.2115, -0.6760,  ...,  0.2550, -0.3933,  0.1209],\n",
      "        [ 0.0267, -0.0350, -0.1721,  ..., -0.1125, -0.0505,  0.0870],\n",
      "        [ 0.1478,  0.0663, -0.0295,  ...,  0.0213,  0.0476,  0.0709]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.2369e-01,  8.1130e-01, -1.0924e+00,  ...,  1.8478e+00,\n",
      "          7.4130e-01, -7.3197e-03],\n",
      "        [-3.4438e-02,  6.7322e-01, -1.2392e+00,  ..., -2.0762e+00,\n",
      "         -5.9368e-01,  9.5272e-04],\n",
      "        [-4.7612e-01,  7.4775e-01, -3.2928e-01,  ...,  7.2212e-01,\n",
      "          2.3585e+00, -2.0295e-01],\n",
      "        ...,\n",
      "        [-3.5656e-01,  6.3819e-01, -1.0429e+00,  ..., -2.5619e-01,\n",
      "         -1.3591e+00, -8.2653e-01],\n",
      "        [ 1.0416e-01, -1.0311e+00, -6.6524e-01,  ...,  8.2171e-01,\n",
      "          6.9401e-01,  1.9939e-01],\n",
      "        [ 2.5928e-01,  7.6941e-02,  9.3393e-01,  ...,  8.4816e-01,\n",
      "         -1.8516e+00, -4.6530e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.3216,  0.2577, -0.5334,  ..., -0.3565,  0.6437,  2.2072],\n",
      "        [-1.1647,  0.1030,  0.9975,  ...,  1.4725,  1.7576,  0.6245],\n",
      "        [ 0.0269, -0.6944,  0.7575,  ...,  0.3338,  0.4361,  0.1613],\n",
      "        ...,\n",
      "        [ 0.7256,  0.7856, -0.2667,  ..., -0.0908, -0.6961, -0.1404],\n",
      "        [ 0.3078,  0.6330, -1.1718,  ...,  0.7411,  0.2610, -0.4789],\n",
      "        [ 2.6524, -0.3741,  0.3397,  ...,  3.1768,  0.9559,  0.3770]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0486,  0.2114,  0.2098,  0.2368,  0.2159,  0.2088,  0.2240,  0.2277,\n",
      "         0.1994,  0.2164,  0.2139,  0.1852,  0.2199,  0.2367,  0.2069,  0.1954,\n",
      "         0.2052,  0.2128,  0.2325,  0.2207,  0.2114,  0.2217,  0.2247,  0.2210,\n",
      "         0.2271,  0.2041,  0.2066,  0.2010,  0.2138,  0.2041,  0.2079,  0.2065,\n",
      "         0.2451,  0.2082,  0.2384,  0.2747,  0.2135,  0.2172,  0.2302,  0.2053,\n",
      "         0.2362,  0.2107,  0.2297,  0.2070,  0.2198,  0.2390,  0.2226,  0.2166,\n",
      "         0.2336,  0.2098,  0.2195,  0.2537,  0.2247,  0.2162,  0.2179,  0.2143,\n",
      "         0.2157,  0.2012,  0.2226,  0.1892,  0.2130,  0.2134,  0.2416,  0.2176,\n",
      "         0.2577,  0.2138,  0.2208,  0.2604,  0.2228,  0.2044,  0.1730,  0.2170,\n",
      "         0.0325,  0.2094,  0.2567,  0.2201,  0.2458,  0.2319,  0.2249,  0.2287,\n",
      "         0.2081,  0.2187,  0.2145,  0.2119,  0.2208,  0.2192,  0.2708,  0.2273,\n",
      "         0.2250,  0.2275,  0.2238,  0.2141,  0.1999,  0.2340,  0.2080,  0.2324,\n",
      "         0.2207,  0.1205,  0.2090,  0.1984,  0.2366,  0.2140,  0.1989,  0.2223,\n",
      "         0.2291,  0.2225,  0.2222,  0.2086,  0.2331,  0.2167,  0.1975,  0.2096,\n",
      "         0.2257,  0.2234,  0.2172,  0.2126,  0.2014,  0.2403,  0.2318,  0.2238,\n",
      "         0.2319,  0.2397,  0.2960,  0.2224,  0.2389,  0.2183,  0.2252,  0.2084,\n",
      "         0.2241,  0.2243,  0.2274,  0.2196,  0.1839,  0.2334,  0.2355,  0.2253,\n",
      "         0.2173,  0.2153,  0.2296,  0.1946,  0.2133,  0.2010,  0.2252,  0.2442,\n",
      "         0.2268,  0.2393,  0.2147,  0.2533,  0.2060,  0.2160,  0.2108,  0.2325,\n",
      "         0.2117,  0.2373,  0.2154,  0.2338,  0.2329,  0.2303,  0.2385,  0.2261,\n",
      "         0.2283,  0.2220,  0.2253,  0.1783,  0.2258,  0.2344,  0.2234,  0.2436,\n",
      "         0.2248,  0.2213,  0.2316,  0.2212,  0.2250,  0.2237,  0.2274,  0.2205,\n",
      "         0.2222,  0.2209,  0.2118,  0.2154,  0.2019,  0.2036,  0.2089,  0.2369,\n",
      "         0.2243,  0.2353,  0.1838,  0.2377,  0.2325,  0.2168,  0.2183,  0.2005,\n",
      "         0.2014,  0.3215,  0.2266,  0.2124,  0.2538,  0.2234,  0.2099,  0.2459,\n",
      "         0.2208,  0.2205,  0.1898,  0.2226,  0.1859,  0.2478,  0.2382,  0.2419,\n",
      "         0.2087,  0.2236,  0.2107,  0.2444,  0.2059,  0.2200,  0.2099,  0.2190,\n",
      "         0.2333,  0.2303,  0.2237,  0.2538,  0.2222,  0.2291,  0.2077,  0.2080,\n",
      "         0.2329,  0.2126,  0.2226,  0.2042,  0.2327,  0.2465,  0.1897,  0.1311,\n",
      "         0.2325,  0.2211,  0.2175,  0.2081,  0.2059,  0.2100,  0.2421,  0.2189,\n",
      "         0.2316,  0.2151,  0.0541,  0.2348,  0.2296,  0.2178,  0.1993,  0.4072,\n",
      "         0.2054,  0.2343,  0.2165,  0.2153,  0.2081,  0.2352,  0.2164,  0.2278,\n",
      "         0.2012,  0.2293,  0.2499,  0.2202,  0.2237,  0.2160,  0.2075,  0.1980,\n",
      "         0.2374,  0.2214,  0.2427,  0.2150,  0.2190,  0.2318,  0.2186,  0.2356,\n",
      "         0.2158,  0.2210,  0.2166, -0.0322,  0.2187,  0.2285,  0.2114,  0.2206,\n",
      "         0.1518,  0.2279,  0.2247,  0.2317,  0.2220,  0.1980,  0.2134,  0.2154,\n",
      "         0.2131,  0.2103,  0.2129,  0.2362,  0.2102,  0.2128,  0.2291,  0.2145,\n",
      "         0.2048,  0.2336,  0.2188,  0.2184,  0.2120,  0.2076,  0.2149,  0.2303,\n",
      "         0.2355,  0.2027,  0.2172,  0.2340,  0.2279,  0.2472,  0.2139,  0.2407,\n",
      "         0.2379,  0.1878,  0.2414,  0.2268,  0.2083,  0.2082,  0.2243,  0.2158,\n",
      "         0.2557,  0.2290,  0.2260,  0.2277,  0.2228,  0.2143,  0.2156,  0.2127,\n",
      "         0.2068,  0.1830,  0.2199,  0.2265,  0.2271,  0.1995,  0.2328,  0.1910,\n",
      "         0.2290,  0.2320,  0.2401,  0.2111,  0.2412,  0.2406,  0.2131,  0.2162,\n",
      "         0.2215,  0.2036,  0.2089,  0.2126,  0.2091,  0.2249,  0.2260,  0.2181,\n",
      "         0.1726,  0.2234,  0.2284,  0.2416,  0.2107,  0.2118,  0.2463,  0.2098,\n",
      "         0.2242,  0.2172,  0.2035,  0.2286,  0.2024,  0.2115,  0.2355,  0.1570,\n",
      "         0.2251,  0.2208,  0.2256,  0.2071,  0.2152,  0.2304,  0.2208,  0.2090,\n",
      "         0.1934,  0.2016,  0.2313,  0.2452,  0.1979,  0.2441,  0.2128,  0.2113,\n",
      "         0.1942,  0.2275,  0.2308,  0.2133,  0.2255, -0.1299,  0.2188,  0.2451,\n",
      "         0.2062,  0.2053,  0.2062,  0.2059, -0.0365,  0.2408,  0.2297,  0.1915,\n",
      "         0.2190,  0.2391,  0.2213,  0.2111,  0.1985,  0.2362,  0.2301,  0.2336,\n",
      "         0.2205,  0.2067,  0.2079,  0.2305,  0.1978,  0.2077,  0.1500,  0.2338,\n",
      "         0.2150,  0.2126,  0.2239,  0.2351,  0.2171,  0.2564,  0.2533,  0.2197,\n",
      "         0.2120,  0.2380,  0.2167,  0.2382,  0.2458,  0.2491,  0.2284,  0.2166,\n",
      "         0.2001,  0.1850,  0.2290,  0.2117,  0.2377,  0.2425,  0.2515,  0.2124,\n",
      "         0.2093,  0.2105,  0.2330,  0.2351,  0.2177,  0.1827,  0.2153,  0.2254,\n",
      "         0.2062,  0.2253,  0.1853,  0.1966,  0.1921,  0.2042,  0.2261,  0.2127,\n",
      "        -0.0367,  0.2363,  0.2331,  0.2119,  0.2325,  0.2212,  0.2438,  0.2163,\n",
      "         0.2215,  0.2323,  0.1857,  0.2113,  0.2122,  0.2143,  0.2222,  0.2464,\n",
      "         0.2247, -0.0351,  0.2009,  0.2296,  0.2122,  0.2129,  0.2036,  0.1340,\n",
      "         0.2297,  0.2178,  0.2195,  0.2263,  0.2103,  0.1972,  0.2170,  0.2178,\n",
      "         0.2260,  0.1793,  0.2155,  0.1893,  0.2226,  0.2529,  0.2413,  0.2176,\n",
      "         0.2431,  0.2599,  0.2146,  0.2307,  0.2263,  0.2508,  0.2319,  0.2195,\n",
      "         0.2172,  0.2199,  0.2158,  0.2197,  0.2269,  0.2174,  0.2313, -0.0309],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0142, -0.0155,  0.1017,  ...,  0.0122,  0.0451,  0.0004],\n",
      "        [-0.0046,  0.0695, -0.0624,  ..., -0.0639,  0.0076,  0.0226],\n",
      "        [ 0.0077,  0.0172, -0.0302,  ..., -0.0321, -0.0121,  0.0017],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0236, -0.0497,  ...,  0.0920,  0.0743, -0.0257],\n",
      "        [-0.0040, -0.0321, -0.0054,  ...,  0.0438,  0.0096,  0.0271],\n",
      "        [ 0.0067, -0.0031,  0.0329,  ..., -0.0139, -0.0370,  0.0368]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0862, -0.6344, -0.0588,  ...,  0.1275, -0.5204, -0.0386],\n",
      "        [-0.0534,  0.3488,  0.4326,  ..., -0.3952, -0.0814, -0.1003],\n",
      "        [ 0.1278, -0.0219,  0.2344,  ..., -0.0802, -0.1081,  0.0446],\n",
      "        ...,\n",
      "        [ 0.0185,  0.1567, -0.4958,  ...,  0.4821,  0.0231, -0.0013],\n",
      "        [-0.0028,  0.2143,  0.2047,  ..., -0.1296, -0.1470, -0.1243],\n",
      "        [-0.0010, -0.1445, -0.0593,  ..., -0.2221,  0.1464, -0.0922]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.4519e-02,  9.9238e-01,  4.3639e-01,  ...,  3.3310e-01,\n",
      "         -4.2449e-01,  5.1173e-02],\n",
      "        [-7.9186e-02, -9.8924e-01,  7.4898e-01,  ...,  3.6478e-01,\n",
      "          2.5879e-01,  6.2963e-04],\n",
      "        [-1.4903e-01, -2.0991e+00, -1.0965e+00,  ..., -5.0909e-01,\n",
      "         -4.4202e-01,  1.2727e-02],\n",
      "        ...,\n",
      "        [-1.9093e-01, -7.6727e-01,  4.6080e-01,  ...,  2.2777e-01,\n",
      "         -2.6762e-01,  6.3391e-02],\n",
      "        [ 9.3323e-02, -3.1129e-01,  1.8971e+00,  ...,  7.8824e-01,\n",
      "         -1.5363e-01,  1.4234e-01],\n",
      "        [ 7.1857e-02, -1.2097e-01,  4.3896e-01,  ...,  4.9453e-02,\n",
      "         -1.0087e-01, -3.5270e-03]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.4906, -0.3695,  0.6238,  ...,  4.5279, -4.6895,  0.3547],\n",
      "        [ 1.0810, -0.4363, -1.5568,  ..., -0.4425,  0.8566, -0.1213],\n",
      "        [ 0.8704,  0.6214, -0.1337,  ..., -1.4496, -0.0742, -0.0640],\n",
      "        ...,\n",
      "        [ 0.8173, -0.6757,  0.4352,  ..., -0.1994,  1.3633, -0.1162],\n",
      "        [ 0.5315,  0.5131,  1.0092,  ..., -0.6396,  0.3810,  0.1201],\n",
      "        [ 1.2320,  0.6530,  0.6633,  ...,  2.5870, -4.6157, -4.5806]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2424,  0.2360,  0.2581,  0.3110,  0.3445,  0.2869,  0.3179,  0.3107,\n",
      "         0.2834,  0.3082,  0.2434,  0.2469,  0.3046,  0.3686,  0.2690,  0.2695,\n",
      "         0.3276,  0.2419,  0.3242,  0.3154,  0.2547,  0.2990,  0.3006,  0.2812,\n",
      "         0.2645,  0.2498,  0.2325,  0.2569,  0.2762,  0.2979,  0.2719,  0.2689,\n",
      "         0.2981,  0.2368,  0.3380,  0.4320,  0.2972,  0.2609,  0.3048,  0.2777,\n",
      "         0.2992,  0.3052,  0.3243,  0.3037,  0.2871,  0.3179,  0.2824,  0.2946,\n",
      "         0.3345,  0.2878,  0.2829,  0.3090,  0.3214,  0.2759,  0.2773,  0.3456,\n",
      "         0.2802,  0.3010,  0.2899,  0.2328,  0.3057,  0.3120,  0.3012,  0.3076,\n",
      "         0.3671,  0.2850,  0.2937,  0.6127,  0.2805,  0.2484,  1.1775,  0.2681,\n",
      "         0.0780,  0.3088,  0.3054,  0.2804,  0.3361,  0.2777,  0.2688,  0.3928,\n",
      "         0.3414,  0.3443,  0.3692,  0.2978,  0.2979,  0.2664,  0.3477,  0.3003,\n",
      "         0.3050,  0.2887,  0.2879,  0.2772,  0.3228,  0.3093,  0.2671,  0.3218,\n",
      "         0.2732, -0.1954,  0.2935,  0.2268,  0.2847,  0.2875,  0.2794,  0.3419,\n",
      "         0.3417,  0.3173,  0.2548,  0.2632,  0.3203,  0.3253,  0.2076,  0.2744,\n",
      "         0.2911,  0.2716,  0.3089,  0.2975,  0.2733,  0.3159,  0.3136,  0.2533,\n",
      "         0.2940,  0.3082,  0.0457,  0.3282,  0.2941,  0.2956,  0.3082,  0.2645,\n",
      "         0.3091,  0.3156,  0.3409,  0.2964,  0.2184,  0.2842,  0.3139,  0.2990,\n",
      "         0.2998,  0.3165,  0.2993,  0.2420,  0.2549,  0.2865,  0.3140,  0.3495,\n",
      "         0.3089,  0.3356,  0.2883,  0.3744,  0.2742,  0.2995,  0.2788,  0.4005,\n",
      "         0.2956,  0.2861,  0.2988,  0.2653,  0.3100,  0.6714,  0.2944,  0.3036,\n",
      "         0.2976,  0.2844,  0.3897,  0.2229,  0.2775,  0.3300,  0.2698,  0.2839,\n",
      "         0.3060,  0.3047,  0.3008,  0.3268,  0.2916,  0.2602,  0.2920,  0.2787,\n",
      "         0.2822,  0.3296,  0.2773,  0.2511,  0.2412,  0.2911,  0.3047,  0.3275,\n",
      "         0.2923,  0.3255,  0.2085,  0.3147,  0.2973,  0.3491,  0.3265,  0.2819,\n",
      "         0.2527,  0.4374,  0.2806,  0.2504,  0.3722,  0.4836,  0.3015,  0.3030,\n",
      "         0.2645,  0.3313,  0.2577,  0.2454,  0.2041,  0.2888,  0.2966,  0.2972,\n",
      "         0.2450,  0.3510,  0.3264,  0.3057,  0.2674,  0.3141,  0.3017,  0.2328,\n",
      "         0.3260,  0.3068,  0.4648,  0.3457,  0.3037,  0.3020,  0.2535,  0.2244,\n",
      "         0.2943,  0.2763,  0.2600,  0.3542,  0.2829,  0.3019,  0.2675, -0.1807,\n",
      "         0.3063,  0.0453,  0.2933,  0.3317,  0.2714,  0.2407,  0.3217,  0.3289,\n",
      "         0.2208,  0.2751, -0.0776,  0.3364,  0.3532,  0.3044,  0.2555,  0.0376,\n",
      "         0.2510,  0.2948,  0.2921,  0.2821,  0.2663,  0.3173,  0.3122,  0.3665,\n",
      "         0.2897,  0.2802,  0.3160,  0.3022,  0.3337,  0.2880,  0.2901,  0.2421,\n",
      "         0.3013,  0.2753,  0.3070,  0.3015,  0.3576,  0.4086,  0.2641,  0.3047,\n",
      "         0.2567,  0.2697,  0.2534, -0.0709,  0.3443,  0.2581,  0.2686,  0.2940,\n",
      "         0.1213,  0.2699,  0.2978,  0.3363,  0.3218,  0.2770,  0.3008,  0.2686,\n",
      "         0.3269,  0.3173,  0.2355,  0.2850,  0.2586,  0.3021,  0.3096,  0.2582,\n",
      "         0.2915,  0.3628,  0.2864,  0.2723,  0.3214,  0.3020,  0.2481,  0.2690,\n",
      "         0.3094,  0.2518,  0.2739,  0.2808,  0.3268,  0.2882,  0.2638,  0.2934,\n",
      "         0.3113,  0.2745,  0.3261,  0.3569,  0.2754,  0.2759,  0.2657,  0.3528,\n",
      "         0.2871,  0.3241,  0.3523,  0.3629,  0.2390,  0.2494,  0.3840,  0.3064,\n",
      "         0.2592,  0.3846,  0.3049,  0.3265,  0.3207,  0.3068,  0.2909,  0.2615,\n",
      "         0.2896,  0.3549,  0.3039,  0.2862,  0.3207,  0.2934,  0.3153,  0.3161,\n",
      "         0.3189,  0.2597,  0.2628,  0.2372,  0.3126,  0.2731,  0.2908,  0.2839,\n",
      "         0.3729,  0.3328,  0.2946,  0.3889,  0.2782,  0.2934,  0.2953,  0.2750,\n",
      "         0.2740,  0.2668,  0.2471,  0.3020,  0.2625,  0.2864,  0.2911,  0.2394,\n",
      "         0.3137,  0.2851,  0.2944,  0.2875,  0.2586,  0.3149,  0.2971,  0.2752,\n",
      "         0.2849,  0.2627,  0.2879,  0.3029,  0.2620,  0.3050,  0.3162,  0.2852,\n",
      "         0.2473,  0.3839,  0.3245,  0.3006,  0.3200, -0.1668,  0.2759,  0.3233,\n",
      "         0.2505,  0.2936,  0.2392,  0.2730,  0.0869,  0.3304,  0.3222,  0.2590,\n",
      "         0.2943,  0.3875,  0.2850,  0.3194,  0.3036,  0.2781,  0.3118,  0.2865,\n",
      "         0.3231,  0.2798,  0.3540,  0.3524,  0.2727,  0.2823,  0.2009,  0.3983,\n",
      "         0.3263,  0.2422,  0.3054,  0.2982,  0.2978,  0.3266,  0.3217,  0.3312,\n",
      "         0.2964,  0.2956,  0.2842,  0.3142,  0.9170,  0.3103,  0.3252,  0.2674,\n",
      "         0.2641,  0.2435,  0.3215,  0.2859,  0.3794,  0.2934,  0.3189,  0.2538,\n",
      "         0.2911,  0.2680,  0.3124,  0.3106,  0.2584, -0.2350,  0.2878,  0.2848,\n",
      "         0.2628,  0.2876,  0.3157,  0.3144,  0.2359,  0.3217,  0.3551,  0.2653,\n",
      "        -0.0810,  0.4430,  0.4168,  0.2703,  0.3565,  0.2922,  0.2672,  0.3261,\n",
      "         0.2766,  0.3419,  0.2559,  0.3090,  0.2878,  0.2478,  0.3058,  0.3004,\n",
      "         0.3107, -0.0537,  0.2818,  0.3210,  0.2776,  0.4045,  0.2322,  0.1131,\n",
      "         0.3092,  0.3510,  0.2789,  0.2709,  0.2679,  0.2837,  0.2441,  0.2940,\n",
      "         0.2934,  0.4847,  0.2637,  0.2259,  0.3058,  0.3120,  0.3376,  0.2810,\n",
      "         0.3035,  0.4362,  0.2956,  0.2673,  0.2698,  0.3179,  0.2740,  0.2573,\n",
      "         0.2844,  0.3248,  0.2595,  0.2697,  0.2878,  0.2766,  0.3517, -0.0809],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0017,  0.0308,  0.0041,  ..., -0.0741, -0.0014,  0.0083],\n",
      "        [ 0.0280,  0.0063,  0.0199,  ..., -0.0425, -0.0213, -0.0022],\n",
      "        [-0.0395,  0.0211,  0.0650,  ..., -0.0246, -0.0821,  0.0105],\n",
      "        ...,\n",
      "        [ 0.0104, -0.0708, -0.0199,  ...,  0.0739, -0.0024, -0.0073],\n",
      "        [ 0.0101, -0.0290,  0.0314,  ...,  0.0264, -0.0508, -0.0095],\n",
      "        [-0.0016,  0.0429, -0.0735,  ..., -0.0723,  0.1171,  0.0096]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052, -0.6167,  0.3462,  ...,  0.1095,  0.7985, -0.0423],\n",
      "        [-0.1906,  0.3384, -0.2551,  ...,  0.0515,  0.0633,  0.2871],\n",
      "        [-0.1600,  0.1940,  0.3271,  ..., -0.3804, -0.7604, -0.7628],\n",
      "        ...,\n",
      "        [-0.4179,  1.0780,  1.1617,  ..., -0.6111,  0.7717,  0.5979],\n",
      "        [ 0.8041, -0.0959,  0.7743,  ...,  0.6198, -0.6030,  0.3088],\n",
      "        [-0.3581,  0.6685,  0.0749,  ...,  0.4536,  1.1826,  0.5527]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.8919e-01,  6.1728e-01, -1.3938e-01,  ..., -2.9334e-01,\n",
      "          9.8316e-01, -4.4273e-01],\n",
      "        [ 1.3719e+00,  6.2183e-03,  5.3804e-01,  ..., -4.0146e-02,\n",
      "         -3.9021e-01,  3.7090e-02],\n",
      "        [-7.0800e-01, -6.3195e-01,  7.1747e-01,  ...,  1.6443e+00,\n",
      "          7.1115e-01, -7.2349e-02],\n",
      "        ...,\n",
      "        [ 1.6233e+00, -6.5042e-02,  6.2704e-01,  ...,  3.2980e-01,\n",
      "         -7.9496e-01, -4.6434e-01],\n",
      "        [-8.2088e-01,  3.9323e-01, -2.6735e-01,  ..., -3.6462e-04,\n",
      "          2.9356e-01, -2.4897e-01],\n",
      "        [-9.7209e-01, -3.2121e-01,  1.1524e+00,  ..., -6.3201e-01,\n",
      "          1.7960e-01,  4.0470e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-4.0484,  2.7012, -0.8762,  ..., -0.7464, -1.6431,  0.2374],\n",
      "        [ 1.4652,  0.0902, -0.5194,  ...,  0.0892, -0.2154, -0.4794],\n",
      "        [ 0.2304, -0.1871,  1.7802,  ..., -0.9059, -0.1079,  0.3758],\n",
      "        ...,\n",
      "        [-0.3426, -0.2427, -0.1461,  ..., -0.5821,  1.3148, -0.7166],\n",
      "        [ 1.7725,  0.6284, -0.2035,  ...,  0.8102,  0.2999,  0.3778],\n",
      "        [ 1.1361, -1.5332, -2.0897,  ...,  3.1973,  3.7490, -1.9094]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 4.5130e-02,  1.6226e-01,  1.5569e-01,  1.7165e-01,  1.9263e-01,\n",
      "         1.4878e-01,  1.8390e-01,  1.5261e-01,  1.4855e-01,  1.4354e-01,\n",
      "         1.5679e-01,  1.3380e-01,  1.7147e-01,  1.9882e-01,  1.8669e-01,\n",
      "         2.2258e-01,  1.8225e-01,  1.3886e-01,  1.7995e-01,  1.9489e-01,\n",
      "         1.6308e-01,  1.8232e-01,  1.9898e-01,  1.8262e-01,  1.4683e-01,\n",
      "         1.7557e-01,  1.2871e-01,  1.3513e-01,  1.7461e-01,  2.0492e-01,\n",
      "         1.7781e-01,  1.5341e-01,  1.8824e-01,  1.4430e-01,  2.1960e-01,\n",
      "         2.5128e-01,  1.9525e-01,  1.7193e-01,  1.6446e-01,  1.8799e-01,\n",
      "         1.7678e-01,  1.8518e-01,  1.8243e-01,  1.8207e-01,  1.8552e-01,\n",
      "         1.5074e-01,  1.6267e-01,  1.6942e-01,  1.7037e-01,  1.6353e-01,\n",
      "         1.6834e-01,  1.9581e-01,  2.1464e-01,  1.8176e-01,  1.8137e-01,\n",
      "         1.5732e-01,  1.8430e-01,  1.7438e-01,  1.8183e-01,  1.3905e-01,\n",
      "         1.8651e-01,  1.5494e-01,  1.8740e-01,  1.7590e-01,  1.7424e-01,\n",
      "         1.7092e-01,  1.8138e-01,  3.4273e-01,  1.8967e-01,  1.4190e-01,\n",
      "         3.5041e-01,  1.6363e-01,  3.0545e-02,  1.3554e-01,  1.8589e-01,\n",
      "         2.2096e-01,  2.3130e-01,  1.7487e-01,  1.6123e-01,  1.9771e-01,\n",
      "         2.1604e-01,  1.8835e-01,  2.2843e-01,  1.3762e-01,  1.8599e-01,\n",
      "         1.7067e-01,  1.9468e-01,  1.5369e-01,  1.7870e-01,  1.7637e-01,\n",
      "         1.8968e-01,  1.7494e-01,  1.9710e-01,  1.6354e-01,  1.3787e-01,\n",
      "         1.3784e-01,  1.6043e-01,  1.1160e-01,  1.6111e-01,  1.3013e-01,\n",
      "         1.7588e-01,  1.9864e-01,  1.6435e-01,  2.1839e-01,  1.7272e-01,\n",
      "         1.8674e-01,  1.7702e-01,  1.6001e-01,  1.5992e-01,  1.6155e-01,\n",
      "         1.2071e-01,  1.7103e-01,  1.6076e-01,  1.7351e-01,  1.6293e-01,\n",
      "         1.7988e-01,  1.6411e-01,  1.9385e-01,  1.2319e-01,  1.5934e-01,\n",
      "         1.7335e-01,  1.8626e-01,  3.7478e-01,  1.8712e-01,  1.8251e-01,\n",
      "         1.6435e-01,  1.8332e-01,  1.6668e-01,  1.8454e-01,  1.9209e-01,\n",
      "         1.7038e-01,  1.7779e-01,  1.3731e-01,  1.8592e-01,  1.8184e-01,\n",
      "         1.4536e-01,  2.0150e-01,  1.5669e-01,  1.7574e-01,  1.3757e-01,\n",
      "         1.7724e-01,  1.7776e-01,  1.5486e-01,  1.7515e-01,  1.8877e-01,\n",
      "         1.7342e-01,  1.5170e-01,  2.3373e-01,  1.5032e-01,  1.4471e-01,\n",
      "         1.9291e-01,  1.9099e-01,  1.6963e-01,  1.4752e-01,  1.7360e-01,\n",
      "         1.5009e-01,  1.9645e-01,  2.0167e-01,  1.7798e-01,  1.6216e-01,\n",
      "         1.6566e-01,  1.7497e-01,  1.9002e-01,  1.2055e-01,  1.5428e-01,\n",
      "         1.8810e-01,  1.8454e-01,  1.8839e-01,  1.6614e-01,  1.7819e-01,\n",
      "         1.7648e-01,  1.4924e-01,  1.8245e-01,  1.6333e-01,  1.7694e-01,\n",
      "         1.5333e-01,  1.5135e-01,  1.8100e-01,  1.5400e-01,  1.7762e-01,\n",
      "         1.4327e-01,  1.8470e-01,  2.0350e-01,  1.9850e-01,  1.5377e-01,\n",
      "         1.9288e-01,  1.5296e-01,  1.9347e-01,  1.6695e-01,  1.8451e-01,\n",
      "         1.9014e-01,  1.4118e-01,  1.8047e-01,  2.5416e-01,  1.7790e-01,\n",
      "         1.6271e-01,  1.9225e-01,  1.6381e-01,  1.6663e-01,  1.7794e-01,\n",
      "         1.8082e-01,  1.6486e-01,  1.3846e-01,  1.5195e-01,  1.3995e-01,\n",
      "         1.7470e-01,  1.9216e-01,  1.7009e-01,  1.6394e-01,  2.1971e-01,\n",
      "         2.2090e-01,  2.1786e-01,  1.6019e-01,  1.6946e-01,  1.7232e-01,\n",
      "         1.2998e-01,  1.9229e-01,  1.8671e-01,  1.7350e-01,  2.1621e-01,\n",
      "         1.6939e-01,  2.0874e-01,  1.4785e-01,  1.5416e-01,  2.0315e-01,\n",
      "         1.8097e-01,  1.4910e-01,  1.6004e-01,  1.8952e-01,  1.8647e-01,\n",
      "         1.4366e-01, -9.6099e-02,  1.7922e-01,  2.6964e-01,  1.5910e-01,\n",
      "         2.1284e-01,  2.0484e-01,  1.6347e-01,  1.8277e-01,  1.6292e-01,\n",
      "         1.1986e-01,  1.6075e-01, -3.2645e-02,  2.1330e-01,  2.0995e-01,\n",
      "         1.7410e-01,  1.3898e-01,  5.8059e-01,  2.0102e-01,  1.6844e-01,\n",
      "         1.5642e-01,  1.8015e-01,  1.3672e-01,  1.8770e-01,  1.6416e-01,\n",
      "         1.9061e-01,  1.8532e-01,  1.7984e-01,  1.8655e-01,  1.6934e-01,\n",
      "         1.6346e-01,  1.7432e-01,  1.5587e-01,  1.5188e-01,  1.6948e-01,\n",
      "         1.5275e-01,  1.8073e-01,  1.6679e-01,  2.0877e-01,  1.9192e-01,\n",
      "         1.6599e-01,  1.6903e-01,  1.4290e-01,  1.6292e-01,  1.4335e-01,\n",
      "         4.7070e-04,  2.4240e-01,  1.6209e-01,  1.5526e-01,  1.8887e-01,\n",
      "        -1.0871e-01,  1.6427e-01,  1.5189e-01,  1.9103e-01,  2.1222e-01,\n",
      "         1.2145e-01,  1.4822e-01,  1.4900e-01,  2.1000e-01,  1.7364e-01,\n",
      "         1.3028e-01,  1.8543e-01,  1.6459e-01,  2.0514e-01,  1.6617e-01,\n",
      "         1.5927e-01,  1.9745e-01,  1.9063e-01,  1.7735e-01,  1.7807e-01,\n",
      "         1.9355e-01,  1.7696e-01,  1.2810e-01,  1.8681e-01,  1.9947e-01,\n",
      "         1.3403e-01,  1.3595e-01,  1.6194e-01,  1.7848e-01,  1.8338e-01,\n",
      "         1.5056e-01,  1.5528e-01,  1.5888e-01,  1.4803e-01,  2.2788e-01,\n",
      "         1.7179e-01,  1.9081e-01,  1.5070e-01,  1.9638e-01,  1.8819e-01,\n",
      "         1.6744e-01,  1.9042e-01,  1.8517e-01,  1.8573e-01,  1.6926e-01,\n",
      "         1.3352e-01,  1.6779e-01,  1.6461e-01,  1.3097e-01,  1.3590e-01,\n",
      "         1.6472e-01,  1.6054e-01,  1.7856e-01,  1.8083e-01,  1.8820e-01,\n",
      "         1.6296e-01,  2.0183e-01,  1.6799e-01,  1.6384e-01,  1.6679e-01,\n",
      "         1.9464e-01,  1.6642e-01,  1.6483e-01,  1.8715e-01,  1.9871e-01,\n",
      "         1.7496e-01,  1.4418e-01,  1.2786e-01,  1.5036e-01,  2.0018e-01,\n",
      "         1.9202e-01,  2.1004e-01,  1.2050e-01,  1.9090e-01,  1.7990e-01,\n",
      "         2.1279e-01,  1.5510e-01,  1.8161e-01,  1.8633e-01,  1.3962e-01,\n",
      "         1.5134e-01,  1.4651e-01,  1.6501e-01,  1.7470e-01,  1.6220e-01,\n",
      "         1.6241e-01,  1.8108e-01,  1.4550e-01,  2.0580e-01,  1.7775e-01,\n",
      "         1.3984e-01,  1.6667e-01,  1.4205e-01,  2.1255e-01,  1.7988e-01,\n",
      "         1.6222e-01,  1.3489e-01,  1.6456e-01,  1.9567e-01,  1.7305e-01,\n",
      "         1.4115e-01,  1.9989e-01,  1.9513e-01,  1.5929e-01,  1.3914e-01,\n",
      "         2.6939e-01,  1.6665e-01,  1.9066e-01,  1.8765e-01,  8.9844e-02,\n",
      "         1.4541e-01,  2.1190e-01,  1.6756e-01,  1.6748e-01,  1.4596e-01,\n",
      "         1.5988e-01,  8.4280e-04,  1.8032e-01,  1.8663e-01,  1.5177e-01,\n",
      "         1.4952e-01,  1.8732e-01,  1.3521e-01,  2.0678e-01,  1.7583e-01,\n",
      "         1.8186e-01,  1.3871e-01,  1.5649e-01,  1.8371e-01,  1.4020e-01,\n",
      "         1.4865e-01,  2.2085e-01,  1.9471e-01,  1.7361e-01,  1.3087e-01,\n",
      "         2.4935e-01,  1.7488e-01, -1.5073e-01,  1.7203e-01,  1.6936e-01,\n",
      "         1.5037e-01,  1.8402e-01,  2.0893e-01,  1.7215e-01,  2.0496e-01,\n",
      "         1.7595e-01,  2.0556e-01,  1.7509e-01,  4.5399e-01,  2.0117e-01,\n",
      "         2.0193e-01,  2.0327e-01,  1.6650e-01,  1.6205e-01,  1.5914e-01,\n",
      "         1.7390e-01,  2.1204e-01,  1.8122e-01,  2.0565e-01,  1.4021e-01,\n",
      "         1.7480e-01,  1.6409e-01,  1.8842e-01,  1.7542e-01,  1.4679e-01,\n",
      "         1.4132e-01,  1.6692e-01,  1.5639e-01,  1.6446e-01,  1.5932e-01,\n",
      "         1.7324e-01,  1.6823e-01,  1.4538e-01,  1.4542e-01,  1.9687e-01,\n",
      "         1.8261e-01,  4.4550e-02,  2.0765e-01,  2.4322e-01,  1.6148e-01,\n",
      "         1.8122e-01,  1.7635e-01,  1.7219e-01,  1.9150e-01,  1.6115e-01,\n",
      "         1.9804e-01,  1.1140e-01,  1.5255e-01,  1.7733e-01,  1.6318e-01,\n",
      "         1.7747e-01,  1.8182e-01,  1.9006e-01, -2.3818e-03,  1.7124e-01,\n",
      "         1.6917e-01,  1.5017e-01,  1.7295e-01,  1.6290e-01, -9.4344e-02,\n",
      "         1.4691e-01,  1.7106e-01,  1.9925e-01,  1.4615e-01,  1.7370e-01,\n",
      "         1.5925e-01,  1.8003e-01,  1.6053e-01,  1.8701e-01,  1.7165e-01,\n",
      "         1.7465e-01,  1.4397e-01,  2.0398e-01,  1.7512e-01,  1.9573e-01,\n",
      "         1.6469e-01,  1.8036e-01,  2.3264e-01,  1.8151e-01,  1.7227e-01,\n",
      "         2.4683e-01,  1.8229e-01,  1.6072e-01,  1.6102e-01,  1.7265e-01,\n",
      "         1.6806e-01,  1.5579e-01,  1.7891e-01,  1.7123e-01,  1.9237e-01,\n",
      "         1.6896e-01, -2.9018e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0434, -0.2534,  0.3223,  ...,  0.0858, -0.1565, -0.0256],\n",
      "        [ 0.0224,  0.0555, -0.0033,  ...,  0.0613, -0.3990, -0.0919],\n",
      "        [ 0.1060,  0.1496,  0.0539,  ..., -0.1209, -0.2438, -0.0367],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0307, -0.3988,  ...,  0.0335,  0.1567,  0.0418],\n",
      "        [ 0.4605, -0.5632, -0.4529,  ..., -0.4083, -0.0374, -0.1197],\n",
      "        [ 0.1427,  0.1997, -0.1595,  ...,  0.4598,  0.1002, -0.0278]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3034, -0.3993, -0.7330,  ..., -0.6472, -0.3566,  0.0216],\n",
      "        [-0.1687, -0.4811,  0.1249,  ...,  0.9246, -0.5676,  0.3544],\n",
      "        [-0.0529,  0.5990,  1.7858,  ..., -0.0262, -1.0405,  0.1288],\n",
      "        ...,\n",
      "        [ 0.5346, -0.9914,  0.6700,  ..., -0.4247, -1.6144,  0.0336],\n",
      "        [-1.1677,  0.5172,  1.7523,  ...,  0.1662, -1.2168,  0.2320],\n",
      "        [ 0.0638, -2.8798,  1.1645,  ..., -0.1998, -0.8701, -0.6957]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5351, -1.3104,  0.9748,  ...,  1.3557,  0.3374, -0.5764],\n",
      "        [ 0.2134,  2.0384, -0.3622,  ...,  1.5753, -1.8056,  0.4109],\n",
      "        [ 0.3134,  0.3348, -0.2666,  ...,  0.8571, -0.0164,  0.3293],\n",
      "        ...,\n",
      "        [ 0.7999, -0.6751,  0.5763,  ...,  0.2434, -0.7302,  0.3297],\n",
      "        [ 0.4987,  0.4168,  0.0420,  ..., -0.8533,  0.4803, -0.1284],\n",
      "        [ 1.3596, -2.5670,  1.3842,  ..., -1.4989,  3.8708, -0.4454]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0361,  0.2411,  0.2432,  0.2609,  0.2442,  0.2424,  0.2660,  0.2632,\n",
      "         0.2583,  0.2317,  0.2244,  0.2163,  0.2690,  0.3040,  0.2389,  0.2508,\n",
      "         0.2280,  0.2486,  0.2839,  0.2814,  0.2502,  0.3013,  0.2617,  0.2712,\n",
      "         0.2564,  0.2321,  0.2351,  0.2229,  0.2567,  0.2587,  0.2545,  0.2375,\n",
      "         0.2948,  0.2196,  0.3025,  0.3550,  0.2867,  0.2607,  0.2586,  0.2690,\n",
      "         0.2721,  0.2810,  0.2822,  0.2485,  0.2806,  0.2729,  0.2316,  0.2514,\n",
      "         0.2902,  0.2466,  0.2572,  0.2773,  0.2515,  0.2683,  0.2424,  0.2551,\n",
      "         0.2302,  0.2542,  0.2868,  0.2384,  0.2627,  0.2413,  0.2542,  0.2578,\n",
      "         0.2896,  0.2436,  0.2770,  0.3282,  0.2497,  0.2367,  0.2319,  0.2629,\n",
      "         0.0508,  0.2154,  0.2572,  0.2706,  0.2923,  0.2838,  0.2684,  0.2702,\n",
      "         0.2622,  0.2521,  0.2485,  0.2448,  0.2707,  0.2433,  0.2870,  0.2562,\n",
      "         0.2594,  0.2618,  0.2580,  0.2651,  0.2381,  0.2538,  0.2259,  0.2535,\n",
      "         0.2619,  0.1569,  0.2342,  0.2167,  0.2511,  0.2558,  0.2591,  0.2924,\n",
      "         0.2655,  0.2681,  0.2515,  0.2419,  0.2713,  0.2516,  0.2286,  0.2249,\n",
      "         0.2598,  0.2499,  0.2409,  0.2223,  0.2568,  0.2887,  0.2377,  0.2437,\n",
      "         0.2690,  0.2485,  0.2785,  0.2441,  0.2830,  0.2858,  0.2922,  0.2501,\n",
      "         0.2853,  0.2573,  0.2740,  0.2601,  0.2016,  0.2578,  0.2864,  0.2660,\n",
      "         0.2625,  0.2702,  0.2876,  0.2360,  0.2653,  0.2476,  0.2557,  0.3002,\n",
      "         0.2720,  0.3044,  0.2438,  0.2859,  0.2283,  0.2408,  0.2591,  0.2581,\n",
      "         0.2275,  0.2550,  0.2541,  0.2872,  0.2720,  0.2561,  0.2705,  0.2485,\n",
      "         0.2669,  0.2740,  0.2686,  0.2028,  0.2559,  0.2630,  0.2390,  0.2581,\n",
      "         0.2458,  0.2630,  0.2574,  0.2592,  0.2661,  0.2255,  0.2932,  0.2480,\n",
      "         0.2656,  0.3011,  0.2211,  0.2752,  0.2341,  0.2457,  0.2439,  0.2930,\n",
      "         0.2280,  0.2855,  0.2242,  0.2476,  0.2590,  0.2684,  0.2761,  0.2653,\n",
      "         0.2455,  0.4361,  0.2454,  0.2247,  0.2822,  0.2967,  0.2288,  0.2748,\n",
      "         0.2552,  0.2444,  0.2177,  0.2313,  0.2274,  0.2699,  0.2840,  0.2869,\n",
      "         0.2407,  0.2845,  0.2182,  0.2783,  0.2472,  0.2476,  0.2450,  0.2345,\n",
      "         0.2639,  0.2582,  0.2509,  0.3084,  0.2803,  0.2493,  0.2256,  0.2339,\n",
      "         0.2632,  0.2538,  0.2327,  0.2336,  0.2861,  0.2704,  0.2318, -0.1623,\n",
      "         0.2654,  0.2268,  0.2457,  0.2667,  0.2579,  0.2217,  0.2886,  0.2737,\n",
      "         0.2403,  0.2568,  0.1090,  0.2720,  0.2680,  0.2463,  0.2436,  0.4403,\n",
      "         0.2579,  0.2693,  0.2541,  0.2470,  0.2197,  0.2672,  0.2553,  0.2875,\n",
      "         0.2344,  0.2556,  0.2794,  0.2698,  0.2617,  0.2296,  0.2310,  0.2368,\n",
      "         0.2643,  0.2949,  0.2651,  0.2486,  0.2949,  0.2760,  0.2283,  0.2674,\n",
      "         0.2246,  0.2634,  0.2333,  0.0386,  0.2448,  0.2727,  0.2328,  0.2731,\n",
      "         0.1849,  0.2374,  0.2529,  0.2859,  0.2713,  0.2374,  0.2329,  0.2522,\n",
      "         0.2361,  0.2447,  0.2290,  0.2626,  0.2184,  0.2525,  0.2699,  0.2638,\n",
      "         0.2455,  0.2962,  0.2550,  0.2380,  0.2439,  0.2347,  0.2245,  0.2547,\n",
      "         0.2969,  0.2254,  0.2292,  0.2511,  0.2810,  0.2826,  0.2491,  0.2762,\n",
      "         0.2612,  0.2359,  0.2813,  0.2870,  0.2638,  0.2546,  0.2394,  0.2830,\n",
      "         0.2707,  0.2818,  0.2405,  0.2717,  0.2519,  0.2250,  0.2418,  0.2471,\n",
      "         0.2394, -0.2111,  0.2626,  0.2571,  0.2740,  0.2521,  0.2822,  0.2453,\n",
      "         0.2705,  0.2626,  0.2705,  0.2606,  0.2732,  0.2656,  0.2552,  0.2553,\n",
      "         0.2690,  0.2160,  0.2502,  0.2281,  0.2415,  0.2506,  0.2360,  0.2664,\n",
      "         0.2124,  0.2682,  0.2792,  0.2762,  0.2405,  0.2426,  0.2812,  0.2254,\n",
      "         0.2393,  0.2523,  0.2230,  0.2580,  0.2461,  0.2656,  0.2475,  0.1982,\n",
      "         0.2589,  0.2520,  0.2419,  0.2554,  0.2538,  0.2535,  0.2658,  0.2243,\n",
      "         0.2277,  0.2508,  0.2883,  0.2721,  0.2113,  0.2802,  0.2741,  0.2361,\n",
      "         0.2246,  0.2544,  0.2548,  0.2394,  0.2763,  0.1666,  0.2666,  0.3064,\n",
      "         0.2474,  0.2229,  0.2185,  0.2607, -0.0426,  0.2781,  0.2730,  0.2249,\n",
      "         0.2785,  0.3076,  0.2395,  0.2460,  0.2516,  0.2734,  0.2541,  0.2554,\n",
      "         0.2548,  0.2553,  0.2399,  0.3040,  0.2968,  0.2411,  0.1968,  0.2515,\n",
      "         0.2738,  0.2553,  0.2496,  0.2437,  0.2723,  0.3192,  0.3062,  0.2745,\n",
      "         0.2745,  0.2608,  0.2614,  0.2647,  0.3175,  0.2899,  0.2716,  0.2439,\n",
      "         0.2509,  0.2332,  0.2523,  0.2683,  0.3135,  0.3029,  0.2745,  0.2249,\n",
      "         0.2574,  0.2439,  0.2985,  0.2751,  0.2480,  0.2086,  0.2798,  0.2111,\n",
      "         0.2445,  0.2637,  0.2499,  0.2338,  0.2420,  0.2306,  0.2804,  0.2468,\n",
      "        -0.0480,  0.2882,  0.3393,  0.2544,  0.2432,  0.2525,  0.2624,  0.2513,\n",
      "         0.2353,  0.2905,  0.2278,  0.2593,  0.2516,  0.2317,  0.2691,  0.2713,\n",
      "         0.2676,  0.0554,  0.2482,  0.2727,  0.2461,  0.2449,  0.2410,  0.1379,\n",
      "         0.2387,  0.2354,  0.2533,  0.2654,  0.2571,  0.2337,  0.2343,  0.2253,\n",
      "         0.2765,  0.1899,  0.2405,  0.2413,  0.2803,  0.2684,  0.3000,  0.2459,\n",
      "         0.2626,  0.3221,  0.2535,  0.2603,  0.2491,  0.3250,  0.2592,  0.2471,\n",
      "         0.2481,  0.2654,  0.2459,  0.2515,  0.2758,  0.2628,  0.2418,  0.0548],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0025,  0.0262,  0.0675,  ...,  0.0122, -0.0688, -0.0225],\n",
      "        [ 0.0293,  0.0256,  0.0113,  ..., -0.0507,  0.0249, -0.0284],\n",
      "        [-0.0208,  0.0046, -0.0304,  ...,  0.0010,  0.0355,  0.0230],\n",
      "        ...,\n",
      "        [ 0.0282, -0.0399, -0.0190,  ...,  0.0680,  0.0103, -0.0483],\n",
      "        [ 0.0099, -0.0814, -0.0251,  ...,  0.0162,  0.0136, -0.0175],\n",
      "        [-0.0375,  0.0069, -0.0126,  ..., -0.0279, -0.0208,  0.0506]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0186, -0.1995,  0.1253,  ...,  0.0336, -0.3320,  0.1042],\n",
      "        [-0.1566,  0.1265,  0.0428,  ...,  0.2311,  0.0713,  0.2141],\n",
      "        [ 0.1781,  0.2782, -0.1128,  ...,  0.1456, -0.0859, -0.1187],\n",
      "        ...,\n",
      "        [-0.1510,  0.4062, -0.0272,  ..., -0.0087, -0.0566,  0.2289],\n",
      "        [-0.1616, -0.2610, -0.0945,  ...,  0.1159,  0.3807,  0.1633],\n",
      "        [ 0.2276, -0.3285, -0.2682,  ..., -0.2377,  0.1790, -0.2054]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2610,  0.4642,  0.3245,  ..., -0.7620,  0.8458,  0.0548],\n",
      "        [ 0.1120, -1.0608,  0.0288,  ...,  0.3835,  0.4335, -0.0574],\n",
      "        [-0.3316,  1.3742,  0.4813,  ...,  0.5089, -0.2136,  0.0885],\n",
      "        ...,\n",
      "        [ 0.0740, -0.6211, -0.7085,  ...,  1.0048, -0.6677, -0.0467],\n",
      "        [ 0.0179,  0.3353, -1.4129,  ...,  1.6722, -0.1471,  0.0543],\n",
      "        [-0.3003,  0.4556,  0.5397,  ..., -0.2168, -0.8546,  0.0710]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.3243,  0.7987,  0.7738,  ..., -0.1774, -5.9476, -4.7286],\n",
      "        [-0.5223, -0.4823, -1.7182,  ..., -1.6563,  1.6081, -0.2445],\n",
      "        [-0.1686,  0.1401, -0.4422,  ..., -0.3851, -0.3027,  1.6284],\n",
      "        ...,\n",
      "        [ 0.3290, -0.3552, -0.5134,  ..., -0.2471, -0.9689, -0.4429],\n",
      "        [ 0.5793, -0.5104, -0.2648,  ..., -1.2065,  0.8666,  0.3878],\n",
      "        [ 5.5754, -9.1196,  0.0582,  ...,  0.9165, -8.4650, -1.1578]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0673,  0.2566,  0.2996,  0.3280,  0.3148,  0.2481,  0.3358,  0.3114,\n",
      "         0.3360,  0.2917,  0.2587,  0.2416,  0.3567,  0.4048,  0.3440,  0.3009,\n",
      "         0.3258,  0.3148,  0.3626,  0.3913,  0.3010,  0.3921,  0.2988,  0.2986,\n",
      "         0.2934,  0.2493,  0.2865,  0.2525,  0.3114,  0.3514,  0.2958,  0.2486,\n",
      "         0.3229, -0.3000,  0.3630,  0.4639,  0.3797,  0.2763,  0.3281,  0.3070,\n",
      "         0.3045,  0.3145,  0.3226,  0.3570,  0.3269,  0.2691,  0.2585,  0.2950,\n",
      "         0.3482,  0.3008,  0.2864,  0.3478,  0.3810,  0.3009,  0.3010,  0.3397,\n",
      "         0.2670,  0.3082,  0.3968,  0.2583,  0.3330,  0.3004,  0.3465,  0.3397,\n",
      "         0.3062,  0.2845,  0.3395,  0.5735,  0.2842,  0.2872,  1.0514,  0.3534,\n",
      "         0.0677,  0.2631,  0.3007,  0.3268,  0.3651,  0.3304,  0.3347,  0.3665,\n",
      "         0.3706,  0.3223,  0.3504,  0.2782,  0.3295,  0.3320,  0.3702,  0.3068,\n",
      "         0.3387,  0.3165,  0.3181,  0.2966,  0.2845,  0.2724,  0.2678,  0.2953,\n",
      "         0.3010,  0.2223,  0.2456,  0.2558,  0.2984,  0.3021,  0.3422,  0.3928,\n",
      "         0.3458,  0.3276,  0.2968,  0.2717,  0.3641,  0.3029,  0.2756,  0.2417,\n",
      "         0.3069,  0.3338,  0.2937,  0.2931,  0.3485,  0.3087,  0.2725,  0.2869,\n",
      "         0.3101,  0.3114,  0.0756,  0.3048,  0.3268,  0.2927,  0.3740,  0.3132,\n",
      "         0.3619,  0.3429,  0.2972,  0.3458,  0.2549,  0.3012,  0.3213,  0.3169,\n",
      "         0.3245,  0.3328,  0.3510,  0.2739,  0.2934,  0.2908,  0.3295,  0.3578,\n",
      "         0.3237,  0.3265,  0.3283,  0.3911,  0.2496,  0.3584,  0.3116,  0.2993,\n",
      "         0.2993,  0.2998,  0.3774,  0.2882,  0.3722,  0.3572,  0.3036,  0.3288,\n",
      "         0.3123,  0.3427,  0.3387,  0.2301,  0.3217,  0.3325,  0.2877,  0.3104,\n",
      "         0.2813,  0.2924,  0.3349,  0.3100,  0.3362,  0.2691,  0.3271,  0.3032,\n",
      "         0.3352,  0.3788,  0.2744,  0.3327,  0.2978,  0.2946,  0.2712,  0.3704,\n",
      "         0.2981,  0.4079,  0.2520,  0.3036,  0.2885,  0.3643,  0.3415,  0.2735,\n",
      "         0.2864,  0.5136,  0.3293,  0.3025,  0.3842,  0.4595,  0.2824,  0.3122,\n",
      "         0.3451,  0.2813,  0.2582,  0.2684,  0.2482,  0.3614,  0.3060,  0.3646,\n",
      "         0.3070,  0.3636,  0.2895,  0.3668,  0.3386,  0.3080,  0.3393,  0.2802,\n",
      "         0.3229,  0.3251,  0.3582,  0.3793,  0.3723,  0.3278,  0.2501,  0.2762,\n",
      "         0.3063,  0.2999,  0.2685,  0.3125,  0.3161,  0.2921,  0.3401,  0.2076,\n",
      "         0.3417,  0.0757,  0.3106,  0.4156,  0.3076,  0.2774,  0.3467,  0.3594,\n",
      "         0.2397,  0.2920, -0.1055,  0.3912,  0.3195,  0.3336,  0.2863,  0.1372,\n",
      "         0.2651,  0.2579,  0.2933,  0.3158,  0.2632,  0.3405,  0.3387,  0.3373,\n",
      "         0.2909,  0.2980,  0.3950,  0.3138,  0.3552,  0.2585,  0.2666,  0.2836,\n",
      "         0.2858,  0.2991,  0.3248,  0.2709,  0.4161,  0.3256,  0.2947,  0.3115,\n",
      "         0.2759,  0.2968,  0.2891,  0.0573,  0.3280,  0.3116,  0.3029,  0.3622,\n",
      "         0.1478,  0.2758,  0.3143,  0.3937,  0.3203,  0.3015,  0.2705,  0.2747,\n",
      "         0.2937,  0.3227,  0.2554,  0.3277,  0.2525,  0.3552,  0.3362,  0.2926,\n",
      "         0.2579,  0.4048,  0.2690,  0.3247,  0.3298,  0.2999,  0.2471,  0.3310,\n",
      "         0.3319,  0.2509,  0.2939,  0.2915,  0.3791,  0.3538,  0.3132,  0.3227,\n",
      "         0.3086,  0.3096,  0.3350,  0.3714,  0.3513,  0.2975,  0.2652,  0.3194,\n",
      "         0.3034,  0.3774,  0.3346,  0.3761,  0.2924,  0.2565,  0.3477,  0.2981,\n",
      "         0.2604,  0.2817,  0.3133,  0.3011,  0.2742,  0.2973,  0.3201,  0.3016,\n",
      "         0.3359,  0.3219,  0.3356,  0.2949,  0.2897,  0.3100,  0.2937,  0.3063,\n",
      "         0.3857,  0.2576,  0.3120,  0.2449,  0.2381,  0.3049,  0.3140,  0.3083,\n",
      "         0.5583,  0.3136,  0.3426,  0.4185,  0.2825,  0.2900,  0.3289,  0.2521,\n",
      "         0.2628,  0.3025,  0.2804,  0.3345,  0.3225,  0.3305,  0.3085,  0.2341,\n",
      "         0.3385,  0.3080,  0.2513,  0.3181,  0.3173,  0.3250,  0.3034,  0.2623,\n",
      "         0.2887,  0.2697,  0.3321,  0.3114,  0.2567,  0.3351,  0.3691,  0.2901,\n",
      "         0.2677,  0.3610,  0.2893,  0.3184,  0.3785,  0.1880,  0.3032,  0.4044,\n",
      "         0.2896,  0.2892,  0.2793,  0.3121,  0.1320,  0.3221,  0.3041,  0.2775,\n",
      "         0.3577,  0.3993,  0.2561,  0.3582,  0.3039,  0.3419,  0.3100,  0.2786,\n",
      "         0.3083,  0.3108,  0.2939,  0.3836,  0.3523,  0.2900,  0.2658,  0.3619,\n",
      "         0.3394,  0.2770,  0.3206,  0.2715,  0.3134,  0.4410,  0.3815,  0.3816,\n",
      "         0.3791,  0.3284,  0.3653,  0.3341,  0.9015,  0.3982,  0.3853,  0.2549,\n",
      "         0.3389,  0.2505,  0.3298,  0.2917,  0.3969,  0.3736,  0.3975,  0.2417,\n",
      "         0.3246,  0.2901,  0.3710,  0.3639,  0.2564, -0.2382,  0.3498,  0.2517,\n",
      "         0.2892,  0.3404,  0.3017,  0.3030,  0.2631,  0.2685,  0.2979,  0.3244,\n",
      "        -0.0721,  0.4114,  0.4088,  0.3333,  0.3889,  0.3078,  0.2993,  0.3710,\n",
      "         0.2849,  0.3358, -0.3072,  0.3127,  0.3205,  0.2412,  0.3740,  0.3294,\n",
      "         0.3113,  0.0515,  0.3147,  0.3507,  0.2895,  0.3448,  0.3063,  0.1192,\n",
      "         0.2702,  0.3479,  0.3006,  0.3332,  0.2898,  0.2679,  0.3034,  0.2903,\n",
      "         0.2928,  0.3826,  0.3163,  0.2832,  0.3531,  0.3031,  0.3909,  0.2713,\n",
      "         0.2941,  0.4805,  0.3098,  0.3095,  0.3105,  0.3707,  0.3730,  0.3186,\n",
      "         0.3305,  0.3102,  0.2993,  0.3048,  0.3346,  0.3073,  0.3660,  0.0701],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0296, -0.1050, -0.0695,  ..., -0.0086,  0.0317, -0.0159],\n",
      "        [-0.0609, -0.0421,  0.0516,  ...,  0.0131, -0.0148, -0.0107],\n",
      "        [ 0.0175,  0.0736, -0.0012,  ..., -0.0124,  0.0271,  0.0015],\n",
      "        ...,\n",
      "        [ 0.0246, -0.0201, -0.0088,  ..., -0.0678, -0.0284, -0.0086],\n",
      "        [-0.0025, -0.0257,  0.0084,  ...,  0.0791, -0.0060,  0.0043],\n",
      "        [ 0.0114, -0.0627, -0.1172,  ..., -0.0494, -0.0643,  0.0015]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2722, -0.5117, -0.4442,  ...,  0.8242,  0.0169, -0.2548],\n",
      "        [ 0.1377, -0.0310, -0.0946,  ...,  0.2854, -0.3374, -0.0097],\n",
      "        [ 0.3440, -0.5588,  0.7548,  ..., -0.1994,  0.8689,  0.2336],\n",
      "        ...,\n",
      "        [-0.2237, -0.4609, -0.3220,  ..., -0.4308, -0.0740,  0.1974],\n",
      "        [ 0.6901,  0.4823,  0.6140,  ...,  0.1371, -0.3399,  0.1519],\n",
      "        [ 0.2511,  0.5305, -0.3082,  ..., -0.0073,  0.3928, -0.4542]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8858, -0.9855,  2.2291,  ..., -0.3229, -1.5240,  0.0903],\n",
      "        [ 0.2553, -0.4723, -0.5270,  ...,  1.5055, -0.6142, -0.0199],\n",
      "        [-0.9236,  0.6162,  1.5649,  ..., -1.9255,  1.8077, -0.3183],\n",
      "        ...,\n",
      "        [-1.1597, -0.2640,  1.2438,  ..., -0.9949, -0.4021, -0.9112],\n",
      "        [-1.3158,  0.4614, -0.8074,  ...,  0.0357, -0.2475, -0.7296],\n",
      "        [ 1.0271,  1.5431,  1.1037,  ..., -1.1772,  1.9260,  0.5714]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0671, -1.5162,  8.7930,  ...,  4.5811,  4.2886,  4.0862],\n",
      "        [-0.3698, -0.6273, -0.7034,  ...,  0.8049, -0.8394,  0.4379],\n",
      "        [ 2.0966, -0.8875,  0.8608,  ...,  0.2942,  0.3993,  0.2579],\n",
      "        ...,\n",
      "        [ 0.0303,  0.9709, -0.9663,  ...,  0.7567,  1.3219, -1.2175],\n",
      "        [-0.6055, -2.6679,  0.0749,  ...,  1.7054,  0.6711,  1.0199],\n",
      "        [ 6.4164,  0.7031, -1.4039,  ..., -2.3888, -2.7938,  4.7180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0505,  0.1170,  0.1401,  0.1426,  0.1312,  0.1202,  0.1450,  0.1436,\n",
      "         0.1172,  0.1130,  0.1112,  0.1064,  0.1563,  0.1945,  0.1669,  0.1602,\n",
      "         0.1463,  0.1447,  0.1750,  0.1725,  0.1326,  0.1833,  0.1402,  0.1502,\n",
      "         0.1326,  0.1455,  0.1228,  0.1168,  0.1419,  0.1608,  0.1377,  0.1239,\n",
      "         0.1358,  0.1035,  0.1963,  0.2819,  0.1565,  0.1312,  0.1448,  0.1865,\n",
      "         0.1449,  0.1544,  0.1416,  0.1845,  0.1837,  0.1455,  0.1078,  0.1381,\n",
      "         0.1445,  0.1567,  0.1417,  0.1930,  0.1840,  0.1429,  0.1678,  0.1253,\n",
      "         0.1290,  0.1537,  0.1909,  0.1275,  0.1305,  0.1454,  0.1660,  0.1510,\n",
      "         0.1406,  0.1468,  0.1351,  0.4116,  0.1861,  0.1088,  0.2678,  0.1443,\n",
      "         0.0380,  0.1087,  0.1298,  0.2015,  0.2136,  0.1504,  0.1467,  0.1831,\n",
      "         0.2302,  0.1881,  0.2099,  0.1104,  0.1442,  0.1129,  0.1845,  0.1172,\n",
      "         0.1426,  0.1699,  0.1677,  0.1539,  0.1626,  0.1182,  0.1182,  0.1270,\n",
      "         0.1347,  0.1203,  0.1428,  0.1117,  0.1202,  0.1431,  0.1496,  0.1684,\n",
      "         0.1465,  0.1515,  0.1542,  0.1234,  0.1388,  0.1265,  0.1123,  0.1158,\n",
      "         0.1319,  0.1261,  0.1165,  0.1451,  0.1604,  0.1270,  0.0971,  0.1315,\n",
      "         0.1367,  0.1680, -0.2359,  0.1391,  0.1379,  0.1328,  0.1750,  0.1283,\n",
      "         0.1512,  0.1267,  0.1453,  0.1609, -0.1170,  0.1368,  0.1626,  0.1222,\n",
      "         0.1724,  0.1496,  0.1448,  0.1240,  0.1581,  0.1481,  0.1216,  0.1554,\n",
      "         0.1906,  0.1585,  0.1215,  0.1848,  0.1122,  0.1172,  0.1418,  0.1572,\n",
      "         0.1309,  0.1232,  0.1807,  0.1487,  0.1748,  0.1539,  0.1550,  0.1326,\n",
      "         0.1715,  0.1410,  0.1582, -0.0792,  0.1118,  0.1578,  0.1065,  0.1504,\n",
      "         0.1030,  0.1355,  0.1729,  0.1187,  0.1408, -0.1007,  0.1630, -0.1141,\n",
      "         0.1658,  0.1815,  0.1257,  0.1768,  0.1286,  0.1503,  0.1486,  0.2121,\n",
      "         0.1193,  0.1840,  0.1038,  0.1229,  0.1211,  0.1756,  0.1570,  0.1442,\n",
      "         0.1260,  0.2497,  0.1339,  0.1235,  0.1738,  0.1851,  0.1407,  0.1535,\n",
      "         0.1424,  0.1319,  0.1037,  0.1307,  0.1204,  0.1589,  0.1599,  0.1554,\n",
      "         0.1172,  0.1913,  0.1636,  0.1596,  0.1248,  0.1288,  0.1642,  0.1402,\n",
      "         0.1703,  0.1695,  0.1566,  0.1907,  0.2132,  0.1726,  0.0858,  0.1138,\n",
      "         0.1413,  0.1215,  0.1136,  0.1328,  0.1497,  0.1404,  0.1370, -0.0944,\n",
      "         0.1490,  0.1638,  0.1660,  0.2007,  0.1683,  0.1183,  0.1413,  0.1888,\n",
      "         0.1074,  0.1355, -0.0883,  0.1975,  0.1919,  0.1371,  0.0962,  0.3589,\n",
      "         0.1936,  0.1313,  0.1297,  0.1459,  0.1075,  0.1761,  0.1518,  0.1961,\n",
      "         0.1503,  0.1317,  0.1750,  0.1515,  0.1375,  0.1415,  0.1149,  0.1092,\n",
      "         0.1434,  0.1484,  0.1336,  0.1391,  0.2804,  0.1571,  0.1380,  0.1488,\n",
      "         0.0968,  0.1389,  0.1140,  0.0254,  0.2534,  0.1318,  0.1412,  0.1842,\n",
      "         0.0848,  0.1359,  0.1698,  0.1941,  0.1377,  0.1026,  0.1256,  0.1395,\n",
      "         0.1884,  0.1657,  0.0896,  0.1273,  0.1284,  0.1875,  0.1273,  0.1388,\n",
      "         0.1476,  0.1776,  0.1629,  0.1378,  0.1616,  0.1300,  0.1053,  0.1374,\n",
      "         0.1670,  0.0978,  0.1126,  0.1366,  0.1417,  0.1383,  0.1787,  0.1314,\n",
      "         0.1390,  0.1229,  0.1715,  0.1518,  0.2155,  0.1398,  0.1747,  0.1315,\n",
      "         0.1336,  0.1591,  0.1799,  0.1498,  0.1435,  0.0972,  0.1522,  0.1121,\n",
      "         0.0948, -0.1479,  0.1293,  0.1348,  0.1582,  0.1363,  0.1537,  0.1692,\n",
      "         0.1548,  0.1230,  0.1371,  0.1319,  0.1527,  0.1636,  0.1552,  0.1482,\n",
      "         0.1796,  0.1188,  0.1147,  0.1281,  0.1186,  0.1672,  0.1508,  0.1565,\n",
      "         0.1661,  0.1214,  0.1433,  0.1679,  0.1253,  0.1189,  0.1837,  0.1164,\n",
      "         0.1424,  0.1366,  0.1370,  0.1483,  0.1411,  0.1257,  0.1585,  0.1292,\n",
      "         0.1647,  0.1253,  0.1080,  0.1260,  0.1589,  0.1579,  0.1382,  0.1128,\n",
      "         0.1133,  0.1250,  0.1500,  0.1581,  0.1197,  0.1399,  0.1997,  0.1340,\n",
      "         0.1009,  0.2954,  0.1697,  0.1297,  0.1716, -0.0921,  0.1142,  0.1925,\n",
      "         0.1201,  0.1349,  0.1234,  0.1527,  0.0625,  0.1264,  0.1317,  0.1308,\n",
      "         0.1644,  0.2100,  0.1133,  0.1797,  0.1548,  0.1475,  0.1489,  0.1462,\n",
      "         0.1371,  0.1085,  0.1322,  0.1990,  0.1731,  0.1233,  0.0892,  0.2145,\n",
      "         0.1466,  0.1520,  0.1659,  0.1206,  0.1169,  0.2177,  0.1928,  0.1846,\n",
      "         0.1716,  0.1494,  0.1694,  0.1587,  0.3657,  0.1661,  0.2005,  0.1244,\n",
      "         0.1571,  0.1091,  0.1411,  0.1411,  0.1943,  0.1340,  0.1994,  0.1261,\n",
      "         0.1388,  0.1085,  0.1770,  0.1815,  0.0950,  0.1075,  0.1568,  0.1231,\n",
      "         0.1158,  0.1292,  0.1574,  0.1143,  0.1144,  0.1083,  0.1737,  0.1326,\n",
      "        -0.0344,  0.1757,  0.2202,  0.1379,  0.1831,  0.1369,  0.1494,  0.1678,\n",
      "         0.1532,  0.1680,  0.1063,  0.1791,  0.1610,  0.1144,  0.1424,  0.1390,\n",
      "         0.1550, -0.0580,  0.1231,  0.1602,  0.1133,  0.1652,  0.1368,  0.0800,\n",
      "         0.1300,  0.1915,  0.1829,  0.1408,  0.1372,  0.1287,  0.1533,  0.1298,\n",
      "         0.1530,  0.1376,  0.1270,  0.1220,  0.2146,  0.1396,  0.1909,  0.1320,\n",
      "         0.1639,  0.1977,  0.1463,  0.1380,  0.1888,  0.1984,  0.1502,  0.1591,\n",
      "         0.1444,  0.1487,  0.1086,  0.1384,  0.1661,  0.1586,  0.1473, -0.0360],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1233, -0.1551, -0.0408,  ...,  0.0592,  0.0808,  0.0411],\n",
      "        [-0.0394, -0.3428,  0.0626,  ..., -0.2928, -0.2245,  0.0448],\n",
      "        [-0.2216, -0.1287,  0.0904,  ...,  0.2252, -0.1727,  0.0660],\n",
      "        ...,\n",
      "        [-0.0599,  0.2949,  0.1322,  ..., -0.5474, -0.0848, -0.0359],\n",
      "        [-0.0720, -0.1823,  0.0635,  ...,  0.1899, -0.0040,  0.0594],\n",
      "        [-0.0533,  0.2599, -0.2663,  ...,  0.1778, -0.0265,  0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2354,  0.1790, -1.2123,  ...,  1.2384,  0.0426, -0.2122],\n",
      "        [-0.0541, -0.3785, -0.1543,  ...,  1.1690,  2.3312,  0.7652],\n",
      "        [-0.2387, -2.1707, -2.7082,  ...,  0.0521,  0.1505,  0.3493],\n",
      "        ...,\n",
      "        [-0.2082,  0.1338,  0.6345,  ...,  0.2605, -0.1748,  0.1706],\n",
      "        [ 0.0637,  2.5810, -2.2646,  ..., -0.2770,  1.6246, -0.1545],\n",
      "        [-0.2644, -1.9754, -0.1235,  ...,  1.0565, -1.5286, -0.4147]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6290,  2.8238,  3.3945,  ...,  1.7105, -1.7144, -2.1249],\n",
      "        [-0.4217,  0.5374, -0.8228,  ..., -0.6237,  2.1958,  0.6002],\n",
      "        [-1.4104, -0.5016,  2.3963,  ...,  1.2634,  0.3820, -0.7999],\n",
      "        ...,\n",
      "        [-1.0605, -0.1218, -0.3692,  ..., -0.1084,  2.1401, -2.1069],\n",
      "        [-0.3000,  0.5096,  0.3555,  ...,  0.6630, -0.7653, -1.1878],\n",
      "        [ 0.4874,  3.8316,  0.2791,  ..., -3.5890,  0.2598,  0.9019]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0332,  0.2579,  0.2810,  0.2975,  0.2523,  0.2321,  0.2714,  0.2554,\n",
      "         0.2255,  0.2691,  0.2674,  0.2118,  0.3036,  0.3759,  0.2756,  0.3202,\n",
      "         0.2530,  0.2392,  0.3053,  0.3055,  0.2457,  0.3426,  0.2630,  0.2878,\n",
      "         0.2618,  0.2249,  0.2598,  0.2477,  0.2524,  0.2934,  0.2472,  0.2391,\n",
      "         0.2714,  0.2576,  0.3395,  0.4610,  0.3231,  0.2571,  0.2718,  0.3213,\n",
      "         0.2790,  0.3135,  0.2821,  0.2645,  0.3102,  0.3078,  0.2551,  0.2675,\n",
      "         0.3223,  0.2591,  0.2499,  0.3179,  0.2718,  0.2408,  0.2852,  0.2702,\n",
      "         0.2427,  0.2683,  0.3172,  0.2207,  0.2748,  0.2776,  0.2809,  0.2536,\n",
      "         0.2830,  0.2517,  0.2999,  0.4605,  0.2790,  0.2498,  0.3732,  0.2713,\n",
      "        -0.0753,  0.2473,  0.2549,  0.3414,  0.2905,  0.3078,  0.3244,  0.2886,\n",
      "         0.3637,  0.2766,  0.2855,  0.2454,  0.2743,  0.3035,  0.3279,  0.2767,\n",
      "         0.2641,  0.2859,  0.2775,  0.2682,  0.2511,  0.2620,  0.2455,  0.2981,\n",
      "         0.2857,  0.2184,  0.2442,  0.2441,  0.2489,  0.2580,  0.3068,  0.3059,\n",
      "         0.2657,  0.2906,  0.2657,  0.2495,  0.2776,  0.2535,  0.2809,  0.2330,\n",
      "         0.2650,  0.2723,  0.2595,  0.2605,  0.2864,  0.2402,  0.2204,  0.2591,\n",
      "         0.2764,  0.2771,  0.3052,  0.2560,  0.2890,  0.2908,  0.3451,  0.2759,\n",
      "         0.2961,  0.2861,  0.2552,  0.2817,  0.2132,  0.2663,  0.2747,  0.2759,\n",
      "         0.2933,  0.2976,  0.3157,  0.2436,  0.2765,  0.2551,  0.2763,  0.3499,\n",
      "         0.3033,  0.3359,  0.2623,  0.3179,  0.2233,  0.2480,  0.2550,  0.2611,\n",
      "         0.2565,  0.2600,  0.3006,  0.2857,  0.3221,  0.2670,  0.2579,  0.2919,\n",
      "         0.3057,  0.3056,  0.2713,  0.2413,  0.2660,  0.3082,  0.2662,  0.2642,\n",
      "         0.2509,  0.2578,  0.2656,  0.2588,  0.3146,  0.2441,  0.3408,  0.2710,\n",
      "         0.3150,  0.3841,  0.2339,  0.3168,  0.2516,  0.2566,  0.2396,  0.3433,\n",
      "         0.2453,  0.3459,  0.2702,  0.2417,  0.2498,  0.3552,  0.2893,  0.2929,\n",
      "         0.2887,  0.6672,  0.2544,  0.2394,  0.2782,  0.3163,  0.2353,  0.3172,\n",
      "         0.2791,  0.2438,  0.2783,  0.2394,  0.2690,  0.2995,  0.3058,  0.3072,\n",
      "         0.2863,  0.2738,  0.2679,  0.3051,  0.2615,  0.2497,  0.2732,  0.2770,\n",
      "         0.2727,  0.2610,  0.2791,  0.3434,  0.2989,  0.2957,  0.2300,  0.2517,\n",
      "         0.2489,  0.2625,  0.2178,  0.2366,  0.2763,  0.2688,  0.2701,  0.2776,\n",
      "         0.3005,  0.2654,  0.2713,  0.3037,  0.2692,  0.2504,  0.2781,  0.2836,\n",
      "         0.2510,  0.2548, -0.1470,  0.3334,  0.2753,  0.2398,  0.2551,  0.5807,\n",
      "         0.2706,  0.2634,  0.2274,  0.2547,  0.2345,  0.2928,  0.2588,  0.3826,\n",
      "         0.2710,  0.2167,  0.3014,  0.2980,  0.2572,  0.2400,  0.2437,  0.2734,\n",
      "         0.2529,  0.3211,  0.2729,  0.2539,  0.3637,  0.2902,  0.2531,  0.2621,\n",
      "         0.2148,  0.2772,  0.2652,  0.0535,  0.3083,  0.2666,  0.2497,  0.2840,\n",
      "         0.2045,  0.2511,  0.2748,  0.3104,  0.2960,  0.2411,  0.2419,  0.2675,\n",
      "         0.2378,  0.2636,  0.2480,  0.2807,  0.2179,  0.2978,  0.2751,  0.2768,\n",
      "         0.2796,  0.3455,  0.2609,  0.2865,  0.3056,  0.2415,  0.2363,  0.2971,\n",
      "         0.3595,  0.2570,  0.2305,  0.2594,  0.2819,  0.3102,  0.2968,  0.2636,\n",
      "         0.2797,  0.2525,  0.2977,  0.3163,  0.3292,  0.2621,  0.2875,  0.2619,\n",
      "         0.2570,  0.2928,  0.3050,  0.2932,  0.2775,  0.2277,  0.2658,  0.2447,\n",
      "         0.2210,  0.3249,  0.2686,  0.2623,  0.2824,  0.2434,  0.2992,  0.2928,\n",
      "         0.2973,  0.2708,  0.2847,  0.2798,  0.2475,  0.2836,  0.3052,  0.2657,\n",
      "         0.3080,  0.2433,  0.2519,  0.2503,  0.2421,  0.2540,  0.2766,  0.2826,\n",
      "         0.3125,  0.2611,  0.3090,  0.2885,  0.2385,  0.2546,  0.3076,  0.2530,\n",
      "         0.2308,  0.2386,  0.2408,  0.2996,  0.2534,  0.2644,  0.2625,  0.2496,\n",
      "         0.2749,  0.2621,  0.2085,  0.2689,  0.3255,  0.2837,  0.2697,  0.2303,\n",
      "         0.2554,  0.2639,  0.2850,  0.2605,  0.2082,  0.2681,  0.3379,  0.2659,\n",
      "         0.2342,  0.3027,  0.2821,  0.2576,  0.3043, -0.2518,  0.2848,  0.4031,\n",
      "         0.2696,  0.2467,  0.2705,  0.2660,  0.0974,  0.2899,  0.2515,  0.2406,\n",
      "         0.3315,  0.2959,  0.2625,  0.2849,  0.2658,  0.2784,  0.2365,  0.2596,\n",
      "         0.2702,  0.2435,  0.2640,  0.3445,  0.3535,  0.2337,  0.2350,  0.2775,\n",
      "         0.2731,  0.2662,  0.2686,  0.2397,  0.3011,  0.3879,  0.3280,  0.3431,\n",
      "         0.3199,  0.2845,  0.2997,  0.2884,  0.4910,  0.3227,  0.3176,  0.2230,\n",
      "         0.2708,  0.2522,  0.2730,  0.2549,  0.3447,  0.3079,  0.2939,  0.2331,\n",
      "         0.3023,  0.2618,  0.3544,  0.3271,  0.2619,  0.2375,  0.3080,  0.2740,\n",
      "         0.2531,  0.2714,  0.3021,  0.2609,  0.2638,  0.2325,  0.2565,  0.2504,\n",
      "         0.0768,  0.2853,  0.4052,  0.2895,  0.2877,  0.2872,  0.2727,  0.2856,\n",
      "         0.2691,  0.3268,  0.3393,  0.2686,  0.2391,  0.2476,  0.2774,  0.2918,\n",
      "         0.2945,  0.0913,  0.2896,  0.2758,  0.2246,  0.3040,  0.2642,  0.1723,\n",
      "         0.2303,  0.2649,  0.2816,  0.2927,  0.3000,  0.2289,  0.2837,  0.2418,\n",
      "         0.2811,  0.2252,  0.2642,  0.2448,  0.3060,  0.2785,  0.3386,  0.2699,\n",
      "         0.2774,  0.3778,  0.2883,  0.2648,  0.2705,  0.3572,  0.3118,  0.2676,\n",
      "         0.2878,  0.2662,  0.2448,  0.2700,  0.2929,  0.2706,  0.2483,  0.0404],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0003, -0.0234, -0.0213,  ...,  0.0187,  0.0334,  0.0188],\n",
      "        [ 0.0033,  0.0281, -0.0686,  ...,  0.0427,  0.1065, -0.0205],\n",
      "        [-0.0281,  0.0216,  0.0284,  ...,  0.0062,  0.0337,  0.0171],\n",
      "        ...,\n",
      "        [-0.0099, -0.0496, -0.0337,  ..., -0.0019,  0.0136, -0.0246],\n",
      "        [-0.0137, -0.0290, -0.0074,  ...,  0.0234, -0.0431, -0.0079],\n",
      "        [ 0.0183,  0.0145,  0.0067,  ..., -0.0202, -0.0103, -0.0336]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0676, -0.4974,  0.6141,  ..., -0.3030, -0.2037, -0.0400],\n",
      "        [ 0.0813, -0.1057, -0.4156,  ...,  0.7811,  0.7208,  0.0801],\n",
      "        [ 0.0472,  0.2873,  0.5564,  ..., -0.5188,  0.5945, -0.2034],\n",
      "        ...,\n",
      "        [-0.2838, -0.6670,  0.2406,  ..., -0.2167,  0.1992, -0.0488],\n",
      "        [-0.0557,  0.3146, -0.2208,  ...,  0.5412, -0.4268, -0.1110],\n",
      "        [-0.0224, -0.0438,  0.2370,  ..., -0.4414,  0.1372,  0.1479]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0759, -0.3420,  0.6052,  ..., -0.2265,  1.1225, -0.1828],\n",
      "        [-0.0192, -1.1082, -0.7936,  ..., -0.2137,  0.4704,  0.0574],\n",
      "        [ 0.1290,  0.7842,  0.2980,  ...,  2.0084,  0.3085, -0.3491],\n",
      "        ...,\n",
      "        [-0.0573,  0.5244, -0.6908,  ..., -0.3500, -0.8239,  0.0426],\n",
      "        [ 1.1190,  0.3785, -0.4988,  ...,  0.8587,  1.2889,  0.0883],\n",
      "        [-0.5941, -0.5801, -0.6729,  ..., -0.5909, -0.5070,  0.1842]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.9252, -1.2491, -0.2864,  ..., -4.6549,  6.3290, -1.9635],\n",
      "        [ 0.7892, -0.9370,  0.5171,  ...,  1.2279, -0.1932, -0.8800],\n",
      "        [ 0.2999,  0.6518,  1.0257,  ..., -0.9182, -1.2501,  0.6384],\n",
      "        ...,\n",
      "        [ 0.2948, -0.6776,  0.0663,  ..., -1.7993,  2.1867, -0.0877],\n",
      "        [-0.9939, -0.0072,  0.6514,  ..., -0.4367,  0.2095, -0.6279],\n",
      "        [ 6.4957, -1.1936, -4.3457,  ..., -2.8830, -0.6985, -5.4791]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-4.9753e-02,  2.6040e-01,  2.5731e-01,  2.4584e-01,  2.4686e-01,\n",
      "         2.0830e-01,  2.6279e-01,  2.3922e-01,  2.2433e-01,  2.2803e-01,\n",
      "         2.1401e-01,  2.1983e-01,  2.5034e-01,  4.1443e-01,  2.5446e-01,\n",
      "         2.8045e-01,  2.6922e-01,  2.1926e-01,  3.0681e-01,  2.9781e-01,\n",
      "         2.2471e-01,  3.3081e-01,  2.9490e-01,  1.9303e-01,  2.9511e-01,\n",
      "         1.8703e-01,  1.9840e-01,  2.1563e-01,  2.7755e-01,  2.6837e-01,\n",
      "         2.1921e-01,  2.1873e-01,  2.4089e-01,  2.3284e-01,  3.6477e-01,\n",
      "         4.7462e-01,  3.0380e-01,  2.2671e-01,  2.4443e-01,  3.4039e-01,\n",
      "         2.7288e-01,  2.8341e-01,  2.1623e-01,  2.8628e-01,  2.9916e-01,\n",
      "         2.9659e-01,  2.5540e-01,  1.9541e-01,  2.6210e-01,  2.7931e-01,\n",
      "         3.3212e-01,  2.1133e-01,  2.5587e-01,  2.2539e-01,  2.8388e-01,\n",
      "         2.5091e-01,  2.1127e-01,  2.5382e-01,  3.3915e-01,  2.0424e-01,\n",
      "         2.5405e-01,  2.5612e-01,  3.2883e-01,  2.5479e-01,  2.7206e-01,\n",
      "         2.1451e-01,  2.7057e-01,  7.6233e-01,  2.9522e-01,  2.6105e-01,\n",
      "         1.3762e+00,  1.8333e-01, -5.7537e-02,  3.0739e-01,  2.6534e-01,\n",
      "         3.2198e-01,  2.6520e-01,  3.7352e-01,  3.9759e-01,  2.9157e-01,\n",
      "         4.3130e-01,  2.7890e-01,  3.9209e-01,  1.9310e-01,  2.4185e-01,\n",
      "         2.4895e-01,  4.1331e-01,  2.9427e-01,  2.0820e-01,  3.1330e-01,\n",
      "         2.5472e-01,  2.1633e-01,  2.6642e-01,  2.1511e-01,  1.9547e-01,\n",
      "         2.9864e-01,  2.4079e-01, -1.6689e-01,  2.0419e-01,  2.2566e-01,\n",
      "         2.2633e-01,  2.5022e-01,  4.2596e-01,  3.0325e-01,  2.5126e-01,\n",
      "         2.7659e-01,  2.5774e-01,  2.0800e-01,  2.6653e-01,  2.8396e-01,\n",
      "         2.2022e-01,  2.3668e-01,  2.0967e-01,  2.5000e-01,  2.2688e-01,\n",
      "         2.2464e-01,  2.5977e-01,  2.4918e-01,  1.6300e-01,  2.0496e-01,\n",
      "         2.5910e-01,  2.5633e-01, -3.2693e-03,  2.3266e-01,  3.0249e-01,\n",
      "         2.4041e-01,  2.7403e-01,  2.3432e-01,  2.8854e-01,  2.9413e-01,\n",
      "         2.1120e-01,  2.8652e-01,  1.6721e-01,  2.6324e-01,  2.0949e-01,\n",
      "         2.4085e-01,  3.1403e-01,  2.8576e-01,  3.3438e-01,  2.5218e-01,\n",
      "         3.2857e-01,  1.9762e-01,  2.1470e-01,  4.1997e-01,  3.2509e-01,\n",
      "         3.7756e-01,  2.2833e-01,  2.8790e-01,  1.7624e-01,  2.5613e-01,\n",
      "         2.5447e-01,  2.1900e-01,  2.4331e-01,  2.2814e-01,  3.0550e-01,\n",
      "         2.2385e-01,  3.6850e-01,  3.9909e-01,  2.6218e-01,  2.8030e-01,\n",
      "         3.0557e-01,  3.4539e-01,  3.2364e-01,  1.8451e-01,  2.3194e-01,\n",
      "         2.5603e-01,  2.5824e-01,  2.1972e-01,  1.9345e-01,  2.2167e-01,\n",
      "         2.8034e-01,  2.0733e-01,  3.3087e-01,  2.5523e-01,  3.6489e-01,\n",
      "         2.9239e-01,  3.9574e-01,  5.3448e-01,  2.3290e-01,  2.9200e-01,\n",
      "         2.2884e-01,  2.5085e-01,  2.2537e-01,  4.1181e-01,  2.2977e-01,\n",
      "         3.4671e-01,  2.2420e-01,  1.9980e-01,  1.9111e-01,  3.5166e-01,\n",
      "         3.3480e-01,  3.4405e-01,  2.3790e-01,  8.2885e-01,  2.9004e-01,\n",
      "         2.0609e-01,  3.1465e-01,  5.4292e-01,  2.5467e-01,  2.9752e-01,\n",
      "         2.7625e-01,  2.3048e-01,  2.4523e-01,  2.1203e-01,  2.0170e-01,\n",
      "         3.3091e-01,  2.6130e-01,  3.0227e-01,  2.5882e-01,  3.5855e-01,\n",
      "         2.6419e-01,  2.6333e-01,  2.6259e-01,  2.5343e-01,  3.5213e-01,\n",
      "         3.2924e-01,  2.4101e-01,  2.9596e-01,  4.1990e-01,  2.9924e-01,\n",
      "         3.4254e-01,  2.8793e-01,  2.2040e-01, -2.6684e-01,  2.4275e-01,\n",
      "         2.9064e-01,  2.0169e-01,  2.5751e-01,  2.1987e-01,  2.2647e-01,\n",
      "         2.5188e-01,  3.0914e-01,  2.6242e-01,  1.4281e-02,  2.6980e-01,\n",
      "         3.6366e-01,  2.2078e-01,  2.0858e-01,  2.5817e-01,  3.8617e-01,\n",
      "         2.0837e-01,  2.3549e-01,  1.2752e-01,  3.7447e-01,  3.3141e-01,\n",
      "         2.2521e-01,  2.5633e-01, -2.2698e-04,  2.3749e-01,  2.0863e-01,\n",
      "         2.4539e-01,  2.3880e-01,  2.0460e-01,  3.1206e-01,  3.1157e-01,\n",
      "         3.6982e-01,  2.6522e-01,  2.2561e-01,  2.9743e-01,  3.1316e-01,\n",
      "         2.7189e-01,  2.3372e-01,  2.0028e-01,  2.1347e-01,  2.5200e-01,\n",
      "         2.9598e-01,  2.4802e-01,  2.0065e-01,  4.2096e-01,  3.9134e-01,\n",
      "         2.3297e-01,  2.7406e-01,  2.1087e-01,  2.3902e-01,  2.8471e-01,\n",
      "         5.2009e-02,  3.6968e-01,  3.3677e-01,  2.1015e-01,  2.6748e-01,\n",
      "         1.8833e-01,  2.4549e-01,  2.8915e-01,  2.7855e-01,  2.5281e-01,\n",
      "         2.0329e-01,  2.5889e-01,  2.2133e-01,  1.9308e-01,  2.8452e-01,\n",
      "         2.1538e-01,  3.1304e-01,  1.9162e-01,  3.7718e-01,  2.9385e-01,\n",
      "         2.2593e-01,  3.0583e-01,  3.2881e-01,  2.5364e-01,  2.8018e-01,\n",
      "         3.8652e-01,  2.1690e-01,  2.1037e-01,  2.4203e-01,  2.6834e-01,\n",
      "         2.0822e-01,  2.4104e-01,  3.1536e-01,  3.1542e-01,  2.6430e-01,\n",
      "         2.5323e-01,  3.0472e-01,  2.3640e-01,  2.8118e-01,  2.5027e-01,\n",
      "         3.7199e-01,  2.9712e-01,  1.7755e-01,  2.4439e-01,  2.9124e-01,\n",
      "         2.0414e-01,  2.9048e-01,  3.0025e-01,  2.3948e-01,  3.2744e-01,\n",
      "         1.9512e-01,  3.9695e-01,  3.0162e-01,  1.9145e-01, -2.4640e-01,\n",
      "         2.3932e-01,  2.0796e-01,  2.2653e-01,  2.2303e-01,  2.6335e-01,\n",
      "         2.8925e-01,  3.9040e-01,  2.2118e-01,  2.2820e-01,  2.7864e-01,\n",
      "         2.1059e-01,  2.7405e-01,  2.5867e-01,  2.4176e-01,  2.8736e-01,\n",
      "         1.8761e-01,  1.8144e-01, -2.1339e-01,  2.0994e-01,  2.7056e-01,\n",
      "         3.1691e-01,  2.8318e-01,  1.1981e+00,  2.8011e-01,  2.9196e-01,\n",
      "         3.2254e-01,  2.0713e-01,  2.3481e-01,  3.0061e-01,  1.8966e-01,\n",
      "         1.9354e-01,  2.3145e-01,  2.0489e-01,  2.8663e-01,  2.6960e-01,\n",
      "         2.0553e-01,  2.4030e-01,  2.6140e-01,  2.9851e-01,  2.7998e-01,\n",
      "         1.8387e-01,  2.1651e-01,  3.8452e-01,  2.6189e-01,  2.3761e-01,\n",
      "         2.1714e-01,  2.4242e-01,  2.6968e-01,  2.3819e-01,  2.4721e-01,\n",
      "         1.8938e-01,  2.9354e-01,  3.1673e-01,  1.9365e-01,  2.0701e-01,\n",
      "         3.3818e-01,  2.3204e-01,  2.6790e-01,  2.7837e-01,  2.9198e-01,\n",
      "         2.5978e-01,  4.2540e-01,  2.5165e-01,  2.5803e-01,  1.9207e-01,\n",
      "         2.8750e-01,  7.2039e-02,  3.0916e-01,  2.4150e-01,  2.3408e-01,\n",
      "         2.7860e-01,  3.1963e-01,  2.1795e-01,  2.9368e-01,  2.6426e-01,\n",
      "         2.4850e-01,  2.2265e-01,  2.4622e-01,  2.4961e-01,  2.4383e-01,\n",
      "         2.4040e-01,  5.7459e-01,  3.5573e-01,  1.8902e-01,  1.9847e-01,\n",
      "         3.6599e-01,  3.3279e-01,  2.8149e-01,  2.2463e-01,  2.1436e-01,\n",
      "         2.9404e-01,  3.7713e-01,  3.6213e-01,  3.4186e-01,  3.2488e-01,\n",
      "         3.4280e-01,  2.8233e-01,  3.4001e-01,  8.6339e-01,  2.7143e-01,\n",
      "         3.3388e-01,  1.5628e-01,  2.9447e-01,  2.1855e-01,  2.8287e-01,\n",
      "         2.7326e-01,  4.0904e-01,  3.3373e-01,  2.9530e-01,  1.9994e-01,\n",
      "         2.8191e-01,  2.7322e-01,  3.0952e-01,  3.6988e-01,  1.8922e-01,\n",
      "         2.1582e-01,  3.0954e-01,  2.1379e-01,  2.1561e-01,  2.6369e-01,\n",
      "         4.5569e-01,  2.9236e-01,  2.0583e-01,  2.5365e-01,  2.5559e-01,\n",
      "         2.0867e-01,  6.8465e-02,  3.2952e-01,  3.7701e-01,  3.2502e-01,\n",
      "         3.8806e-01,  3.2442e-01,  2.3998e-01,  3.0022e-01,  2.5383e-01,\n",
      "         3.9538e-01,  3.3503e-01,  2.6499e-01,  2.6274e-01,  1.9401e-01,\n",
      "         3.0390e-01,  2.4991e-01,  2.7931e-01,  5.6543e-02,  2.5820e-01,\n",
      "         2.8219e-01,  2.0431e-01,  3.7494e-01,  2.2700e-01,  1.5092e-01,\n",
      "         2.7797e-01,  3.3395e-01,  2.3485e-01,  2.8662e-01,  2.6718e-01,\n",
      "         2.2574e-01,  2.6861e-01,  2.5613e-01,  2.3646e-01,  4.4551e-01,\n",
      "         2.5341e-01,  2.8779e-01,  3.6177e-01,  2.7930e-01,  2.6688e-01,\n",
      "         2.4604e-01,  2.7239e-01,  3.8044e-01,  3.1281e-01,  3.2528e-01,\n",
      "         3.0176e-01,  3.3524e-01,  2.5409e-01,  3.4156e-01,  3.1177e-01,\n",
      "         2.9607e-01,  2.2613e-01,  2.8863e-01,  2.2837e-01,  2.1606e-01,\n",
      "         2.6016e-01,  5.7545e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0049,  0.0796, -0.0338,  ..., -0.0212,  0.1406, -0.0191],\n",
      "        [-0.0199, -0.0092, -0.0704,  ...,  0.0211,  0.0091,  0.0234],\n",
      "        [ 0.0918, -0.0458,  0.0691,  ...,  0.1048,  0.1281, -0.0413],\n",
      "        ...,\n",
      "        [-0.0058,  0.1284,  0.0476,  ..., -0.0723,  0.0278,  0.0159],\n",
      "        [ 0.0369,  0.0608,  0.0267,  ...,  0.1008, -0.0712, -0.0414],\n",
      "        [ 0.0423,  0.0568,  0.0493,  ...,  0.0430, -0.0327, -0.0007]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8841,  1.2601, -0.6281,  ...,  0.9166, -0.9863, -0.3711],\n",
      "        [ 0.9313, -0.2162,  0.5043,  ..., -0.1338, -0.7477,  0.8755],\n",
      "        [-0.0438,  0.0026, -0.0829,  ...,  0.0140, -0.1100,  0.1339],\n",
      "        ...,\n",
      "        [-0.4227, -0.2879, -0.1824,  ...,  1.2490, -0.3371,  0.0232],\n",
      "        [ 0.9417,  0.9734,  0.3309,  ..., -0.3189,  0.1659, -0.1860],\n",
      "        [-0.3381,  0.7708, -0.2335,  ...,  0.0117,  0.2430, -0.7037]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.2999, -0.7976,  2.9704,  ...,  0.2682, -1.7009,  0.1956],\n",
      "        [ 1.7671, -1.0825,  1.2731,  ...,  0.5899, -0.2432, -0.4069],\n",
      "        [ 0.2448, -0.6641,  1.9992,  ..., -0.8308,  4.0790, -0.1651],\n",
      "        ...,\n",
      "        [-1.3037,  1.0417, -0.9238,  ...,  1.0250,  3.1478, -0.8172],\n",
      "        [-0.0486, -1.0720, -1.3837,  ..., -0.8680, -0.6735, -0.0986],\n",
      "        [ 1.4265,  0.5626,  2.1233,  ...,  1.6137,  0.6838,  0.0437]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3863,  5.4889, -4.0104,  ..., -0.0460,  1.8467,  1.2557],\n",
      "        [ 4.6527,  1.0113, -2.2362,  ...,  1.3867,  2.3171, -1.0044],\n",
      "        [-1.1546, -2.6591,  0.8170,  ...,  1.7652, -0.8933,  1.6398],\n",
      "        ...,\n",
      "        [ 0.4268,  0.7389,  0.3845,  ...,  1.7019,  2.3315,  1.3760],\n",
      "        [ 2.5542,  0.8966,  0.7154,  ...,  1.6661,  1.7863,  0.1179],\n",
      "        [-5.8310, -3.9269,  2.6395,  ..., -6.8916, -0.4970, -4.3579]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0319,  0.0903,  0.1003,  0.0978,  0.0846,  0.0956,  0.1122,  0.0902,\n",
      "         0.0817,  0.0882,  0.0746,  0.0673,  0.1069,  0.1366,  0.1259,  0.1233,\n",
      "         0.1032,  0.1140,  0.1015,  0.1260,  0.0902,  0.1265,  0.0934,  0.1146,\n",
      "         0.0901,  0.0934,  0.0867,  0.0816,  0.0854,  0.1322,  0.1030,  0.0694,\n",
      "         0.0985, -0.0715,  0.2163,  0.1878,  0.1329,  0.0866,  0.0922,  0.1837,\n",
      "         0.0928,  0.1073,  0.0966,  0.1621,  0.1718,  0.0829,  0.0859,  0.0799,\n",
      "         0.1273,  0.1168,  0.0856,  0.1368,  0.1671,  0.0950,  0.1046,  0.0829,\n",
      "         0.1056,  0.1228,  0.1448,  0.0922,  0.1068,  0.0969,  0.0926,  0.1193,\n",
      "         0.0929,  0.0940,  0.1009,  0.2939,  0.1213,  0.0741,  0.0930,  0.1115,\n",
      "         0.0228,  0.0944,  0.1027,  0.1641,  0.1629,  0.1201,  0.1095,  0.1267,\n",
      "         0.2250,  0.1435,  0.1495,  0.0771,  0.1126,  0.0812,  0.1328,  0.0868,\n",
      "         0.0911,  0.1657,  0.1340,  0.0982,  0.1427,  0.0954,  0.0866,  0.1071,\n",
      "         0.1025, -0.0927,  0.1021,  0.0701,  0.1014,  0.0909,  0.1310,  0.1213,\n",
      "         0.1164,  0.1211,  0.1178,  0.0853,  0.1102,  0.0896,  0.0854,  0.0911,\n",
      "         0.0999,  0.1046,  0.0860,  0.0878,  0.1049,  0.0931,  0.0806,  0.0966,\n",
      "         0.0853,  0.1145,  0.1709,  0.1018,  0.0808,  0.0908,  0.1333,  0.1029,\n",
      "         0.1259,  0.0884,  0.0848,  0.1116,  0.0686,  0.0972,  0.1261,  0.0945,\n",
      "         0.1239,  0.1148,  0.1364,  0.0953,  0.1257,  0.0970,  0.1027,  0.1063,\n",
      "         0.1491,  0.0871,  0.0954,  0.1365,  0.0683,  0.0865,  0.1057,  0.1626,\n",
      "         0.0834,  0.0758,  0.2054,  0.0992,  0.1279,  0.0862,  0.0909,  0.1033,\n",
      "         0.1309,  0.0900,  0.1133,  0.0750,  0.0795,  0.1098,  0.0866,  0.0965,\n",
      "         0.0749,  0.1101,  0.1133,  0.0813,  0.1134,  0.0732,  0.1094,  0.0857,\n",
      "         0.1048,  0.1107,  0.1016,  0.1153,  0.0970,  0.0849,  0.1055,  0.1860,\n",
      "         0.0922,  0.1283,  0.0951,  0.0917,  0.1049,  0.1402,  0.1286,  0.0875,\n",
      "         0.0965,  0.1568,  0.1092,  0.1050,  0.1199,  0.1244,  0.0954,  0.1122,\n",
      "         0.1163,  0.1104,  0.0650,  0.0976,  0.0952,  0.1117,  0.1511,  0.0976,\n",
      "         0.0811,  0.1893,  0.0995,  0.1208,  0.1041,  0.0852,  0.1075,  0.0828,\n",
      "         0.1184,  0.1254,  0.0910,  0.1534,  0.1675,  0.1348,  0.0758,  0.0779,\n",
      "         0.1119,  0.0889,  0.0719,  0.0790,  0.1168,  0.0996,  0.0863, -0.0604,\n",
      "         0.1041,  0.1002,  0.1057,  0.1434,  0.1102,  0.0781,  0.0902,  0.1092,\n",
      "         0.0792,  0.0877,  0.0820,  0.1025,  0.1308,  0.0962, -0.0846,  0.2344,\n",
      "         0.1050,  0.1018,  0.0892,  0.1090,  0.0816,  0.1150,  0.1083,  0.1152,\n",
      "         0.1034,  0.1079,  0.1308,  0.1196,  0.1129,  0.0971,  0.0833,  0.0867,\n",
      "         0.0987,  0.1077,  0.1009,  0.1118,  0.2338,  0.1283,  0.1309,  0.1043,\n",
      "         0.0817,  0.1044,  0.0767,  0.0138,  0.3404,  0.1134,  0.1156,  0.1338,\n",
      "         0.0551,  0.1039,  0.1928,  0.1225,  0.1102,  0.0799,  0.0954,  0.0945,\n",
      "         0.1270,  0.1075,  0.0777,  0.1031,  0.0837,  0.2022,  0.0886,  0.1011,\n",
      "         0.0987,  0.1238,  0.1183,  0.0982,  0.1214,  0.1066,  0.0662,  0.1048,\n",
      "         0.1104, -0.0764,  0.1014,  0.1003,  0.1150,  0.0953,  0.1788,  0.0885,\n",
      "         0.0859,  0.0759,  0.1136,  0.1004,  0.1383,  0.0931,  0.1230,  0.1101,\n",
      "         0.1167,  0.1444,  0.1370,  0.1048,  0.0992,  0.0773,  0.1142,  0.0990,\n",
      "         0.0682,  0.1294,  0.0965,  0.0858,  0.0986,  0.0975,  0.1028,  0.1200,\n",
      "         0.1533,  0.0986,  0.0891,  0.1070,  0.1163,  0.1055,  0.1157,  0.1036,\n",
      "         0.1147,  0.1126,  0.0901,  0.0718,  0.0882,  0.1306,  0.1324,  0.1232,\n",
      "         0.1000,  0.0955,  0.1283,  0.1074,  0.0935,  0.0865,  0.1391,  0.1008,\n",
      "         0.1161,  0.0859,  0.0913,  0.1211,  0.1124,  0.1028,  0.1110,  0.0799,\n",
      "         0.1155,  0.0886,  0.0891,  0.0999,  0.1016,  0.1053,  0.1179,  0.0734,\n",
      "         0.0972,  0.1009,  0.0982,  0.1049,  0.1109,  0.1287,  0.1485,  0.0923,\n",
      "         0.0823,  0.3400,  0.1147,  0.0949,  0.1028, -0.0758,  0.0892,  0.1264,\n",
      "         0.0897,  0.0923,  0.0818,  0.1035, -0.0204,  0.0911,  0.0886,  0.0959,\n",
      "         0.1053,  0.2532,  0.0753,  0.1260,  0.0896,  0.1073,  0.0913,  0.1013,\n",
      "         0.0909,  0.0794,  0.0980,  0.1980,  0.1253,  0.1143,  0.0745,  0.2552,\n",
      "         0.1139,  0.1163,  0.1140,  0.0951,  0.0772,  0.1962,  0.1178,  0.1388,\n",
      "         0.1301,  0.1036,  0.1114,  0.1123,  0.1657,  0.1172,  0.1408,  0.0919,\n",
      "         0.1039,  0.0808,  0.0936,  0.0946,  0.1291,  0.1001,  0.1444,  0.0801,\n",
      "         0.1076,  0.0871,  0.1183,  0.1418,  0.0779,  0.0654,  0.1062,  0.0940,\n",
      "         0.0761,  0.1168,  0.0946,  0.1112,  0.0982,  0.0854,  0.0993,  0.0886,\n",
      "         0.0204,  0.1277,  0.1341,  0.1086,  0.1180,  0.1070,  0.0983,  0.1131,\n",
      "         0.1510,  0.1147,  0.0751,  0.1092,  0.1027,  0.0766,  0.1055,  0.1016,\n",
      "         0.0922,  0.0398,  0.0911,  0.1184,  0.0712,  0.0959,  0.1301, -0.0508,\n",
      "         0.0890,  0.1220,  0.1265,  0.0988,  0.1018,  0.0858,  0.0984,  0.0916,\n",
      "         0.1034,  0.1028,  0.0911,  0.0857,  0.2021,  0.1034,  0.1006,  0.0896,\n",
      "         0.1091,  0.1908,  0.1035,  0.0882,  0.1513,  0.1318,  0.1098,  0.1347,\n",
      "         0.1176,  0.0978,  0.1181,  0.0972,  0.1137,  0.1133,  0.1051,  0.0132],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2778, -0.1211, -0.1542,  ...,  0.2892, -0.2163,  0.1012],\n",
      "        [-0.2027,  0.2970,  0.1802,  ..., -0.2114,  0.0253, -0.0691],\n",
      "        [-0.0789, -0.0043, -0.1764,  ..., -0.2310, -0.1989, -0.0054],\n",
      "        ...,\n",
      "        [-0.0004,  0.2155,  0.2151,  ...,  0.0583,  0.2400, -0.0027],\n",
      "        [-0.0944, -0.1335,  0.1907,  ...,  0.1425, -0.2834,  0.0266],\n",
      "        [-0.0781,  0.1355,  0.0680,  ..., -0.0301, -0.0622,  0.0253]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.7265, -5.3531,  0.0651,  ..., -3.5149, -1.3336,  0.3809],\n",
      "        [ 0.1480, -2.7280, -0.6830,  ...,  3.7328, -0.8068,  0.9692],\n",
      "        [-0.1218,  1.8420,  0.3764,  ..., -1.3336,  5.7973, -0.1485],\n",
      "        ...,\n",
      "        [ 0.0590,  2.9341,  0.5739,  ...,  3.3042,  1.1995, -0.5327],\n",
      "        [-0.2821, -1.0824,  0.7735,  ..., -0.7211,  0.3755,  0.2108],\n",
      "        [-0.4001,  1.8713, -3.6058,  ...,  2.5420,  1.3418,  0.1744]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.2741, -1.2032, -1.8812,  ...,  3.0599, -0.9035, -0.8057],\n",
      "        [-1.3427,  0.7123,  2.9808,  ..., -0.7049,  1.0802, -1.6866],\n",
      "        [ 0.6496, -1.1785, -0.5642,  ..., -0.7631, -0.0196,  0.8050],\n",
      "        ...,\n",
      "        [-0.8867, -0.7793, -0.5162,  ...,  2.3107,  0.1003, -0.4411],\n",
      "        [-1.8226,  1.0449,  0.6469,  ...,  1.0933,  1.5085,  1.0744],\n",
      "        [ 2.3613, -4.5499, -9.2330,  ..., -3.2463, -1.2749, -6.3065]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0627,  0.2111,  0.2494,  0.2480,  0.2165,  0.2013,  0.2142,  0.1950,\n",
      "         0.1628,  0.1971,  0.2167,  0.1982,  0.2788,  0.3764,  0.2414,  0.3257,\n",
      "         0.2483,  0.2394,  0.2584,  0.2688,  0.2153,  0.2855,  0.2306,  0.2192,\n",
      "         0.2660,  0.1629,  0.1904,  0.2351,  0.2224,  0.2531,  0.2157,  0.1868,\n",
      "         0.2029,  0.2337,  0.2559,  0.5815,  0.2404,  0.1782,  0.2284,  0.2705,\n",
      "         0.2072,  0.2809,  0.2506,  0.2266,  0.2525,  0.3169,  0.2198,  0.1975,\n",
      "         0.2930,  0.2317,  0.2350,  0.2695,  0.2227,  0.1929,  0.2259,  0.2090,\n",
      "         0.1930,  0.2195,  0.2958,  0.1946,  0.2224,  0.2324,  0.2708,  0.1891,\n",
      "         0.2024,  0.2264,  0.2545,  0.5524,  0.2373,  0.2078,  0.6430,  0.2055,\n",
      "         0.0436,  0.2192,  0.2016,  0.3297,  0.2600,  0.2748,  0.2841,  0.2483,\n",
      "         0.3249,  0.2123,  0.2269,  0.1856,  0.2102,  0.2463,  0.2669,  0.2271,\n",
      "         0.2248,  0.2055,  0.2313,  0.1936,  0.2121,  0.1890,  0.1906,  0.2406,\n",
      "         0.2197,  0.2019,  0.1927,  0.1992,  0.1707,  0.1930,  0.2708,  0.2172,\n",
      "         0.2134,  0.2382,  0.2338,  0.1992,  0.2051,  0.1937,  0.2444,  0.2018,\n",
      "         0.2217,  0.2429,  0.2084,  0.2186,  0.2261,  0.1757,  0.1679,  0.2143,\n",
      "         0.2052,  0.1972,  0.2735,  0.2079,  0.2425,  0.2120,  0.2627,  0.2088,\n",
      "         0.2507,  0.3038,  0.1784,  0.2485,  0.1737,  0.2429,  0.2049,  0.2274,\n",
      "         0.2394,  0.2623,  0.3057,  0.1981,  0.2233,  0.2263,  0.2308,  0.3496,\n",
      "         0.2406,  0.3695,  0.2378,  0.2690,  0.1692,  0.2235,  0.2279,  0.2211,\n",
      "         0.1987,  0.2086,  0.2517,  0.1889,  0.2866,  0.3169,  0.2468,  0.2635,\n",
      "         0.3716,  0.2753,  0.2406,  0.2083,  0.2183,  0.2458,  0.1970,  0.1908,\n",
      "         0.1702,  0.1841,  0.2299,  0.2149,  0.2761,  0.2258,  0.3173,  0.2022,\n",
      "         0.2864,  0.3554,  0.1804,  0.3390,  0.1948,  0.2249,  0.2077,  0.3197,\n",
      "         0.1909,  0.3040,  0.2325,  0.1602,  0.1854,  0.3316,  0.2440,  0.3789,\n",
      "         0.2389,  0.7083,  0.2026,  0.2009,  0.2285,  0.2881,  0.1783,  0.2666,\n",
      "         0.2506,  0.1886,  0.2145,  0.1897,  0.2368,  0.2674,  0.2511,  0.2527,\n",
      "         0.2467,  0.2127,  0.2456,  0.2241,  0.2119,  0.2075,  0.2807,  0.2660,\n",
      "         0.2187,  0.2351,  0.2323,  0.3068,  0.2781,  0.2504,  0.1695,  0.2299,\n",
      "         0.2121,  0.1959,  0.1752,  0.1893,  0.2051,  0.2368,  0.1967,  0.3007,\n",
      "         0.2566,  0.1851,  0.2294,  0.2895,  0.2329,  0.1835,  0.2285,  0.2731,\n",
      "         0.2625,  0.2135, -0.2329,  0.3102,  0.2268,  0.2032,  0.2511,  0.4910,\n",
      "         0.2199,  0.2239,  0.2070,  0.2073,  0.2043,  0.2483,  0.2269,  0.3810,\n",
      "         0.2164,  0.1603,  0.2705,  0.2490,  0.2125,  0.2125,  0.1788,  0.2334,\n",
      "         0.1849,  0.2571,  0.2358,  0.2129,  0.3633,  0.2377,  0.1925,  0.2127,\n",
      "         0.1895,  0.2149,  0.2082,  0.0520,  0.2350,  0.2045,  0.2171,  0.2610,\n",
      "         0.1988,  0.1892,  0.2390,  0.2782,  0.2472,  0.2109,  0.1904,  0.2398,\n",
      "         0.2108,  0.2342,  0.1922,  0.2354,  0.1680,  0.2682,  0.2005,  0.2153,\n",
      "         0.2517,  0.2671,  0.2183,  0.2274,  0.2904,  0.1949,  0.1884,  0.2226,\n",
      "         0.3006,  0.2091,  0.2188,  0.2183,  0.2547,  0.2247,  0.2943,  0.2419,\n",
      "         0.2015,  0.2208,  0.2432,  0.2725,  0.3452,  0.1958,  0.2676,  0.2116,\n",
      "         0.2005,  0.2643,  0.2319,  0.1876,  0.2673,  0.1727,  0.2322,  0.2352,\n",
      "         0.1532,  0.2777,  0.2230,  0.2056,  0.2425,  0.1797,  0.2416,  0.2653,\n",
      "         0.2483,  0.1976,  0.1992,  0.2378,  0.1936,  0.2559,  0.2701,  0.2072,\n",
      "         0.2741,  0.2270,  0.2086,  0.1983,  0.1819,  0.2180,  0.2395,  0.2242,\n",
      "         0.3596,  0.2103,  0.2833,  0.2616,  0.1892,  0.1924,  0.2373,  0.2104,\n",
      "         0.1949,  0.1878,  0.2051,  0.2358,  0.2288,  0.2422,  0.2023,  0.1966,\n",
      "         0.2256,  0.2231,  0.1504,  0.2370,  0.2868,  0.2505,  0.2217,  0.1819,\n",
      "         0.2293,  0.2185,  0.2278,  0.2124,  0.1677,  0.2262,  0.3116,  0.1999,\n",
      "         0.1948,  0.2415,  0.2577,  0.2136,  0.2712,  0.2517,  0.2563,  0.3685,\n",
      "         0.2560,  0.2170,  0.2250,  0.2424,  0.1254,  0.2563,  0.1790,  0.2141,\n",
      "         0.2590,  0.2506,  0.1815,  0.2178,  0.2051,  0.2364,  0.1867,  0.2076,\n",
      "         0.1989,  0.2060,  0.2000,  0.2601,  0.3070,  0.2471,  0.1850,  0.2327,\n",
      "         0.2150,  0.2276,  0.2206,  0.1838,  0.2932,  0.3232,  0.2897,  0.3358,\n",
      "         0.2637,  0.2600,  0.2637,  0.2547,  0.6183,  0.2979,  0.3073,  0.1619,\n",
      "         0.2457,  0.2210,  0.2255,  0.2068,  0.3147,  0.2637,  0.2788,  0.1807,\n",
      "         0.2703,  0.1938,  0.2713,  0.2960,  0.1743,  0.1958,  0.2822,  0.2522,\n",
      "         0.2217,  0.2316,  0.3198,  0.2245,  0.2010,  0.1912,  0.2241,  0.2168,\n",
      "         0.0377,  0.2256,  0.3597,  0.2398,  0.2637,  0.2082,  0.2225,  0.2655,\n",
      "         0.1958,  0.2870,  0.3510,  0.2339,  0.2114,  0.1805,  0.2146,  0.2431,\n",
      "         0.2433,  0.0858,  0.2657,  0.2187,  0.1715,  0.2503,  0.2429,  0.1553,\n",
      "         0.2102,  0.2877,  0.2523,  0.2599,  0.2541,  0.2088,  0.2339,  0.1964,\n",
      "         0.1960,  0.2526,  0.1984,  0.2151,  0.2292,  0.2392,  0.2700,  0.2206,\n",
      "         0.2272,  0.4261,  0.2754,  0.2541,  0.2243,  0.3258,  0.2766,  0.2537,\n",
      "         0.2474,  0.2092,  0.2138,  0.2330,  0.2365,  0.1842,  0.2194,  0.0481],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.5583e-01,  1.6458e-01,  1.8197e-01,  2.0792e-01,  1.5886e-01,\n",
      "         1.4222e-01,  1.5845e-01,  1.4269e-01,  1.3648e-01,  1.5702e-01,\n",
      "         1.6670e-01,  1.3271e-01,  1.7980e-01,  3.2683e-01,  2.0897e-01,\n",
      "         2.6234e-01,  1.8381e-01,  1.8566e-01,  1.8115e-01,  1.9588e-01,\n",
      "         1.5456e-01,  2.1353e-01,  1.5126e-01,  1.6348e-01,  1.8062e-01,\n",
      "         1.4414e-01,  1.7974e-01,  2.0646e-01,  1.7899e-01,  2.0434e-01,\n",
      "         1.6415e-01,  1.4987e-01,  1.3866e-01,  2.2488e-01,  1.7041e-01,\n",
      "         6.1698e-01,  1.8228e-01,  1.7578e-01,  1.6113e-01,  2.4024e-01,\n",
      "         1.6280e-01,  2.2871e-01,  1.6127e-01,  1.8426e-01,  2.1641e-01,\n",
      "         2.6774e-01,  1.8475e-01,  1.5955e-01,  2.5002e-01,  1.9592e-01,\n",
      "         1.5467e-01,  2.0025e-01,  1.7020e-01,  1.4393e-01,  1.9788e-01,\n",
      "         1.5900e-01,  1.4895e-01,  1.5042e-01,  2.6026e-01,  1.5933e-01,\n",
      "         1.5081e-01,  2.0102e-01,  1.9843e-01,  1.5577e-01,  1.5257e-01,\n",
      "         1.5654e-01,  1.6762e-01,  7.5296e-01,  1.6636e-01,  1.5402e-01,\n",
      "         4.6294e-02,  1.6461e-01, -6.6102e-03,  1.7538e-01,  1.5686e-01,\n",
      "         2.5396e-01,  1.9640e-01,  2.0717e-01,  2.0110e-01,  2.0370e-01,\n",
      "         2.1669e-01,  1.6539e-01,  1.6955e-01,  1.2699e-01,  1.4506e-01,\n",
      "         1.7137e-01,  1.8019e-01,  1.7089e-01,  1.6468e-01,  2.1281e-01,\n",
      "         1.7568e-01,  1.3533e-01,  1.5223e-01,  1.4241e-01,  1.4643e-01,\n",
      "         2.1323e-01,  1.6563e-01,  3.7506e-01,  1.3279e-01,  1.4821e-01,\n",
      "         1.6229e-01,  1.5335e-01,  1.9785e-01,  1.5253e-01,  1.5380e-01,\n",
      "         1.8114e-01,  1.5337e-01,  1.5664e-01,  1.6879e-01,  1.5306e-01,\n",
      "         1.9569e-01,  1.5582e-01,  1.8156e-01,  2.0031e-01,  1.4573e-01,\n",
      "         1.6565e-01,  1.7020e-01,  1.5614e-01,  1.4373e-01,  1.7403e-01,\n",
      "         1.5083e-01,  1.5831e-01,  1.6520e-01,  1.5690e-01,  1.9232e-01,\n",
      "         1.5903e-01,  2.0464e-01,  1.4832e-01,  1.7730e-01,  1.8827e-01,\n",
      "         1.4112e-01,  1.9067e-01,  1.6094e-01,  1.4695e-01,  1.5871e-01,\n",
      "         1.6182e-01,  2.0271e-01,  1.8662e-01,  2.0484e-01,  1.4663e-01,\n",
      "         1.8820e-01,  2.1564e-01,  1.7099e-01,  2.4158e-01,  1.9468e-01,\n",
      "         2.7900e-01,  1.8111e-01,  2.3911e-01,  1.5275e-01,  1.6435e-01,\n",
      "         1.6918e-01,  1.7360e-01,  1.4911e-01,  1.4707e-01,  2.3178e-01,\n",
      "         1.3859e-01,  2.2257e-01,  1.7704e-01,  1.8228e-01,  2.3528e-01,\n",
      "         2.8649e-01,  1.8691e-01,  2.0105e-01,  1.5032e-01,  1.7764e-01,\n",
      "         1.6979e-01,  1.4514e-01,  1.3593e-01,  1.2960e-01,  1.5276e-01,\n",
      "         1.8807e-01,  1.7210e-01,  2.1407e-01,  2.1323e-01,  2.3099e-01,\n",
      "         1.4930e-01,  2.0003e-01,  2.2725e-01,  1.4609e-01,  3.5971e-01,\n",
      "         1.4036e-01,  1.7294e-01,  1.4863e-01,  1.9655e-01,  1.4858e-01,\n",
      "         2.5492e-01,  2.2977e-01,  1.2809e-01,  1.3660e-01,  2.7187e-01,\n",
      "         1.8013e-01,  2.6223e-01,  1.4699e-01,  2.1151e+00,  1.5537e-01,\n",
      "         1.6117e-01,  1.8694e-01,  1.8826e-01,  1.4469e-01,  1.9167e-01,\n",
      "         2.0336e-01,  1.5928e-01,  2.0197e-01,  1.4118e-01,  2.0147e-01,\n",
      "         1.8554e-01,  1.9080e-01,  1.6589e-01,  2.3433e-01,  1.4772e-01,\n",
      "         1.9256e-01,  1.6619e-01,  1.5463e-01,  1.4645e-01,  2.2029e-01,\n",
      "         1.9336e-01,  1.7481e-01,  1.7506e-01,  2.1029e-01,  2.3300e-01,\n",
      "         1.8893e-01,  1.7255e-01,  1.3724e-01,  2.0819e-01,  1.6483e-01,\n",
      "         1.5570e-01,  1.4373e-01,  1.5941e-01,  1.5687e-01,  2.1235e-01,\n",
      "         1.5520e-01,  2.5171e+00,  2.1923e-01,  1.2453e-01,  1.8497e-01,\n",
      "         1.9306e-01,  1.7169e-01,  1.4926e-01,  1.4328e-01,  1.7837e-01,\n",
      "         2.2950e-01,  1.7026e-01,  2.4611e+00,  2.5711e-01,  1.4745e-01,\n",
      "         1.6339e-01,  2.5688e-01,  1.6264e-01,  1.5801e-01,  1.6309e-01,\n",
      "         1.4103e-01,  1.6114e-01,  1.4158e-01,  1.7461e-01,  1.7149e-01,\n",
      "         2.7059e-01,  1.7242e-01,  1.2586e-01,  1.8509e-01,  1.6231e-01,\n",
      "         1.5975e-01,  1.6281e-01,  1.7397e-01,  1.7470e-01,  1.4098e-01,\n",
      "         2.2063e-01,  1.8709e-01,  1.8378e-01,  4.0520e-01,  1.6544e-01,\n",
      "         1.6063e-01,  1.6041e-01,  1.4579e-01,  1.6455e-01,  1.5979e-01,\n",
      "         7.1167e-01,  1.7078e-01,  1.5860e-01,  1.7149e-01,  1.6616e-01,\n",
      "         1.9971e-01,  1.5718e-01,  1.7235e-01,  1.8438e-01,  1.5239e-01,\n",
      "         1.7162e-01,  1.3605e-01,  1.5651e-01,  1.4684e-01,  1.7189e-01,\n",
      "         1.4549e-01,  1.7558e-01,  1.3983e-01,  1.9443e-01,  1.5784e-01,\n",
      "         1.5582e-01,  1.9934e-01,  1.9525e-01,  1.4703e-01,  2.4204e-01,\n",
      "         1.8359e-01,  1.5044e-01,  2.0058e-01,  1.6991e-01,  2.8148e-01,\n",
      "         1.7974e-01,  1.6118e-01,  1.7444e-01,  1.8981e-01,  1.8370e-01,\n",
      "         2.2226e-01,  1.5726e-01,  1.4730e-01,  1.6256e-01,  1.8383e-01,\n",
      "         2.4254e-01,  2.5519e-01,  1.7940e-01,  1.6245e-01,  1.4989e-01,\n",
      "         1.5379e-01,  1.9139e-01,  1.9344e-01,  1.3398e-01,  1.8337e-01,\n",
      "         1.5040e-01,  1.7911e-01,  1.6171e-01,  1.3523e-01,  5.4605e+00,\n",
      "         1.5881e-01,  1.4423e-01,  1.5600e-01,  1.4916e-01,  1.7011e-01,\n",
      "         1.9777e-01,  2.0215e-01,  1.2799e-01,  1.5923e-01,  2.1676e-01,\n",
      "         1.3977e-01,  1.7835e-01,  2.4116e-01,  1.7344e-01,  2.1488e-01,\n",
      "         1.5579e-01,  1.6865e-01,  1.7583e-01,  1.4176e-01,  1.5000e-01,\n",
      "         1.7954e-01,  1.7976e-01,  2.8477e-01,  1.5168e-01,  2.2093e-01,\n",
      "         2.0215e-01,  1.3400e-01,  1.4388e-01,  1.7456e-01,  1.5423e-01,\n",
      "         1.5707e-01,  1.5100e-01,  1.4948e-01,  1.5853e-01,  1.6074e-01,\n",
      "         1.7330e-01,  1.5760e-01,  1.4226e-01,  1.7912e-01,  1.8282e-01,\n",
      "         1.2702e-01,  1.8516e-01,  1.9899e-01,  1.6360e-01,  1.6236e-01,\n",
      "         1.5465e-01,  1.5235e-01,  1.7416e-01,  1.7015e-01,  1.5917e-01,\n",
      "         1.4212e-01,  1.6291e-01,  2.5345e-01,  1.6701e-01,  1.4763e-01,\n",
      "         1.6975e-01,  2.0256e-01,  1.6515e-01,  1.9852e-01,  1.2991e+00,\n",
      "         1.9594e-01,  2.6771e-01,  1.9862e-01,  1.5207e-01,  1.6803e-01,\n",
      "         1.5831e-01,  4.0148e+00,  1.6928e-01,  1.3382e-01,  1.7820e-01,\n",
      "         2.0096e-01,  2.1817e-01,  1.1961e-01,  1.8248e-01,  1.6298e-01,\n",
      "         1.7030e-01,  1.4106e-01,  1.6028e-01,  1.3935e-01,  1.5241e-01,\n",
      "         1.7868e-01,  2.1180e-01,  1.8998e-01,  2.3722e-01,  1.4837e-01,\n",
      "         1.7459e-01,  1.8545e-01,  2.3156e-01,  1.7222e-01,  1.5377e-01,\n",
      "         2.5178e-01,  2.8675e-01,  2.0507e-01,  2.5137e-01,  1.8816e-01,\n",
      "         1.5631e-01,  1.6928e-01,  1.6876e-01,  8.0431e+00,  2.0267e-01,\n",
      "         2.2498e-01,  1.4882e-01,  1.8083e-01,  1.8124e-01,  2.2278e-01,\n",
      "         1.7481e-01,  2.3087e-01,  1.8128e-01,  2.1446e-01,  1.4030e-01,\n",
      "         2.1781e-01,  1.4043e-01,  2.1164e-01,  2.1963e-01,  1.5551e-01,\n",
      "         1.7168e-01,  2.0861e-01,  1.8555e-01,  1.4485e-01,  1.8797e-01,\n",
      "         3.8759e-01,  2.1366e-01,  1.6399e-01,  1.6895e-01,  1.5024e-01,\n",
      "         1.6269e-01,  1.9745e-02,  1.9682e-01,  2.7993e-01,  1.9728e-01,\n",
      "         2.1243e-01,  1.5368e-01,  1.8379e-01,  1.8075e-01,  1.4863e-01,\n",
      "         2.5591e-01,  1.1998e+00,  1.5990e-01,  1.6892e-01,  1.4405e-01,\n",
      "         1.6796e-01,  1.7778e-01,  1.5223e-01,  2.2864e+00,  2.5031e-01,\n",
      "         1.6878e-01,  1.5282e-01,  1.7926e-01,  1.5576e-01,  1.9016e-01,\n",
      "         1.4083e-01,  2.2560e-01,  2.2790e-01,  1.9366e-01,  2.2626e-01,\n",
      "         1.6086e-01,  1.7701e-01,  1.4979e-01,  1.4872e-01,  1.4451e-01,\n",
      "         1.4825e-01,  1.7975e-01,  1.8818e-01,  1.7596e-01,  1.9973e-01,\n",
      "         1.6966e-01,  1.5584e-01,  4.9063e-01,  1.9658e-01,  1.8339e-01,\n",
      "         1.8400e-01,  2.9085e-01,  2.0234e-01,  1.8496e-01,  2.1258e-01,\n",
      "         1.5033e-01,  1.6261e-01,  1.8824e-01,  1.7955e-01,  1.6545e-01,\n",
      "         1.9544e-01,  2.7973e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.2732,  0.9292, -1.0205,  ...,  0.2288, -2.2574, -3.7006],\n",
      "        [-0.2558,  0.0417,  1.3518,  ..., -0.7974, -0.1019, -0.0231],\n",
      "        [-0.2269,  0.0265,  0.8257,  ..., -1.3897,  0.9686,  0.0203],\n",
      "        ...,\n",
      "        [-2.2725,  0.9263, -1.0138,  ...,  0.3040, -2.2434, -3.7124],\n",
      "        [-2.2806,  0.9670, -0.9895,  ...,  0.2817, -2.2224, -3.6577],\n",
      "        [-2.2950,  0.8968, -1.0096,  ...,  0.2397, -2.1929, -3.7084]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    # print(name)\n",
    "    print(param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c826ecbb-6ee0-400b-8983-e856af03923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd8a0aa03224a5dbd2f06f846dfe0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.62669491767883\n",
      "142.5286384820938\n",
      "143.50277489423752\n",
      "146.23862969875336\n",
      "140.5147095322609\n",
      "141.00257223844528\n",
      "139.3914748430252\n",
      "136.42909741401672\n",
      "141.59473425149918\n",
      "141.94694358110428\n",
      "140.41304218769073\n",
      "139.03717106580734\n",
      "140.07656514644623\n",
      "139.0496900677681\n",
      "133.51497408747673\n",
      "143.52370661497116\n",
      "135.01737874746323\n",
      "144.30945867300034\n",
      "137.03003215789795\n",
      "132.50970220565796\n",
      "134.4957340657711\n",
      "137.367799192667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1afaa1908ab4af9a6f40fd30e7477c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.06550937891006\n",
      "130.4050893187523\n",
      "121.49772857874632\n",
      "129.01674707233906\n",
      "127.08680990338326\n",
      "125.33649933338165\n",
      "119.89576543867588\n",
      "125.69496434926987\n",
      "121.67850044369698\n",
      "138.26129053533077\n",
      "131.07128305733204\n",
      "127.71054154634476\n",
      "124.89710092544556\n",
      "129.9786283671856\n",
      "121.57243762910366\n",
      "119.14458272606134\n",
      "126.05276826024055\n",
      "133.2459368109703\n",
      "122.86035543680191\n",
      "123.20752841234207\n",
      "106.55835398286581\n",
      "115.82176097482443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9030c011e24b10bae88ae9b122964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114.60257506370544\n",
      "114.78088916838169\n",
      "107.76556182652712\n",
      "113.857197413221\n",
      "96.57973081246018\n",
      "106.73437930736691\n",
      "97.5641563758254\n",
      "95.66529198735952\n",
      "93.67192112002522\n",
      "111.64739824831486\n",
      "116.35455247201025\n",
      "103.08623736910522\n",
      "100.48711034096777\n",
      "114.12756237387657\n",
      "106.53347855061293\n",
      "104.6505661890842\n",
      "110.28618248924613\n",
      "116.79008141160011\n",
      "121.74708787724376\n",
      "104.89155095815659\n",
      "107.77591145224869\n",
      "94.34633373469114\n",
      "88.32246338948607\n"
     ]
    }
   ],
   "source": [
    "# Fine tuninng\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "loss_array = []\n",
    "loss_per_100 = 0\n",
    "c = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # print(batch)\n",
    "        batch_label = [index_to_ans[int(a[0])-1] if a[0].isdigit() else a[0] for a in batch['choices']['label']]\n",
    "        highest_prob_index = int(batch['answerKey'][0]) - 1 if batch['answerKey'][0].isdigit() else batch_label.index(batch['answerKey'][0]) \n",
    "        labels = [0.0] * len(batch_label)\n",
    "        labels[highest_prob_index] = 1.0\n",
    "        labels = torch.tensor(labels, requires_grad=True).to(\"cuda\")\n",
    "        # labels.retain_grad()\n",
    "        # print(labels)\n",
    "        # labels = batch['labels'][0].to(device)\n",
    "        batch = batch['input_ids'].to(device)\n",
    "        outputs = model(batch, decoder_input_ids=torch.tensor([[0]]).to(device))\n",
    "        # print(outputs)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        preds = logits[0, 0, tuple(list(ans_id_dict.keys()))]\n",
    "        \n",
    "        # preds_prod = []\n",
    "        # for i in range(1):\n",
    "        #     pred_prob = []\n",
    "        #     for t in ans_id_dict.keys():\n",
    "        #         if ans_id_dict[t] in batch_label:\n",
    "        #             pred_prob.append(logits[..., t][0][0].item())\n",
    "        #     preds_prod.append(pred_prob)\n",
    "        # preds = torch.tensor(preds_prod[0], requires_grad=True).to(\"cuda\")\n",
    "        # preds.retain_grad()\n",
    "        # print(preds)\n",
    "        # preds.requires_grad_()\n",
    "        if len(preds) != len(labels):\n",
    "            continue\n",
    "\n",
    "        \n",
    "        loss = loss_fct(preds, labels)\n",
    "        # loss.retain_grad()\n",
    "        loss_per_100 += loss.item()\n",
    "        if c%100==99:\n",
    "            print(loss_per_100)\n",
    "            loss_array.append(loss_per_100)\n",
    "            loss_per_100 = 0\n",
    "        # loss = outputs.loss\n",
    "        # print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # print(loss.grad, preds.grad, labels.grad)\n",
    "        # for param in model.parameters():\n",
    "        #     # print(name)\n",
    "        #     # print(param)\n",
    "        #     print(param.grad)\n",
    "        #     break\n",
    "            \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # progress_bar.update(1)\n",
    "        c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "adf97c3f-d053-4cbd-825d-8d6ff4d740ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{71: 'A', 272: 'B', 205: 'C', 309: 'D'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "aecc1b4a-2ac7-4292-8ff7-155124ed60dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "54b8810d-a335-491a-b78e-4270f13eb9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight\n",
      "Parameter containing:\n",
      "tensor([[ 3.0842e+00,  1.9254e-01, -2.7279e-01,  ...,  5.0338e-01,\n",
      "         -5.0113e-01,  4.1227e+00],\n",
      "        [-9.0299e+00,  9.4837e+00, -5.7072e-01,  ...,  1.2272e+01,\n",
      "          6.3371e+00,  9.9253e+01],\n",
      "        [ 7.4780e+00,  2.9949e+00, -4.1963e+00,  ..., -6.2720e+00,\n",
      "          1.0989e+01,  1.8465e+01],\n",
      "        ...,\n",
      "        [-4.4965e-01, -3.3481e-01, -3.8736e-01,  ..., -2.0925e-01,\n",
      "         -1.9933e+00, -9.1097e-01],\n",
      "        [-1.0199e+00, -8.0590e-01,  4.3409e-01,  ..., -5.9127e-02,\n",
      "         -9.1876e-01, -9.2657e-01],\n",
      "        [ 1.0043e+00,  1.5183e-01, -2.4818e-01,  ..., -1.8493e-01,\n",
      "         -2.7056e-01,  1.7908e+00]], device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0671, -0.0989,  ..., -0.0978, -0.0593,  0.0457],\n",
      "        [ 0.0937,  0.0238,  0.0696,  ..., -0.0138,  0.0104, -0.1074],\n",
      "        [-0.1015, -0.0440, -0.0720,  ...,  0.1680,  0.0483,  0.0362],\n",
      "        ...,\n",
      "        [-0.0408, -0.0334, -0.1107,  ...,  0.0397,  0.0611, -0.0232],\n",
      "        [ 0.0929, -0.0743, -0.0316,  ...,  0.0473,  0.0238,  0.0274],\n",
      "        [ 0.0946,  0.0147, -0.0539,  ..., -0.0500, -0.0595, -0.0873]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1273,  0.3903,  0.5319,  ..., -0.6235,  0.4498,  0.0703],\n",
      "        [-0.4336, -1.0889, -0.0451,  ..., -1.7447, -0.1486, -1.1927],\n",
      "        [-0.1240, -0.8413, -0.2103,  ...,  0.3633, -0.2394,  0.0957],\n",
      "        ...,\n",
      "        [-0.5635, -0.2633, -0.1393,  ..., -0.5172, -0.5117, -0.1270],\n",
      "        [-0.1917,  0.3156, -0.5244,  ...,  0.0676, -0.2987,  0.2911],\n",
      "        [ 0.4029, -0.2129, -0.2009,  ..., -0.0476, -0.2175, -0.0161]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.6815,  0.3230, -0.0520,  ...,  0.2811,  0.3789, -0.0388],\n",
      "        [ 0.0373, -0.0581,  0.4757,  ...,  1.0135,  0.3266, -0.3457],\n",
      "        [-0.1984, -0.4395,  0.2240,  ...,  0.0406,  0.2322,  0.0563],\n",
      "        ...,\n",
      "        [ 0.6446, -0.6548,  0.0977,  ...,  0.2723,  0.3133,  0.1099],\n",
      "        [ 0.5351,  0.3514, -0.1866,  ..., -0.1668,  0.0523,  0.2968],\n",
      "        [-0.0634, -0.3910, -0.2740,  ..., -0.0166, -0.0937, -0.2901]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6292, -0.2008,  0.4424,  ..., -0.2170,  0.2619,  0.5838],\n",
      "        [-0.6205, -0.0790,  0.5157,  ..., -0.2136, -0.6427,  0.2868],\n",
      "        [ 0.1301, -0.1420, -1.1101,  ..., -0.3281, -0.7565, -0.1212],\n",
      "        ...,\n",
      "        [ 0.3416, -0.8364, -0.0052,  ...,  0.0403, -0.1818, -0.0989],\n",
      "        [-0.5174,  1.1519, -0.2170,  ..., -0.1782,  0.2821, -0.4152],\n",
      "        [ 0.3341,  0.0822,  0.1205,  ...,  0.1180, -0.4135,  0.8773]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter containing:\n",
      "tensor([[ 5.0652e+00, -1.1234e+01,  5.6757e-01, -1.0316e+01,  4.0690e+00,\n",
      "         -1.6761e+01],\n",
      "        [ 7.8778e+00,  8.4475e+00, -1.6642e+00,  3.0609e+00,  7.4574e+00,\n",
      "          5.4820e+00],\n",
      "        [ 6.0811e+00,  6.6934e+00, -1.9002e+00,  3.5499e+00,  7.4193e+00,\n",
      "          5.5853e+00],\n",
      "        [ 5.1839e+00,  5.6683e+00, -2.1065e+00,  3.5757e+00,  7.1214e+00,\n",
      "          5.6091e+00],\n",
      "        [ 4.4211e+00,  5.0100e+00, -2.2567e+00,  3.7724e+00,  6.7873e+00,\n",
      "          5.5197e+00],\n",
      "        [ 4.2258e+00,  4.5678e+00, -2.6873e+00,  3.8370e+00,  6.6068e+00,\n",
      "          5.5367e+00],\n",
      "        [ 3.7657e+00,  4.0154e+00, -2.7027e+00,  3.8975e+00,  6.3410e+00,\n",
      "          5.4213e+00],\n",
      "        [ 3.6424e+00,  3.7212e+00, -2.5726e+00,  3.9279e+00,  6.2578e+00,\n",
      "          5.4675e+00],\n",
      "        [ 3.0435e+00,  2.8483e+00, -3.5453e+00,  3.8498e+00,  5.8077e+00,\n",
      "          5.2948e+00],\n",
      "        [ 2.4482e+00,  1.8200e+00, -4.0232e+00,  3.8944e+00,  5.2633e+00,\n",
      "          5.1028e+00],\n",
      "        [ 1.8397e+00,  9.1749e-01, -4.4217e+00,  3.8788e+00,  4.6958e+00,\n",
      "          4.8568e+00],\n",
      "        [ 1.5344e+00,  2.3970e-02, -4.8642e+00,  3.8907e+00,  4.0433e+00,\n",
      "          4.5948e+00],\n",
      "        [ 1.0470e+00, -1.3265e+00, -5.1972e+00,  3.7790e+00,  3.3395e+00,\n",
      "          4.2876e+00],\n",
      "        [ 4.8752e-01, -2.3745e+00, -5.8035e+00,  3.7213e+00,  2.4886e+00,\n",
      "          3.9717e+00],\n",
      "        [ 5.8434e-01, -3.5841e+00, -5.7671e+00,  3.6314e+00,  1.6078e+00,\n",
      "          3.6407e+00],\n",
      "        [-2.5419e+00, -5.8710e+00, -7.9908e+00,  3.5110e+00, -2.8935e+00,\n",
      "          2.2445e+00],\n",
      "        [ 7.9809e-02, -7.9809e-02,  3.3286e-01, -3.5818e-01,  3.1339e-01,\n",
      "         -2.7836e-01],\n",
      "        [ 3.7112e+00, -2.5148e-01,  8.8047e+00,  4.0275e+00,  1.0028e+00,\n",
      "         -8.1554e+00],\n",
      "        [ 3.1740e+00,  1.0354e-02,  7.6512e+00,  4.3579e+00,  1.5026e+00,\n",
      "          2.4064e+00],\n",
      "        [ 3.1317e+00,  2.4559e-01,  7.0351e+00,  4.5303e+00,  1.2038e+00,\n",
      "          2.5442e+00],\n",
      "        [ 3.1318e+00,  2.1446e-01,  6.5614e+00,  4.6277e+00,  1.2099e+00,\n",
      "          2.5443e+00],\n",
      "        [ 2.9455e+00,  1.0083e-01,  6.2237e+00,  4.6306e+00,  1.2482e+00,\n",
      "          2.5550e+00],\n",
      "        [ 2.9810e+00,  1.8940e-01,  5.8864e+00,  4.7495e+00,  1.1488e+00,\n",
      "          2.6953e+00],\n",
      "        [ 2.7799e+00,  1.7990e-01,  5.6770e+00,  4.7674e+00,  1.1569e+00,\n",
      "          2.5345e+00],\n",
      "        [ 2.6940e+00,  1.8521e-01,  5.2225e+00,  4.8237e+00,  1.0898e+00,\n",
      "          2.4809e+00],\n",
      "        [ 2.5322e+00,  2.6570e-01,  4.5760e+00,  4.8144e+00,  1.0112e+00,\n",
      "          2.3645e+00],\n",
      "        [ 2.2950e+00,  1.5048e-01,  4.0592e+00,  4.8380e+00,  8.5697e-01,\n",
      "          2.2377e+00],\n",
      "        [ 1.9238e+00,  1.6915e-01,  3.5331e+00,  4.8666e+00,  6.5468e-01,\n",
      "          2.1507e+00],\n",
      "        [ 1.4754e+00,  1.2329e-01,  2.8972e+00,  4.8726e+00,  5.7044e-01,\n",
      "          1.9442e+00],\n",
      "        [ 8.3816e-01,  5.7568e-02,  2.2624e+00,  4.8767e+00,  3.4394e-01,\n",
      "          1.8279e+00],\n",
      "        [-1.6592e-01, -1.4224e-01,  1.8491e+00,  4.8675e+00,  1.2460e-01,\n",
      "          1.6641e+00],\n",
      "        [-6.7216e+00, -1.1927e-01,  4.2488e-01,  4.9003e+00, -7.2145e-01,\n",
      "          1.0982e+00]], device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.0604, 0.0901, 0.0718, 0.0799, 0.0746, 0.0783, 0.0945, 0.0914, 0.0550,\n",
      "        0.1077, 0.0839, 0.0672, 0.0749, 0.0640, 0.0701, 0.0880, 0.0771, 0.0682,\n",
      "        0.1017, 0.0579, 0.0866, 0.0599, 0.1085, 0.0876, 0.0580, 0.0718, 0.0563,\n",
      "        0.0944, 0.0767, 0.0884, 0.0860, 0.0988, 0.1219, 0.0750, 0.0660, 0.0781,\n",
      "        0.0933, 0.0696, 0.0766, 0.0609, 0.0782, 0.0737, 0.0854, 0.0430, 0.0701,\n",
      "        0.1342, 0.1055, 0.0656, 0.0621, 0.0612, 0.0576, 0.0851, 0.0850, 0.0846,\n",
      "        0.1034, 0.0725, 0.0572, 0.0932, 0.0783, 0.0659, 0.0723, 0.0792, 0.0767,\n",
      "        0.0763, 0.0872, 0.0855, 0.0664, 0.0561, 0.1182, 0.0876, 0.1014, 0.0599,\n",
      "        0.0730, 0.0795, 0.1000, 0.0854, 0.0711, 0.0795, 0.0542, 0.0523, 0.0980,\n",
      "        0.0754, 0.0779, 0.0856, 0.0672, 0.0806, 0.0842, 0.0796, 0.0837, 0.1045,\n",
      "        0.0629, 0.0608, 0.0913, 0.0803, 0.0728, 0.0759, 0.0818, 0.0731, 0.0784,\n",
      "        0.0703, 0.0805, 0.0742, 0.0673, 0.0655, 0.0631, 0.0702, 0.0507, 0.0813,\n",
      "        0.0583, 0.0856, 0.0826, 0.0873, 0.1168, 0.0579, 0.0727, 0.0744, 0.0569,\n",
      "        0.0744, 0.0858, 0.0824, 0.0752, 0.1070, 0.0831, 0.0948, 0.0700, 0.0746,\n",
      "        0.0895, 0.0875, 0.0550, 0.0735, 0.0863, 0.0798, 0.0706, 0.0720, 0.0855,\n",
      "        0.0804, 0.1405, 0.0614, 0.0723, 0.1002, 0.0668, 0.1040, 0.0820, 0.0809,\n",
      "        0.0855, 0.0926, 0.0775, 0.0789, 0.0661, 0.0638, 0.0958, 0.1119, 0.0730,\n",
      "        0.0800, 0.0724, 0.0844, 0.0807, 0.1037, 0.0934, 0.0852, 0.0716, 0.0907,\n",
      "        0.0907, 0.0439, 0.0844, 0.0874, 0.0715, 0.0794, 0.0710, 0.0993, 0.0800,\n",
      "        0.0767, 0.0737, 0.0781, 0.0774, 0.0781, 0.0921, 0.0544, 0.0741, 0.0800,\n",
      "        0.0754, 0.0723, 0.0962, 0.0738, 0.0782, 0.0825, 0.0886, 0.1074, 0.0829,\n",
      "        0.0698, 0.0629, 0.1134, 0.0666, 0.1071, 0.0780, 0.0480, 0.0830, 0.0877,\n",
      "        0.0844, 0.0607, 0.0735, 0.0863, 0.0625, 0.0808, 0.0412, 0.0775, 0.0869,\n",
      "        0.0675, 0.0631, 0.0900, 0.1125, 0.0651, 0.0886, 0.0581, 0.0752, 0.0796,\n",
      "        0.0930, 0.0765, 0.0872, 0.1120, 0.0709, 0.0516, 0.0829, 0.0813, 0.0899,\n",
      "        0.0461, 0.0629, 0.0781, 0.0861, 0.0785, 0.0616, 0.0716, 0.0638, 0.0659,\n",
      "        0.0763, 0.0630, 0.0782, 0.0791, 0.0916, 0.0948, 0.0816, 0.0716, 0.0821,\n",
      "        0.0590, 0.0863, 0.0904, 0.0876, 0.0840, 0.0722, 0.0750, 0.0778, 0.0715,\n",
      "        0.0671, 0.0648, 0.0634, 0.0588, 0.0807, 0.0914, 0.0677, 0.0875, 0.0793,\n",
      "        0.0732, 0.0620, 0.0639, 0.1062, 0.0763, 0.0792, 0.0885, 0.0645, 0.0626,\n",
      "        0.0673, 0.0673, 0.0673, 0.0711, 0.0885, 0.0712, 0.0790, 0.0946, 0.0720,\n",
      "        0.0688, 0.0644, 0.0748, 0.0679, 0.1124, 0.0743, 0.0721, 0.0932, 0.0583,\n",
      "        0.1106, 0.0667, 0.0792, 0.0527, 0.0761, 0.0716, 0.0835, 0.0815, 0.0654,\n",
      "        0.0631, 0.0887, 0.0684, 0.1063, 0.0785, 0.0681, 0.0961, 0.0540, 0.1151,\n",
      "        0.0992, 0.0627, 0.0538, 0.0721, 0.0779, 0.0971, 0.0796, 0.0889, 0.0885,\n",
      "        0.1107, 0.0783, 0.0758, 0.0959, 0.0725, 0.1032, 0.0636, 0.0649, 0.1039,\n",
      "        0.0613, 0.0568, 0.0785, 0.0841, 0.0752, 0.0899, 0.0884, 0.0835, 0.0871,\n",
      "        0.0442, 0.0685, 0.0585, 0.0713, 0.0845, 0.0712, 0.0632, 0.0594, 0.0765,\n",
      "        0.0783, 0.0762, 0.0620, 0.0856, 0.1037, 0.0797, 0.0677, 0.0686, 0.0703,\n",
      "        0.0820, 0.0607, 0.0863, 0.0748, 0.0695, 0.0859, 0.0858, 0.1017, 0.0766,\n",
      "        0.0714, 0.0729, 0.0802, 0.0801, 0.0614, 0.0833, 0.0800, 0.0166, 0.0888,\n",
      "        0.0961, 0.0968, 0.0925, 0.1106, 0.0851, 0.0719, 0.0653, 0.0974, 0.0659,\n",
      "        0.0630, 0.0710, 0.0937, 0.0588, 0.0668, 0.0690, 0.0637, 0.0925, 0.1368,\n",
      "        0.0931, 0.0783, 0.0597, 0.0805, 0.0713, 0.0818, 0.0758, 0.1052, 0.0802,\n",
      "        0.0950, 0.0770, 0.0716, 0.0691, 0.0545, 0.0736, 0.0889, 0.0616, 0.0578,\n",
      "        0.1035, 0.0917, 0.0859, 0.1018, 0.0683, 0.0740, 0.0801, 0.0435, 0.0622,\n",
      "        0.0333, 0.0965, 0.0734, 0.0676, 0.0608, 0.0887, 0.0713, 0.0881, 0.0686,\n",
      "        0.0551, 0.0534, 0.0805, 0.0844, 0.0806, 0.0668, 0.0642, 0.0564, 0.0852,\n",
      "        0.0689, 0.0848, 0.0839, 0.0920, 0.0758, 0.0777, 0.0697, 0.0983, 0.0647,\n",
      "        0.0700, 0.0670, 0.0715, 0.0810, 0.0778, 0.0583, 0.1158, 0.1144, 0.0534,\n",
      "        0.0737, 0.0851, 0.0694, 0.0654, 0.1083, 0.0675, 0.0646, 0.0621, 0.0686,\n",
      "        0.0571, 0.0987, 0.0842, 0.0727, 0.0524, 0.0748, 0.0830, 0.0793, 0.0646,\n",
      "        0.0904, 0.0813, 0.0727, 0.0869, 0.0904, 0.0562, 0.0758, 0.1144, 0.0827,\n",
      "        0.0666, 0.0507, 0.0642, 0.0888, 0.0706, 0.0864, 0.0966, 0.0749, 0.0755,\n",
      "        0.1178, 0.0816, 0.1122, 0.0705, 0.0683, 0.0671, 0.0623, 0.1043, 0.0829,\n",
      "        0.0521, 0.0812, 0.0959, 0.0857, 0.0997, 0.0920, 0.1058, 0.0907, 0.1044,\n",
      "        0.0799, 0.0664, 0.0789, 0.0754, 0.0583, 0.0909, 0.0671, 0.0885],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1575,  0.3127,  0.4494,  ..., -0.6385, -0.2564,  0.9690],\n",
      "        [-0.0357, -0.0469, -0.0235,  ..., -0.9306, -0.5493, -0.0526],\n",
      "        [-0.5008,  0.6708, -0.7712,  ..., -0.5597, -0.2594, -0.3419],\n",
      "        ...,\n",
      "        [-0.8456, -0.9150, -0.7412,  ...,  0.3200, -0.0895,  0.0139],\n",
      "        [-0.0510, -0.0599,  0.0789,  ..., -0.9560,  0.0627, -0.3712],\n",
      "        [-0.5022,  0.0696, -0.0442,  ..., -0.1619,  0.1256, -0.5085]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2910, -0.5462,  0.1166,  ..., -0.1101, -0.3845, -0.2183],\n",
      "        [ 0.1700, -0.6832,  1.0829,  ..., -0.7116,  0.7346,  0.2800],\n",
      "        [-0.0804,  1.0460,  0.4298,  ...,  0.6803,  0.1556, -0.2181],\n",
      "        ...,\n",
      "        [ 0.0336, -0.0136,  0.3304,  ...,  0.3064,  0.4771,  0.0636],\n",
      "        [-1.0841, -0.4471, -0.4153,  ...,  0.7682, -1.1176,  0.1750],\n",
      "        [ 0.3524,  0.4877, -0.0106,  ...,  0.8920,  0.2008, -0.1936]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0266, -0.1575,  0.1867,  ...,  0.4175,  0.2407, -0.1126],\n",
      "        [-0.1113,  0.3464, -0.0275,  ..., -0.2061,  0.3377,  0.2621],\n",
      "        [ 0.3507, -0.3439, -0.3635,  ..., -0.1231, -0.0214, -0.0562],\n",
      "        ...,\n",
      "        [ 0.0541, -0.2970, -0.1678,  ..., -0.0878, -0.2468, -0.0079],\n",
      "        [ 0.2982, -0.0053,  0.5058,  ...,  0.0762,  0.3432,  0.3249],\n",
      "        [ 1.2118,  0.0595, -0.3252,  ..., -0.3141,  0.2570,  0.1877]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1103, 0.0979, 0.0536, 0.0989, 0.0758, 0.0743, 0.1126, 0.0734, 0.0603,\n",
      "        0.0882, 0.0935, 0.0459, 0.0741, 0.0744, 0.0571, 0.0979, 0.0976, 0.0607,\n",
      "        0.0993, 0.0614, 0.0866, 0.0648, 0.1053, 0.0876, 0.0670, 0.0495, 0.0642,\n",
      "        0.0792, 0.0699, 0.0735, 0.0751, 0.1016, 0.1416, 0.0627, 0.0580, 0.0741,\n",
      "        0.0795, 0.0587, 0.0785, 0.0504, 0.0773, 0.0741, 0.0886, 0.0542, 0.0629,\n",
      "        0.1161, 0.0983, 0.0666, 0.0597, 0.0579, 0.0608, 0.0841, 0.0683, 0.0621,\n",
      "        0.0862, 0.0782, 0.0691, 0.0970, 0.0767, 0.0661, 0.0847, 0.0818, 0.0792,\n",
      "        0.0750, 0.1300, 0.0615, 0.0491, 0.0676, 0.1098, 0.0696, 0.1074, 0.0635,\n",
      "        0.0968, 0.0670, 0.0896, 0.0728, 0.0549, 0.0655, 0.0379, 0.0540, 0.0803,\n",
      "        0.0796, 0.0910, 0.0788, 0.0573, 0.0801, 0.0849, 0.0781, 0.0629, 0.0972,\n",
      "        0.0592, 0.0585, 0.0847, 0.0769, 0.0912, 0.0596, 0.0737, 0.0813, 0.0565,\n",
      "        0.0712, 0.0795, 0.0757, 0.0561, 0.0652, 0.0811, 0.0709, 0.0694, 0.0713,\n",
      "        0.0534, 0.0776, 0.0751, 0.0836, 0.1138, 0.0556, 0.0533, 0.0605, 0.0503,\n",
      "        0.0740, 0.0597, 0.0595, 0.0762, 0.0995, 0.0897, 0.0846, 0.0730, 0.0701,\n",
      "        0.0712, 0.0665, 0.0542, 0.0612, 0.0802, 0.0821, 0.0800, 0.0758, 0.0568,\n",
      "        0.0770, 0.1942, 0.0643, 0.0736, 0.1066, 0.0570, 0.1086, 0.0705, 0.0690,\n",
      "        0.0826, 0.0904, 0.0686, 0.0678, 0.0714, 0.0514, 0.0907, 0.0950, 0.0735,\n",
      "        0.0892, 0.0577, 0.0773, 0.0789, 0.0948, 0.0854, 0.0888, 0.0741, 0.0883,\n",
      "        0.0965, 0.0390, 0.0686, 0.0979, 0.0535, 0.0924, 0.0763, 0.1057, 0.0785,\n",
      "        0.0703, 0.0750, 0.0699, 0.0907, 0.0716, 0.0730, 0.0423, 0.0694, 0.0944,\n",
      "        0.0746, 0.0476, 0.1103, 0.0647, 0.0662, 0.0858, 0.1110, 0.0854, 0.0765,\n",
      "        0.0792, 0.0642, 0.0987, 0.0744, 0.0996, 0.0779, 0.0526, 0.0841, 0.0781,\n",
      "        0.0818, 0.0795, 0.0660, 0.0856, 0.0709, 0.0704, 0.0797, 0.0638, 0.0728,\n",
      "        0.0564, 0.0804, 0.0855, 0.1110, 0.0833, 0.0909, 0.0635, 0.0689, 0.0737,\n",
      "        0.0774, 0.0693, 0.0727, 0.0990, 0.0658, 0.0537, 0.0762, 0.0812, 0.0791,\n",
      "        0.0666, 0.0709, 0.0792, 0.0687, 0.0855, 0.0470, 0.0612, 0.0580, 0.0742,\n",
      "        0.0857, 0.0756, 0.0779, 0.1004, 0.0969, 0.0666, 0.0644, 0.0728, 0.0720,\n",
      "        0.0537, 0.0769, 0.0671, 0.0842, 0.0758, 0.0838, 0.0779, 0.0931, 0.0711,\n",
      "        0.0655, 0.0619, 0.0600, 0.0555, 0.0624, 0.0761, 0.0863, 0.0780, 0.0892,\n",
      "        0.0552, 0.0662, 0.0933, 0.0962, 0.0749, 0.0772, 0.0806, 0.0786, 0.0643,\n",
      "        0.0666, 0.0700, 0.0663, 0.0702, 0.0845, 0.0564, 0.0861, 0.0887, 0.0723,\n",
      "        0.0607, 0.0696, 0.0661, 0.0685, 0.1109, 0.0644, 0.0622, 0.0945, 0.0533,\n",
      "        0.0892, 0.0644, 0.0635, 0.0496, 0.0815, 0.0632, 0.0826, 0.0609, 0.0727,\n",
      "        0.0409, 0.1043, 0.0580, 0.1056, 0.0650, 0.0690, 0.0873, 0.0651, 0.1008,\n",
      "        0.1056, 0.0591, 0.0537, 0.0619, 0.0557, 0.0763, 0.1028, 0.0781, 0.0773,\n",
      "        0.0843, 0.0713, 0.0705, 0.1022, 0.0788, 0.1014, 0.0658, 0.0585, 0.1111,\n",
      "        0.0756, 0.0715, 0.0827, 0.0639, 0.0805, 0.0849, 0.0808, 0.0881, 0.0928,\n",
      "        0.0440, 0.0899, 0.0636, 0.0697, 0.0697, 0.0786, 0.1077, 0.0734, 0.0691,\n",
      "        0.0672, 0.0692, 0.0606, 0.0800, 0.0853, 0.0684, 0.0563, 0.0635, 0.0763,\n",
      "        0.0779, 0.0863, 0.0749, 0.0659, 0.0684, 0.0641, 0.0663, 0.0953, 0.0522,\n",
      "        0.0721, 0.0841, 0.0887, 0.0610, 0.0630, 0.0888, 0.0696, 0.0349, 0.0854,\n",
      "        0.0736, 0.1073, 0.0694, 0.1266, 0.0872, 0.0674, 0.0735, 0.0831, 0.0661,\n",
      "        0.0686, 0.0866, 0.0818, 0.0730, 0.0707, 0.0669, 0.0595, 0.0784, 0.1092,\n",
      "        0.1017, 0.0589, 0.0473, 0.0632, 0.0678, 0.0834, 0.0902, 0.1001, 0.0754,\n",
      "        0.1017, 0.0574, 0.0743, 0.0767, 0.0558, 0.0651, 0.0742, 0.0686, 0.0722,\n",
      "        0.0801, 0.0653, 0.0776, 0.0970, 0.0792, 0.0790, 0.0644, 0.0610, 0.0587,\n",
      "        0.0293, 0.0761, 0.0736, 0.0824, 0.0525, 0.0944, 0.0589, 0.0782, 0.0707,\n",
      "        0.0405, 0.0316, 0.0801, 0.0756, 0.0817, 0.0718, 0.0700, 0.0561, 0.0880,\n",
      "        0.0621, 0.0707, 0.0834, 0.0833, 0.0722, 0.0815, 0.0795, 0.0687, 0.0610,\n",
      "        0.0508, 0.0549, 0.0785, 0.0738, 0.0833, 0.0536, 0.1012, 0.0779, 0.0571,\n",
      "        0.0746, 0.0783, 0.0613, 0.0640, 0.0983, 0.0605, 0.0765, 0.0655, 0.0730,\n",
      "        0.0620, 0.0982, 0.0899, 0.0692, 0.0456, 0.0652, 0.0879, 0.0650, 0.0454,\n",
      "        0.0916, 0.0952, 0.0713, 0.0880, 0.0781, 0.0651, 0.0669, 0.0996, 0.0884,\n",
      "        0.0709, 0.0434, 0.0707, 0.0744, 0.0813, 0.0713, 0.0918, 0.0846, 0.0699,\n",
      "        0.1117, 0.0819, 0.0969, 0.0875, 0.0722, 0.0662, 0.0555, 0.0738, 0.0834,\n",
      "        0.0622, 0.0731, 0.0824, 0.0782, 0.0984, 0.0803, 0.1045, 0.0630, 0.0884,\n",
      "        0.0786, 0.0559, 0.0692, 0.0836, 0.0514, 0.0756, 0.0717, 0.1407],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0167,  0.0012, -0.0657,  ...,  0.0740, -0.0661, -0.1470],\n",
      "        [ 0.0524, -0.0237, -0.0373,  ...,  0.0496,  0.0036, -0.0864],\n",
      "        [-0.0532,  0.0971, -0.0609,  ...,  0.0988, -0.0460, -0.0353],\n",
      "        ...,\n",
      "        [-0.0247, -0.1337, -0.0629,  ...,  0.0087, -0.0397,  0.0392],\n",
      "        [ 0.0250,  0.0829, -0.0310,  ..., -0.0335,  0.0717,  0.0436],\n",
      "        [ 0.0682,  0.0132,  0.0848,  ...,  0.0456,  0.0325,  0.0136]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.8203, -0.3400,  0.0096,  ..., -0.4911,  0.4370, -0.0450],\n",
      "        [ 1.0550, -0.3630, -0.0772,  ..., -0.9041,  0.0644, -0.6144],\n",
      "        [ 0.2266, -0.3668,  0.4157,  ...,  0.6134, -0.4190,  0.0974],\n",
      "        ...,\n",
      "        [ 0.2766, -0.0858, -0.4946,  ...,  0.3240,  0.0078,  0.0699],\n",
      "        [ 0.2302,  0.0230, -0.3358,  ...,  0.1292, -0.1649,  0.5112],\n",
      "        [-0.2720,  0.2876,  0.4519,  ...,  0.6133,  0.5095, -0.1339]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.7110, -0.2612,  0.1762,  ..., -0.0683,  0.0987, -0.0164],\n",
      "        [-0.3511, -0.1441,  0.8803,  ...,  0.8346, -0.3574,  0.0313],\n",
      "        [-0.5371, -0.0625,  0.2090,  ...,  0.5329,  0.6662,  0.1210],\n",
      "        ...,\n",
      "        [ 0.2425,  0.0872,  0.2023,  ...,  0.1647,  0.2702, -0.1706],\n",
      "        [-0.1195, -0.5357,  0.4805,  ...,  0.2241, -0.1043, -0.1023],\n",
      "        [-0.0777, -0.4340, -0.4001,  ..., -0.2822, -0.2276,  0.0509]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3712, -0.0415,  0.3334,  ..., -0.3502,  0.7947,  0.1245],\n",
      "        [ 0.6789,  0.2445, -0.1947,  ..., -0.2674, -0.8623,  0.4596],\n",
      "        [ 0.3898, -0.2762,  0.0365,  ...,  0.0661,  0.1549, -0.0736],\n",
      "        ...,\n",
      "        [-0.3560,  0.2932,  0.4369,  ...,  0.9466, -0.8012, -0.1033],\n",
      "        [ 0.9831,  1.0828,  0.6137,  ..., -0.1544,  0.3290,  0.1635],\n",
      "        [ 0.4414, -0.0826, -0.1270,  ..., -0.1260, -0.2215,  0.3419]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1041, 0.1053, 0.0920, 0.1293, 0.1030, 0.1098, 0.0583, 0.1144, 0.0800,\n",
      "        0.1380, 0.1138, 0.0398, 0.1143, 0.1174, 0.0921, 0.1269, 0.1094, 0.0976,\n",
      "        0.1383, 0.0870, 0.0986, 0.0731, 0.1160, 0.1063, 0.1092, 0.0769, 0.0825,\n",
      "        0.1117, 0.1225, 0.1041, 0.1239, 0.1479, 0.1003, 0.1212, 0.0939, 0.1054,\n",
      "        0.1393, 0.0959, 0.1080, 0.0908, 0.0963, 0.1003, 0.1428, 0.0807, 0.1065,\n",
      "        0.1671, 0.1101, 0.0901, 0.1050, 0.0972, 0.1007, 0.1311, 0.1215, 0.1153,\n",
      "        0.1340, 0.1137, 0.1046, 0.1397, 0.1029, 0.1066, 0.1142, 0.1475, 0.1149,\n",
      "        0.1188, 0.1510, 0.1237, 0.0841, 0.1144, 0.1428, 0.1137, 0.1313, 0.0995,\n",
      "        0.1280, 0.1047, 0.1147, 0.1112, 0.0979, 0.1219, 0.0340, 0.1035, 0.1203,\n",
      "        0.1051, 0.1310, 0.1290, 0.1078, 0.1337, 0.1252, 0.1206, 0.1031, 0.1585,\n",
      "        0.0884, 0.0961, 0.1172, 0.1280, 0.1205, 0.1052, 0.1119, 0.1163, 0.1051,\n",
      "        0.0867, 0.1008, 0.1066, 0.1045, 0.1054, 0.0816, 0.1150, 0.0868, 0.1128,\n",
      "        0.0711, 0.1217, 0.1040, 0.1073, 0.1325, 0.0832, 0.1050, 0.0963, 0.0754,\n",
      "        0.1020, 0.1060, 0.1132, 0.1151, 0.1414, 0.1205, 0.1475, 0.0893, 0.1188,\n",
      "        0.1207, 0.1081, 0.0855, 0.0806, 0.1128, 0.1176, 0.0978, 0.0840, 0.1073,\n",
      "        0.1069, 0.0873, 0.0891, 0.0900, 0.1264, 0.0890, 0.1536, 0.1137, 0.1269,\n",
      "        0.1132, 0.1343, 0.1016, 0.1137, 0.1048, 0.0937, 0.1316, 0.1337, 0.1082,\n",
      "        0.1274, 0.1000, 0.1152, 0.1065, 0.1645, 0.1205, 0.1087, 0.1036, 0.1121,\n",
      "        0.1296, 0.0282, 0.0948, 0.1222, 0.1015, 0.1303, 0.1005, 0.1450, 0.1157,\n",
      "        0.1156, 0.1208, 0.1187, 0.1140, 0.1035, 0.1119, 0.0856, 0.1196, 0.1111,\n",
      "        0.0931, 0.0918, 0.1406, 0.0988, 0.1101, 0.1149, 0.1473, 0.1442, 0.1073,\n",
      "        0.0990, 0.1126, 0.1465, 0.1068, 0.1449, 0.1372, 0.0842, 0.1155, 0.1135,\n",
      "        0.1133, 0.1124, 0.0858, 0.1078, 0.1112, 0.1122, 0.0652, 0.1012, 0.1010,\n",
      "        0.0899, 0.1268, 0.1319, 0.1516, 0.0967, 0.1126, 0.1119, 0.1139, 0.1109,\n",
      "        0.1178, 0.1131, 0.1082, 0.1524, 0.0958, 0.0942, 0.1224, 0.1137, 0.1089,\n",
      "        0.0845, 0.1280, 0.1104, 0.1129, 0.1251, 0.0855, 0.0926, 0.0958, 0.1091,\n",
      "        0.1413, 0.1089, 0.1369, 0.1337, 0.1185, 0.1148, 0.1107, 0.1127, 0.1233,\n",
      "        0.0988, 0.1149, 0.1061, 0.1180, 0.1202, 0.0814, 0.1285, 0.0725, 0.1265,\n",
      "        0.1206, 0.0931, 0.1026, 0.0940, 0.0976, 0.1075, 0.1074, 0.1207, 0.1223,\n",
      "        0.1033, 0.1033, 0.1166, 0.1245, 0.1415, 0.1049, 0.1337, 0.0812, 0.0962,\n",
      "        0.0947, 0.1068, 0.1181, 0.1100, 0.1242, 0.1082, 0.0924, 0.1201, 0.1101,\n",
      "        0.1186, 0.1077, 0.1186, 0.1089, 0.1366, 0.1176, 0.0970, 0.1151, 0.0909,\n",
      "        0.1402, 0.0983, 0.1107, 0.0994, 0.1317, 0.0863, 0.0928, 0.0637, 0.1016,\n",
      "        0.0813, 0.1284, 0.1062, 0.1532, 0.1239, 0.1264, 0.1409, 0.0830, 0.1479,\n",
      "        0.1178, 0.1030, 0.0698, 0.1021, 0.1118, 0.1118, 0.1128, 0.1082, 0.1397,\n",
      "        0.1233, 0.1233, 0.1285, 0.1244, 0.1124, 0.1476, 0.1001, 0.1111, 0.1463,\n",
      "        0.1041, 0.0877, 0.1336, 0.1065, 0.1019, 0.1199, 0.1246, 0.1086, 0.1233,\n",
      "        0.0368, 0.1099, 0.0880, 0.1102, 0.1120, 0.0807, 0.1013, 0.1042, 0.1092,\n",
      "        0.1033, 0.1004, 0.1066, 0.1087, 0.1047, 0.0989, 0.1106, 0.0917, 0.1134,\n",
      "        0.1178, 0.0914, 0.1307, 0.1099, 0.1062, 0.1446, 0.1138, 0.1358, 0.0869,\n",
      "        0.1178, 0.1231, 0.1275, 0.0960, 0.0856, 0.1341, 0.1093, 0.0239, 0.1422,\n",
      "        0.1171, 0.1392, 0.0943, 0.1322, 0.1393, 0.1018, 0.1099, 0.1254, 0.0958,\n",
      "        0.1093, 0.1262, 0.1396, 0.1154, 0.1127, 0.1071, 0.1025, 0.1167, 0.1696,\n",
      "        0.1336, 0.1120, 0.0954, 0.1077, 0.1114, 0.1104, 0.1130, 0.1557, 0.1074,\n",
      "        0.1132, 0.1076, 0.0925, 0.1121, 0.0847, 0.0939, 0.1241, 0.1196, 0.1245,\n",
      "        0.0995, 0.1535, 0.1162, 0.1299, 0.1304, 0.1062, 0.1008, 0.0440, 0.0904,\n",
      "        0.0303, 0.1165, 0.1053, 0.1141, 0.1110, 0.1185, 0.1145, 0.1472, 0.1119,\n",
      "        0.0866, 0.0741, 0.1125, 0.1225, 0.1084, 0.0845, 0.1034, 0.0766, 0.1396,\n",
      "        0.1106, 0.1208, 0.1116, 0.1094, 0.1069, 0.1108, 0.1223, 0.0961, 0.1164,\n",
      "        0.0913, 0.0983, 0.1074, 0.1302, 0.1344, 0.0741, 0.1607, 0.1128, 0.0968,\n",
      "        0.0867, 0.1229, 0.0989, 0.1076, 0.1415, 0.0998, 0.0897, 0.1055, 0.0744,\n",
      "        0.0937, 0.1381, 0.1049, 0.1105, 0.0714, 0.0856, 0.1134, 0.0920, 0.0910,\n",
      "        0.1325, 0.1360, 0.1212, 0.1069, 0.0956, 0.0944, 0.1160, 0.1346, 0.1147,\n",
      "        0.1130, 0.0665, 0.1018, 0.1146, 0.1080, 0.1224, 0.1066, 0.0947, 0.1222,\n",
      "        0.1405, 0.1025, 0.1347, 0.1213, 0.1132, 0.0973, 0.1060, 0.1406, 0.1195,\n",
      "        0.1042, 0.1266, 0.1270, 0.1233, 0.1256, 0.1311, 0.1248, 0.1104, 0.1532,\n",
      "        0.1278, 0.1029, 0.1294, 0.1096, 0.0850, 0.1193, 0.1239, 0.1061],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1679,  0.1236,  0.6602,  ...,  0.1601, -0.0686, -1.1255],\n",
      "        [ 0.1144,  0.1374,  0.2988,  ..., -0.2428,  1.1987, -0.5799],\n",
      "        [-0.6243, -0.5400, -0.4300,  ...,  0.0720, -0.5474,  0.1472],\n",
      "        ...,\n",
      "        [-0.4324, -0.2151, -0.7727,  ..., -0.5561, -0.5693,  0.2010],\n",
      "        [ 0.6047,  0.5840,  0.8911,  ...,  0.8785, -0.3033,  0.8346],\n",
      "        [ 0.0463, -0.1216, -0.0790,  ..., -0.0325, -0.5352, -0.0682]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2814,  0.8564,  0.2633,  ...,  0.1065, -0.0299,  0.9091],\n",
      "        [-0.2765, -0.1565, -0.7038,  ..., -0.2755, -0.9045, -0.3625],\n",
      "        [-0.0556, -0.6205,  0.0896,  ..., -0.0037,  1.2599,  0.9265],\n",
      "        ...,\n",
      "        [-0.7481, -0.5238, -0.2567,  ...,  0.8529, -0.4405,  0.3844],\n",
      "        [ 0.0123, -0.1135,  0.6593,  ..., -0.6455,  1.0780,  0.6768],\n",
      "        [-0.1458, -0.3857, -0.0164,  ..., -0.1421,  0.4786, -0.1273]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0592, -0.5524,  0.3757,  ...,  0.4714, -0.6674,  0.3226],\n",
      "        [-0.2673, -0.1386,  0.3385,  ...,  0.0268, -0.1119, -0.0169],\n",
      "        [ 0.3552, -0.0752, -0.1591,  ...,  0.2497,  0.1136, -0.0557],\n",
      "        ...,\n",
      "        [ 0.2756, -0.4044,  0.3361,  ..., -0.0065,  0.1641, -0.4741],\n",
      "        [ 0.0172, -0.3748,  0.4814,  ..., -0.1649, -0.5009, -0.4298],\n",
      "        [ 0.1515,  0.1064, -0.0648,  ..., -0.1012, -0.1839, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1252, 0.1244, 0.0722, 0.1164, 0.1099, 0.1070, 0.1235, 0.1095, 0.0749,\n",
      "        0.1368, 0.0897, 0.0456, 0.0960, 0.0821, 0.0821, 0.1167, 0.0918, 0.0855,\n",
      "        0.1153, 0.0882, 0.0917, 0.0699, 0.1091, 0.1051, 0.0783, 0.0706, 0.0636,\n",
      "        0.0832, 0.1068, 0.0959, 0.1077, 0.1322, 0.1946, 0.1018, 0.0788, 0.1027,\n",
      "        0.1168, 0.0895, 0.0986, 0.0751, 0.1037, 0.1052, 0.1092, 0.0781, 0.0927,\n",
      "        0.1357, 0.1091, 0.1113, 0.0711, 0.0958, 0.0894, 0.1034, 0.0869, 0.1079,\n",
      "        0.1215, 0.1155, 0.0804, 0.1367, 0.0918, 0.0852, 0.1031, 0.0819, 0.1119,\n",
      "        0.1018, 0.1436, 0.0938, 0.0706, 0.0827, 0.1213, 0.0986, 0.1188, 0.0800,\n",
      "        0.0988, 0.1043, 0.1085, 0.0911, 0.0886, 0.1036, 0.0358, 0.0871, 0.1183,\n",
      "        0.0873, 0.0935, 0.0772, 0.0846, 0.1132, 0.1104, 0.0885, 0.1009, 0.1207,\n",
      "        0.0870, 0.0807, 0.1004, 0.0991, 0.0986, 0.0796, 0.1050, 0.1049, 0.0846,\n",
      "        0.0950, 0.0951, 0.0979, 0.0961, 0.0963, 0.0771, 0.0908, 0.0914, 0.0953,\n",
      "        0.0660, 0.0987, 0.0858, 0.0943, 0.1465, 0.0694, 0.0792, 0.0928, 0.0484,\n",
      "        0.0872, 0.0876, 0.0982, 0.1018, 0.1131, 0.1201, 0.0991, 0.1060, 0.1179,\n",
      "        0.0990, 0.0941, 0.0741, 0.0657, 0.1049, 0.0843, 0.0939, 0.0752, 0.0813,\n",
      "        0.0951, 0.3473, 0.0855, 0.0900, 0.1225, 0.0771, 0.1471, 0.0842, 0.1150,\n",
      "        0.0796, 0.1201, 0.0918, 0.1015, 0.0895, 0.0846, 0.0971, 0.1071, 0.0789,\n",
      "        0.1227, 0.0915, 0.0928, 0.1149, 0.1235, 0.0951, 0.0941, 0.0811, 0.1201,\n",
      "        0.1036, 0.0351, 0.0955, 0.0969, 0.0779, 0.1251, 0.0912, 0.1004, 0.0993,\n",
      "        0.0977, 0.0899, 0.0959, 0.1126, 0.0871, 0.0995, 0.0763, 0.0978, 0.1083,\n",
      "        0.0991, 0.0740, 0.1200, 0.0714, 0.0953, 0.1054, 0.1324, 0.1089, 0.1023,\n",
      "        0.0771, 0.0928, 0.1343, 0.0852, 0.1252, 0.1145, 0.0745, 0.1001, 0.0895,\n",
      "        0.0982, 0.1005, 0.0827, 0.1035, 0.1014, 0.0885, 0.0986, 0.0974, 0.0975,\n",
      "        0.0846, 0.0954, 0.1194, 0.1293, 0.0855, 0.0992, 0.0774, 0.1044, 0.1018,\n",
      "        0.0973, 0.0971, 0.0933, 0.1153, 0.0671, 0.0740, 0.1029, 0.1041, 0.0840,\n",
      "        0.0627, 0.0871, 0.0935, 0.0827, 0.1139, 0.0663, 0.0971, 0.0792, 0.0950,\n",
      "        0.1167, 0.0835, 0.0995, 0.1331, 0.1054, 0.0937, 0.0995, 0.0964, 0.0757,\n",
      "        0.0793, 0.0857, 0.0888, 0.0911, 0.0925, 0.0815, 0.1054, 0.0817, 0.1012,\n",
      "        0.0846, 0.0959, 0.0826, 0.0895, 0.0884, 0.1021, 0.0941, 0.1119, 0.0936,\n",
      "        0.0696, 0.0862, 0.1236, 0.1022, 0.1182, 0.1040, 0.1244, 0.0769, 0.0642,\n",
      "        0.0976, 0.0887, 0.0906, 0.0944, 0.1247, 0.0890, 0.0811, 0.0981, 0.0985,\n",
      "        0.1014, 0.0799, 0.0921, 0.0869, 0.1466, 0.1131, 0.0808, 0.1034, 0.0816,\n",
      "        0.1286, 0.0911, 0.0933, 0.0751, 0.1018, 0.0762, 0.0976, 0.0503, 0.0909,\n",
      "        0.0640, 0.1068, 0.0938, 0.1337, 0.0918, 0.0918, 0.1240, 0.0937, 0.1143,\n",
      "        0.1174, 0.0753, 0.0726, 0.0890, 0.1034, 0.0981, 0.1081, 0.1040, 0.1080,\n",
      "        0.1289, 0.0933, 0.0941, 0.1204, 0.0893, 0.1376, 0.0833, 0.0849, 0.1394,\n",
      "        0.1005, 0.0860, 0.1113, 0.0787, 0.0936, 0.1106, 0.1087, 0.1060, 0.1185,\n",
      "        0.0472, 0.0994, 0.0776, 0.0927, 0.0978, 0.0745, 0.1192, 0.0924, 0.0916,\n",
      "        0.0841, 0.0853, 0.0805, 0.1067, 0.1088, 0.0927, 0.0887, 0.0834, 0.0862,\n",
      "        0.0963, 0.1022, 0.0953, 0.0959, 0.0804, 0.1139, 0.0798, 0.1164, 0.0718,\n",
      "        0.0886, 0.0891, 0.1103, 0.0824, 0.0650, 0.1065, 0.0810, 0.0326, 0.1182,\n",
      "        0.0841, 0.1355, 0.0978, 0.1331, 0.1155, 0.0931, 0.1035, 0.0967, 0.0985,\n",
      "        0.0912, 0.0968, 0.1094, 0.1062, 0.0793, 0.0866, 0.0809, 0.1080, 0.1490,\n",
      "        0.1155, 0.0820, 0.0823, 0.1167, 0.0657, 0.1000, 0.1027, 0.1284, 0.1026,\n",
      "        0.1024, 0.0985, 0.1028, 0.1097, 0.0698, 0.0718, 0.1183, 0.0878, 0.0859,\n",
      "        0.1033, 0.0958, 0.0848, 0.0909, 0.1045, 0.0962, 0.0858, 0.0287, 0.0912,\n",
      "        0.0289, 0.1045, 0.0883, 0.0988, 0.0787, 0.1144, 0.0905, 0.1213, 0.1103,\n",
      "        0.0706, 0.0566, 0.0928, 0.1071, 0.0949, 0.0708, 0.0891, 0.0696, 0.1138,\n",
      "        0.0854, 0.1044, 0.1057, 0.0920, 0.0962, 0.0960, 0.0999, 0.0778, 0.0733,\n",
      "        0.0864, 0.0807, 0.1034, 0.0952, 0.1117, 0.0806, 0.1109, 0.0961, 0.0747,\n",
      "        0.0692, 0.1089, 0.0963, 0.0868, 0.1310, 0.0788, 0.0891, 0.0899, 0.0695,\n",
      "        0.0841, 0.1264, 0.1062, 0.0872, 0.0567, 0.0702, 0.1069, 0.0856, 0.0511,\n",
      "        0.0926, 0.1244, 0.1021, 0.0887, 0.0890, 0.0814, 0.0865, 0.1126, 0.0995,\n",
      "        0.0843, 0.0640, 0.0951, 0.1062, 0.0861, 0.1099, 0.1036, 0.0899, 0.1109,\n",
      "        0.1362, 0.1128, 0.1214, 0.0898, 0.0918, 0.0861, 0.0838, 0.1056, 0.1034,\n",
      "        0.0797, 0.0981, 0.1135, 0.1061, 0.1057, 0.1106, 0.1480, 0.1001, 0.1187,\n",
      "        0.1034, 0.0827, 0.1221, 0.0964, 0.0643, 0.1152, 0.1061, 0.1486],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0717,  0.0236,  0.0213,  ...,  0.0300, -0.0353, -0.0493],\n",
      "        [ 0.0718,  0.0346,  0.0672,  ...,  0.0648, -0.0041, -0.0095],\n",
      "        [-0.0799,  0.0173,  0.0819,  ...,  0.0051,  0.0453,  0.0502],\n",
      "        ...,\n",
      "        [ 0.0651, -0.0089, -0.0021,  ...,  0.0303, -0.0699, -0.0137],\n",
      "        [ 0.0218,  0.1072, -0.0221,  ..., -0.0060, -0.1032, -0.0416],\n",
      "        [ 0.0929, -0.0337,  0.0347,  ...,  0.0050, -0.0001,  0.0948]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2650, -0.3795,  0.6392,  ...,  0.5397,  0.5342, -0.3662],\n",
      "        [-0.2062,  0.1378, -0.3571,  ...,  0.1527, -0.8924, -0.6237],\n",
      "        [-0.2247,  0.0595, -0.2936,  ...,  0.0410, -0.5299,  0.2250],\n",
      "        ...,\n",
      "        [ 0.2123, -0.4846,  0.0263,  ..., -0.1312, -0.4152, -0.2778],\n",
      "        [ 0.2546,  0.6137, -0.7832,  ...,  0.1608, -0.4944, -0.0937],\n",
      "        [ 0.5151, -0.5504,  0.4542,  ...,  0.0426, -0.0826,  0.1906]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0392,  0.7291,  0.8788,  ...,  0.6471,  0.8652, -0.2177],\n",
      "        [-0.1561, -0.7600, -0.2398,  ..., -0.1443,  0.1300, -0.0026],\n",
      "        [-0.3698,  0.4629,  1.0878,  ...,  0.8678,  0.2996,  0.0105],\n",
      "        ...,\n",
      "        [-0.1647,  1.0309,  0.5118,  ..., -0.4838, -0.3990,  0.2742],\n",
      "        [-0.4068,  0.3883,  0.3537,  ...,  1.0888,  0.4117,  0.2579],\n",
      "        [-0.2128, -0.4125, -0.1726,  ...,  0.2083,  0.2612, -0.1582]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0369, -0.2200,  0.2377,  ..., -0.3612, -0.4682,  0.8567],\n",
      "        [ 0.0309,  0.3046,  0.0165,  ..., -0.0839,  0.0994,  0.2648],\n",
      "        [ 0.3265,  0.3884, -0.7796,  ..., -0.0878,  0.0774, -0.5921],\n",
      "        ...,\n",
      "        [-0.0954, -0.4950,  0.1745,  ...,  0.2912, -0.6369, -1.3845],\n",
      "        [ 0.5874, -0.3753, -1.1684,  ..., -0.0032, -0.0315, -0.3866],\n",
      "        [ 0.7394,  0.4557, -0.5346,  ..., -0.0207,  0.5655,  0.0216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1373, 0.1299, 0.0807, 0.1269, 0.1018, 0.1033, 0.0377, 0.1040, 0.1014,\n",
      "        0.1614, 0.1381, 0.0591, 0.0932, 0.1296, 0.1009, 0.1414, 0.1047, 0.1034,\n",
      "        0.1376, 0.0766, 0.1061, 0.0930, 0.1225, 0.1367, 0.0987, 0.0893, 0.0706,\n",
      "        0.1218, 0.1256, 0.1174, 0.1159, 0.1257, 0.0844, 0.1058, 0.0989, 0.1009,\n",
      "        0.1265, 0.1062, 0.1071, 0.0972, 0.1092, 0.1213, 0.1377, 0.0995, 0.1250,\n",
      "        0.1649, 0.1095, 0.1205, 0.0953, 0.1132, 0.1009, 0.1300, 0.1048, 0.1242,\n",
      "        0.1267, 0.1102, 0.1087, 0.1188, 0.1163, 0.1025, 0.1205, 0.1060, 0.1318,\n",
      "        0.1118, 0.1377, 0.1412, 0.1020, 0.1171, 0.1459, 0.0970, 0.1381, 0.0956,\n",
      "        0.1202, 0.1099, 0.1006, 0.1024, 0.1061, 0.1345, 0.0407, 0.1072, 0.1354,\n",
      "        0.1237, 0.1232, 0.1101, 0.1137, 0.1110, 0.1107, 0.1243, 0.1263, 0.1381,\n",
      "        0.0754, 0.0973, 0.1343, 0.1196, 0.1157, 0.1042, 0.1063, 0.1203, 0.0958,\n",
      "        0.0874, 0.0984, 0.0896, 0.1079, 0.1076, 0.0982, 0.0977, 0.1220, 0.1304,\n",
      "        0.0798, 0.1043, 0.1114, 0.1112, 0.1690, 0.0826, 0.1004, 0.1099, 0.0770,\n",
      "        0.0967, 0.1218, 0.1387, 0.0990, 0.1351, 0.1319, 0.1468, 0.1134, 0.1101,\n",
      "        0.1170, 0.0978, 0.0809, 0.0843, 0.1246, 0.1145, 0.1016, 0.0758, 0.0951,\n",
      "        0.1092, 0.0607, 0.0972, 0.0941, 0.1429, 0.1049, 0.1513, 0.0979, 0.1199,\n",
      "        0.1142, 0.1290, 0.1159, 0.1063, 0.1153, 0.0882, 0.1433, 0.1369, 0.1167,\n",
      "        0.1465, 0.1144, 0.1148, 0.1183, 0.1425, 0.1253, 0.1069, 0.1249, 0.1178,\n",
      "        0.1248, 0.0397, 0.1119, 0.1096, 0.0970, 0.1633, 0.1092, 0.1364, 0.1278,\n",
      "        0.1111, 0.1132, 0.1178, 0.1092, 0.1066, 0.1137, 0.0731, 0.1041, 0.1178,\n",
      "        0.1229, 0.0921, 0.1424, 0.1040, 0.1042, 0.1263, 0.1418, 0.1205, 0.1214,\n",
      "        0.0964, 0.1071, 0.1190, 0.1040, 0.1333, 0.1266, 0.0825, 0.1271, 0.1191,\n",
      "        0.1217, 0.1050, 0.0932, 0.1137, 0.1179, 0.1073, 0.0939, 0.1024, 0.0959,\n",
      "        0.0926, 0.1152, 0.1178, 0.1565, 0.0996, 0.1196, 0.0890, 0.1227, 0.1161,\n",
      "        0.1170, 0.1355, 0.1003, 0.1308, 0.1092, 0.1056, 0.1144, 0.1307, 0.1090,\n",
      "        0.0809, 0.1308, 0.0994, 0.1209, 0.1442, 0.0961, 0.1039, 0.0979, 0.1019,\n",
      "        0.1462, 0.1291, 0.1258, 0.1293, 0.1314, 0.1186, 0.1210, 0.1113, 0.1352,\n",
      "        0.0921, 0.1130, 0.1348, 0.1262, 0.1167, 0.0934, 0.1162, 0.0903, 0.1262,\n",
      "        0.1164, 0.1182, 0.0942, 0.0930, 0.1115, 0.1088, 0.1365, 0.1275, 0.1205,\n",
      "        0.1023, 0.1086, 0.1123, 0.1398, 0.1334, 0.1177, 0.1180, 0.0857, 0.0877,\n",
      "        0.0970, 0.1106, 0.1150, 0.1150, 0.1191, 0.0882, 0.0978, 0.1287, 0.1012,\n",
      "        0.1246, 0.1139, 0.1069, 0.0783, 0.1020, 0.1279, 0.1145, 0.1328, 0.1017,\n",
      "        0.1524, 0.0992, 0.1240, 0.0868, 0.1219, 0.1048, 0.1212, 0.0848, 0.0996,\n",
      "        0.0852, 0.1224, 0.0850, 0.1425, 0.1135, 0.1336, 0.1438, 0.0936, 0.1453,\n",
      "        0.1167, 0.1001, 0.0793, 0.0953, 0.1043, 0.1156, 0.0906, 0.1225, 0.1094,\n",
      "        0.1353, 0.1245, 0.1155, 0.1052, 0.0965, 0.1571, 0.0940, 0.1007, 0.1603,\n",
      "        0.1143, 0.1094, 0.1355, 0.1080, 0.0960, 0.1083, 0.1109, 0.1151, 0.1145,\n",
      "        0.0558, 0.1158, 0.1160, 0.1262, 0.1327, 0.1114, 0.1200, 0.1225, 0.0956,\n",
      "        0.1199, 0.1090, 0.1123, 0.0930, 0.1198, 0.0969, 0.1089, 0.1036, 0.1069,\n",
      "        0.1063, 0.1023, 0.1155, 0.1163, 0.1133, 0.1182, 0.1074, 0.1184, 0.0963,\n",
      "        0.0991, 0.1048, 0.1357, 0.0982, 0.0709, 0.1122, 0.1039, 0.0363, 0.1194,\n",
      "        0.1440, 0.1415, 0.1097, 0.1536, 0.1261, 0.1119, 0.1247, 0.1184, 0.1024,\n",
      "        0.0988, 0.1110, 0.1269, 0.1342, 0.1191, 0.1112, 0.0986, 0.1273, 0.1794,\n",
      "        0.1259, 0.1008, 0.0880, 0.1356, 0.1057, 0.1032, 0.1203, 0.1340, 0.1225,\n",
      "        0.1231, 0.1274, 0.1058, 0.1212, 0.1046, 0.0933, 0.1280, 0.1126, 0.1073,\n",
      "        0.1243, 0.1237, 0.1355, 0.1088, 0.1140, 0.1196, 0.1146, 0.0517, 0.1195,\n",
      "        0.0384, 0.1047, 0.1041, 0.1153, 0.0970, 0.1294, 0.1013, 0.1221, 0.1270,\n",
      "        0.0783, 0.0799, 0.1011, 0.0916, 0.1015, 0.1060, 0.1112, 0.1016, 0.1529,\n",
      "        0.1081, 0.0936, 0.1190, 0.1001, 0.1217, 0.1251, 0.1159, 0.0946, 0.1202,\n",
      "        0.0892, 0.1186, 0.1354, 0.1213, 0.1150, 0.0761, 0.1758, 0.1069, 0.0977,\n",
      "        0.0864, 0.0987, 0.0957, 0.1025, 0.1486, 0.0883, 0.1070, 0.1166, 0.0883,\n",
      "        0.1044, 0.1326, 0.0971, 0.0926, 0.0786, 0.0761, 0.1337, 0.0891, 0.0745,\n",
      "        0.1189, 0.1430, 0.0991, 0.1176, 0.1063, 0.1082, 0.1196, 0.1474, 0.0936,\n",
      "        0.1113, 0.0914, 0.1214, 0.1087, 0.1154, 0.1285, 0.1266, 0.0916, 0.1176,\n",
      "        0.1309, 0.1047, 0.1219, 0.1098, 0.1327, 0.0964, 0.1110, 0.1147, 0.1323,\n",
      "        0.0886, 0.1134, 0.1480, 0.1114, 0.1129, 0.1339, 0.1130, 0.1059, 0.1381,\n",
      "        0.1072, 0.1004, 0.1315, 0.1283, 0.0897, 0.0978, 0.1221, 0.1247],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 5.6958e-01, -6.8268e-01, -1.0881e-01,  ..., -5.6809e-02,\n",
      "         -1.4005e-01, -2.2458e-01],\n",
      "        [-7.2580e-03, -1.6454e-01,  7.3387e-01,  ..., -1.3448e-02,\n",
      "         -6.6601e-02,  3.0725e-01],\n",
      "        [ 1.0062e-01,  9.0814e-02,  2.6199e-03,  ..., -7.3633e-04,\n",
      "         -3.1052e-02, -1.5927e-01],\n",
      "        ...,\n",
      "        [-5.4874e-01, -8.2218e-01, -4.1948e-01,  ..., -1.8496e-01,\n",
      "         -3.9066e-01,  1.8648e+00],\n",
      "        [-5.3516e-01, -7.2279e-02,  2.5600e-01,  ..., -3.5613e-02,\n",
      "          1.0743e+00, -4.7563e-01],\n",
      "        [ 3.2937e-01, -1.2356e-01, -4.7975e-01,  ..., -4.2018e-01,\n",
      "         -7.3880e-01, -2.5136e-01]], device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.7101, -0.3271, -1.0863,  ...,  1.0581,  0.1202, -0.0994],\n",
      "        [-0.0089, -0.8364, -0.0061,  ...,  0.2968,  0.0950,  0.4962],\n",
      "        [ 0.2740, -0.7093,  0.4923,  ...,  0.2044, -0.1993,  0.1975],\n",
      "        ...,\n",
      "        [ 0.6756,  0.2537, -0.3033,  ..., -0.8790,  0.9024,  2.5025],\n",
      "        [ 0.7751,  0.7276, -0.9111,  ...,  0.1185, -0.3570,  0.2783],\n",
      "        [-0.0302, -1.5840,  0.4486,  ...,  0.6133, -0.0187,  0.1626]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0668,  0.1227,  0.4518,  ..., -0.2222, -0.7307,  0.5789],\n",
      "        [ 0.0555,  0.4796, -0.1139,  ...,  0.1597,  0.7047,  0.2442],\n",
      "        [-0.7803,  0.0020,  0.1431,  ...,  0.0443,  0.3783,  0.2813],\n",
      "        ...,\n",
      "        [ 0.2651, -0.2423, -0.0553,  ...,  0.4264,  0.7366,  0.6572],\n",
      "        [ 0.3909, -0.1265, -0.1657,  ..., -0.0242, -0.3469, -0.2975],\n",
      "        [ 0.4929, -0.4868, -0.0681,  ..., -0.1018, -0.7519,  0.5689]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.2.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1291, 0.1166, 0.0823, 0.1274, 0.1204, 0.0994, 0.1298, 0.0984, 0.0754,\n",
      "        0.1352, 0.1096, 0.0479, 0.0996, 0.0943, 0.0785, 0.1229, 0.0795, 0.0787,\n",
      "        0.0902, 0.0731, 0.0941, 0.0603, 0.0922, 0.1044, 0.0724, 0.0750, 0.0645,\n",
      "        0.1004, 0.0971, 0.0828, 0.0940, 0.1074, 0.1869, 0.0887, 0.0826, 0.0950,\n",
      "        0.1134, 0.0741, 0.0884, 0.0711, 0.1022, 0.0979, 0.1159, 0.0729, 0.0861,\n",
      "        0.1209, 0.1047, 0.1184, 0.0820, 0.0944, 0.0845, 0.1078, 0.0945, 0.1107,\n",
      "        0.1246, 0.1102, 0.0869, 0.1252, 0.0824, 0.0930, 0.1039, 0.0887, 0.0924,\n",
      "        0.1024, 0.1361, 0.1207, 0.0634, 0.0823, 0.1035, 0.0818, 0.1200, 0.0785,\n",
      "        0.0901, 0.0955, 0.0936, 0.0930, 0.0989, 0.1154, 0.0322, 0.0830, 0.1174,\n",
      "        0.0987, 0.0973, 0.0901, 0.0994, 0.0994, 0.1061, 0.0935, 0.1106, 0.1242,\n",
      "        0.0912, 0.0733, 0.0882, 0.0947, 0.1016, 0.0995, 0.0970, 0.1082, 0.0779,\n",
      "        0.0869, 0.0806, 0.0954, 0.0995, 0.1036, 0.0823, 0.0821, 0.0958, 0.0911,\n",
      "        0.0682, 0.1000, 0.0781, 0.0875, 0.1439, 0.0663, 0.0881, 0.0949, 0.0661,\n",
      "        0.0930, 0.0864, 0.1089, 0.1029, 0.1051, 0.1034, 0.1095, 0.0985, 0.1046,\n",
      "        0.0951, 0.0857, 0.0810, 0.0701, 0.0899, 0.1001, 0.0926, 0.0651, 0.0800,\n",
      "        0.0914, 0.6521, 0.0867, 0.0943, 0.1070, 0.0761, 0.1436, 0.0876, 0.1058,\n",
      "        0.0895, 0.1336, 0.0897, 0.0971, 0.0848, 0.0868, 0.1163, 0.1160, 0.0909,\n",
      "        0.1171, 0.0845, 0.0991, 0.0979, 0.1170, 0.1009, 0.0974, 0.0866, 0.1316,\n",
      "        0.1130, 0.0300, 0.1014, 0.0964, 0.0883, 0.1157, 0.1003, 0.1050, 0.1181,\n",
      "        0.0980, 0.1097, 0.0935, 0.1095, 0.0945, 0.0882, 0.0790, 0.0991, 0.1044,\n",
      "        0.0915, 0.0842, 0.1093, 0.0745, 0.0912, 0.1050, 0.1324, 0.1049, 0.0923,\n",
      "        0.0844, 0.0804, 0.1172, 0.0801, 0.0979, 0.1170, 0.0672, 0.0978, 0.0992,\n",
      "        0.0970, 0.0996, 0.0706, 0.0884, 0.0809, 0.0906, 0.0915, 0.0885, 0.0757,\n",
      "        0.0753, 0.1098, 0.1147, 0.1093, 0.0941, 0.1008, 0.0860, 0.0964, 0.1122,\n",
      "        0.0987, 0.1114, 0.0886, 0.1391, 0.0690, 0.0742, 0.0947, 0.0998, 0.0724,\n",
      "        0.0739, 0.1013, 0.0962, 0.0856, 0.1184, 0.0766, 0.0957, 0.0794, 0.0911,\n",
      "        0.1165, 0.1027, 0.0951, 0.1211, 0.0997, 0.0971, 0.1046, 0.0942, 0.0997,\n",
      "        0.0975, 0.0815, 0.0942, 0.1031, 0.0883, 0.0737, 0.1005, 0.0714, 0.1050,\n",
      "        0.0947, 0.0953, 0.0835, 0.0879, 0.0971, 0.1030, 0.0874, 0.1035, 0.0996,\n",
      "        0.0674, 0.0746, 0.1119, 0.1012, 0.1105, 0.0803, 0.1165, 0.0796, 0.0803,\n",
      "        0.1160, 0.0849, 0.0939, 0.1003, 0.1139, 0.0983, 0.0926, 0.1027, 0.0873,\n",
      "        0.1100, 0.0894, 0.0838, 0.0937, 0.1224, 0.1065, 0.0767, 0.1119, 0.0961,\n",
      "        0.1167, 0.1057, 0.0877, 0.0817, 0.1172, 0.0826, 0.0939, 0.0538, 0.0810,\n",
      "        0.0647, 0.0980, 0.0992, 0.1152, 0.1106, 0.0949, 0.1284, 0.0989, 0.1059,\n",
      "        0.1012, 0.0721, 0.0648, 0.0838, 0.0960, 0.0949, 0.0947, 0.1000, 0.1010,\n",
      "        0.1127, 0.1064, 0.1152, 0.1044, 0.0862, 0.1289, 0.0784, 0.0901, 0.1265,\n",
      "        0.1010, 0.0816, 0.1142, 0.0949, 0.0945, 0.0964, 0.0955, 0.0934, 0.0962,\n",
      "        0.0465, 0.1076, 0.0906, 0.0812, 0.1013, 0.0773, 0.1185, 0.1014, 0.1022,\n",
      "        0.0965, 0.0885, 0.0912, 0.0895, 0.1021, 0.1006, 0.1029, 0.0932, 0.0914,\n",
      "        0.1066, 0.0984, 0.0958, 0.0952, 0.0847, 0.1050, 0.0941, 0.1056, 0.0691,\n",
      "        0.0861, 0.1000, 0.0890, 0.0724, 0.0651, 0.1038, 0.0778, 0.0318, 0.1087,\n",
      "        0.1060, 0.1170, 0.1008, 0.1129, 0.1061, 0.0883, 0.1119, 0.0892, 0.1000,\n",
      "        0.0880, 0.1050, 0.1189, 0.0995, 0.0955, 0.0849, 0.0820, 0.1105, 0.1382,\n",
      "        0.1095, 0.0873, 0.0881, 0.1055, 0.0878, 0.0934, 0.0967, 0.1267, 0.0955,\n",
      "        0.1058, 0.0978, 0.0974, 0.1089, 0.0701, 0.0844, 0.1231, 0.1124, 0.1059,\n",
      "        0.0859, 0.0947, 0.0998, 0.1049, 0.1019, 0.0962, 0.0907, 0.0468, 0.0916,\n",
      "        0.0294, 0.0980, 0.0921, 0.1006, 0.0781, 0.1142, 0.0990, 0.1176, 0.0990,\n",
      "        0.0735, 0.0715, 0.0939, 0.0964, 0.0884, 0.0835, 0.0840, 0.0935, 0.1223,\n",
      "        0.0906, 0.1010, 0.0988, 0.0906, 0.0766, 0.0845, 0.1052, 0.0850, 0.0868,\n",
      "        0.0903, 0.0801, 0.1072, 0.0921, 0.1069, 0.0829, 0.0823, 0.0824, 0.0805,\n",
      "        0.0725, 0.0869, 0.0948, 0.0756, 0.1181, 0.0881, 0.0886, 0.1014, 0.0708,\n",
      "        0.0901, 0.1096, 0.0877, 0.0785, 0.0671, 0.0846, 0.1046, 0.0999, 0.0576,\n",
      "        0.0975, 0.1312, 0.0965, 0.0967, 0.0770, 0.0817, 0.1031, 0.1334, 0.0859,\n",
      "        0.0958, 0.0595, 0.1114, 0.1065, 0.0917, 0.1143, 0.1018, 0.0885, 0.1050,\n",
      "        0.1206, 0.1052, 0.1133, 0.1031, 0.1105, 0.0809, 0.0936, 0.0872, 0.1052,\n",
      "        0.0899, 0.1002, 0.1199, 0.1187, 0.1093, 0.1229, 0.1220, 0.0922, 0.1155,\n",
      "        0.1062, 0.0951, 0.1103, 0.1051, 0.0713, 0.1023, 0.0927, 0.2009],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0210,  0.0020,  0.0629,  ...,  0.0840, -0.0335, -0.0110],\n",
      "        [ 0.0323,  0.1382,  0.0124,  ..., -0.1183, -0.1635,  0.0430],\n",
      "        [-0.0329,  0.0088,  0.0339,  ..., -0.0463, -0.0795,  0.0675],\n",
      "        ...,\n",
      "        [-0.0565,  0.0302, -0.0419,  ..., -0.0697,  0.0293,  0.0135],\n",
      "        [ 0.0869,  0.0126,  0.0449,  ...,  0.0592, -0.0170,  0.0695],\n",
      "        [-0.0814,  0.0108, -0.0209,  ..., -0.0766, -0.0028, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0051, -0.5743, -0.3726,  ..., -0.1057,  0.4277, -0.4029],\n",
      "        [ 0.1810, -0.0970, -0.1880,  ...,  0.5077,  0.1122, -0.4097],\n",
      "        [-0.2956,  0.5016, -0.2487,  ..., -0.4006, -0.3981,  0.6106],\n",
      "        ...,\n",
      "        [-0.1039,  0.5526,  0.3218,  ..., -0.1888, -0.1716, -0.0601],\n",
      "        [ 0.2228, -0.1778,  0.2299,  ...,  0.1267, -0.0503,  0.1900],\n",
      "        [-0.4199,  0.5263, -0.3631,  ..., -0.2131, -0.0996, -0.0730]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2526,  0.0206,  0.2287,  ..., -0.6715,  0.2813,  0.7315],\n",
      "        [ 0.1994, -0.5212, -0.3553,  ..., -0.0046,  0.2965, -0.1090],\n",
      "        [-0.1148, -0.4600, -0.1505,  ...,  0.8062,  0.0979,  0.0685],\n",
      "        ...,\n",
      "        [-0.7945, -0.1607,  0.6125,  ..., -0.0256, -0.4481, -0.2607],\n",
      "        [ 0.9163, -0.1060,  1.1423,  ..., -0.9743,  0.8804,  0.4823],\n",
      "        [ 0.0358,  0.0413,  0.3029,  ...,  1.0005,  0.4418,  0.1881]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.5249, -0.4842, -0.2244,  ..., -1.2082, -0.2389, -0.5521],\n",
      "        [-0.1407,  0.1689, -0.0181,  ..., -0.9045, -0.5693,  0.5323],\n",
      "        [ 0.3644, -0.1588, -0.9510,  ...,  0.3232,  1.3372,  0.2583],\n",
      "        ...,\n",
      "        [-0.6874,  0.2296,  0.4531,  ...,  0.6647, -1.0676,  0.6240],\n",
      "        [ 1.0460,  0.4419, -0.0393,  ..., -0.3785, -0.0020, -0.0967],\n",
      "        [-0.3430,  0.2191,  0.2607,  ..., -0.2239,  0.7571, -0.2373]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1802, 0.1660, 0.1280, 0.1477, 0.1486, 0.1432, 0.0478, 0.1663, 0.1188,\n",
      "        0.2075, 0.1628, 0.0632, 0.1471, 0.1456, 0.1452, 0.2066, 0.1382, 0.1315,\n",
      "        0.1832, 0.1221, 0.1332, 0.0951, 0.1498, 0.1618, 0.1503, 0.1157, 0.1137,\n",
      "        0.1513, 0.1520, 0.1600, 0.1490, 0.1667, 0.1676, 0.1501, 0.1326, 0.1384,\n",
      "        0.1492, 0.1418, 0.1588, 0.1233, 0.1502, 0.1437, 0.1569, 0.1172, 0.1329,\n",
      "        0.1786, 0.1472, 0.1669, 0.1404, 0.1276, 0.1341, 0.1379, 0.1362, 0.1679,\n",
      "        0.1405, 0.1773, 0.1314, 0.1617, 0.1488, 0.1362, 0.1338, 0.1427, 0.1660,\n",
      "        0.1444, 0.1633, 0.1605, 0.1348, 0.1623, 0.1731, 0.1478, 0.1757, 0.1169,\n",
      "        0.1385, 0.1424, 0.1368, 0.1363, 0.1515, 0.1614, 0.0542, 0.1428, 0.1587,\n",
      "        0.1304, 0.1482, 0.1394, 0.1532, 0.1618, 0.1541, 0.1658, 0.1586, 0.1464,\n",
      "        0.1426, 0.1156, 0.1296, 0.1696, 0.1622, 0.1440, 0.1496, 0.1384, 0.1257,\n",
      "        0.1311, 0.1232, 0.1265, 0.1425, 0.1421, 0.1410, 0.1458, 0.1448, 0.1298,\n",
      "        0.1027, 0.1333, 0.1388, 0.1362, 0.2060, 0.1167, 0.1428, 0.1421, 0.1132,\n",
      "        0.1237, 0.1258, 0.1560, 0.1474, 0.1525, 0.1530, 0.1530, 0.1259, 0.1337,\n",
      "        0.1435, 0.1358, 0.1147, 0.1366, 0.1563, 0.1468, 0.1289, 0.1143, 0.1224,\n",
      "        0.1644, 0.0440, 0.1381, 0.1313, 0.1477, 0.1306, 0.1770, 0.1674, 0.1598,\n",
      "        0.1546, 0.1513, 0.1521, 0.1423, 0.1373, 0.1096, 0.1682, 0.1421, 0.1562,\n",
      "        0.1586, 0.1481, 0.1646, 0.1456, 0.1562, 0.1391, 0.1483, 0.1577, 0.1871,\n",
      "        0.1680, 0.0721, 0.1407, 0.1379, 0.1427, 0.1694, 0.1303, 0.1697, 0.1555,\n",
      "        0.1203, 0.1432, 0.1345, 0.1635, 0.1468, 0.1271, 0.1362, 0.1429, 0.1499,\n",
      "        0.1367, 0.1118, 0.1955, 0.1352, 0.1533, 0.1512, 0.1572, 0.1852, 0.1496,\n",
      "        0.1335, 0.1331, 0.1528, 0.1355, 0.1489, 0.1315, 0.1223, 0.1627, 0.1630,\n",
      "        0.1309, 0.1481, 0.1464, 0.1452, 0.1347, 0.1461, 0.1180, 0.1155, 0.1216,\n",
      "        0.1379, 0.1687, 0.1486, 0.1709, 0.1566, 0.1581, 0.1353, 0.1640, 0.1398,\n",
      "        0.1393, 0.1843, 0.1343, 0.1608, 0.1349, 0.1375, 0.1429, 0.1400, 0.1612,\n",
      "        0.1163, 0.1510, 0.1260, 0.1515, 0.1540, 0.1319, 0.1304, 0.1332, 0.1244,\n",
      "        0.1893, 0.1538, 0.1468, 0.1537, 0.1755, 0.1704, 0.1478, 0.1400, 0.1516,\n",
      "        0.1346, 0.1348, 0.1582, 0.1599, 0.1569, 0.1240, 0.1462, 0.1203, 0.1457,\n",
      "        0.1579, 0.1377, 0.1196, 0.1288, 0.1418, 0.1632, 0.1540, 0.1641, 0.1403,\n",
      "        0.1223, 0.1516, 0.1717, 0.1797, 0.1354, 0.1726, 0.1711, 0.1609, 0.1182,\n",
      "        0.1407, 0.1439, 0.1464, 0.1366, 0.1749, 0.1254, 0.1416, 0.1614, 0.1462,\n",
      "        0.1722, 0.1324, 0.1572, 0.1245, 0.1596, 0.1484, 0.1140, 0.1614, 0.1422,\n",
      "        0.1741, 0.1415, 0.1288, 0.1228, 0.1747, 0.1332, 0.1306, 0.0978, 0.1368,\n",
      "        0.0993, 0.1504, 0.1312, 0.1720, 0.1357, 0.1495, 0.1708, 0.1390, 0.1594,\n",
      "        0.1394, 0.1265, 0.1148, 0.1335, 0.1352, 0.1603, 0.1209, 0.1544, 0.1422,\n",
      "        0.1642, 0.1383, 0.1583, 0.1572, 0.1454, 0.1735, 0.1330, 0.1400, 0.1888,\n",
      "        0.1443, 0.1393, 0.1573, 0.1414, 0.1356, 0.1702, 0.1353, 0.1304, 0.1434,\n",
      "        0.0830, 0.1414, 0.1401, 0.1551, 0.1700, 0.1383, 0.1558, 0.1340, 0.1596,\n",
      "        0.1482, 0.1227, 0.1516, 0.1325, 0.1707, 0.1496, 0.1443, 0.1132, 0.1472,\n",
      "        0.1555, 0.1390, 0.1668, 0.1495, 0.1419, 0.1452, 0.1333, 0.1374, 0.1160,\n",
      "        0.1377, 0.1467, 0.1440, 0.1225, 0.1076, 0.1321, 0.1353, 0.0415, 0.1587,\n",
      "        0.1517, 0.1543, 0.1430, 0.1686, 0.1532, 0.1170, 0.1636, 0.1359, 0.1504,\n",
      "        0.1357, 0.1704, 0.1582, 0.1428, 0.1491, 0.1341, 0.1290, 0.1476, 0.2021,\n",
      "        0.1500, 0.1563, 0.1344, 0.1403, 0.1324, 0.1390, 0.1602, 0.1881, 0.1346,\n",
      "        0.1500, 0.1468, 0.1473, 0.1308, 0.1435, 0.1121, 0.1459, 0.1523, 0.1367,\n",
      "        0.1575, 0.1338, 0.1431, 0.1512, 0.1571, 0.1437, 0.1443, 0.0918, 0.1621,\n",
      "        0.0357, 0.1306, 0.1379, 0.1573, 0.1557, 0.1625, 0.1478, 0.1786, 0.1408,\n",
      "        0.1258, 0.1389, 0.1424, 0.1577, 0.1232, 0.1213, 0.1520, 0.1484, 0.1675,\n",
      "        0.1297, 0.1463, 0.1536, 0.1207, 0.1356, 0.1335, 0.1514, 0.1180, 0.1472,\n",
      "        0.1202, 0.1285, 0.1849, 0.1156, 0.1521, 0.1368, 0.2015, 0.1412, 0.1469,\n",
      "        0.1214, 0.1328, 0.1437, 0.1521, 0.1574, 0.1122, 0.1428, 0.1585, 0.1164,\n",
      "        0.1360, 0.1375, 0.1584, 0.1184, 0.1131, 0.1120, 0.1732, 0.1227, 0.1139,\n",
      "        0.1700, 0.1589, 0.1370, 0.1449, 0.1382, 0.1428, 0.1913, 0.1891, 0.1471,\n",
      "        0.1433, 0.1074, 0.1502, 0.1413, 0.1559, 0.1777, 0.1777, 0.1379, 0.1688,\n",
      "        0.1747, 0.1378, 0.1625, 0.1563, 0.1625, 0.1260, 0.1515, 0.1352, 0.1560,\n",
      "        0.1230, 0.1531, 0.1775, 0.1499, 0.1591, 0.1585, 0.1825, 0.1438, 0.1441,\n",
      "        0.1574, 0.1473, 0.1515, 0.1794, 0.1104, 0.1436, 0.1492, 0.1403],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5322, -0.2205, -0.3416,  ...,  0.2043,  0.0169, -0.0989],\n",
      "        [-0.2019, -0.2599,  0.3491,  ..., -0.3686,  0.3291,  0.0512],\n",
      "        [ 0.0326,  0.3522,  0.4343,  ...,  0.2805, -0.0341,  0.2542],\n",
      "        ...,\n",
      "        [ 0.4569,  0.3693,  0.5249,  ...,  0.5443, -0.4634,  0.0129],\n",
      "        [-0.2821, -0.4978,  0.3610,  ...,  0.3163, -0.1572, -0.4212],\n",
      "        [-0.7399, -0.2171, -0.3617,  ...,  0.5363,  0.5114, -0.4185]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2271, -1.1927,  0.5014,  ...,  0.1570,  0.7063, -0.0872],\n",
      "        [ 0.0474, -0.1075,  0.7945,  ..., -0.5017,  1.4886,  0.1163],\n",
      "        [ 0.0644, -0.6721,  0.7569,  ...,  0.4224, -0.1512, -1.1818],\n",
      "        ...,\n",
      "        [-0.6562,  1.2240,  0.6139,  ...,  0.0862,  0.1071,  0.6972],\n",
      "        [-2.7028,  2.2276,  2.3607,  ...,  0.8148,  1.0803,  1.0078],\n",
      "        [ 0.1513, -1.3411,  0.1546,  ...,  0.3953,  0.5983,  0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.9601, -0.0262,  0.2123,  ...,  1.0168, -0.5249, -0.1303],\n",
      "        [ 0.1209, -0.0091,  0.5653,  ..., -0.8223,  0.2019, -0.2576],\n",
      "        [ 0.1355,  0.2402, -0.1007,  ...,  1.1905,  1.1747,  0.3112],\n",
      "        ...,\n",
      "        [-0.2496,  0.5420, -0.7341,  ...,  0.3028, -0.0334,  0.2328],\n",
      "        [-0.1306,  1.0143, -0.2458,  ...,  0.5208, -0.5778, -0.0321],\n",
      "        [ 0.0657,  0.0466, -0.0571,  ..., -0.3905, -0.2775,  0.4066]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1270, 0.1391, 0.0810, 0.1079, 0.1095, 0.1016, 0.0404, 0.1127, 0.0745,\n",
      "        0.1196, 0.1091, 0.0383, 0.0983, 0.0856, 0.0901, 0.1322, 0.1079, 0.0778,\n",
      "        0.1082, 0.0741, 0.0941, 0.0699, 0.1073, 0.0980, 0.0834, 0.0830, 0.0610,\n",
      "        0.0986, 0.1074, 0.0831, 0.1020, 0.1181, 0.1447, 0.0953, 0.0944, 0.0962,\n",
      "        0.1044, 0.0939, 0.0818, 0.0664, 0.1052, 0.1034, 0.1248, 0.0763, 0.0957,\n",
      "        0.1056, 0.0928, 0.1112, 0.0956, 0.1017, 0.0788, 0.1056, 0.1031, 0.1072,\n",
      "        0.1154, 0.1142, 0.1041, 0.1056, 0.0973, 0.0838, 0.1119, 0.0969, 0.0996,\n",
      "        0.1091, 0.1175, 0.1093, 0.0730, 0.0950, 0.1178, 0.0954, 0.1165, 0.0821,\n",
      "        0.0995, 0.0967, 0.0874, 0.0968, 0.0881, 0.1147, 0.0424, 0.0949, 0.1133,\n",
      "        0.1071, 0.0829, 0.0979, 0.0973, 0.1069, 0.1007, 0.1010, 0.0982, 0.1023,\n",
      "        0.0808, 0.0811, 0.0964, 0.1101, 0.1232, 0.1130, 0.1078, 0.1277, 0.0730,\n",
      "        0.0825, 0.0728, 0.1019, 0.1039, 0.1031, 0.0877, 0.0866, 0.1046, 0.0948,\n",
      "        0.0829, 0.1016, 0.0876, 0.1022, 0.1308, 0.0705, 0.0999, 0.1074, 0.0647,\n",
      "        0.0906, 0.0841, 0.1232, 0.0961, 0.1090, 0.0962, 0.1073, 0.0898, 0.1026,\n",
      "        0.0972, 0.0994, 0.0891, 0.0789, 0.1056, 0.0912, 0.0946, 0.0825, 0.0800,\n",
      "        0.0873, 0.2233, 0.0915, 0.0916, 0.1055, 0.0806, 0.1165, 0.0903, 0.1107,\n",
      "        0.1034, 0.1359, 0.0882, 0.1062, 0.1005, 0.0944, 0.1072, 0.1131, 0.0873,\n",
      "        0.1108, 0.0789, 0.0937, 0.1015, 0.1001, 0.0983, 0.1025, 0.0878, 0.1233,\n",
      "        0.1071, 0.0371, 0.0965, 0.1102, 0.0975, 0.1120, 0.0906, 0.0894, 0.1149,\n",
      "        0.0922, 0.0982, 0.0890, 0.1289, 0.0887, 0.0958, 0.0661, 0.1157, 0.1072,\n",
      "        0.0699, 0.0761, 0.1111, 0.0923, 0.0933, 0.1093, 0.1221, 0.1139, 0.1081,\n",
      "        0.0811, 0.0868, 0.1056, 0.0916, 0.0919, 0.1065, 0.0840, 0.0876, 0.1080,\n",
      "        0.1016, 0.1040, 0.0886, 0.0959, 0.0852, 0.0838, 0.0955, 0.0853, 0.0866,\n",
      "        0.0877, 0.1168, 0.1210, 0.1211, 0.0886, 0.1050, 0.0866, 0.1163, 0.1000,\n",
      "        0.0945, 0.1002, 0.0910, 0.1182, 0.0691, 0.0828, 0.0998, 0.0978, 0.0802,\n",
      "        0.0656, 0.1094, 0.0920, 0.0879, 0.1192, 0.0839, 0.0889, 0.0888, 0.1033,\n",
      "        0.1174, 0.1012, 0.1014, 0.1114, 0.1135, 0.0959, 0.1098, 0.0949, 0.0983,\n",
      "        0.0977, 0.0867, 0.1196, 0.0903, 0.0967, 0.0825, 0.1055, 0.0756, 0.1139,\n",
      "        0.1002, 0.0972, 0.0934, 0.0883, 0.0929, 0.1016, 0.0968, 0.1139, 0.1003,\n",
      "        0.0845, 0.0901, 0.1075, 0.1081, 0.1213, 0.1038, 0.1178, 0.0864, 0.0905,\n",
      "        0.0987, 0.1020, 0.0977, 0.0947, 0.1143, 0.0992, 0.0945, 0.1141, 0.0869,\n",
      "        0.0974, 0.0962, 0.0987, 0.0795, 0.1255, 0.1010, 0.0890, 0.1089, 0.0874,\n",
      "        0.1189, 0.0953, 0.1020, 0.0891, 0.1087, 0.0757, 0.1023, 0.0660, 0.0943,\n",
      "        0.0768, 0.1097, 0.0978, 0.1338, 0.1001, 0.0961, 0.1147, 0.1079, 0.1041,\n",
      "        0.1175, 0.0780, 0.0781, 0.1033, 0.0863, 0.0946, 0.0915, 0.0957, 0.0969,\n",
      "        0.1149, 0.1097, 0.1140, 0.1035, 0.0870, 0.1202, 0.0867, 0.0871, 0.1126,\n",
      "        0.1053, 0.0843, 0.1103, 0.0980, 0.0966, 0.1149, 0.1054, 0.0921, 0.0988,\n",
      "        0.0648, 0.1100, 0.0877, 0.0895, 0.0965, 0.0767, 0.1204, 0.1029, 0.1132,\n",
      "        0.0969, 0.1025, 0.0963, 0.0946, 0.0941, 0.0952, 0.0901, 0.0904, 0.0837,\n",
      "        0.1035, 0.0995, 0.0896, 0.0939, 0.0966, 0.0996, 0.0926, 0.1068, 0.0875,\n",
      "        0.1012, 0.0879, 0.1005, 0.0773, 0.0726, 0.1088, 0.0907, 0.0434, 0.1084,\n",
      "        0.0947, 0.1280, 0.0982, 0.1084, 0.1213, 0.0762, 0.1052, 0.0951, 0.0938,\n",
      "        0.0947, 0.1006, 0.1025, 0.1032, 0.0863, 0.0960, 0.0863, 0.1120, 0.1212,\n",
      "        0.1009, 0.1060, 0.0917, 0.1088, 0.0812, 0.0943, 0.0978, 0.1288, 0.1027,\n",
      "        0.0968, 0.1090, 0.0883, 0.1059, 0.0850, 0.0775, 0.1179, 0.0962, 0.1003,\n",
      "        0.1092, 0.0966, 0.0952, 0.1100, 0.0881, 0.0929, 0.0885, 0.0411, 0.0943,\n",
      "        0.0366, 0.0951, 0.1078, 0.0866, 0.0823, 0.1047, 0.1023, 0.1055, 0.0948,\n",
      "        0.0718, 0.0695, 0.1000, 0.0972, 0.0888, 0.0932, 0.0861, 0.0890, 0.1220,\n",
      "        0.0982, 0.1132, 0.0973, 0.0772, 0.0925, 0.0988, 0.0928, 0.0797, 0.0988,\n",
      "        0.0779, 0.0730, 0.1169, 0.0985, 0.1064, 0.0811, 0.1147, 0.0847, 0.0959,\n",
      "        0.0784, 0.1053, 0.0919, 0.0848, 0.0972, 0.0822, 0.0936, 0.1165, 0.0836,\n",
      "        0.0979, 0.1021, 0.1108, 0.0846, 0.0729, 0.0782, 0.1031, 0.0907, 0.0912,\n",
      "        0.1061, 0.1096, 0.0822, 0.1002, 0.0778, 0.1031, 0.0963, 0.1218, 0.0860,\n",
      "        0.1029, 0.0708, 0.1132, 0.1011, 0.0932, 0.1214, 0.1026, 0.0938, 0.1132,\n",
      "        0.1220, 0.1027, 0.1235, 0.1105, 0.1226, 0.0792, 0.0825, 0.0985, 0.1143,\n",
      "        0.0886, 0.0987, 0.1080, 0.1037, 0.1045, 0.1125, 0.1110, 0.0965, 0.1129,\n",
      "        0.1047, 0.0815, 0.0939, 0.1073, 0.0706, 0.0893, 0.1114, 0.1550],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0575, -0.0152, -0.0017,  ...,  0.0281,  0.0002, -0.0082],\n",
      "        [-0.0408,  0.0263,  0.0122,  ...,  0.0139,  0.0149, -0.0159],\n",
      "        [-0.0100,  0.0775,  0.0518,  ...,  0.0883, -0.0131, -0.0048],\n",
      "        ...,\n",
      "        [-0.0139, -0.0022,  0.0291,  ..., -0.0010,  0.0214, -0.0835],\n",
      "        [ 0.0187, -0.0282,  0.0104,  ...,  0.0851,  0.0220,  0.0295],\n",
      "        [-0.0026, -0.0082, -0.0479,  ...,  0.0137,  0.0343, -0.0834]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.8898, -0.9212,  0.2037,  ...,  0.6367, -0.1094, -0.3263],\n",
      "        [-0.4890,  0.0498,  0.0333,  ...,  0.6143, -0.2893, -0.7838],\n",
      "        [ 0.5628,  0.1691,  0.3782,  ...,  0.0427,  0.2942,  0.1129],\n",
      "        ...,\n",
      "        [ 0.0606, -0.5090,  0.1537,  ..., -0.2527,  0.1725, -0.4433],\n",
      "        [ 0.2144,  0.4028,  0.0510,  ...,  0.9340, -0.3784, -0.1261],\n",
      "        [-0.0677,  0.0442,  0.2481,  ..., -0.4275,  0.3678, -0.3566]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4202,  1.2373, -0.1808,  ..., -0.3416,  0.4067, -0.4895],\n",
      "        [ 0.0704, -0.4765, -0.3516,  ..., -0.6420,  0.1086, -0.5586],\n",
      "        [ 0.5809,  1.5220,  0.4746,  ...,  0.0380,  0.7855,  0.3125],\n",
      "        ...,\n",
      "        [ 0.3426,  0.0849,  0.0287,  ..., -0.2235,  0.9439,  0.5294],\n",
      "        [ 1.3276,  0.7467,  0.0310,  ..., -0.7633,  0.0570, -0.1045],\n",
      "        [ 0.9005, -0.3150,  0.6070,  ...,  0.8995,  0.4870, -0.0295]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3555,  0.5443, -0.4056,  ...,  0.3459, -0.0016, -0.9763],\n",
      "        [ 0.4640,  0.4333,  0.1550,  ..., -0.0564, -0.2837,  0.1710],\n",
      "        [ 0.3237,  0.3362,  0.6463,  ...,  0.1247, -0.9528,  0.0242],\n",
      "        ...,\n",
      "        [-0.1899,  0.0431,  0.2104,  ..., -0.9738,  0.1832, -1.2198],\n",
      "        [ 0.1641,  0.0608, -0.4126,  ..., -0.3401, -0.2272, -0.4410],\n",
      "        [-0.2419,  0.2815, -0.0981,  ...,  0.2929, -0.2027,  0.3142]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1802, 0.1956, 0.1383, 0.2088, 0.1648, 0.1466, 0.0411, 0.1809, 0.1330,\n",
      "        0.2184, 0.2033, 0.0710, 0.1623, 0.1532, 0.1457, 0.1895, 0.1529, 0.1556,\n",
      "        0.1423, 0.1200, 0.1247, 0.1196, 0.1785, 0.1705, 0.1337, 0.1261, 0.1183,\n",
      "        0.1764, 0.1542, 0.1553, 0.1599, 0.1925, 0.1504, 0.1415, 0.1505, 0.1763,\n",
      "        0.1776, 0.1661, 0.1587, 0.1184, 0.1606, 0.1603, 0.1794, 0.1365, 0.1491,\n",
      "        0.1794, 0.1586, 0.1989, 0.1577, 0.1798, 0.1378, 0.2000, 0.1527, 0.2118,\n",
      "        0.1760, 0.1944, 0.1473, 0.1717, 0.1572, 0.1293, 0.1872, 0.1629, 0.1704,\n",
      "        0.1719, 0.2077, 0.2069, 0.1231, 0.1717, 0.1562, 0.1310, 0.1940, 0.1259,\n",
      "        0.1579, 0.1555, 0.1359, 0.1463, 0.1691, 0.2188, 0.0636, 0.1631, 0.1741,\n",
      "        0.1314, 0.1658, 0.1566, 0.1614, 0.1893, 0.1761, 0.1493, 0.1777, 0.1728,\n",
      "        0.1411, 0.1426, 0.1523, 0.1650, 0.1916, 0.1782, 0.1602, 0.1638, 0.1285,\n",
      "        0.1310, 0.1428, 0.1607, 0.1597, 0.1518, 0.1458, 0.1383, 0.1769, 0.1570,\n",
      "        0.1182, 0.1580, 0.1330, 0.1699, 0.1936, 0.1289, 0.1497, 0.1369, 0.1115,\n",
      "        0.1637, 0.1492, 0.1785, 0.1536, 0.1509, 0.1624, 0.1872, 0.1490, 0.1668,\n",
      "        0.1768, 0.1433, 0.1501, 0.1324, 0.1411, 0.1483, 0.1447, 0.1299, 0.1406,\n",
      "        0.1714, 0.0382, 0.1658, 0.1515, 0.1582, 0.1550, 0.1838, 0.1675, 0.1878,\n",
      "        0.1738, 0.2167, 0.1441, 0.1628, 0.1447, 0.1531, 0.1647, 0.1920, 0.1625,\n",
      "        0.2226, 0.1682, 0.1736, 0.1550, 0.1675, 0.1538, 0.1504, 0.1668, 0.1907,\n",
      "        0.1646, 0.0463, 0.1686, 0.1363, 0.1408, 0.2089, 0.1746, 0.1940, 0.1929,\n",
      "        0.1194, 0.1782, 0.1732, 0.1715, 0.1520, 0.1664, 0.1439, 0.1714, 0.1636,\n",
      "        0.1385, 0.1325, 0.1520, 0.1509, 0.1380, 0.1640, 0.1773, 0.1638, 0.1720,\n",
      "        0.1348, 0.1522, 0.1593, 0.1424, 0.1739, 0.1743, 0.1278, 0.1506, 0.1906,\n",
      "        0.1660, 0.1384, 0.1431, 0.1560, 0.1622, 0.1424, 0.1380, 0.1158, 0.1402,\n",
      "        0.1277, 0.1783, 0.1449, 0.1815, 0.1539, 0.1746, 0.1557, 0.2045, 0.1715,\n",
      "        0.1746, 0.1582, 0.1413, 0.1870, 0.1332, 0.1355, 0.1707, 0.1458, 0.1529,\n",
      "        0.1356, 0.1728, 0.1396, 0.1518, 0.1778, 0.1285, 0.1671, 0.1386, 0.1361,\n",
      "        0.1946, 0.1491, 0.1849, 0.1677, 0.1594, 0.1776, 0.1517, 0.1692, 0.1614,\n",
      "        0.1444, 0.1362, 0.1692, 0.1548, 0.1410, 0.1275, 0.1779, 0.1041, 0.1604,\n",
      "        0.1530, 0.1564, 0.1336, 0.1489, 0.1526, 0.1644, 0.1843, 0.1737, 0.1538,\n",
      "        0.1368, 0.1409, 0.1686, 0.1938, 0.1775, 0.1712, 0.1989, 0.1349, 0.1419,\n",
      "        0.1613, 0.1542, 0.1465, 0.1257, 0.1605, 0.1540, 0.1509, 0.1726, 0.1502,\n",
      "        0.1630, 0.1445, 0.1584, 0.1495, 0.1453, 0.1693, 0.1394, 0.1452, 0.1511,\n",
      "        0.1906, 0.1519, 0.1942, 0.1670, 0.1717, 0.1509, 0.1551, 0.1085, 0.1501,\n",
      "        0.1216, 0.1554, 0.1541, 0.1941, 0.1759, 0.1555, 0.1901, 0.1625, 0.1724,\n",
      "        0.1708, 0.1383, 0.1289, 0.1716, 0.1673, 0.1670, 0.1477, 0.1789, 0.1800,\n",
      "        0.1621, 0.1817, 0.1823, 0.1582, 0.1542, 0.2011, 0.1445, 0.1468, 0.1894,\n",
      "        0.1709, 0.1241, 0.2162, 0.1392, 0.1480, 0.1726, 0.1536, 0.1335, 0.1608,\n",
      "        0.0876, 0.1506, 0.1451, 0.1515, 0.1642, 0.1093, 0.1764, 0.1606, 0.1892,\n",
      "        0.1756, 0.1628, 0.1789, 0.1579, 0.1735, 0.1512, 0.1661, 0.1409, 0.1418,\n",
      "        0.1663, 0.1475, 0.1583, 0.1791, 0.1689, 0.1782, 0.1715, 0.1685, 0.1224,\n",
      "        0.1573, 0.1800, 0.1778, 0.1356, 0.1233, 0.1746, 0.1577, 0.0439, 0.1753,\n",
      "        0.1873, 0.1708, 0.1463, 0.1929, 0.1782, 0.1357, 0.1730, 0.1348, 0.1424,\n",
      "        0.1593, 0.1829, 0.1629, 0.1689, 0.1580, 0.1725, 0.1458, 0.1836, 0.1993,\n",
      "        0.1727, 0.1712, 0.1498, 0.1757, 0.1418, 0.1499, 0.1619, 0.1928, 0.1554,\n",
      "        0.1838, 0.1767, 0.1529, 0.1818, 0.1344, 0.1457, 0.1927, 0.1875, 0.1514,\n",
      "        0.1699, 0.1502, 0.1421, 0.1432, 0.1586, 0.1619, 0.1762, 0.0737, 0.1527,\n",
      "        0.0540, 0.1681, 0.1807, 0.1489, 0.1316, 0.1892, 0.1581, 0.1783, 0.1545,\n",
      "        0.1352, 0.1308, 0.1630, 0.1491, 0.1504, 0.1257, 0.1222, 0.1593, 0.2171,\n",
      "        0.1388, 0.1619, 0.1746, 0.1312, 0.1612, 0.1626, 0.1591, 0.1103, 0.1606,\n",
      "        0.1366, 0.1342, 0.1757, 0.1675, 0.1579, 0.1305, 0.2182, 0.1579, 0.1646,\n",
      "        0.1310, 0.1611, 0.1542, 0.1261, 0.1769, 0.1314, 0.1543, 0.1727, 0.1308,\n",
      "        0.1475, 0.1659, 0.1599, 0.1299, 0.1011, 0.1102, 0.1917, 0.1396, 0.1330,\n",
      "        0.1689, 0.2139, 0.1529, 0.1648, 0.1114, 0.1615, 0.1824, 0.1786, 0.1667,\n",
      "        0.1588, 0.1000, 0.1750, 0.1770, 0.1557, 0.1875, 0.1650, 0.1477, 0.2067,\n",
      "        0.2043, 0.1578, 0.1762, 0.1736, 0.1957, 0.1580, 0.1620, 0.1561, 0.1616,\n",
      "        0.1843, 0.1517, 0.1946, 0.1648, 0.1809, 0.1648, 0.1854, 0.1702, 0.1518,\n",
      "        0.1881, 0.1426, 0.1802, 0.1799, 0.1278, 0.1442, 0.1458, 0.1460],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0809, -0.2089,  0.3044,  ...,  0.5317, -0.2540,  0.3115],\n",
      "        [ 0.5904, -0.1940, -0.0768,  ...,  0.0256, -0.5137,  0.3204],\n",
      "        [ 0.3290,  0.4329, -0.0783,  ..., -0.1002, -0.0855, -0.4341],\n",
      "        ...,\n",
      "        [ 0.3955, -0.5074, -0.4897,  ..., -0.0629, -0.2082, -0.1230],\n",
      "        [-0.2356, -0.1761, -0.7093,  ...,  0.3805,  0.7197, -0.3440],\n",
      "        [-0.1733,  0.3458,  0.4058,  ..., -0.2657, -0.0046,  0.7962]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2513,  1.6673, -0.5061,  ...,  0.3145, -1.4993,  0.0764],\n",
      "        [-1.5684,  0.1876,  1.3446,  ...,  0.8738, -2.6254,  0.0349],\n",
      "        [ 0.7478,  0.5923,  0.2452,  ...,  1.6457,  1.5904,  1.2480],\n",
      "        ...,\n",
      "        [-2.1262,  2.0307, -0.7033,  ...,  0.7264, -0.2337,  0.6030],\n",
      "        [-1.0736,  0.8717,  0.0724,  ...,  0.2938,  0.0074,  0.6532],\n",
      "        [ 0.5758,  0.4944, -0.2749,  ..., -0.3492,  0.3515,  0.6935]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6956, -0.1058, -0.5167,  ..., -1.0014,  0.0404,  0.6673],\n",
      "        [ 0.6297, -0.2263, -0.3568,  ...,  0.4705, -0.0096,  0.2454],\n",
      "        [ 0.1407, -0.1229,  0.1557,  ..., -0.3194,  0.3191,  0.2256],\n",
      "        ...,\n",
      "        [ 0.3117, -0.6089,  0.1266,  ...,  0.5212, -0.3227,  0.7324],\n",
      "        [ 0.5205,  0.5006, -0.4909,  ...,  0.4388, -1.0023,  0.5437],\n",
      "        [-0.5019, -0.2085, -0.0962,  ...,  0.0605, -0.0393, -0.3859]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1277, 0.1343, 0.0772, 0.1247, 0.1013, 0.1080, 0.0407, 0.1072, 0.0970,\n",
      "        0.1253, 0.1152, 0.0444, 0.1133, 0.0801, 0.0875, 0.1387, 0.0948, 0.0899,\n",
      "        0.1129, 0.1032, 0.0990, 0.0843, 0.1079, 0.1029, 0.0984, 0.0921, 0.0916,\n",
      "        0.1029, 0.1143, 0.0912, 0.0985, 0.1146, 0.1433, 0.0948, 0.1078, 0.1066,\n",
      "        0.1011, 0.0934, 0.1013, 0.0918, 0.0976, 0.1128, 0.1230, 0.0849, 0.1019,\n",
      "        0.1156, 0.0977, 0.1209, 0.0909, 0.1198, 0.0855, 0.1138, 0.1052, 0.1204,\n",
      "        0.1052, 0.1176, 0.0917, 0.0950, 0.0949, 0.1015, 0.1194, 0.0936, 0.1225,\n",
      "        0.1093, 0.1328, 0.1062, 0.0716, 0.1187, 0.1167, 0.0927, 0.1327, 0.0938,\n",
      "        0.0945, 0.1062, 0.0944, 0.0991, 0.1035, 0.1187, 0.0401, 0.1070, 0.1094,\n",
      "        0.1038, 0.0923, 0.0936, 0.1116, 0.1155, 0.1075, 0.0966, 0.1170, 0.1155,\n",
      "        0.0938, 0.1017, 0.0933, 0.1017, 0.1292, 0.1119, 0.1192, 0.1068, 0.0899,\n",
      "        0.0769, 0.0939, 0.1039, 0.1027, 0.1056, 0.0924, 0.0911, 0.1139, 0.1048,\n",
      "        0.0830, 0.1183, 0.0833, 0.1040, 0.1159, 0.0847, 0.0954, 0.1007, 0.0686,\n",
      "        0.1000, 0.0894, 0.1199, 0.0978, 0.0934, 0.1030, 0.1139, 0.1010, 0.1019,\n",
      "        0.1048, 0.1064, 0.0898, 0.0947, 0.1061, 0.1089, 0.1045, 0.0974, 0.0867,\n",
      "        0.0886, 0.2412, 0.1022, 0.1046, 0.1098, 0.0850, 0.1226, 0.0888, 0.1104,\n",
      "        0.1082, 0.1377, 0.0965, 0.0964, 0.0963, 0.1022, 0.0906, 0.1121, 0.0987,\n",
      "        0.1184, 0.0952, 0.1049, 0.0980, 0.1024, 0.0935, 0.1045, 0.0901, 0.1332,\n",
      "        0.1023, 0.0336, 0.1122, 0.0930, 0.0934, 0.1147, 0.0952, 0.1127, 0.1053,\n",
      "        0.1035, 0.0988, 0.0954, 0.1106, 0.0969, 0.1142, 0.0843, 0.0993, 0.1165,\n",
      "        0.0829, 0.0894, 0.1155, 0.0842, 0.1034, 0.0981, 0.1064, 0.1122, 0.0999,\n",
      "        0.0909, 0.0967, 0.0988, 0.1096, 0.1037, 0.1063, 0.0865, 0.0963, 0.1123,\n",
      "        0.0962, 0.1156, 0.0838, 0.0952, 0.1083, 0.0850, 0.0932, 0.0959, 0.0967,\n",
      "        0.0891, 0.1114, 0.1047, 0.1167, 0.0966, 0.0908, 0.0900, 0.1046, 0.0957,\n",
      "        0.1075, 0.1107, 0.0961, 0.1108, 0.0765, 0.0923, 0.0966, 0.0863, 0.0901,\n",
      "        0.0977, 0.1024, 0.0930, 0.1037, 0.1170, 0.1042, 0.1036, 0.0980, 0.1093,\n",
      "        0.0994, 0.1109, 0.1065, 0.1066, 0.1151, 0.1229, 0.1193, 0.0940, 0.0854,\n",
      "        0.0872, 0.0934, 0.1116, 0.0938, 0.0896, 0.0968, 0.1076, 0.0877, 0.1089,\n",
      "        0.1022, 0.1108, 0.0842, 0.1074, 0.1020, 0.0960, 0.1081, 0.1263, 0.0906,\n",
      "        0.0933, 0.0941, 0.1111, 0.1091, 0.1052, 0.1123, 0.1163, 0.0988, 0.1035,\n",
      "        0.1118, 0.1020, 0.0911, 0.0813, 0.1093, 0.0925, 0.0952, 0.1134, 0.0891,\n",
      "        0.1248, 0.0916, 0.1052, 0.1059, 0.1012, 0.1120, 0.0902, 0.1179, 0.1061,\n",
      "        0.1155, 0.1008, 0.1119, 0.0957, 0.1092, 0.0978, 0.1047, 0.0769, 0.1067,\n",
      "        0.0809, 0.1068, 0.1083, 0.1216, 0.1088, 0.0949, 0.1134, 0.1054, 0.1195,\n",
      "        0.1026, 0.0804, 0.0765, 0.1082, 0.0959, 0.0993, 0.0924, 0.0975, 0.1080,\n",
      "        0.1092, 0.1130, 0.1208, 0.1034, 0.1040, 0.1136, 0.1058, 0.1003, 0.1160,\n",
      "        0.1049, 0.0813, 0.1286, 0.0975, 0.0948, 0.1256, 0.1024, 0.0828, 0.1136,\n",
      "        0.0612, 0.1220, 0.1004, 0.0981, 0.1269, 0.0850, 0.1175, 0.1060, 0.1017,\n",
      "        0.1044, 0.0990, 0.1034, 0.0995, 0.1080, 0.1089, 0.0902, 0.0957, 0.0923,\n",
      "        0.1157, 0.0954, 0.1001, 0.1053, 0.0870, 0.1223, 0.1046, 0.1118, 0.0919,\n",
      "        0.0932, 0.1106, 0.0957, 0.0903, 0.0880, 0.1180, 0.0923, 0.0382, 0.1042,\n",
      "        0.1045, 0.1302, 0.1020, 0.1274, 0.1139, 0.0850, 0.0913, 0.1004, 0.0927,\n",
      "        0.1069, 0.0903, 0.1108, 0.1052, 0.0889, 0.0977, 0.0939, 0.1222, 0.1334,\n",
      "        0.1240, 0.0917, 0.0991, 0.1240, 0.0985, 0.0939, 0.0931, 0.1087, 0.0876,\n",
      "        0.1056, 0.0988, 0.0999, 0.1160, 0.0919, 0.0830, 0.1120, 0.0938, 0.0990,\n",
      "        0.0998, 0.0953, 0.1085, 0.0974, 0.0853, 0.1051, 0.0982, 0.0480, 0.1191,\n",
      "        0.0412, 0.1065, 0.1127, 0.1017, 0.0988, 0.1153, 0.1066, 0.1247, 0.0929,\n",
      "        0.0898, 0.0847, 0.0979, 0.0950, 0.0943, 0.0962, 0.0895, 0.1021, 0.1267,\n",
      "        0.0972, 0.1168, 0.1009, 0.0897, 0.1003, 0.1081, 0.0843, 0.0816, 0.0973,\n",
      "        0.0968, 0.0990, 0.1155, 0.1120, 0.1061, 0.0824, 0.1145, 0.0900, 0.1071,\n",
      "        0.0862, 0.1093, 0.1120, 0.0998, 0.1248, 0.0983, 0.1096, 0.1126, 0.0861,\n",
      "        0.1014, 0.1040, 0.1074, 0.0954, 0.0899, 0.0900, 0.1059, 0.0915, 0.0921,\n",
      "        0.0976, 0.1210, 0.0878, 0.1086, 0.0771, 0.0959, 0.1071, 0.1074, 0.0978,\n",
      "        0.0989, 0.0967, 0.1128, 0.1147, 0.1127, 0.1053, 0.1041, 0.0884, 0.1248,\n",
      "        0.1178, 0.1035, 0.1119, 0.1216, 0.1289, 0.0958, 0.0980, 0.1095, 0.0941,\n",
      "        0.1087, 0.1134, 0.1116, 0.1176, 0.1127, 0.1066, 0.1134, 0.1017, 0.1068,\n",
      "        0.1138, 0.0879, 0.1074, 0.1168, 0.0857, 0.0929, 0.1051, 0.1556],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0473, -0.0177,  0.0026,  ..., -0.0172, -0.0433, -0.0961],\n",
      "        [ 0.0311,  0.0538, -0.0375,  ..., -0.0527,  0.0068,  0.0528],\n",
      "        [ 0.0368,  0.0123,  0.0267,  ..., -0.0356,  0.0123,  0.0259],\n",
      "        ...,\n",
      "        [ 0.0338, -0.0140, -0.0818,  ...,  0.0116,  0.0180, -0.0084],\n",
      "        [-0.0031,  0.0081, -0.0531,  ...,  0.0752, -0.0729,  0.0049],\n",
      "        [ 0.0337,  0.0280,  0.0086,  ...,  0.0144,  0.0927, -0.0310]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1494, -0.4322, -0.2078,  ...,  0.2598, -0.1284, -0.8323],\n",
      "        [-0.2843, -0.1816, -0.3293,  ..., -0.1709,  0.0128, -0.2631],\n",
      "        [ 0.0492,  0.3039, -0.1183,  ...,  0.3857,  0.2622,  0.4868],\n",
      "        ...,\n",
      "        [ 0.4058, -0.1648, -0.3139,  ..., -0.4439, -0.1390, -0.0355],\n",
      "        [-0.4127,  0.2171, -0.1127,  ...,  0.4185, -0.5342, -0.1289],\n",
      "        [ 0.0562, -0.0695, -0.4588,  ...,  0.1215,  0.3706, -0.7804]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.0740, -0.4373, -0.9284,  ..., -0.1299,  1.0930,  0.4162],\n",
      "        [-0.4464,  0.1344,  0.3330,  ..., -0.0817,  1.0759,  0.4249],\n",
      "        [ 0.3318,  0.1129,  0.7077,  ...,  0.4184,  0.9780, -0.0023],\n",
      "        ...,\n",
      "        [ 0.9539,  0.1063, -0.7730,  ...,  0.6771,  0.5631, -0.1355],\n",
      "        [-0.0201,  0.5249,  0.6993,  ..., -0.7504, -0.2457, -0.6319],\n",
      "        [ 0.1606, -0.0934,  0.0883,  ...,  0.2431,  0.4674,  0.5985]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0446,  1.0491,  0.7051,  ...,  0.6422,  1.7005,  0.1626],\n",
      "        [ 0.4761, -0.4403,  0.9399,  ...,  0.0733,  0.8879,  1.3128],\n",
      "        [-1.0142, -0.2458, -0.2651,  ...,  0.7902, -0.5625, -0.6014],\n",
      "        ...,\n",
      "        [-0.8939, -0.3425, -0.5974,  ...,  0.5155,  0.6102, -0.1421],\n",
      "        [ 0.7525, -0.3721,  0.7677,  ..., -0.3061,  0.5494,  0.6163],\n",
      "        [-0.4571, -0.5642, -0.1139,  ...,  0.4305,  0.5169,  0.0279]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1837, 0.1781, 0.1642, 0.1928, 0.1515, 0.1907, 0.0400, 0.1894, 0.1573,\n",
      "        0.1942, 0.1763, 0.0757, 0.1516, 0.1514, 0.1554, 0.2141, 0.1665, 0.1565,\n",
      "        0.1914, 0.1389, 0.1495, 0.1350, 0.1607, 0.1805, 0.1570, 0.1379, 0.1292,\n",
      "        0.1831, 0.1696, 0.1418, 0.1762, 0.1828, 0.2007, 0.1496, 0.1747, 0.1579,\n",
      "        0.1832, 0.1742, 0.1700, 0.1496, 0.1768, 0.1608, 0.1708, 0.1304, 0.1558,\n",
      "        0.1676, 0.1591, 0.2086, 0.1664, 0.1790, 0.1493, 0.1579, 0.1449, 0.2046,\n",
      "        0.1564, 0.2006, 0.1482, 0.1732, 0.1688, 0.1369, 0.1489, 0.1621, 0.2021,\n",
      "        0.1878, 0.1723, 0.1790, 0.1399, 0.1846, 0.1913, 0.1510, 0.1927, 0.1551,\n",
      "        0.1564, 0.1536, 0.1245, 0.1634, 0.1777, 0.1843, 0.0671, 0.1715, 0.1826,\n",
      "        0.1555, 0.1818, 0.1528, 0.1927, 0.1848, 0.1738, 0.1615, 0.1647, 0.1735,\n",
      "        0.1533, 0.1387, 0.1469, 0.1817, 0.2018, 0.1848, 0.1457, 0.1973, 0.1503,\n",
      "        0.1383, 0.1500, 0.1659, 0.1559, 0.1937, 0.1609, 0.1660, 0.2087, 0.1817,\n",
      "        0.1421, 0.1644, 0.1445, 0.1511, 0.1573, 0.1454, 0.1479, 0.1664, 0.1523,\n",
      "        0.1352, 0.1779, 0.1751, 0.1811, 0.1785, 0.1556, 0.1637, 0.1730, 0.1717,\n",
      "        0.1790, 0.1767, 0.1613, 0.1430, 0.1451, 0.1790, 0.1401, 0.1567, 0.1377,\n",
      "        0.1542, 0.0227, 0.1762, 0.1568, 0.1428, 0.1602, 0.2108, 0.1664, 0.1606,\n",
      "        0.1673, 0.2081, 0.1566, 0.1618, 0.1586, 0.1658, 0.1659, 0.1832, 0.1396,\n",
      "        0.1680, 0.1560, 0.1728, 0.1758, 0.1585, 0.1519, 0.1534, 0.1788, 0.1645,\n",
      "        0.1430, 0.0493, 0.1511, 0.1455, 0.1619, 0.1799, 0.1483, 0.1681, 0.2002,\n",
      "        0.1566, 0.1854, 0.1642, 0.1746, 0.1691, 0.1581, 0.1332, 0.1472, 0.2012,\n",
      "        0.1493, 0.1528, 0.1739, 0.1731, 0.1730, 0.1633, 0.1596, 0.1669, 0.1826,\n",
      "        0.1333, 0.1758, 0.1407, 0.1472, 0.1821, 0.1713, 0.1670, 0.1594, 0.1922,\n",
      "        0.1624, 0.1751, 0.1536, 0.1690, 0.1683, 0.1540, 0.1443, 0.1469, 0.1478,\n",
      "        0.1528, 0.1445, 0.1620, 0.1823, 0.1805, 0.1570, 0.1490, 0.1883, 0.1557,\n",
      "        0.1670, 0.1852, 0.1763, 0.1856, 0.1579, 0.1450, 0.1603, 0.1509, 0.1916,\n",
      "        0.1625, 0.1745, 0.1522, 0.1651, 0.1499, 0.1354, 0.1548, 0.1537, 0.1306,\n",
      "        0.1783, 0.1405, 0.1944, 0.1665, 0.1766, 0.1847, 0.1699, 0.1741, 0.1598,\n",
      "        0.1412, 0.1541, 0.1805, 0.1887, 0.1498, 0.1660, 0.1782, 0.1342, 0.1571,\n",
      "        0.1671, 0.1753, 0.1547, 0.1604, 0.1595, 0.1781, 0.1657, 0.1870, 0.1740,\n",
      "        0.1569, 0.1522, 0.1532, 0.1930, 0.1776, 0.1967, 0.1965, 0.1567, 0.1373,\n",
      "        0.1439, 0.1713, 0.1469, 0.1548, 0.1765, 0.1659, 0.1584, 0.1743, 0.1541,\n",
      "        0.1798, 0.1506, 0.1556, 0.1501, 0.1388, 0.1589, 0.1579, 0.1583, 0.1354,\n",
      "        0.1973, 0.1329, 0.1844, 0.1520, 0.1588, 0.1417, 0.1467, 0.1489, 0.1795,\n",
      "        0.1376, 0.1575, 0.1619, 0.1727, 0.1825, 0.1839, 0.1729, 0.1616, 0.1538,\n",
      "        0.1632, 0.1461, 0.1429, 0.1630, 0.1630, 0.1645, 0.1336, 0.1666, 0.1985,\n",
      "        0.1734, 0.1748, 0.1813, 0.1749, 0.1602, 0.1814, 0.1398, 0.1434, 0.1963,\n",
      "        0.1725, 0.1498, 0.1961, 0.1442, 0.1753, 0.1683, 0.1613, 0.1521, 0.1723,\n",
      "        0.0889, 0.1672, 0.1701, 0.1925, 0.1738, 0.1396, 0.1907, 0.1695, 0.1495,\n",
      "        0.1861, 0.1393, 0.1877, 0.1650, 0.1815, 0.1837, 0.1691, 0.1485, 0.1507,\n",
      "        0.1899, 0.1462, 0.1628, 0.1560, 0.1711, 0.1584, 0.1636, 0.1535, 0.1496,\n",
      "        0.1672, 0.1453, 0.1583, 0.1596, 0.1562, 0.1664, 0.1627, 0.0507, 0.1701,\n",
      "        0.2020, 0.1868, 0.1598, 0.1846, 0.1762, 0.1535, 0.1597, 0.1508, 0.1667,\n",
      "        0.1720, 0.1731, 0.2013, 0.1636, 0.1300, 0.1822, 0.1261, 0.1668, 0.1736,\n",
      "        0.1697, 0.1695, 0.1727, 0.1727, 0.1381, 0.1532, 0.1695, 0.1856, 0.1524,\n",
      "        0.1415, 0.1656, 0.1707, 0.1623, 0.1448, 0.1597, 0.2041, 0.1743, 0.1584,\n",
      "        0.1687, 0.1708, 0.1716, 0.1587, 0.1497, 0.1578, 0.1738, 0.0839, 0.1837,\n",
      "        0.0566, 0.1369, 0.1641, 0.1679, 0.1490, 0.1513, 0.1704, 0.1458, 0.1603,\n",
      "        0.1368, 0.1474, 0.1550, 0.1701, 0.1574, 0.1592, 0.1654, 0.1653, 0.1969,\n",
      "        0.1561, 0.1670, 0.1537, 0.1519, 0.1533, 0.1786, 0.1602, 0.1371, 0.1611,\n",
      "        0.1501, 0.1475, 0.1646, 0.1575, 0.1706, 0.1600, 0.2020, 0.1603, 0.1750,\n",
      "        0.1293, 0.1740, 0.1582, 0.1635, 0.1621, 0.1411, 0.1589, 0.1670, 0.1673,\n",
      "        0.1454, 0.1653, 0.1681, 0.1499, 0.1235, 0.1147, 0.1726, 0.1581, 0.1457,\n",
      "        0.1705, 0.1848, 0.1762, 0.1673, 0.1588, 0.1575, 0.1723, 0.1913, 0.1421,\n",
      "        0.1658, 0.1525, 0.1724, 0.1760, 0.1699, 0.1641, 0.1633, 0.1399, 0.1738,\n",
      "        0.1713, 0.1602, 0.1853, 0.1639, 0.1620, 0.1555, 0.1574, 0.1593, 0.1682,\n",
      "        0.1652, 0.1538, 0.1769, 0.1645, 0.1783, 0.1745, 0.1898, 0.1639, 0.1735,\n",
      "        0.1670, 0.1723, 0.1575, 0.1764, 0.1483, 0.1460, 0.1438, 0.1154],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0680,  0.2749, -0.5154,  ..., -0.0615, -0.5678, -0.3611],\n",
      "        [ 0.1589, -0.0613, -0.0046,  ...,  0.5456, -0.0442, -0.1149],\n",
      "        [-0.3558, -0.4199, -0.0908,  ...,  0.1644,  0.3364, -0.2178],\n",
      "        ...,\n",
      "        [-0.1859, -0.3423,  0.0155,  ..., -0.4501, -0.5582,  0.0391],\n",
      "        [-0.4196,  0.0975, -0.5475,  ..., -0.1047,  0.0402, -0.4569],\n",
      "        [-0.3467, -0.1722,  0.1123,  ...,  0.1687,  0.4282, -0.6055]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4540,  0.7358, -0.8461,  ...,  1.0282, -1.3165, -1.5986],\n",
      "        [ 0.3079, -0.3144, -0.0028,  ..., -0.5503,  1.0377,  0.5744],\n",
      "        [ 0.6974,  0.2464, -0.3488,  ...,  0.0344, -0.2274,  0.5738],\n",
      "        ...,\n",
      "        [ 1.5293, -1.2517, -1.8295,  ..., -1.9202,  1.7075,  0.8779],\n",
      "        [ 0.1571,  0.8450,  0.3841,  ..., -0.4294,  0.1022, -1.6089],\n",
      "        [ 0.3513,  0.6690,  0.2497,  ..., -1.1860,  0.3687, -0.3971]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6150,  0.2462,  0.7299,  ...,  0.4501, -0.0788,  0.1732],\n",
      "        [-0.4838, -0.1338,  0.1112,  ..., -0.5611, -0.7158, -0.0032],\n",
      "        [ 1.1326,  0.2684, -0.0744,  ...,  0.0164,  0.6649,  0.6033],\n",
      "        ...,\n",
      "        [ 0.0077,  1.0203, -1.2406,  ..., -0.1930,  0.0257,  0.4106],\n",
      "        [ 0.1287, -0.2001, -0.9950,  ..., -0.0511, -0.4645, -0.6618],\n",
      "        [ 0.0912,  0.0665,  0.1481,  ...,  0.0112, -0.2662, -0.0557]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1287, 0.1309, 0.1051, 0.1278, 0.1065, 0.1240, 0.0526, 0.1181, 0.1052,\n",
      "        0.1305, 0.1224, 0.0691, 0.1149, 0.0950, 0.0979, 0.1404, 0.1083, 0.0932,\n",
      "        0.1085, 0.1071, 0.0999, 0.0909, 0.1062, 0.0974, 0.0974, 0.0980, 0.0907,\n",
      "        0.1097, 0.1131, 0.1043, 0.1112, 0.1072, 0.1875, 0.0959, 0.1215, 0.1221,\n",
      "        0.1021, 0.1142, 0.1217, 0.1005, 0.0995, 0.1030, 0.1317, 0.0944, 0.1063,\n",
      "        0.1120, 0.1081, 0.1352, 0.1045, 0.1230, 0.0986, 0.1354, 0.0970, 0.1395,\n",
      "        0.1122, 0.1148, 0.1059, 0.1186, 0.1155, 0.0976, 0.1065, 0.1118, 0.1005,\n",
      "        0.1116, 0.1203, 0.0994, 0.0846, 0.1176, 0.1125, 0.1112, 0.1387, 0.1016,\n",
      "        0.1149, 0.1026, 0.1063, 0.1070, 0.1046, 0.1207, 0.0530, 0.1179, 0.1095,\n",
      "        0.1075, 0.1019, 0.1105, 0.1217, 0.1059, 0.1145, 0.1257, 0.1230, 0.1318,\n",
      "        0.1015, 0.0965, 0.1101, 0.1170, 0.1217, 0.1164, 0.1096, 0.1242, 0.1034,\n",
      "        0.1038, 0.1004, 0.1085, 0.1067, 0.1140, 0.1160, 0.0988, 0.1173, 0.1018,\n",
      "        0.1045, 0.1030, 0.0818, 0.1049, 0.1198, 0.0970, 0.1120, 0.1146, 0.0915,\n",
      "        0.1178, 0.1128, 0.1280, 0.0987, 0.0973, 0.1152, 0.1271, 0.1052, 0.1097,\n",
      "        0.0985, 0.1211, 0.1151, 0.0982, 0.1109, 0.1099, 0.1071, 0.0987, 0.0989,\n",
      "        0.0942, 0.2230, 0.1216, 0.0960, 0.1183, 0.1103, 0.1175, 0.0885, 0.1292,\n",
      "        0.1135, 0.1392, 0.1073, 0.1101, 0.1157, 0.1143, 0.1157, 0.1203, 0.1164,\n",
      "        0.1192, 0.1000, 0.0990, 0.1102, 0.1071, 0.1042, 0.1094, 0.1232, 0.1150,\n",
      "        0.1095, 0.0403, 0.1207, 0.1050, 0.1008, 0.1098, 0.1065, 0.1113, 0.1187,\n",
      "        0.0986, 0.1154, 0.1033, 0.1189, 0.1041, 0.1075, 0.0926, 0.1186, 0.1168,\n",
      "        0.1012, 0.0894, 0.1001, 0.0913, 0.1164, 0.1359, 0.0959, 0.1157, 0.1056,\n",
      "        0.0989, 0.1057, 0.0891, 0.1035, 0.1041, 0.0973, 0.0980, 0.1149, 0.1247,\n",
      "        0.1132, 0.1097, 0.1039, 0.1063, 0.0963, 0.1072, 0.1116, 0.1009, 0.0868,\n",
      "        0.1049, 0.1178, 0.1022, 0.1212, 0.0980, 0.1023, 0.1007, 0.1264, 0.1184,\n",
      "        0.1104, 0.1318, 0.1059, 0.1005, 0.0938, 0.1009, 0.1059, 0.1063, 0.0980,\n",
      "        0.1057, 0.1119, 0.0956, 0.1031, 0.1127, 0.0964, 0.0957, 0.0996, 0.1042,\n",
      "        0.1086, 0.1051, 0.1171, 0.1212, 0.0992, 0.1104, 0.1256, 0.1094, 0.1068,\n",
      "        0.0957, 0.1040, 0.1132, 0.0970, 0.1040, 0.1034, 0.1124, 0.0917, 0.1089,\n",
      "        0.0969, 0.1044, 0.0992, 0.1084, 0.1133, 0.1036, 0.1102, 0.1173, 0.0906,\n",
      "        0.0978, 0.1065, 0.1182, 0.1118, 0.1183, 0.1103, 0.1126, 0.1237, 0.0892,\n",
      "        0.1041, 0.1221, 0.1037, 0.0967, 0.1036, 0.1243, 0.1131, 0.1268, 0.0986,\n",
      "        0.1302, 0.0932, 0.1070, 0.0940, 0.1136, 0.1205, 0.1129, 0.1183, 0.1131,\n",
      "        0.1121, 0.1075, 0.1132, 0.1006, 0.1071, 0.1016, 0.1284, 0.0807, 0.1054,\n",
      "        0.0942, 0.1126, 0.1158, 0.1196, 0.1029, 0.1029, 0.1323, 0.1225, 0.1200,\n",
      "        0.0901, 0.1051, 0.0954, 0.1048, 0.1164, 0.1113, 0.0930, 0.1047, 0.1140,\n",
      "        0.1226, 0.1185, 0.1437, 0.1163, 0.1088, 0.1064, 0.1103, 0.1075, 0.1209,\n",
      "        0.1099, 0.0968, 0.1206, 0.1058, 0.0965, 0.1195, 0.1099, 0.0898, 0.1201,\n",
      "        0.0867, 0.1110, 0.0936, 0.1115, 0.1144, 0.0901, 0.1277, 0.0993, 0.1026,\n",
      "        0.1144, 0.0956, 0.1217, 0.0918, 0.1128, 0.1043, 0.1057, 0.1036, 0.1015,\n",
      "        0.1106, 0.1141, 0.1005, 0.1246, 0.0992, 0.1167, 0.0990, 0.1107, 0.1207,\n",
      "        0.0982, 0.1085, 0.1019, 0.0939, 0.0958, 0.1058, 0.1013, 0.0476, 0.1198,\n",
      "        0.1118, 0.1423, 0.1191, 0.1159, 0.1039, 0.0957, 0.1131, 0.0913, 0.0862,\n",
      "        0.1049, 0.1148, 0.1247, 0.1098, 0.0922, 0.0994, 0.0869, 0.1267, 0.1169,\n",
      "        0.1328, 0.1116, 0.1009, 0.1321, 0.1026, 0.1167, 0.1054, 0.1198, 0.1038,\n",
      "        0.1138, 0.1145, 0.1104, 0.1269, 0.0939, 0.1014, 0.1236, 0.1050, 0.0985,\n",
      "        0.1149, 0.1167, 0.1163, 0.0986, 0.1193, 0.1197, 0.1137, 0.0588, 0.1109,\n",
      "        0.0434, 0.0873, 0.1114, 0.1024, 0.1065, 0.1185, 0.1209, 0.1324, 0.0930,\n",
      "        0.1049, 0.1038, 0.1077, 0.1041, 0.1054, 0.1050, 0.1100, 0.1109, 0.1301,\n",
      "        0.1037, 0.1128, 0.1228, 0.1022, 0.1081, 0.1008, 0.1255, 0.0897, 0.0987,\n",
      "        0.1100, 0.0988, 0.1411, 0.1093, 0.1157, 0.0916, 0.1103, 0.1039, 0.1091,\n",
      "        0.0978, 0.1048, 0.0994, 0.1021, 0.1151, 0.1035, 0.1050, 0.1177, 0.1013,\n",
      "        0.1016, 0.1044, 0.1213, 0.0976, 0.0917, 0.0915, 0.1131, 0.1045, 0.0871,\n",
      "        0.1200, 0.1073, 0.1024, 0.1073, 0.0945, 0.1023, 0.1049, 0.1191, 0.1005,\n",
      "        0.1124, 0.0966, 0.1132, 0.1040, 0.1084, 0.1297, 0.1214, 0.1101, 0.1185,\n",
      "        0.1227, 0.1155, 0.1051, 0.1141, 0.1182, 0.0923, 0.1078, 0.0924, 0.1282,\n",
      "        0.1268, 0.1174, 0.1160, 0.1086, 0.1122, 0.0998, 0.1349, 0.0978, 0.1231,\n",
      "        0.1147, 0.1034, 0.1189, 0.1251, 0.1019, 0.0990, 0.1003, 0.1940],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0011,  0.0040, -0.0037,  ..., -0.0221, -0.0396,  0.0760],\n",
      "        [ 0.0522,  0.0315, -0.0604,  ..., -0.0016,  0.0357, -0.0941],\n",
      "        [ 0.0337, -0.0036, -0.0308,  ..., -0.0192,  0.0566,  0.0065],\n",
      "        ...,\n",
      "        [ 0.0727, -0.0107, -0.0180,  ..., -0.0442,  0.0489, -0.0332],\n",
      "        [ 0.0464,  0.0274, -0.0201,  ...,  0.0124,  0.0196, -0.0303],\n",
      "        [ 0.0139, -0.0021, -0.0304,  ..., -0.0264,  0.0288,  0.0425]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-3.0559e-01,  6.9144e-02, -3.1269e-02,  ..., -2.8613e-01,\n",
      "          5.9419e-02,  1.1356e-01],\n",
      "        [-3.4758e-01,  2.1341e-01, -8.9794e-02,  ..., -2.7132e-01,\n",
      "         -7.8254e-02, -3.7422e-01],\n",
      "        [-1.7419e-01, -1.3741e-01,  4.8154e-01,  ...,  4.1039e-01,\n",
      "         -1.1951e-01, -7.6392e-02],\n",
      "        ...,\n",
      "        [-2.6908e-01, -6.0931e-04,  2.8513e-01,  ..., -3.4490e-01,\n",
      "          8.1538e-02, -3.3409e-01],\n",
      "        [-3.3349e-01, -5.3310e-01, -1.1320e-01,  ...,  2.0604e-01,\n",
      "         -2.4308e-01, -3.0924e-01],\n",
      "        [ 1.0669e-01, -2.5091e-01,  1.3618e-01,  ...,  6.5679e-01,\n",
      "          9.5111e-01, -4.1710e-01]], device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.0097,  0.4970,  1.2019,  ...,  0.4512, -0.1300, -0.8582],\n",
      "        [ 0.1112,  0.2638,  0.4608,  ..., -0.1616, -1.2150, -0.5843],\n",
      "        [ 0.4653,  0.0073,  0.4839,  ..., -0.2731,  1.2784, -0.2364],\n",
      "        ...,\n",
      "        [ 0.4084,  0.5944, -0.5270,  ...,  0.2231,  0.4075,  0.6669],\n",
      "        [-0.4485,  0.7430,  0.9547,  ...,  0.2937,  0.2521,  0.0571],\n",
      "        [ 0.8122, -1.1261, -0.0814,  ...,  0.8005, -0.0732, -0.2400]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6647, -0.5458, -1.0302,  ..., -0.7705,  0.0825,  0.0298],\n",
      "        [-0.9210, -0.3174,  0.1361,  ...,  0.3328, -1.4094,  0.8164],\n",
      "        [ 0.2103, -0.4666, -0.3962,  ..., -0.9339, -0.1969, -0.2711],\n",
      "        ...,\n",
      "        [-0.3676,  0.7744, -0.4208,  ...,  0.2122, -0.4684,  0.1387],\n",
      "        [ 0.9145,  0.2543,  0.9861,  ..., -0.5736,  0.1002, -0.0475],\n",
      "        [ 1.2070,  0.2760, -0.1737,  ..., -0.4172,  0.2347,  0.0406]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1831, 0.1646, 0.1540, 0.1687, 0.1685, 0.1580, 0.0395, 0.1764, 0.1413,\n",
      "        0.1678, 0.1892, 0.0914, 0.1633, 0.1348, 0.1636, 0.2133, 0.1597, 0.1471,\n",
      "        0.1422, 0.1529, 0.1466, 0.1205, 0.1614, 0.1833, 0.1709, 0.1559, 0.1547,\n",
      "        0.1591, 0.1581, 0.1492, 0.1617, 0.1458, 0.2043, 0.1558, 0.1569, 0.1822,\n",
      "        0.1697, 0.1628, 0.1926, 0.1403, 0.1493, 0.1398, 0.1637, 0.1632, 0.1706,\n",
      "        0.1681, 0.1635, 0.1911, 0.1563, 0.1884, 0.1650, 0.1663, 0.1706, 0.1919,\n",
      "        0.1658, 0.1667, 0.1604, 0.1617, 0.1543, 0.1274, 0.1463, 0.1423, 0.1669,\n",
      "        0.1665, 0.2011, 0.1827, 0.1461, 0.1423, 0.1840, 0.1473, 0.1633, 0.1585,\n",
      "        0.1803, 0.1759, 0.1385, 0.1541, 0.1737, 0.1737, 0.0667, 0.1718, 0.1726,\n",
      "        0.1621, 0.1898, 0.1965, 0.1664, 0.1691, 0.1570, 0.1670, 0.1568, 0.1532,\n",
      "        0.1520, 0.1518, 0.1688, 0.1783, 0.1947, 0.1611, 0.1637, 0.1609, 0.1636,\n",
      "        0.1437, 0.1586, 0.1532, 0.1508, 0.1602, 0.1744, 0.1499, 0.1683, 0.1705,\n",
      "        0.1259, 0.1487, 0.1505, 0.1546, 0.1645, 0.1415, 0.1586, 0.1574, 0.1359,\n",
      "        0.1474, 0.1763, 0.1951, 0.1690, 0.1814, 0.1382, 0.1693, 0.1532, 0.1444,\n",
      "        0.1504, 0.1894, 0.1660, 0.1468, 0.1387, 0.1524, 0.1504, 0.1615, 0.1575,\n",
      "        0.1807, 0.0067, 0.1762, 0.1663, 0.1927, 0.1566, 0.1663, 0.1630, 0.1684,\n",
      "        0.1816, 0.1797, 0.1442, 0.1458, 0.1520, 0.1546, 0.1597, 0.2016, 0.1628,\n",
      "        0.1313, 0.1583, 0.1552, 0.1346, 0.1581, 0.1582, 0.1572, 0.1801, 0.1323,\n",
      "        0.1568, 0.0412, 0.1595, 0.1677, 0.1799, 0.1777, 0.1450, 0.1655, 0.1634,\n",
      "        0.1435, 0.1496, 0.1687, 0.1751, 0.1543, 0.1796, 0.1515, 0.1829, 0.1809,\n",
      "        0.1307, 0.1468, 0.1593, 0.1639, 0.1641, 0.1540, 0.1454, 0.1652, 0.1996,\n",
      "        0.1518, 0.1746, 0.0956, 0.1565, 0.1899, 0.1424, 0.1611, 0.1957, 0.1791,\n",
      "        0.1664, 0.1351, 0.1488, 0.1501, 0.1578, 0.1436, 0.1805, 0.1685, 0.1651,\n",
      "        0.1323, 0.1626, 0.1454, 0.1739, 0.1589, 0.1509, 0.1637, 0.1681, 0.1781,\n",
      "        0.1832, 0.1720, 0.1524, 0.1866, 0.1483, 0.1616, 0.1656, 0.1395, 0.1730,\n",
      "        0.1718, 0.1879, 0.1729, 0.1428, 0.1743, 0.1189, 0.1715, 0.1375, 0.1429,\n",
      "        0.1624, 0.1489, 0.1847, 0.1574, 0.1776, 0.1672, 0.1714, 0.1559, 0.1631,\n",
      "        0.1524, 0.1335, 0.1761, 0.1630, 0.1537, 0.1627, 0.1637, 0.1703, 0.1667,\n",
      "        0.1590, 0.1682, 0.1461, 0.1681, 0.1642, 0.1587, 0.1703, 0.1790, 0.1820,\n",
      "        0.1514, 0.1499, 0.1783, 0.1859, 0.1832, 0.1723, 0.1683, 0.1526, 0.1393,\n",
      "        0.1542, 0.1562, 0.1659, 0.1586, 0.1620, 0.1617, 0.1688, 0.1773, 0.1379,\n",
      "        0.1774, 0.1357, 0.1687, 0.1476, 0.1676, 0.1529, 0.1798, 0.1623, 0.1729,\n",
      "        0.1806, 0.1321, 0.1875, 0.1557, 0.1642, 0.1569, 0.1457, 0.1807, 0.1689,\n",
      "        0.1440, 0.1432, 0.1473, 0.1540, 0.1669, 0.1562, 0.1712, 0.1645, 0.1676,\n",
      "        0.1639, 0.1393, 0.1484, 0.1475, 0.1570, 0.1912, 0.1402, 0.1530, 0.1786,\n",
      "        0.1753, 0.1780, 0.1778, 0.1547, 0.1693, 0.1613, 0.1626, 0.1540, 0.1955,\n",
      "        0.1480, 0.1735, 0.1818, 0.1631, 0.1686, 0.1460, 0.1404, 0.1795, 0.1620,\n",
      "        0.0943, 0.1502, 0.1826, 0.1732, 0.1843, 0.1415, 0.1896, 0.1538, 0.1795,\n",
      "        0.1558, 0.1329, 0.1857, 0.1624, 0.1620, 0.1865, 0.1672, 0.1616, 0.1556,\n",
      "        0.1664, 0.1474, 0.1538, 0.1598, 0.1830, 0.1497, 0.1459, 0.1626, 0.1625,\n",
      "        0.1756, 0.1765, 0.1416, 0.1485, 0.1558, 0.1590, 0.1392, 0.0507, 0.1714,\n",
      "        0.1964, 0.0940, 0.1784, 0.1816, 0.1514, 0.1262, 0.1531, 0.1525, 0.1394,\n",
      "        0.1492, 0.1517, 0.1710, 0.1651, 0.1675, 0.1743, 0.1413, 0.1768, 0.1861,\n",
      "        0.1503, 0.1714, 0.1521, 0.1793, 0.1346, 0.1826, 0.1680, 0.1836, 0.1581,\n",
      "        0.1651, 0.1699, 0.1579, 0.1727, 0.1436, 0.1477, 0.1659, 0.1535, 0.1583,\n",
      "        0.1617, 0.1634, 0.1655, 0.1519, 0.1762, 0.1634, 0.1549, 0.0695, 0.1759,\n",
      "        0.0646, 0.1595, 0.1744, 0.1763, 0.1350, 0.1522, 0.1853, 0.1586, 0.1794,\n",
      "        0.1644, 0.1632, 0.1510, 0.1835, 0.1822, 0.1493, 0.1470, 0.1618, 0.1915,\n",
      "        0.1394, 0.1634, 0.1433, 0.1542, 0.1445, 0.1463, 0.1743, 0.1531, 0.1727,\n",
      "        0.1630, 0.1584, 0.1598, 0.1642, 0.1556, 0.1318, 0.2185, 0.1375, 0.1763,\n",
      "        0.1384, 0.1635, 0.1513, 0.1604, 0.1601, 0.1516, 0.1795, 0.1824, 0.1931,\n",
      "        0.1376, 0.1720, 0.1738, 0.1353, 0.1585, 0.1433, 0.1827, 0.1525, 0.1314,\n",
      "        0.1703, 0.1964, 0.1500, 0.1592, 0.1512, 0.1487, 0.1935, 0.1780, 0.1547,\n",
      "        0.1501, 0.1666, 0.1586, 0.1383, 0.1569, 0.1457, 0.1600, 0.1523, 0.1628,\n",
      "        0.1568, 0.1612, 0.1588, 0.1907, 0.1691, 0.1508, 0.1685, 0.1857, 0.1647,\n",
      "        0.1783, 0.1701, 0.2144, 0.1768, 0.1713, 0.1626, 0.1734, 0.1596, 0.1779,\n",
      "        0.1765, 0.1613, 0.1462, 0.1438, 0.1542, 0.1471, 0.1495, 0.1336],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1569, -0.0286,  0.0791,  ...,  0.2823, -0.1995, -0.0021],\n",
      "        [ 0.0887,  0.0462,  0.3156,  ...,  0.2550,  0.0487,  0.0543],\n",
      "        [-0.4480, -0.1278, -0.0461,  ..., -0.3398, -0.3032, -0.1645],\n",
      "        ...,\n",
      "        [-0.0152, -0.2274,  0.2960,  ...,  0.0739,  0.2603, -0.1600],\n",
      "        [-0.2454, -0.2956, -0.4338,  ..., -0.0642, -0.1829, -0.1071],\n",
      "        [-0.1973, -0.2066,  0.2670,  ..., -0.2712, -0.3218, -0.1704]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-2.5376e-01,  6.3879e-01,  2.0366e+00,  ..., -6.2669e-01,\n",
      "         -6.8899e-01, -7.3862e-01],\n",
      "        [-8.9018e-01, -5.7037e-01, -1.0488e+00,  ..., -2.2550e-01,\n",
      "          4.8352e-01,  2.3742e-01],\n",
      "        [-6.1522e-05,  3.5137e-01,  1.2698e+00,  ..., -2.1555e-01,\n",
      "          1.2212e+00, -5.6952e-01],\n",
      "        ...,\n",
      "        [-1.5075e+00,  3.0817e-01, -3.5964e-01,  ...,  6.9972e-02,\n",
      "         -1.5990e-01, -9.3732e-01],\n",
      "        [-1.0596e+00,  3.9012e-01,  4.8973e-01,  ..., -2.2157e-01,\n",
      "          5.8800e-01, -1.8010e+00],\n",
      "        [-4.3956e-01,  7.0633e-02, -2.1072e-01,  ..., -7.1526e-01,\n",
      "         -2.4698e+00,  6.3616e-01]], device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.5503, -0.4774,  0.2102,  ..., -0.3132,  0.1947, -0.6133],\n",
      "        [-0.2086,  0.8505, -0.3341,  ..., -0.3079, -0.9583, -0.5267],\n",
      "        [-0.1811,  0.1452, -0.0742,  ..., -1.1945,  0.0128,  0.0017],\n",
      "        ...,\n",
      "        [-0.7517,  0.9914,  0.5364,  ...,  0.1681, -0.2565, -0.4219],\n",
      "        [ 0.1678, -0.2481,  0.3416,  ..., -1.3714,  0.6623, -0.0618],\n",
      "        [ 0.0478, -0.2717, -0.0284,  ...,  0.9039,  0.9469,  0.2514]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1342, 0.1387, 0.1202, 0.1158, 0.1200, 0.1049, 0.0583, 0.1162, 0.1161,\n",
      "        0.1319, 0.1373, 0.0641, 0.1082, 0.1049, 0.1227, 0.1575, 0.1086, 0.1230,\n",
      "        0.1238, 0.1112, 0.1031, 0.1108, 0.1198, 0.1125, 0.1320, 0.1098, 0.1194,\n",
      "        0.1153, 0.1079, 0.1145, 0.1215, 0.1203, 0.2449, 0.1377, 0.1124, 0.1139,\n",
      "        0.0974, 0.1072, 0.1236, 0.0998, 0.1266, 0.1215, 0.1246, 0.1123, 0.1119,\n",
      "        0.1159, 0.1243, 0.1295, 0.1106, 0.1242, 0.1145, 0.1222, 0.1147, 0.1253,\n",
      "        0.1060, 0.1262, 0.1024, 0.1150, 0.1123, 0.1049, 0.1276, 0.1153, 0.1254,\n",
      "        0.1153, 0.1307, 0.1135, 0.1128, 0.1151, 0.1140, 0.1063, 0.1308, 0.0936,\n",
      "        0.1043, 0.1100, 0.0947, 0.1198, 0.1128, 0.1138, 0.0606, 0.1308, 0.1112,\n",
      "        0.1068, 0.1232, 0.1164, 0.1273, 0.1146, 0.1433, 0.1148, 0.1529, 0.1321,\n",
      "        0.1122, 0.1049, 0.1136, 0.1142, 0.1209, 0.1277, 0.1166, 0.1252, 0.1139,\n",
      "        0.1203, 0.1061, 0.1222, 0.1150, 0.1193, 0.1339, 0.1169, 0.1273, 0.1041,\n",
      "        0.1114, 0.1210, 0.1087, 0.1057, 0.1155, 0.1076, 0.1151, 0.1303, 0.1057,\n",
      "        0.1248, 0.1088, 0.1099, 0.1097, 0.1201, 0.1295, 0.1211, 0.1130, 0.1182,\n",
      "        0.1117, 0.1268, 0.1163, 0.1340, 0.1327, 0.1150, 0.1140, 0.1070, 0.1168,\n",
      "        0.1157, 0.5934, 0.1365, 0.1064, 0.1131, 0.1104, 0.1175, 0.1075, 0.1222,\n",
      "        0.1175, 0.1204, 0.1091, 0.1085, 0.1020, 0.1233, 0.1062, 0.1225, 0.1203,\n",
      "        0.1081, 0.1088, 0.1233, 0.1201, 0.1000, 0.1346, 0.1276, 0.1439, 0.1361,\n",
      "        0.1166, 0.0351, 0.1041, 0.1123, 0.1073, 0.1156, 0.1150, 0.1319, 0.1219,\n",
      "        0.0996, 0.1320, 0.0956, 0.1366, 0.1100, 0.1074, 0.1135, 0.1211, 0.1205,\n",
      "        0.1187, 0.1123, 0.1048, 0.1065, 0.1109, 0.1161, 0.0961, 0.1181, 0.1160,\n",
      "        0.0972, 0.1186, 0.0859, 0.1029, 0.1226, 0.1143, 0.1081, 0.1242, 0.1117,\n",
      "        0.1194, 0.1118, 0.1047, 0.1140, 0.1161, 0.1184, 0.1372, 0.1277, 0.1108,\n",
      "        0.1087, 0.1199, 0.1038, 0.1127, 0.1055, 0.1265, 0.1130, 0.1346, 0.1146,\n",
      "        0.1261, 0.1433, 0.1225, 0.1278, 0.1032, 0.1105, 0.1135, 0.1150, 0.1101,\n",
      "        0.1146, 0.1076, 0.1189, 0.1140, 0.1222, 0.1118, 0.1056, 0.1069, 0.0981,\n",
      "        0.1066, 0.1136, 0.1280, 0.1317, 0.1100, 0.1080, 0.1173, 0.1154, 0.1051,\n",
      "        0.1015, 0.1317, 0.1267, 0.1118, 0.1183, 0.1304, 0.1336, 0.1355, 0.1090,\n",
      "        0.1162, 0.1142, 0.0950, 0.1157, 0.1211, 0.1071, 0.1197, 0.1174, 0.1181,\n",
      "        0.0986, 0.1300, 0.1184, 0.1100, 0.1259, 0.1185, 0.1171, 0.1267, 0.1125,\n",
      "        0.1180, 0.1130, 0.1074, 0.1134, 0.1062, 0.1214, 0.1054, 0.1194, 0.1010,\n",
      "        0.1256, 0.1030, 0.1143, 0.1039, 0.1182, 0.1123, 0.1156, 0.1073, 0.1144,\n",
      "        0.1162, 0.1204, 0.1229, 0.1195, 0.1065, 0.1150, 0.1229, 0.1055, 0.1245,\n",
      "        0.1203, 0.1101, 0.1082, 0.1212, 0.1128, 0.1268, 0.1215, 0.1140, 0.1237,\n",
      "        0.1132, 0.1251, 0.1069, 0.1083, 0.1079, 0.1267, 0.0884, 0.1225, 0.1230,\n",
      "        0.1233, 0.1037, 0.1338, 0.1319, 0.1318, 0.1209, 0.1111, 0.0989, 0.1159,\n",
      "        0.1388, 0.1148, 0.1247, 0.1111, 0.1252, 0.1177, 0.1133, 0.1175, 0.1130,\n",
      "        0.0931, 0.1120, 0.1134, 0.1095, 0.1312, 0.1017, 0.1526, 0.1108, 0.1351,\n",
      "        0.1173, 0.1215, 0.1135, 0.1227, 0.1248, 0.1190, 0.1165, 0.1145, 0.1096,\n",
      "        0.1181, 0.1209, 0.1043, 0.1245, 0.1286, 0.1203, 0.1177, 0.1234, 0.1323,\n",
      "        0.1185, 0.1270, 0.1154, 0.1165, 0.1136, 0.1122, 0.1176, 0.0434, 0.1129,\n",
      "        0.1424, 0.1599, 0.1210, 0.1103, 0.1184, 0.1204, 0.1214, 0.1101, 0.1052,\n",
      "        0.1169, 0.1190, 0.1080, 0.0988, 0.1067, 0.1204, 0.1165, 0.1208, 0.1231,\n",
      "        0.1493, 0.1193, 0.1229, 0.1312, 0.1263, 0.1059, 0.1190, 0.1152, 0.1132,\n",
      "        0.1305, 0.1155, 0.1100, 0.1014, 0.1076, 0.1216, 0.1099, 0.1181, 0.1097,\n",
      "        0.1175, 0.1147, 0.1176, 0.1157, 0.1212, 0.1384, 0.1109, 0.0632, 0.1099,\n",
      "        0.0716, 0.1155, 0.1193, 0.1265, 0.1159, 0.1179, 0.1131, 0.1314, 0.1073,\n",
      "        0.1142, 0.1211, 0.1201, 0.1232, 0.1030, 0.1061, 0.1083, 0.1190, 0.1174,\n",
      "        0.1127, 0.1078, 0.1231, 0.1090, 0.1245, 0.1118, 0.1195, 0.1112, 0.1148,\n",
      "        0.1142, 0.1218, 0.1133, 0.1175, 0.1340, 0.1124, 0.1452, 0.1051, 0.1384,\n",
      "        0.1053, 0.0994, 0.1093, 0.1236, 0.1127, 0.1067, 0.1284, 0.1408, 0.1263,\n",
      "        0.1165, 0.1100, 0.1228, 0.1026, 0.1206, 0.0888, 0.1168, 0.1084, 0.1300,\n",
      "        0.1110, 0.1403, 0.1018, 0.1151, 0.1216, 0.1128, 0.1170, 0.1227, 0.1238,\n",
      "        0.1272, 0.1228, 0.1139, 0.1298, 0.1199, 0.1074, 0.1125, 0.1189, 0.1127,\n",
      "        0.1325, 0.1159, 0.1209, 0.1132, 0.1288, 0.1174, 0.1151, 0.1294, 0.1117,\n",
      "        0.1328, 0.1134, 0.1206, 0.1219, 0.1089, 0.1111, 0.1477, 0.1064, 0.1123,\n",
      "        0.1446, 0.1128, 0.1085, 0.1468, 0.1040, 0.0968, 0.1141, 0.3619],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0125,  0.0041, -0.0122,  ...,  0.0239, -0.0269,  0.0575],\n",
      "        [ 0.0191,  0.0460,  0.0070,  ..., -0.0082,  0.0186, -0.0098],\n",
      "        [ 0.0385,  0.0194, -0.0025,  ..., -0.0129, -0.0545, -0.0163],\n",
      "        ...,\n",
      "        [-0.0118, -0.0038,  0.0131,  ..., -0.0117,  0.0165,  0.0204],\n",
      "        [ 0.0086,  0.0107,  0.0105,  ...,  0.0129,  0.0327,  0.0130],\n",
      "        [ 0.0200,  0.0344, -0.0001,  ...,  0.0077, -0.0365,  0.0297]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0107,  0.1076,  0.2294,  ..., -0.1018,  0.2580,  0.0560],\n",
      "        [-0.1412, -0.0559, -0.1105,  ...,  0.1003,  0.0083, -0.1781],\n",
      "        [ 0.3120,  0.2932, -0.0680,  ..., -0.0874,  0.1189,  0.3697],\n",
      "        ...,\n",
      "        [ 0.2724,  0.0658, -0.0051,  ...,  0.0996,  0.0840, -0.0545],\n",
      "        [-0.1281, -0.0166, -0.2829,  ..., -0.1432,  0.1079,  0.0797],\n",
      "        [ 0.1641, -0.0735, -0.1345,  ...,  0.0602, -0.2746,  0.3301]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.8221, -0.2228,  0.7395,  ...,  0.8704, -0.9625,  0.6067],\n",
      "        [-0.5532, -0.5289, -1.1831,  ..., -1.0486, -0.7974, -0.9680],\n",
      "        [ 1.1318, -0.5695,  0.8636,  ..., -0.3706, -0.0096,  1.1986],\n",
      "        ...,\n",
      "        [ 0.0123,  0.0411, -1.0296,  ...,  0.6140,  0.0148,  1.4800],\n",
      "        [ 0.9247, -0.3009, -1.7937,  ...,  0.4735, -0.9152,  0.7106],\n",
      "        [ 0.9593,  0.6251,  0.0674,  ...,  0.3277,  1.2294,  0.3186]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-7.5694e-01,  5.3843e-01, -7.8724e-01,  ..., -3.5190e-02,\n",
      "         -5.3759e-01,  2.9016e-01],\n",
      "        [ 2.4182e-01, -7.6502e-01,  5.2064e-01,  ..., -2.2932e+00,\n",
      "         -1.7582e+00, -2.1654e+00],\n",
      "        [ 5.8982e-01,  1.2127e+00, -2.1370e-01,  ...,  1.1815e+00,\n",
      "         -3.3060e-01,  7.1322e-01],\n",
      "        ...,\n",
      "        [ 1.1384e+00, -1.1543e+00,  1.7920e-01,  ..., -8.2258e-01,\n",
      "         -2.9430e-02, -3.0750e-01],\n",
      "        [ 2.7840e-01,  2.9011e-01, -9.2940e-01,  ..., -1.0484e-03,\n",
      "         -6.3321e-01,  1.3407e+00],\n",
      "        [ 5.2302e-01, -8.0435e-02,  2.3943e-01,  ...,  4.9415e-01,\n",
      "         -9.9160e-01, -3.3744e-01]], device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.2249, 0.1879, 0.1832, 0.1576, 0.1854, 0.1959, 0.0206, 0.2055, 0.1951,\n",
      "        0.1767, 0.2137, 0.1013, 0.2091, 0.1782, 0.1842, 0.2288, 0.1946, 0.2017,\n",
      "        0.1824, 0.1822, 0.1836, 0.1559, 0.2038, 0.2058, 0.1790, 0.1951, 0.1789,\n",
      "        0.1961, 0.2022, 0.1904, 0.1757, 0.1538, 0.1824, 0.1910, 0.1818, 0.1998,\n",
      "        0.1848, 0.1806, 0.2089, 0.1796, 0.1938, 0.1782, 0.1964, 0.1766, 0.1914,\n",
      "        0.1799, 0.1824, 0.1962, 0.1902, 0.1943, 0.1992, 0.1955, 0.2094, 0.2297,\n",
      "        0.1839, 0.2028, 0.1789, 0.1878, 0.1816, 0.1600, 0.1980, 0.2022, 0.2066,\n",
      "        0.1779, 0.1740, 0.2146, 0.1907, 0.1692, 0.1515, 0.1824, 0.1721, 0.1607,\n",
      "        0.1956, 0.2018, 0.1605, 0.1835, 0.2072, 0.1970, 0.0379, 0.2028, 0.2198,\n",
      "        0.1840, 0.1989, 0.2430, 0.2087, 0.2068, 0.1820, 0.2180, 0.2320, 0.1573,\n",
      "        0.1600, 0.2058, 0.1953, 0.2096, 0.2112, 0.2054, 0.1956, 0.1926, 0.1664,\n",
      "        0.2103, 0.1995, 0.1864, 0.1706, 0.1888, 0.2138, 0.1905, 0.2116, 0.1816,\n",
      "        0.1755, 0.1801, 0.1754, 0.1932, 0.1652, 0.1853, 0.1798, 0.1874, 0.1728,\n",
      "        0.1988, 0.2041, 0.2009, 0.1848, 0.2160, 0.1843, 0.1879, 0.1752, 0.1763,\n",
      "        0.1921, 0.2277, 0.2012, 0.1959, 0.1679, 0.1655, 0.1776, 0.1809, 0.1897,\n",
      "        0.2159, 0.0323, 0.2256, 0.2062, 0.1995, 0.1966, 0.2225, 0.1859, 0.1896,\n",
      "        0.2246, 0.2120, 0.1823, 0.1868, 0.2130, 0.1709, 0.1710, 0.2531, 0.2150,\n",
      "        0.1596, 0.1936, 0.1859, 0.1493, 0.1925, 0.1841, 0.1739, 0.1956, 0.1581,\n",
      "        0.1802, 0.0626, 0.1648, 0.1804, 0.2021, 0.2268, 0.1565, 0.1985, 0.2141,\n",
      "        0.1908, 0.1949, 0.1868, 0.1773, 0.1909, 0.1890, 0.1831, 0.2169, 0.1746,\n",
      "        0.1559, 0.2066, 0.1850, 0.2084, 0.1964, 0.1967, 0.1117, 0.2039, 0.2041,\n",
      "        0.1710, 0.2020, 0.0764, 0.1865, 0.2018, 0.1769, 0.1962, 0.1971, 0.2069,\n",
      "        0.2201, 0.1572, 0.1838, 0.1806, 0.2040, 0.1878, 0.1869, 0.1860, 0.2001,\n",
      "        0.1843, 0.1888, 0.1733, 0.2073, 0.1954, 0.1793, 0.1877, 0.2004, 0.1726,\n",
      "        0.2138, 0.1951, 0.2053, 0.1998, 0.1730, 0.1870, 0.1924, 0.1922, 0.2374,\n",
      "        0.1924, 0.2060, 0.1908, 0.1906, 0.1826, 0.1814, 0.1850, 0.1728, 0.1700,\n",
      "        0.1708, 0.1743, 0.2018, 0.1799, 0.1705, 0.1803, 0.2032, 0.2043, 0.1975,\n",
      "        0.1761, 0.1637, 0.1781, 0.1793, 0.1908, 0.1850, 0.1751, 0.1930, 0.1792,\n",
      "        0.1792, 0.2030, 0.1561, 0.2068, 0.1791, 0.1914, 0.1802, 0.2127, 0.2141,\n",
      "        0.1923, 0.1839, 0.1697, 0.2000, 0.2156, 0.2026, 0.1939, 0.2044, 0.1760,\n",
      "        0.1857, 0.1948, 0.2069, 0.1675, 0.1853, 0.1943, 0.1991, 0.2004, 0.1600,\n",
      "        0.1910, 0.1851, 0.2051, 0.1859, 0.1630, 0.1971, 0.2107, 0.1751, 0.1854,\n",
      "        0.1964, 0.1602, 0.1751, 0.1959, 0.1793, 0.1969, 0.1870, 0.2154, 0.2032,\n",
      "        0.1836, 0.1940, 0.1805, 0.2013, 0.1985, 0.1891, 0.2024, 0.1783, 0.1765,\n",
      "        0.1871, 0.1863, 0.2086, 0.1721, 0.1903, 0.2213, 0.1836, 0.1898, 0.2010,\n",
      "        0.2093, 0.1984, 0.2240, 0.1737, 0.2167, 0.1783, 0.1951, 0.1836, 0.1986,\n",
      "        0.2081, 0.1944, 0.1980, 0.2087, 0.1786, 0.1935, 0.1566, 0.2038, 0.1988,\n",
      "        0.1268, 0.1760, 0.2070, 0.2344, 0.2079, 0.1708, 0.1989, 0.1815, 0.2074,\n",
      "        0.2047, 0.1807, 0.2011, 0.1766, 0.2155, 0.2010, 0.1731, 0.1779, 0.1988,\n",
      "        0.2116, 0.1950, 0.1772, 0.1876, 0.2026, 0.1834, 0.2030, 0.1817, 0.1812,\n",
      "        0.2078, 0.1953, 0.1804, 0.1601, 0.1940, 0.1950, 0.1853, 0.0397, 0.2466,\n",
      "        0.1895, 0.0184, 0.2154, 0.2013, 0.1807, 0.1860, 0.1757, 0.1860, 0.1638,\n",
      "        0.1731, 0.1803, 0.1956, 0.1766, 0.1965, 0.2176, 0.1638, 0.1849, 0.2147,\n",
      "        0.1850, 0.2122, 0.1748, 0.2007, 0.1983, 0.2106, 0.1877, 0.2053, 0.1810,\n",
      "        0.1647, 0.1877, 0.1833, 0.1849, 0.1672, 0.1955, 0.1749, 0.1873, 0.1716,\n",
      "        0.1978, 0.1739, 0.2042, 0.1763, 0.1955, 0.1920, 0.1990, 0.0765, 0.2017,\n",
      "        0.0910, 0.1747, 0.1804, 0.2104, 0.1935, 0.1955, 0.2079, 0.1200, 0.2041,\n",
      "        0.1893, 0.2129, 0.1795, 0.1827, 0.1946, 0.1780, 0.1795, 0.1884, 0.1992,\n",
      "        0.1347, 0.1939, 0.1555, 0.1909, 0.1585, 0.1964, 0.1833, 0.1753, 0.1928,\n",
      "        0.1777, 0.2077, 0.1559, 0.1980, 0.2080, 0.1777, 0.2292, 0.1998, 0.2205,\n",
      "        0.1901, 0.1662, 0.2208, 0.2006, 0.1510, 0.1671, 0.2148, 0.2180, 0.2246,\n",
      "        0.1701, 0.1889, 0.2098, 0.1648, 0.2127, 0.1691, 0.1848, 0.1861, 0.1985,\n",
      "        0.1978, 0.2296, 0.1944, 0.1651, 0.1929, 0.1914, 0.2089, 0.1882, 0.1922,\n",
      "        0.1985, 0.2062, 0.1646, 0.1781, 0.2031, 0.1673, 0.1809, 0.1919, 0.1614,\n",
      "        0.1818, 0.2004, 0.1722, 0.1896, 0.2001, 0.1947, 0.1780, 0.2176, 0.1854,\n",
      "        0.1993, 0.1723, 0.2014, 0.1861, 0.2485, 0.1843, 0.2186, 0.1930, 0.1907,\n",
      "        0.1891, 0.1887, 0.1700, 0.1891, 0.1891, 0.1697, 0.1867, 0.1031],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1540, -0.0403,  0.0916,  ..., -0.0046,  0.0454,  0.0560],\n",
      "        [-0.1661, -0.1913,  0.4297,  ...,  0.2976,  0.2744,  0.3388],\n",
      "        [ 0.0953,  0.0062, -0.0758,  ...,  0.0540, -0.2211, -0.0854],\n",
      "        ...,\n",
      "        [-0.0705,  0.0043, -0.1890,  ...,  0.0533, -0.1846,  0.1012],\n",
      "        [-0.0265, -0.2679, -0.3228,  ..., -0.1516,  0.3212,  0.1548],\n",
      "        [ 0.0311, -0.1447,  0.7675,  ..., -0.1829,  0.6412,  0.1241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.5386,  0.8622,  1.3652,  ...,  1.4732, -0.7710, -0.4673],\n",
      "        [-0.4442,  0.1694, -1.3022,  ...,  1.7764,  0.1724, -0.7232],\n",
      "        [-0.1645,  0.4111, -0.6617,  ...,  0.9522,  2.0063,  0.3374],\n",
      "        ...,\n",
      "        [ 0.5696,  1.6541,  1.5210,  ..., -1.5783, -0.0589, -0.4398],\n",
      "        [-0.1443, -0.6743, -1.0245,  ...,  0.5347,  0.7095,  0.8817],\n",
      "        [ 0.8474,  0.3181, -0.2656,  ..., -2.0994,  2.5585, -0.6817]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.9457, -0.0666,  0.7363,  ..., -0.2273,  0.8030,  0.8892],\n",
      "        [-0.5358, -0.2120, -1.5983,  ..., -1.2186,  0.2544, -0.2157],\n",
      "        [-0.5760,  0.1750,  0.2685,  ...,  0.0141,  0.0153, -0.5536],\n",
      "        ...,\n",
      "        [ 1.4497,  0.3670,  0.5936,  ...,  0.1448, -0.6335,  0.4659],\n",
      "        [ 0.3085,  0.0513, -0.8995,  ...,  0.2659,  0.8249, -0.4160],\n",
      "        [-0.2123, -0.0907, -0.0903,  ...,  0.5385, -0.3935, -0.2297]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1106, 0.1110, 0.0998, 0.0846, 0.1198, 0.1093, 0.0646, 0.1132, 0.1053,\n",
      "        0.1022, 0.1103, 0.0800, 0.1154, 0.0980, 0.1072, 0.1457, 0.1143, 0.1026,\n",
      "        0.1102, 0.1190, 0.1085, 0.1137, 0.1043, 0.0925, 0.1344, 0.1279, 0.1114,\n",
      "        0.1190, 0.1046, 0.1087, 0.1000, 0.1001, 0.3494, 0.1166, 0.1051, 0.1244,\n",
      "        0.1079, 0.1259, 0.1106, 0.1073, 0.1155, 0.1010, 0.1114, 0.1198, 0.1015,\n",
      "        0.1138, 0.1149, 0.1159, 0.0966, 0.1093, 0.1084, 0.1275, 0.1144, 0.0995,\n",
      "        0.0996, 0.1000, 0.1050, 0.1145, 0.1174, 0.1124, 0.1078, 0.0943, 0.1216,\n",
      "        0.0900, 0.1122, 0.0958, 0.1174, 0.1025, 0.1132, 0.1255, 0.1078, 0.1098,\n",
      "        0.1178, 0.0951, 0.1175, 0.1079, 0.1158, 0.1167, 0.0901, 0.1127, 0.1015,\n",
      "        0.1278, 0.0987, 0.1108, 0.1159, 0.1009, 0.1126, 0.1058, 0.1369, 0.1074,\n",
      "        0.0946, 0.0885, 0.1090, 0.0973, 0.1021, 0.1257, 0.1102, 0.0976, 0.1069,\n",
      "        0.1072, 0.1144, 0.1103, 0.1177, 0.0996, 0.1234, 0.1067, 0.1110, 0.1099,\n",
      "        0.1170, 0.1084, 0.1107, 0.0955, 0.1187, 0.1108, 0.0997, 0.1075, 0.1061,\n",
      "        0.1016, 0.1087, 0.1035, 0.1048, 0.1325, 0.1114, 0.0987, 0.1169, 0.1005,\n",
      "        0.1091, 0.1190, 0.1052, 0.1293, 0.1029, 0.1062, 0.0986, 0.1155, 0.1090,\n",
      "        0.1139, 0.9796, 0.1215, 0.1104, 0.0948, 0.1181, 0.1156, 0.1026, 0.1078,\n",
      "        0.1050, 0.1171, 0.1018, 0.1004, 0.1045, 0.1035, 0.1069, 0.1141, 0.1044,\n",
      "        0.0885, 0.1184, 0.0954, 0.1051, 0.1005, 0.1209, 0.1135, 0.1293, 0.1131,\n",
      "        0.1064, 0.0522, 0.1035, 0.1116, 0.0945, 0.1067, 0.1050, 0.1057, 0.1119,\n",
      "        0.1010, 0.1212, 0.1078, 0.1215, 0.1045, 0.1171, 0.1060, 0.1152, 0.1213,\n",
      "        0.1122, 0.1369, 0.1143, 0.1081, 0.1137, 0.1317, 0.0805, 0.1208, 0.0923,\n",
      "        0.1036, 0.1234, 0.0548, 0.1131, 0.1018, 0.1011, 0.1036, 0.1183, 0.1063,\n",
      "        0.1316, 0.1186, 0.1074, 0.1027, 0.1113, 0.1223, 0.1273, 0.1151, 0.1280,\n",
      "        0.1052, 0.0943, 0.1072, 0.0946, 0.1060, 0.1165, 0.1083, 0.1233, 0.0942,\n",
      "        0.1333, 0.1341, 0.1339, 0.0993, 0.1193, 0.1008, 0.1067, 0.1069, 0.1152,\n",
      "        0.1024, 0.0948, 0.1195, 0.1066, 0.1141, 0.1067, 0.1141, 0.1094, 0.0963,\n",
      "        0.0956, 0.1003, 0.1234, 0.0982, 0.1057, 0.1045, 0.1196, 0.1015, 0.1072,\n",
      "        0.1176, 0.1040, 0.1151, 0.0965, 0.1067, 0.1300, 0.1051, 0.1394, 0.1020,\n",
      "        0.1180, 0.0976, 0.1079, 0.1091, 0.1169, 0.1019, 0.1089, 0.0885, 0.1091,\n",
      "        0.1160, 0.1145, 0.1139, 0.1047, 0.0939, 0.1015, 0.1109, 0.1335, 0.0996,\n",
      "        0.1015, 0.1142, 0.1142, 0.1128, 0.0967, 0.1110, 0.1139, 0.1118, 0.1003,\n",
      "        0.1164, 0.1148, 0.1278, 0.1181, 0.1191, 0.1168, 0.1073, 0.1062, 0.1132,\n",
      "        0.1177, 0.0942, 0.0990, 0.1065, 0.0977, 0.1086, 0.1187, 0.1412, 0.1193,\n",
      "        0.1109, 0.1117, 0.0969, 0.1101, 0.0918, 0.1033, 0.1048, 0.1158, 0.1183,\n",
      "        0.0975, 0.0922, 0.1304, 0.0830, 0.1063, 0.1045, 0.1149, 0.1065, 0.1124,\n",
      "        0.1035, 0.0887, 0.1252, 0.1088, 0.1147, 0.1031, 0.0978, 0.1064, 0.1138,\n",
      "        0.1011, 0.1107, 0.1034, 0.1084, 0.1168, 0.1303, 0.1062, 0.1118, 0.1008,\n",
      "        0.0991, 0.1045, 0.1130, 0.1286, 0.1166, 0.1036, 0.1610, 0.1106, 0.0989,\n",
      "        0.1137, 0.1054, 0.1012, 0.0919, 0.1141, 0.1166, 0.0968, 0.1236, 0.1176,\n",
      "        0.1095, 0.1231, 0.1115, 0.1054, 0.1150, 0.0968, 0.1132, 0.0939, 0.1584,\n",
      "        0.1135, 0.1173, 0.0934, 0.1061, 0.1165, 0.1153, 0.1179, 0.0487, 0.1157,\n",
      "        0.1281, 0.1873, 0.1048, 0.0975, 0.1052, 0.0955, 0.1071, 0.0928, 0.1142,\n",
      "        0.1019, 0.1006, 0.0998, 0.0896, 0.1169, 0.1014, 0.1064, 0.1168, 0.0976,\n",
      "        0.1071, 0.1237, 0.1108, 0.1184, 0.1161, 0.1124, 0.1110, 0.1135, 0.1266,\n",
      "        0.1051, 0.1051, 0.1007, 0.0931, 0.1131, 0.1110, 0.1106, 0.1018, 0.1013,\n",
      "        0.1071, 0.1102, 0.1124, 0.1214, 0.1199, 0.1020, 0.1119, 0.0941, 0.1121,\n",
      "        0.0685, 0.0988, 0.1145, 0.1082, 0.1158, 0.0978, 0.1069, 0.0943, 0.1171,\n",
      "        0.1189, 0.1197, 0.1112, 0.1240, 0.1030, 0.1284, 0.1110, 0.1100, 0.1020,\n",
      "        0.1071, 0.1113, 0.1055, 0.1201, 0.1045, 0.1175, 0.1160, 0.1149, 0.1078,\n",
      "        0.1059, 0.1164, 0.0861, 0.0991, 0.1020, 0.1085, 0.1295, 0.1033, 0.1163,\n",
      "        0.1064, 0.1150, 0.1135, 0.1138, 0.0965, 0.1081, 0.1094, 0.1203, 0.1524,\n",
      "        0.1116, 0.1058, 0.1218, 0.1108, 0.1283, 0.1162, 0.1122, 0.1070, 0.1315,\n",
      "        0.1220, 0.1100, 0.1006, 0.1084, 0.1210, 0.1117, 0.1093, 0.1100, 0.1211,\n",
      "        0.1178, 0.1205, 0.1002, 0.0913, 0.1078, 0.1085, 0.1432, 0.1084, 0.0871,\n",
      "        0.1088, 0.0996, 0.0882, 0.1122, 0.1027, 0.1203, 0.1111, 0.1312, 0.0953,\n",
      "        0.1015, 0.1091, 0.0890, 0.0994, 0.1211, 0.1177, 0.1288, 0.1105, 0.1064,\n",
      "        0.1049, 0.1265, 0.1151, 0.0821, 0.1140, 0.1185, 0.0999, 0.2576],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.final_layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.1574,  0.1407,  0.1922,  0.0935,  0.1503,  0.1513, -0.0098,  0.1431,\n",
      "         0.1991,  0.1219,  0.1052,  0.0690,  0.1703,  0.1429,  0.1844,  0.1858,\n",
      "         0.1673,  0.1839,  0.1105,  0.1691,  0.1565,  0.1445,  0.1546,  0.0985,\n",
      "         0.2041,  0.2312,  0.1951,  0.1705,  0.1343,  0.1579,  0.1597,  0.1012,\n",
      "         0.2424,  0.1797,  0.1690,  0.1697,  0.1385,  0.1636,  0.1558,  0.1776,\n",
      "         0.1951,  0.1303,  0.1798,  0.1590,  0.1417,  0.1213,  0.1434,  0.1140,\n",
      "         0.1831,  0.1478,  0.1879,  0.1448,  0.1698,  0.1716,  0.1508,  0.1384,\n",
      "         0.1595,  0.1518,  0.1399,  0.1482,  0.1284,  0.1674,  0.1550,  0.1283,\n",
      "         0.1011,  0.1324,  0.1889,  0.1223,  0.1246,  0.1948,  0.1158,  0.1685,\n",
      "         0.1651,  0.1679,  0.1613,  0.1895,  0.1960,  0.1368,  0.0316,  0.1893,\n",
      "         0.1337,  0.1553,  0.1712,  0.1601,  0.1593,  0.1505,  0.1619,  0.1360,\n",
      "         0.1950,  0.1476,  0.1544,  0.1652,  0.1856,  0.1603,  0.1505,  0.1970,\n",
      "         0.1468,  0.1360,  0.1644,  0.1987,  0.1743,  0.1590,  0.1586,  0.1475,\n",
      "         0.1722,  0.1582,  0.1424,  0.1481,  0.1880,  0.1374,  0.1797,  0.1508,\n",
      "         0.0796,  0.2010,  0.1810,  0.1702,  0.2065,  0.1694,  0.1775,  0.1424,\n",
      "         0.1579,  0.1769,  0.1582,  0.1523,  0.1649,  0.1452,  0.1500,  0.2011,\n",
      "         0.2021,  0.2107,  0.1101,  0.1487,  0.1411,  0.1729,  0.1910,  0.1545,\n",
      "         0.0027,  0.1909,  0.2031,  0.1497,  0.1812,  0.1383,  0.1437,  0.1306,\n",
      "         0.1456,  0.1272,  0.1651,  0.1622,  0.1583,  0.1591,  0.1274,  0.1425,\n",
      "         0.1704,  0.1000,  0.1705,  0.1505,  0.1397,  0.1364,  0.1548,  0.1811,\n",
      "         0.2031,  0.1409,  0.1485,  0.0039,  0.1514,  0.1825,  0.1744,  0.1396,\n",
      "         0.1484,  0.1456,  0.1402,  0.1686,  0.1231,  0.1880,  0.1482,  0.1731,\n",
      "         0.1663,  0.1805,  0.1698,  0.1663,  0.1757,  0.1934,  0.1574,  0.1883,\n",
      "         0.1691,  0.1542,  0.0867,  0.1677,  0.1568,  0.1704,  0.1759,  0.0657,\n",
      "         0.1485,  0.1483,  0.1257,  0.1921,  0.1626,  0.1474,  0.1473,  0.1301,\n",
      "         0.1804,  0.1781,  0.1661,  0.1834,  0.1900,  0.1544,  0.1772,  0.1830,\n",
      "         0.1347,  0.1076,  0.1293,  0.1740,  0.1485,  0.1936,  0.1389,  0.1490,\n",
      "         0.1705,  0.1535,  0.1917,  0.1449,  0.1832,  0.1710,  0.1443,  0.1652,\n",
      "         0.2127,  0.1828,  0.1423,  0.1795,  0.1804,  0.1667,  0.1791,  0.1668,\n",
      "         0.1731,  0.1545,  0.1015,  0.1505,  0.1621,  0.1413,  0.1412,  0.1569,\n",
      "         0.1783,  0.1457,  0.1579,  0.1630,  0.1492,  0.1443,  0.1569,  0.1955,\n",
      "         0.1613,  0.1660,  0.1141,  0.1342,  0.1631,  0.1508,  0.1699,  0.1698,\n",
      "         0.1889,  0.1571,  0.1423,  0.1631,  0.1482,  0.1747,  0.1842,  0.1482,\n",
      "         0.1549,  0.1360,  0.1390,  0.1420,  0.2250,  0.1724,  0.1485,  0.1668,\n",
      "         0.1593,  0.1917,  0.1253,  0.1548,  0.1917,  0.1503,  0.1496,  0.1560,\n",
      "         0.1686,  0.2006,  0.1789,  0.1450,  0.1379,  0.1482,  0.1356,  0.1773,\n",
      "         0.1520,  0.1524,  0.1545,  0.1883,  0.1305,  0.1888,  0.1505,  0.1656,\n",
      "         0.1310,  0.1870,  0.1455,  0.1508,  0.1118,  0.1302,  0.1456,  0.1421,\n",
      "         0.1635,  0.1796,  0.1573,  0.1567,  0.2019,  0.1405,  0.1754,  0.1897,\n",
      "         0.1664,  0.1320,  0.1636,  0.1243,  0.1305,  0.1471,  0.1680,  0.1723,\n",
      "         0.1364,  0.1651,  0.1630,  0.1484,  0.1401,  0.1937,  0.1563,  0.1718,\n",
      "         0.1615,  0.1479,  0.1529,  0.1534,  0.1469,  0.1019,  0.1526,  0.1629,\n",
      "         0.1605,  0.1831,  0.1499,  0.1191,  0.1418,  0.1582,  0.1523,  0.1594,\n",
      "         0.1508,  0.1351,  0.1586,  0.1951,  0.1605,  0.1405,  0.1510,  0.1754,\n",
      "         0.1670,  0.1704,  0.1769,  0.1577,  0.1449,  0.1663,  0.1438,  0.1937,\n",
      "         0.1693,  0.1719,  0.1367,  0.1972,  0.2357,  0.1325,  0.1984,  0.0054,\n",
      "         0.1749,  0.1301,  0.0004,  0.1693,  0.1379,  0.1184,  0.1841,  0.1386,\n",
      "         0.1375,  0.1654,  0.1444,  0.1540,  0.1739,  0.1142,  0.1639,  0.1647,\n",
      "         0.1931,  0.1112,  0.1129,  0.1338,  0.1888,  0.1914,  0.1600,  0.1758,\n",
      "         0.1735,  0.1592,  0.1598,  0.1553,  0.1608,  0.1352,  0.1518,  0.1356,\n",
      "         0.1695,  0.1897,  0.1324,  0.1312,  0.1527,  0.1536,  0.1520,  0.1427,\n",
      "         0.1515,  0.1597,  0.1584,  0.1579,  0.0278,  0.1663,  0.0623,  0.1507,\n",
      "         0.1510,  0.1570,  0.1841,  0.1609,  0.1711,  0.0706,  0.1772,  0.1844,\n",
      "         0.2128,  0.1547,  0.1532,  0.1671,  0.1907,  0.1491,  0.1355,  0.1119,\n",
      "         0.1464,  0.1542,  0.1164,  0.1427,  0.1432,  0.1676,  0.1760,  0.1989,\n",
      "         0.1709,  0.1599,  0.1889,  0.0856,  0.1582,  0.1574,  0.2095,  0.0831,\n",
      "         0.1809,  0.1816,  0.1909,  0.1591,  0.1661,  0.1711,  0.1311,  0.1609,\n",
      "         0.1560,  0.1468,  0.2103,  0.1510,  0.1533,  0.1900,  0.1619,  0.1863,\n",
      "         0.1570,  0.1354,  0.1674,  0.2250,  0.1587,  0.1450,  0.1628,  0.1335,\n",
      "         0.1979,  0.1863,  0.1648,  0.1139,  0.1519,  0.1551,  0.2294,  0.1218,\n",
      "         0.1324,  0.1538,  0.1032,  0.1814,  0.1829,  0.1063,  0.1351,  0.1533,\n",
      "         0.1367,  0.1545,  0.1263,  0.1704,  0.1404,  0.1140,  0.1438,  0.1646,\n",
      "         0.1477,  0.0917,  0.1545,  0.1688,  0.1506,  0.1590,  0.1988,  0.1585,\n",
      "         0.1527,  0.1811,  0.1437,  0.0940,  0.1835,  0.1537,  0.1782,  0.1459],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0315,  0.0092, -0.0254,  ...,  0.0724, -0.0740,  0.0247],\n",
      "        [-0.1084,  0.0024,  0.1099,  ...,  0.0699,  0.0318, -0.0037],\n",
      "        [-0.0229, -0.0304,  0.0025,  ...,  0.0195,  0.0332,  0.1942],\n",
      "        ...,\n",
      "        [-0.1555,  0.0096, -0.0960,  ...,  0.0429,  0.1018,  0.0859],\n",
      "        [-0.0764, -0.0163, -0.1105,  ..., -0.0115, -0.0601, -0.0804],\n",
      "        [-0.0466, -0.0601, -0.0260,  ..., -0.0424, -0.0016,  0.0135]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.0674,  0.3533,  0.2549,  ...,  0.1148,  0.0428,  0.1330],\n",
      "        [ 0.4666, -0.0509,  0.0665,  ...,  0.4755,  0.3059,  0.1893],\n",
      "        [ 0.3759,  0.1503,  0.0561,  ...,  0.1757,  0.4971,  0.4911],\n",
      "        ...,\n",
      "        [ 0.6937,  0.3348, -0.4505,  ..., -0.6623, -0.9822, -0.0656],\n",
      "        [-0.6880,  0.0353, -0.0202,  ...,  0.7871,  0.1769,  0.0076],\n",
      "        [ 0.1735, -0.4157, -0.5939,  ...,  0.2145,  0.1383,  0.0873]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2801, -0.2851,  0.0634,  ...,  0.2321, -0.3005, -0.0875],\n",
      "        [-0.1166, -0.0966, -0.2758,  ...,  0.1592, -0.0048, -0.0094],\n",
      "        [-0.1856,  0.1452, -0.0108,  ...,  0.0988, -0.4574,  0.1397],\n",
      "        ...,\n",
      "        [ 0.4592, -0.0209,  0.2739,  ...,  0.1686, -0.7860, -0.1021],\n",
      "        [ 0.0887, -0.1503, -0.3654,  ..., -0.5106, -0.1554, -0.0392],\n",
      "        [-0.1054, -0.4311,  1.0622,  ..., -0.0208,  0.0665,  0.0782]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0336,  0.8236, -0.1539,  ...,  0.6656, -0.0349,  0.0841],\n",
      "        [-0.6043, -0.1132,  0.4675,  ...,  0.0099, -0.2306, -0.1241],\n",
      "        [-0.3667,  0.2441, -0.1306,  ..., -0.0688,  0.1003,  0.1018],\n",
      "        ...,\n",
      "        [ 0.2409, -0.2573, -0.7883,  ...,  0.1259, -0.0679,  0.5602],\n",
      "        [-0.0175, -0.8880, -0.6844,  ...,  0.0767,  0.2615, -0.1604],\n",
      "        [-0.4246,  0.4552, -0.5823,  ..., -0.3798, -0.5631, -0.6977]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter containing:\n",
      "tensor([[  1.3309, -25.0684,   4.5966,   3.5978,   1.0863,   2.6494],\n",
      "        [  3.2396,   1.2928,   6.7383,   6.7517,   1.7236,   5.8109],\n",
      "        [  3.0284,   1.9706,   5.0617,   4.6147,   1.7640,   4.8850],\n",
      "        [  2.7109,   2.1119,   4.1258,   3.2862,   1.8601,   4.2537],\n",
      "        [  2.4486,   2.1582,   3.4502,   2.4943,   1.8241,   3.7729],\n",
      "        [  2.2440,   2.1260,   2.8746,   1.8291,   1.8635,   3.3330],\n",
      "        [  2.0671,   2.1177,   2.4297,   1.4130,   1.8511,   2.9484],\n",
      "        [  1.8565,   2.1144,   2.0724,   1.0432,   1.8526,   2.6227],\n",
      "        [  1.7002,   2.0320,   1.6673,   0.5973,   1.8717,   2.3342],\n",
      "        [  1.5703,   2.0097,   1.3982,   0.4213,   1.8601,   2.0276],\n",
      "        [  1.4808,   1.9686,   1.1189,   0.1603,   1.8388,   1.8078],\n",
      "        [  1.2726,   1.9364,   0.9549,  -0.1139,   1.7841,   1.5671],\n",
      "        [  1.2136,   1.8877,   0.6829,  -0.2759,   1.7669,   1.2921],\n",
      "        [  1.0820,   1.8468,   0.5018,  -0.4308,   1.7814,   1.0511],\n",
      "        [  1.0188,   1.7869,   0.3725,  -0.6293,   1.7798,   0.9539],\n",
      "        [  0.8958,   1.7697,   0.1030,  -0.7419,   1.7498,   0.7173],\n",
      "        [  0.7561,   1.6846,  -0.0947,  -1.0164,   1.6947,   0.4314],\n",
      "        [  0.5467,   1.6026,  -0.4141,  -1.2599,   1.6763,   0.0801],\n",
      "        [  0.4144,   1.5137,  -0.6327,  -1.4858,   1.6730,  -0.2159],\n",
      "        [  0.2724,   1.4023,  -0.9296,  -1.7282,   1.6033,  -0.5195],\n",
      "        [  0.0459,   1.3303,  -1.0984,  -1.9174,   1.4884,  -0.7955],\n",
      "        [ -0.0759,   1.2041,  -1.3251,  -2.0864,   1.4405,  -1.0568],\n",
      "        [ -0.1561,   1.0995,  -1.5328,  -2.2171,   1.3762,  -1.3537],\n",
      "        [ -0.3623,   0.9781,  -1.7294,  -2.3802,   1.3676,  -1.5976],\n",
      "        [ -0.4065,   0.9024,  -1.9378,  -2.4919,   1.2508,  -1.7967],\n",
      "        [ -0.4898,   0.7665,  -2.0044,  -2.6324,   1.1739,  -1.9143],\n",
      "        [ -0.5081,   0.6145,  -2.1390,  -2.7199,   1.1307,  -2.0488],\n",
      "        [ -0.5978,   0.5308,  -2.2350,  -2.8545,   1.0549,  -2.1995],\n",
      "        [ -0.5483,   0.3806,  -2.3214,  -2.9220,   1.0140,  -2.2943],\n",
      "        [ -0.4604,   0.2813,  -2.3907,  -2.9494,   0.9259,  -2.3720],\n",
      "        [ -0.4484,   0.1637,  -2.4934,  -3.0118,   0.8979,  -2.5188],\n",
      "        [ 25.2450,  -0.3021,  -2.5262,  -2.9680,   0.8117,  -2.5343]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.0649, 0.1076, 0.0953, 0.1219, 0.1033, 0.1037, 0.1529, 0.1227, 0.0826,\n",
      "        0.1378, 0.1238, 0.0656, 0.0977, 0.1089, 0.0942, 0.1548, 0.0985, 0.0809,\n",
      "        0.1428, 0.0782, 0.1120, 0.0672, 0.1171, 0.1247, 0.0774, 0.1013, 0.0758,\n",
      "        0.0999, 0.1057, 0.1150, 0.1101, 0.1505, 0.1359, 0.1066, 0.0901, 0.0922,\n",
      "        0.1211, 0.1083, 0.1058, 0.0762, 0.1101, 0.0902, 0.1176, 0.0718, 0.1075,\n",
      "        0.1645, 0.1462, 0.1084, 0.1006, 0.0961, 0.0925, 0.1011, 0.0981, 0.1053,\n",
      "        0.1214, 0.1044, 0.0926, 0.1359, 0.1048, 0.0967, 0.1042, 0.1169, 0.1074,\n",
      "        0.1034, 0.0746, 0.1225, 0.1040, 0.0873, 0.1406, 0.1157, 0.1177, 0.0943,\n",
      "        0.0869, 0.1042, 0.1225, 0.1039, 0.0758, 0.1063, 0.0889, 0.0829, 0.1115,\n",
      "        0.1091, 0.1191, 0.1125, 0.0961, 0.1260, 0.1032, 0.1016, 0.1013, 0.1265,\n",
      "        0.1002, 0.0949, 0.1087, 0.1206, 0.0866, 0.1049, 0.1018, 0.1188, 0.1175,\n",
      "        0.1017, 0.1142, 0.1097, 0.0773, 0.0853, 0.0692, 0.0972, 0.0641, 0.1045,\n",
      "        0.0808, 0.1016, 0.0962, 0.1132, 0.1270, 0.0752, 0.0842, 0.0852, 0.0764,\n",
      "        0.1074, 0.1132, 0.1141, 0.1030, 0.1484, 0.0619, 0.0574, 0.0961, 0.1154,\n",
      "        0.1093, 0.0928, 0.0722, 0.0808, 0.1254, 0.1191, 0.0823, 0.0914, 0.0966,\n",
      "        0.1108, 0.1225, 0.0750, 0.0891, 0.1014, 0.0814, 0.1520, 0.1098, 0.1141,\n",
      "        0.1220, 0.1229, 0.0955, 0.0870, 0.1109, 0.0860, 0.1321, 0.1413, 0.0890,\n",
      "        0.1302, 0.0883, 0.1027, 0.1122, 0.1331, 0.1174, 0.1102, 0.0739, 0.1139,\n",
      "        0.1153, 0.0469, 0.1134, 0.1198, 0.0869, 0.1202, 0.1096, 0.1267, 0.0999,\n",
      "        0.1045, 0.0965, 0.1120, 0.0358, 0.0950, 0.1142, 0.0601, 0.1076, 0.1152,\n",
      "        0.0973, 0.0877, 0.1006, 0.0863, 0.1160, 0.1068, 0.1232, 0.1307, 0.1208,\n",
      "        0.0830, 0.0871, 0.1205, 0.1051, 0.1409, 0.1260, 0.0760, 0.1200, 0.1430,\n",
      "        0.1185, 0.0747, 0.0807, 0.1247, 0.1039, 0.1146, 0.0298, 0.0908, 0.1034,\n",
      "        0.0797, 0.1246, 0.0700, 0.1531, 0.0883, 0.1079, 0.0921, 0.1116, 0.1336,\n",
      "        0.1372, 0.1165, 0.1092, 0.1222, 0.0883, 0.0765, 0.1266, 0.1279, 0.1108,\n",
      "        0.0531, 0.1081, 0.0893, 0.1284, 0.1178, 0.1000, 0.0982, 0.0806, 0.0476,\n",
      "        0.1214, 0.0759, 0.1204, 0.0766, 0.1241, 0.1147, 0.1137, 0.1031, 0.1221,\n",
      "        0.0920, 0.1223, 0.1121, 0.1023, 0.0907, 0.0924, 0.1075, 0.0737, 0.1200,\n",
      "        0.0960, 0.0884, 0.0885, 0.0841, 0.1055, 0.1189, 0.1174, 0.1031, 0.1288,\n",
      "        0.0882, 0.0996, 0.1078, 0.1280, 0.1038, 0.1168, 0.1012, 0.0377, 0.0613,\n",
      "        0.1034, 0.0926, 0.1095, 0.1028, 0.1216, 0.1046, 0.1110, 0.1136, 0.1164,\n",
      "        0.0869, 0.0336, 0.1067, 0.1088, 0.1417, 0.1017, 0.0971, 0.1134, 0.0839,\n",
      "        0.1598, 0.1011, 0.1141, 0.0899, 0.1176, 0.0743, 0.1043, 0.0949, 0.1093,\n",
      "        0.0663, 0.1246, 0.0996, 0.1577, 0.1032, 0.1216, 0.1279, 0.0670, 0.1395,\n",
      "        0.1312, 0.0828, 0.0608, 0.0902, 0.0907, 0.0951, 0.1060, 0.1237, 0.1082,\n",
      "        0.1427, 0.1127, 0.1002, 0.0889, 0.0931, 0.1451, 0.0939, 0.0919, 0.1243,\n",
      "        0.0910, 0.0932, 0.0958, 0.1238, 0.1095, 0.1083, 0.1030, 0.1268, 0.0526,\n",
      "        0.0415, 0.1008, 0.0798, 0.0937, 0.1135, 0.0896, 0.1077, 0.0931, 0.0965,\n",
      "        0.1004, 0.1029, 0.0955, 0.0991, 0.1273, 0.0973, 0.0809, 0.0585, 0.0998,\n",
      "        0.1100, 0.0583, 0.1161, 0.1204, 0.1080, 0.1205, 0.1039, 0.1184, 0.0862,\n",
      "        0.0971, 0.1102, 0.1239, 0.0899, 0.0889, 0.0933, 0.0933, 0.0422, 0.0974,\n",
      "        0.1075, 0.1198, 0.1134, 0.1290, 0.1176, 0.0948, 0.1149, 0.1226, 0.0831,\n",
      "        0.0947, 0.1002, 0.1065, 0.0994, 0.0993, 0.0917, 0.0832, 0.1104, 0.1547,\n",
      "        0.1140, 0.0863, 0.0869, 0.0897, 0.0844, 0.1056, 0.1147, 0.1154, 0.1074,\n",
      "        0.1334, 0.1100, 0.0939, 0.1067, 0.0723, 0.0892, 0.1220, 0.1039, 0.0979,\n",
      "        0.1121, 0.1111, 0.1045, 0.1154, 0.1138, 0.0885, 0.0865, 0.0600, 0.0918,\n",
      "        0.0383, 0.1030, 0.0903, 0.1036, 0.0750, 0.1305, 0.1164, 0.1279, 0.1020,\n",
      "        0.0786, 0.0606, 0.1076, 0.1376, 0.1047, 0.0531, 0.0886, 0.0819, 0.1244,\n",
      "        0.1004, 0.1118, 0.1320, 0.0841, 0.0954, 0.1102, 0.0937, 0.1118, 0.0933,\n",
      "        0.0899, 0.0758, 0.0876, 0.1117, 0.1132, 0.0748, 0.1618, 0.1161, 0.0814,\n",
      "        0.0372, 0.1078, 0.1043, 0.0944, 0.1389, 0.0997, 0.0684, 0.1147, 0.0831,\n",
      "        0.0924, 0.1348, 0.0882, 0.0971, 0.0772, 0.0863, 0.1170, 0.1039, 0.0689,\n",
      "        0.1343, 0.1190, 0.1007, 0.0919, 0.1405, 0.0604, 0.0941, 0.1250, 0.1230,\n",
      "        0.1056, 0.0584, 0.1050, 0.1141, 0.0962, 0.1207, 0.1228, 0.0989, 0.1285,\n",
      "        0.1295, 0.1210, 0.1373, 0.0330, 0.1001, 0.0971, 0.0903, 0.1372, 0.1343,\n",
      "        0.0757, 0.1262, 0.1236, 0.0980, 0.1260, 0.1044, 0.1298, 0.0947, 0.1105,\n",
      "        0.1044, 0.0856, 0.1301, 0.1058, 0.0751, 0.1219, 0.1137, 0.0369],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 3.1383e-02, -2.1840e-04, -6.4214e-02,  ..., -2.5119e-02,\n",
      "          1.2728e-01, -1.6126e-02],\n",
      "        [ 9.8780e-02,  1.0499e-01,  1.5601e-01,  ..., -5.8230e-02,\n",
      "          7.4627e-02, -1.9932e-02],\n",
      "        [ 1.1151e-03,  2.3767e-03, -2.3072e-02,  ...,  8.3670e-02,\n",
      "          2.0687e-02,  3.3222e-02],\n",
      "        ...,\n",
      "        [ 4.9411e-02,  2.5332e-02,  3.4419e-04,  ..., -9.8732e-02,\n",
      "         -1.5036e-02,  1.8329e-03],\n",
      "        [ 3.7967e-02, -2.5570e-02,  1.8320e-01,  ...,  4.2697e-02,\n",
      "          1.2019e-02, -1.6902e-02],\n",
      "        [ 4.6486e-02,  5.5484e-02, -1.7245e-02,  ...,  1.1395e-01,\n",
      "         -1.1227e-02,  4.8932e-06]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3294, -0.3969,  0.3182,  ..., -0.1915, -0.1610, -0.5179],\n",
      "        [ 0.4359,  0.1101,  0.3714,  ..., -0.7972,  0.2761,  0.0055],\n",
      "        [-0.2435,  0.3951, -0.2592,  ...,  0.1293,  0.3105, -0.2448],\n",
      "        ...,\n",
      "        [-0.2163, -0.4165,  0.2310,  ..., -0.0187, -0.0865, -0.0025],\n",
      "        [ 0.5108,  0.5845,  0.0773,  ...,  0.4989,  0.4363, -0.5816],\n",
      "        [ 0.0637, -0.2704,  0.1770,  ...,  0.1685,  0.3904, -0.2715]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0956, -0.6382, -0.0502,  ...,  0.1685, -0.3495,  0.3979],\n",
      "        [ 0.1906, -0.1887, -0.2293,  ...,  0.1990, -0.2191,  0.2454],\n",
      "        [ 0.2493,  0.2678, -0.5825,  ...,  0.0930, -0.0798, -0.0235],\n",
      "        ...,\n",
      "        [ 0.2066, -0.0894,  0.3296,  ..., -0.3306, -0.3902,  0.0881],\n",
      "        [-0.5606, -0.5647, -0.0957,  ..., -0.1248, -0.2790,  0.4313],\n",
      "        [ 0.0544, -0.1543,  0.0367,  ..., -0.0562, -0.0078, -0.6044]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.5055, -0.9558, -0.3647,  ..., -0.2336,  0.7947, -1.2873],\n",
      "        [-0.4296, -0.0491, -0.1786,  ..., -0.4594,  0.4225, -0.4570],\n",
      "        [ 0.3287,  0.3667,  0.4226,  ...,  0.2726,  0.1445,  0.0490],\n",
      "        ...,\n",
      "        [-0.1040, -0.3344,  0.1089,  ..., -0.0375, -0.0173,  0.3733],\n",
      "        [-0.4192,  0.2197, -0.4403,  ..., -0.5173, -0.0245, -0.1681],\n",
      "        [ 2.4764, -1.8981, -4.9471,  ..., -0.2528,  1.0269, -5.1897]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1214, 0.1111, 0.0807, 0.0922, 0.0985, 0.0763, 0.1188, 0.1012, 0.0725,\n",
      "        0.1179, 0.0952, 0.0821, 0.0817, 0.0933, 0.0865, 0.2044, 0.1001, 0.0728,\n",
      "        0.1139, 0.0631, 0.0952, 0.0787, 0.0945, 0.0933, 0.0725, 0.0764, 0.0668,\n",
      "        0.0953, 0.1058, 0.0929, 0.0994, 0.1216, 0.1258, 0.1120, 0.0755, 0.0999,\n",
      "        0.0975, 0.0867, 0.1013, 0.0634, 0.0929, 0.0823, 0.1135, 0.0788, 0.0811,\n",
      "        0.1558, 0.1297, 0.0858, 0.1015, 0.0761, 0.0898, 0.0981, 0.1007, 0.0899,\n",
      "        0.1023, 0.0824, 0.0975, 0.1058, 0.0870, 0.0820, 0.0862, 0.0943, 0.0851,\n",
      "        0.0974, 0.1322, 0.0878, 0.0952, 0.0896, 0.1230, 0.0848, 0.1091, 0.0703,\n",
      "        0.0438, 0.0892, 0.1112, 0.1142, 0.0888, 0.0837, 0.0675, 0.0832, 0.0974,\n",
      "        0.0948, 0.1111, 0.0878, 0.0830, 0.0987, 0.0974, 0.1008, 0.0947, 0.0996,\n",
      "        0.0710, 0.0942, 0.0911, 0.0869, 0.0817, 0.1090, 0.0999, 0.0969, 0.0832,\n",
      "        0.1121, 0.1026, 0.1054, 0.0745, 0.0831, 0.0941, 0.0987, 0.1052, 0.0792,\n",
      "        0.0715, 0.1116, 0.1064, 0.0996, 0.1344, 0.0798, 0.0964, 0.0935, 0.0602,\n",
      "        0.0912, 0.0998, 0.1028, 0.0934, 0.1332, 0.1191, 0.1164, 0.0905, 0.1094,\n",
      "        0.0948, 0.0924, 0.0642, 0.0909, 0.1027, 0.0900, 0.1113, 0.0862, 0.0974,\n",
      "        0.1067, 0.1650, 0.0939, 0.0849, 0.0931, 0.0859, 0.1299, 0.0750, 0.0866,\n",
      "        0.0972, 0.1234, 0.0821, 0.0803, 0.0830, 0.0711, 0.1034, 0.1308, 0.0969,\n",
      "        0.1015, 0.0805, 0.1063, 0.1007, 0.1354, 0.0955, 0.1232, 0.0962, 0.0967,\n",
      "        0.1030, 0.1066, 0.0931, 0.1216, 0.0838, 0.1007, 0.0871, 0.1155, 0.1052,\n",
      "        0.1020, 0.0898, 0.0908, 0.1031, 0.1029, 0.1100, 0.0669, 0.0989, 0.1001,\n",
      "        0.1107, 0.0788, 0.1306, 0.0717, 0.0738, 0.0877, 0.1025, 0.1131, 0.1023,\n",
      "        0.0951, 0.0741, 0.1216, 0.0956, 0.1151, 0.0967, 0.0758, 0.0964, 0.1059,\n",
      "        0.1146, 0.0724, 0.0961, 0.1065, 0.0983, 0.0966, 0.0741, 0.1014, 0.1080,\n",
      "        0.1016, 0.1032, 0.1349, 0.1332, 0.0887, 0.1001, 0.0761, 0.1215, 0.1412,\n",
      "        0.0946, 0.1098, 0.1272, 0.1038, 0.0769, 0.0976, 0.0754, 0.0966, 0.0869,\n",
      "        0.0760, 0.1021, 0.0964, 0.1149, 0.1255, 0.0928, 0.0756, 0.0809, 0.0854,\n",
      "        0.0996, 0.0766, 0.0993, 0.1192, 0.1156, 0.1129, 0.1431, 0.0918, 0.1146,\n",
      "        0.0943, 0.0869, 0.0954, 0.0938, 0.1268, 0.0962, 0.0945, 0.1472, 0.0989,\n",
      "        0.0928, 0.0782, 0.0643, 0.0841, 0.0896, 0.0906, 0.1054, 0.1016, 0.0923,\n",
      "        0.0959, 0.0931, 0.1086, 0.1201, 0.0982, 0.1030, 0.1087, 0.0901, 0.0750,\n",
      "        0.0972, 0.0986, 0.1045, 0.1011, 0.1024, 0.0837, 0.0981, 0.1050, 0.1012,\n",
      "        0.1086, 0.0965, 0.1000, 0.0888, 0.1206, 0.0978, 0.1116, 0.1001, 0.0818,\n",
      "        0.1604, 0.1183, 0.1097, 0.0776, 0.0875, 0.0808, 0.1002, 0.0945, 0.1055,\n",
      "        0.0857, 0.1012, 0.0844, 0.1063, 0.0814, 0.0956, 0.1048, 0.0941, 0.1766,\n",
      "        0.1072, 0.0750, 0.0729, 0.0691, 0.0961, 0.0985, 0.0991, 0.0880, 0.1026,\n",
      "        0.0948, 0.0867, 0.1093, 0.1309, 0.0823, 0.0907, 0.0835, 0.0928, 0.1009,\n",
      "        0.0970, 0.0941, 0.0963, 0.0964, 0.0914, 0.0841, 0.0957, 0.1039, 0.1138,\n",
      "        0.0648, 0.0968, 0.0777, 0.0796, 0.1064, 0.0851, 0.1379, 0.0768, 0.0761,\n",
      "        0.0898, 0.0856, 0.0824, 0.0901, 0.0982, 0.0961, 0.0959, 0.0839, 0.0903,\n",
      "        0.1122, 0.1016, 0.0942, 0.1123, 0.1154, 0.0960, 0.0764, 0.1156, 0.0834,\n",
      "        0.0952, 0.0969, 0.1014, 0.0830, 0.0862, 0.0993, 0.0971, 0.0606, 0.1132,\n",
      "        0.0955, 0.1158, 0.0921, 0.1201, 0.0834, 0.0822, 0.0880, 0.0925, 0.0845,\n",
      "        0.0892, 0.0928, 0.1233, 0.0773, 0.1015, 0.0956, 0.0860, 0.0909, 0.1415,\n",
      "        0.1010, 0.0876, 0.0766, 0.1134, 0.0790, 0.1137, 0.0933, 0.1130, 0.0962,\n",
      "        0.1054, 0.0997, 0.0933, 0.0886, 0.0753, 0.0802, 0.1045, 0.0783, 0.0849,\n",
      "        0.1108, 0.0967, 0.0979, 0.0968, 0.0959, 0.0785, 0.0925, 0.1326, 0.0786,\n",
      "        0.0531, 0.0898, 0.0837, 0.0939, 0.0795, 0.1163, 0.0945, 0.0939, 0.0922,\n",
      "        0.0665, 0.0668, 0.0971, 0.1029, 0.0811, 0.0733, 0.1035, 0.0694, 0.1211,\n",
      "        0.0810, 0.1005, 0.1079, 0.1073, 0.1050, 0.0881, 0.0969, 0.0988, 0.0729,\n",
      "        0.0827, 0.0933, 0.0721, 0.0868, 0.0893, 0.0853, 0.1347, 0.1009, 0.0779,\n",
      "        0.0937, 0.1023, 0.0936, 0.0855, 0.1171, 0.0871, 0.0693, 0.0890, 0.0908,\n",
      "        0.0753, 0.1003, 0.1092, 0.0806, 0.0728, 0.0689, 0.0971, 0.0716, 0.0656,\n",
      "        0.1062, 0.1132, 0.0983, 0.1031, 0.1295, 0.0928, 0.1119, 0.1265, 0.0928,\n",
      "        0.0981, 0.0739, 0.1081, 0.0985, 0.1176, 0.0928, 0.1105, 0.0954, 0.1036,\n",
      "        0.1217, 0.1024, 0.1142, 0.1451, 0.0765, 0.0854, 0.0834, 0.1344, 0.1032,\n",
      "        0.0795, 0.0923, 0.1388, 0.0951, 0.1365, 0.1124, 0.1146, 0.0901, 0.1177,\n",
      "        0.1026, 0.0699, 0.1128, 0.0874, 0.0816, 0.0929, 0.0810, 0.2336],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2126, -0.0676,  0.1858,  ..., -0.1478,  0.0182,  0.4609],\n",
      "        [ 0.0619, -0.1342,  0.2068,  ...,  0.2332,  0.0616, -0.0931],\n",
      "        [ 0.0928, -0.1929, -0.0169,  ..., -0.5339,  0.6135, -0.3907],\n",
      "        ...,\n",
      "        [ 0.1592,  0.2668, -0.2621,  ...,  0.2685, -1.2015, -0.3122],\n",
      "        [ 0.2792,  0.0351, -0.0268,  ...,  0.9945, -0.4678,  0.4397],\n",
      "        [ 0.0856,  0.2093,  1.3750,  ...,  0.4076, -0.2373,  0.1990]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1267, -0.2629, -0.7924,  ..., -0.1926,  0.0786,  0.2221],\n",
      "        [ 0.0141, -0.2168, -0.7047,  ..., -0.5989,  0.0831,  0.0195],\n",
      "        [-0.2820,  0.0626, -0.4369,  ..., -0.4864,  0.1074,  0.2882],\n",
      "        ...,\n",
      "        [-0.4420,  0.8947, -0.1626,  ...,  0.5296,  0.1293,  0.3418],\n",
      "        [ 0.1583, -0.1237, -0.3968,  ..., -0.7175,  0.3495,  0.2199],\n",
      "        [ 0.0839,  0.3105,  0.0125,  ..., -0.2622, -0.6939, -0.6554]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.7500, -0.2226,  0.2794,  ...,  1.0333, -0.4425, -0.6480],\n",
      "        [-0.2141,  0.0178, -0.3070,  ..., -0.5723,  0.5852, -0.0884],\n",
      "        [ 0.0771, -0.1541,  0.4435,  ..., -0.1547,  0.1103, -0.0551],\n",
      "        ...,\n",
      "        [-0.2222, -0.0968, -0.0808,  ..., -0.7601,  0.2029, -0.8992],\n",
      "        [-0.2998, -0.0485, -0.0624,  ...,  0.2573,  0.3009, -0.2091],\n",
      "        [-0.7991, -0.8736,  0.1075,  ...,  1.5081, -0.6690, -0.8777]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.0.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([0.0585, 0.1113, 0.0763, 0.0957, 0.0847, 0.0980, 0.1135, 0.0871, 0.0738,\n",
      "        0.1114, 0.1022, 0.0995, 0.0788, 0.0740, 0.0833, 0.0977, 0.0941, 0.0831,\n",
      "        0.1087, 0.0749, 0.0871, 0.0716, 0.1181, 0.1007, 0.0756, 0.0903, 0.0730,\n",
      "        0.0936, 0.0904, 0.0815, 0.1021, 0.1153, 0.1248, 0.0930, 0.0604, 0.0628,\n",
      "        0.0909, 0.0910, 0.0989, 0.0742, 0.0982, 0.0860, 0.1015, 0.0592, 0.0792,\n",
      "        0.1345, 0.1108, 0.0890, 0.0769, 0.0808, 0.0751, 0.0979, 0.0773, 0.0880,\n",
      "        0.0958, 0.0926, 0.0746, 0.1048, 0.0862, 0.0911, 0.0892, 0.0990, 0.0987,\n",
      "        0.1017, 0.1279, 0.1047, 0.0765, 0.0778, 0.1190, 0.0897, 0.1053, 0.0772,\n",
      "        0.0211, 0.0893, 0.0925, 0.0720, 0.0728, 0.0905, 0.0817, 0.0720, 0.0988,\n",
      "        0.0884, 0.1004, 0.1009, 0.0843, 0.1150, 0.0881, 0.0971, 0.0889, 0.1102,\n",
      "        0.0757, 0.0795, 0.1021, 0.0903, 0.0913, 0.0822, 0.0970, 0.0964, 0.0837,\n",
      "        0.0881, 0.0928, 0.0837, 0.0738, 0.0867, 0.0845, 0.0868, 0.0788, 0.0836,\n",
      "        0.0653, 0.0980, 0.0984, 0.0924, 0.1266, 0.0724, 0.0817, 0.0718, 0.0633,\n",
      "        0.0930, 0.0913, 0.0848, 0.0848, 0.1157, 0.1087, 0.1211, 0.0875, 0.0906,\n",
      "        0.0852, 0.0746, 0.0567, 0.0707, 0.0922, 0.0953, 0.0920, 0.0870, 0.0892,\n",
      "        0.0933, 0.1356, 0.0826, 0.0769, 0.0983, 0.0819, 0.1271, 0.0900, 0.1023,\n",
      "        0.0840, 0.1029, 0.0794, 0.0725, 0.0894, 0.0824, 0.1005, 0.1230, 0.0805,\n",
      "        0.1027, 0.0816, 0.0920, 0.0931, 0.1081, 0.1058, 0.0872, 0.0840, 0.0886,\n",
      "        0.0894, 0.0578, 0.0938, 0.0962, 0.0871, 0.1034, 0.0847, 0.0961, 0.0911,\n",
      "        0.1045, 0.0859, 0.0962, 0.0898, 0.0791, 0.0880, 0.0562, 0.0932, 0.0860,\n",
      "        0.0866, 0.0834, 0.0973, 0.0655, 0.0844, 0.0806, 0.1182, 0.1130, 0.0922,\n",
      "        0.0672, 0.0717, 0.1143, 0.0817, 0.0967, 0.0947, 0.0702, 0.0889, 0.0882,\n",
      "        0.0931, 0.0675, 0.0789, 0.0992, 0.0864, 0.0845, 0.0782, 0.0935, 0.0904,\n",
      "        0.0734, 0.0946, 0.1169, 0.1158, 0.0883, 0.0916, 0.0749, 0.0716, 0.1081,\n",
      "        0.0909, 0.0980, 0.0681, 0.1039, 0.0753, 0.0768, 0.0963, 0.0934, 0.0839,\n",
      "        0.0697, 0.1034, 0.1008, 0.0910, 0.1200, 0.0768, 0.0925, 0.0683, 0.0944,\n",
      "        0.1091, 0.0718, 0.0991, 0.1014, 0.0972, 0.0822, 0.0728, 0.0865, 0.1017,\n",
      "        0.0808, 0.0949, 0.0886, 0.1048, 0.1146, 0.0876, 0.0807, 0.1033, 0.1001,\n",
      "        0.0857, 0.0776, 0.0761, 0.0754, 0.0772, 0.1027, 0.0974, 0.0967, 0.0989,\n",
      "        0.0861, 0.0858, 0.0887, 0.1186, 0.0884, 0.0905, 0.0866, 0.0591, 0.0730,\n",
      "        0.0827, 0.0761, 0.0984, 0.0870, 0.0854, 0.0871, 0.0847, 0.0985, 0.0910,\n",
      "        0.0773, 0.0850, 0.0856, 0.0835, 0.1117, 0.0876, 0.0732, 0.0860, 0.0719,\n",
      "        0.1123, 0.0886, 0.0994, 0.0669, 0.0988, 0.0813, 0.0866, 0.0877, 0.0897,\n",
      "        0.0558, 0.1045, 0.0784, 0.1051, 0.0857, 0.1029, 0.1013, 0.0767, 0.1139,\n",
      "        0.1103, 0.0706, 0.0627, 0.0763, 0.0787, 0.0890, 0.0837, 0.0944, 0.0992,\n",
      "        0.0973, 0.0822, 0.0918, 0.1145, 0.0785, 0.0963, 0.0595, 0.0761, 0.1107,\n",
      "        0.0847, 0.0870, 0.0775, 0.1040, 0.0953, 0.1046, 0.0969, 0.0983, 0.1046,\n",
      "        0.0620, 0.0871, 0.0713, 0.0812, 0.1044, 0.0820, 0.1167, 0.0868, 0.0832,\n",
      "        0.0993, 0.0789, 0.0735, 0.0945, 0.0944, 0.0904, 0.0826, 0.0868, 0.0850,\n",
      "        0.0938, 0.0884, 0.0952, 0.0918, 0.0769, 0.0926, 0.0832, 0.0995, 0.0776,\n",
      "        0.0742, 0.0920, 0.0943, 0.0764, 0.0688, 0.0961, 0.0906, 0.0833, 0.1086,\n",
      "        0.0959, 0.1046, 0.0989, 0.1085, 0.0922, 0.0679, 0.0886, 0.1143, 0.0809,\n",
      "        0.0901, 0.0814, 0.1058, 0.0837, 0.0963, 0.0774, 0.0825, 0.0843, 0.1057,\n",
      "        0.1036, 0.0715, 0.0820, 0.0841, 0.0781, 0.0919, 0.0924, 0.1282, 0.0899,\n",
      "        0.1069, 0.0855, 0.0837, 0.0900, 0.0730, 0.0665, 0.1062, 0.0746, 0.0871,\n",
      "        0.0962, 0.0922, 0.0960, 0.1052, 0.0834, 0.0815, 0.0823, 0.0721, 0.0762,\n",
      "        0.0400, 0.0829, 0.0788, 0.0928, 0.0702, 0.1058, 0.0808, 0.1083, 0.0831,\n",
      "        0.0703, 0.0452, 0.0874, 0.0918, 0.0808, 0.0242, 0.0814, 0.0685, 0.0978,\n",
      "        0.0827, 0.1034, 0.1054, 0.1033, 0.0789, 0.0845, 0.0793, 0.0884, 0.0720,\n",
      "        0.0793, 0.0708, 0.0847, 0.0879, 0.1116, 0.0666, 0.1191, 0.1044, 0.0674,\n",
      "        0.0748, 0.1104, 0.0897, 0.1018, 0.1157, 0.0881, 0.0594, 0.0805, 0.0738,\n",
      "        0.0900, 0.1134, 0.0965, 0.0865, 0.0664, 0.0788, 0.1077, 0.0876, 0.0705,\n",
      "        0.1003, 0.1143, 0.0960, 0.0906, 0.0954, 0.0908, 0.0876, 0.0881, 0.0904,\n",
      "        0.0914, 0.0615, 0.0915, 0.1068, 0.0791, 0.0992, 0.1120, 0.0859, 0.0964,\n",
      "        0.1066, 0.1050, 0.1149, 0.1185, 0.0860, 0.0727, 0.0738, 0.1004, 0.0956,\n",
      "        0.0711, 0.0933, 0.0852, 0.0955, 0.0986, 0.0978, 0.0920, 0.0865, 0.0988,\n",
      "        0.0838, 0.0797, 0.1017, 0.0776, 0.0669, 0.0937, 0.0911, 0.0105],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0017,  0.0113,  0.1030,  ..., -0.0792,  0.0788,  0.0363],\n",
      "        [-0.0623, -0.0043,  0.0370,  ...,  0.0958,  0.0083, -0.0180],\n",
      "        [ 0.0061,  0.0563, -0.0143,  ...,  0.0215, -0.0472, -0.0277],\n",
      "        ...,\n",
      "        [ 0.0261, -0.0968, -0.0149,  ..., -0.0937,  0.0378,  0.0407],\n",
      "        [ 0.0678,  0.0888, -0.0145,  ...,  0.0832, -0.0592,  0.0355],\n",
      "        [ 0.0715, -0.0125, -0.1131,  ..., -0.0094, -0.0146,  0.0302]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 4.7520e-02,  4.9969e-01,  4.8564e-01,  ...,  1.0196e-01,\n",
      "          2.2419e-01, -2.0359e-01],\n",
      "        [ 7.0395e-02, -2.3659e-01,  2.1504e-02,  ...,  2.1031e-01,\n",
      "         -2.6876e-02, -3.4389e-01],\n",
      "        [ 1.2880e-03, -2.6198e-01, -6.8864e-04,  ..., -2.8482e-01,\n",
      "         -1.8547e-01,  5.8167e-01],\n",
      "        ...,\n",
      "        [-1.6012e-01, -9.0964e-02,  9.2876e-02,  ..., -1.6276e-01,\n",
      "          8.0507e-01,  4.5891e-02],\n",
      "        [ 1.6928e-01,  5.3975e-01, -4.1855e-02,  ..., -5.9935e-01,\n",
      "          2.5872e-01,  5.1044e-01],\n",
      "        [ 5.2816e-01, -2.5724e-01,  1.6661e-02,  ..., -3.1528e-01,\n",
      "          6.0310e-02, -3.4761e-02]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0607, -0.3514,  0.2251,  ..., -0.6108, -0.2261, -0.1257],\n",
      "        [-0.1821,  0.1570, -0.5414,  ...,  0.2606,  0.6088,  0.0042],\n",
      "        [ 0.1405, -0.4792,  0.1267,  ...,  0.5875, -0.0509, -0.0162],\n",
      "        ...,\n",
      "        [-0.1640, -0.0148, -0.1043,  ..., -0.0838, -0.5283,  0.0249],\n",
      "        [-0.1034,  0.2345, -0.1541,  ...,  0.5559,  0.0615, -0.0176],\n",
      "        [ 0.0100,  0.6766, -0.0522,  ..., -0.4067, -0.4952,  0.0066]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 2.9917,  1.7054,  3.0687,  ...,  1.4630,  2.6638, -0.2965],\n",
      "        [-0.5054,  0.7241,  0.8177,  ...,  0.4809, -0.2572,  0.5996],\n",
      "        [-0.4808,  0.8044, -0.1192,  ..., -1.6147,  0.1569,  0.3625],\n",
      "        ...,\n",
      "        [ 0.2206,  0.6708,  0.3470,  ..., -0.2474,  0.7029, -0.4874],\n",
      "        [ 0.1204,  0.1773,  0.6233,  ...,  0.1275, -1.0872,  0.6148],\n",
      "        [ 2.6648,  0.5448,  0.2993,  ...,  5.2173,  1.2520,  2.7251]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0473,  0.1171,  0.1386,  0.1007,  0.1076,  0.1221,  0.1142,  0.1425,\n",
      "         0.0830,  0.1082,  0.1263,  0.0826,  0.0969,  0.1004,  0.0940,  0.1398,\n",
      "         0.1069,  0.0757,  0.1075,  0.0902,  0.1208,  0.1017,  0.1044,  0.1193,\n",
      "         0.0987,  0.1040,  0.0873,  0.1056,  0.1321,  0.1124,  0.1053,  0.1242,\n",
      "         0.1302,  0.0820,  0.1036,  0.0965,  0.1096,  0.1190,  0.1037,  0.0874,\n",
      "         0.1281,  0.0931,  0.1158,  0.0718,  0.1123,  0.1106,  0.1276,  0.0944,\n",
      "         0.1255,  0.0936,  0.0869,  0.1088,  0.1092,  0.1103,  0.0927,  0.1012,\n",
      "         0.1196,  0.1114,  0.0992,  0.0983,  0.0930,  0.1021,  0.1194,  0.1092,\n",
      "         0.1166,  0.1345,  0.1281,  0.0846,  0.1196,  0.1120,  0.1188,  0.0938,\n",
      "        -0.0410,  0.0798,  0.1206,  0.1016,  0.0934,  0.1037,  0.0932,  0.1108,\n",
      "         0.1176,  0.1241,  0.1091,  0.0932,  0.1029,  0.0964,  0.0896,  0.1104,\n",
      "         0.1108,  0.1199,  0.1042,  0.1214,  0.1185,  0.0930,  0.1002,  0.1107,\n",
      "         0.0855,  0.0429,  0.1031,  0.0954,  0.1197,  0.1248,  0.0924,  0.0981,\n",
      "         0.1146,  0.1005,  0.0919,  0.1111,  0.0853,  0.1102,  0.0920,  0.0997,\n",
      "         0.1405,  0.1015,  0.0935,  0.1237,  0.0795,  0.1263,  0.1131,  0.0988,\n",
      "         0.1144,  0.1389,  0.0432,  0.1119,  0.1199,  0.1033,  0.1214,  0.1012,\n",
      "         0.0868,  0.0951,  0.1093,  0.1350,  0.0880,  0.1053,  0.1124,  0.1018,\n",
      "         0.1207,  0.1038,  0.0959,  0.1051,  0.0869,  0.1070,  0.1015,  0.1191,\n",
      "         0.1355,  0.0884,  0.1130,  0.1187,  0.1218,  0.0927,  0.1209,  0.1251,\n",
      "         0.1001,  0.1670,  0.0989,  0.1106,  0.1023,  0.1569,  0.1267,  0.1294,\n",
      "         0.0885,  0.1065,  0.1024,  0.0674,  0.1000,  0.1195,  0.1003,  0.1227,\n",
      "         0.1152,  0.1365,  0.1170,  0.0991,  0.0957,  0.1050,  0.0891,  0.0896,\n",
      "         0.1065,  0.0947,  0.1063,  0.1143,  0.0893,  0.0984,  0.1506,  0.0897,\n",
      "         0.1157,  0.1076,  0.1053,  0.1239,  0.1142,  0.0820,  0.1002,  0.0973,\n",
      "         0.1036,  0.0767,  0.1031,  0.0781,  0.1157,  0.1410,  0.1336,  0.0863,\n",
      "         0.0866,  0.1253,  0.1255,  0.1157,  0.0643,  0.0821,  0.1151,  0.1039,\n",
      "         0.1357,  0.1634,  0.1432,  0.1101,  0.1063,  0.1131,  0.1208,  0.1703,\n",
      "         0.1402,  0.1035,  0.1536,  0.1164,  0.1013,  0.0999,  0.1087,  0.1087,\n",
      "         0.1009,  0.0694,  0.1137,  0.1102,  0.1228,  0.1254,  0.1328, -0.0709,\n",
      "         0.0906,  0.0517,  0.1148,  0.0879,  0.0960,  0.0824,  0.1540,  0.0992,\n",
      "         0.1596,  0.1109,  0.0393,  0.1053,  0.1335,  0.1171,  0.1065,  0.0307,\n",
      "         0.1053,  0.1406,  0.1223,  0.1119,  0.1082,  0.0865,  0.0828,  0.0881,\n",
      "         0.1373,  0.1173,  0.1150,  0.0950,  0.1214,  0.0818,  0.1135,  0.0879,\n",
      "         0.1206,  0.0907,  0.1120,  0.1206,  0.0665,  0.1092,  0.1022,  0.1049,\n",
      "         0.1057,  0.1058,  0.0963,  0.0139,  0.1090,  0.1144,  0.1125,  0.1088,\n",
      "         0.0566,  0.1250,  0.0902,  0.1239,  0.1185,  0.1160,  0.1008,  0.1085,\n",
      "         0.1821,  0.0999,  0.1230,  0.0924,  0.1018,  0.1012,  0.1317,  0.0967,\n",
      "         0.1061,  0.0857,  0.1295,  0.0848,  0.1504,  0.1294,  0.1008,  0.1278,\n",
      "         0.0953,  0.1252,  0.1107,  0.1012,  0.0834,  0.1027,  0.0744,  0.1225,\n",
      "         0.1151,  0.1120,  0.1254,  0.1278,  0.0979,  0.1168,  0.1137,  0.1137,\n",
      "         0.1349,  0.0971,  0.0981,  0.1585,  0.1015,  0.0990,  0.0961,  0.1125,\n",
      "         0.1369,  0.0702,  0.0877,  0.1174,  0.1116,  0.0773,  0.1362,  0.0846,\n",
      "         0.0859,  0.1191,  0.1022,  0.1197,  0.1199,  0.1242,  0.0808,  0.1111,\n",
      "         0.0935,  0.1065,  0.1244,  0.0850,  0.0991,  0.1063,  0.1129,  0.0939,\n",
      "         0.1721,  0.1198,  0.1130,  0.1132,  0.0958,  0.1198,  0.1094,  0.0816,\n",
      "         0.1472,  0.1009,  0.1305,  0.0924,  0.1004,  0.1212,  0.0971,  0.0424,\n",
      "         0.1079,  0.1084,  0.1158,  0.0946,  0.1257,  0.1100,  0.0928,  0.1040,\n",
      "         0.1077,  0.0934,  0.1013,  0.1200,  0.1188,  0.1183,  0.0919,  0.0951,\n",
      "         0.0959,  0.1147,  0.1419,  0.0825,  0.1066,  0.0602,  0.1119,  0.0962,\n",
      "         0.1127,  0.1136,  0.1082,  0.1139, -0.0581,  0.1302,  0.1203,  0.1009,\n",
      "         0.0893,  0.1109,  0.1262,  0.0988,  0.1084,  0.1045,  0.1168,  0.1058,\n",
      "         0.1087,  0.1369,  0.1025,  0.0805,  0.0714,  0.0957,  0.0485,  0.0912,\n",
      "         0.0929,  0.0996,  0.0973,  0.1350,  0.0880,  0.0962,  0.1113,  0.0854,\n",
      "         0.0703,  0.1077,  0.1076,  0.1268,  0.0857,  0.1237,  0.0961,  0.1640,\n",
      "         0.0939,  0.1071,  0.1456,  0.1228,  0.0700,  0.0921,  0.0954,  0.1049,\n",
      "         0.0917,  0.1050,  0.0851,  0.0948,  0.1153,  0.1064,  0.0968,  0.1437,\n",
      "         0.1159,  0.0809,  0.0630,  0.1001,  0.1210,  0.1194,  0.1625,  0.0909,\n",
      "         0.0272,  0.1230,  0.0984,  0.1059,  0.1263,  0.0994,  0.1172,  0.0989,\n",
      "         0.0900,  0.1096,  0.0768,  0.0979,  0.1210,  0.1435,  0.0947,  0.1014,\n",
      "         0.1400,  0.0241,  0.0985,  0.1648,  0.1144,  0.1148,  0.0922,  0.0476,\n",
      "         0.1329,  0.1194,  0.1103,  0.0828,  0.0977,  0.1124,  0.1062,  0.1021,\n",
      "         0.1083,  0.0495,  0.1004,  0.1059,  0.0986,  0.1512,  0.0920,  0.0882,\n",
      "         0.1298,  0.1119,  0.1070,  0.1310,  0.0790,  0.1163,  0.0894,  0.0884,\n",
      "         0.1010,  0.1050,  0.1041,  0.0930,  0.0901,  0.1314,  0.1034,  0.0363],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0050, -0.0661, -0.0934,  ..., -0.0677,  0.0597,  0.0068],\n",
      "        [ 0.0573, -0.0939,  0.0358,  ..., -0.0010, -0.0917, -0.0398],\n",
      "        [ 0.0299, -0.0942,  0.0522,  ...,  0.0133,  0.0589, -0.0306],\n",
      "        ...,\n",
      "        [ 0.0245, -0.0333,  0.0771,  ..., -0.0597, -0.0016, -0.0222],\n",
      "        [ 0.0108,  0.0642,  0.0223,  ...,  0.0435,  0.0403, -0.0010],\n",
      "        [-0.0125,  0.0490, -0.0282,  ..., -0.0427,  0.0469,  0.0048]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3197,  0.1509, -0.1193,  ..., -0.1701,  0.0865, -0.3341],\n",
      "        [ 0.1057, -0.1527,  0.3594,  ..., -0.9745,  0.5945,  0.4909],\n",
      "        [-0.3183, -0.3955, -0.2513,  ...,  0.1291, -0.0322, -0.0987],\n",
      "        ...,\n",
      "        [ 0.1786,  0.0138, -0.0114,  ...,  0.3396,  0.0095,  0.0676],\n",
      "        [-0.1896, -0.3930, -0.1804,  ...,  0.7862,  0.2563, -0.5125],\n",
      "        [-0.1023, -0.3036,  0.0570,  ..., -0.1839,  0.8773, -0.0665]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2577, -0.2416, -0.2164,  ...,  0.4358, -0.1727, -0.5202],\n",
      "        [-0.5616, -0.0098, -0.3601,  ...,  0.1775, -0.1075,  0.2806],\n",
      "        [-0.1394, -0.2239, -0.1680,  ...,  0.1999,  0.2323, -0.0246],\n",
      "        ...,\n",
      "        [ 0.2371, -0.3247, -0.3205,  ...,  0.0890, -0.6792, -0.1018],\n",
      "        [-0.1539, -0.1252,  0.2529,  ..., -0.1933,  0.6307,  0.1590],\n",
      "        [ 0.1066, -0.0201, -0.1698,  ...,  0.0158,  0.1219, -0.1766]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3947, -0.1746,  0.3261,  ..., -5.2095, -1.4787,  4.4654],\n",
      "        [ 0.3130, -0.1580,  0.1894,  ...,  0.4206,  0.0840,  0.0957],\n",
      "        [-0.0452,  0.1353, -0.3154,  ...,  0.4604, -0.8654,  0.7328],\n",
      "        ...,\n",
      "        [ 0.0533,  0.1298,  0.3567,  ..., -0.1846,  0.0506,  0.2100],\n",
      "        [ 0.7473, -0.3662,  0.2325,  ...,  0.7314, -0.4504, -0.1126],\n",
      "        [ 2.2695,  0.2889,  1.7382,  ..., -3.7185, -1.8417,  2.2015]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0403,  0.1218,  0.1006,  0.1315,  0.0961,  0.1389,  0.1197,  0.1273,\n",
      "         0.1034,  0.1060,  0.0909,  0.1109,  0.1023,  0.0846,  0.1104,  0.1895,\n",
      "         0.1029,  0.0977,  0.1222,  0.1058,  0.1329,  0.1141,  0.1120,  0.1247,\n",
      "         0.0954,  0.1121,  0.1067,  0.0886,  0.1138,  0.0967,  0.1070,  0.1271,\n",
      "         0.1415,  0.1099,  0.1071,  0.1169,  0.1347,  0.0839,  0.1259,  0.1272,\n",
      "         0.1279,  0.0878,  0.1219,  0.0843,  0.1012,  0.1170,  0.1316,  0.1153,\n",
      "         0.1093,  0.0754,  0.1067,  0.1224,  0.1130,  0.1321,  0.1146,  0.1013,\n",
      "         0.1216,  0.1373,  0.1167,  0.1039,  0.1013,  0.1135,  0.1073,  0.1172,\n",
      "         0.1211,  0.1383,  0.1229,  0.1143,  0.1514,  0.0836,  0.0884,  0.0899,\n",
      "         0.0334,  0.0837,  0.1324,  0.1242,  0.1127,  0.0988,  0.0900,  0.1387,\n",
      "         0.1143,  0.1294,  0.1179,  0.1114,  0.0886,  0.1119,  0.1352,  0.0891,\n",
      "         0.1137,  0.1197,  0.0985,  0.1194,  0.1187,  0.1245,  0.1068,  0.1181,\n",
      "         0.0962,  0.0521,  0.1031,  0.0957,  0.1313,  0.1128,  0.0921,  0.1256,\n",
      "         0.0854,  0.1145,  0.1140,  0.0994,  0.0808,  0.1129,  0.1115,  0.1070,\n",
      "         0.1244,  0.0824,  0.1096,  0.0969,  0.0767,  0.1098,  0.1212,  0.0874,\n",
      "         0.1172,  0.1284,  0.2097,  0.1299,  0.0994,  0.0995,  0.1067,  0.1354,\n",
      "         0.0917,  0.1061,  0.1079,  0.1317,  0.0807,  0.0977,  0.1163,  0.1182,\n",
      "         0.1281,  0.1036,  0.1197,  0.1044,  0.0867,  0.1002,  0.1138,  0.1119,\n",
      "         0.1220,  0.1050,  0.1121,  0.1042,  0.1260,  0.0783,  0.1318,  0.1125,\n",
      "         0.1150,  0.1311,  0.1253,  0.1178,  0.1157,  0.1108,  0.1104,  0.1143,\n",
      "         0.1050,  0.1045,  0.0964,  0.0835,  0.0980,  0.1336,  0.1022,  0.1332,\n",
      "         0.1164,  0.1391,  0.1199,  0.1013,  0.1023,  0.0964,  0.0877,  0.1008,\n",
      "         0.1004,  0.1031,  0.1091,  0.1016,  0.1077,  0.0905,  0.1295,  0.0967,\n",
      "         0.0904,  0.1241,  0.0791,  0.1377,  0.1049,  0.0714,  0.1246,  0.0996,\n",
      "         0.1150,  0.0873,  0.1088,  0.0834,  0.1303,  0.1121,  0.1245,  0.0944,\n",
      "         0.0714,  0.1300,  0.1216,  0.1158,  0.0809,  0.0989,  0.1188,  0.0844,\n",
      "         0.1096,  0.1502,  0.1337,  0.1114,  0.0976,  0.0922,  0.1479,  0.1416,\n",
      "         0.1510,  0.1043,  0.1417,  0.1076,  0.1020,  0.1087,  0.1077,  0.1069,\n",
      "         0.1356,  0.0933,  0.1311,  0.1044,  0.1241,  0.1133,  0.1073,  0.0794,\n",
      "         0.0975,  0.2058,  0.0959,  0.1078,  0.0968,  0.1037,  0.1289,  0.1193,\n",
      "         0.2024,  0.1027, -0.0292,  0.1039,  0.1638,  0.1253,  0.0909,  0.3312,\n",
      "         0.1132,  0.1320,  0.1264,  0.1282,  0.0879,  0.1167,  0.0926,  0.0845,\n",
      "         0.1447,  0.1111,  0.1018,  0.0987,  0.1124,  0.1072,  0.1090,  0.1020,\n",
      "         0.1371,  0.1056,  0.1135,  0.1301,  0.0784,  0.0924,  0.0992,  0.0899,\n",
      "         0.0999,  0.1175,  0.1163,  0.0357,  0.1147,  0.1087,  0.0899,  0.1086,\n",
      "         0.1128,  0.0869,  0.0853,  0.1331,  0.1121,  0.1312,  0.1134,  0.0876,\n",
      "         0.1640,  0.0891,  0.0748,  0.1155,  0.1046,  0.1106,  0.1051,  0.1129,\n",
      "         0.0919,  0.1126,  0.1268,  0.1133,  0.1277,  0.1026,  0.1188,  0.1358,\n",
      "         0.0901,  0.1430,  0.1201,  0.1012,  0.0852,  0.0906,  0.0966,  0.1102,\n",
      "         0.1385,  0.1171,  0.1366,  0.1220,  0.0944,  0.1150,  0.1270,  0.1232,\n",
      "         0.1105,  0.1057,  0.1012,  0.1216,  0.1173,  0.1056,  0.0927,  0.1019,\n",
      "         0.1194,  0.1128,  0.1095,  0.1119,  0.1057,  0.0931,  0.1211,  0.0913,\n",
      "         0.0895,  0.1109,  0.0964,  0.1131,  0.1182,  0.1091,  0.1026,  0.1220,\n",
      "         0.1035,  0.0947,  0.1279,  0.1061,  0.0899,  0.0994,  0.1328,  0.1064,\n",
      "         0.1371,  0.1340,  0.0961,  0.1143,  0.1184,  0.1160,  0.1287,  0.0880,\n",
      "         0.0968,  0.1146,  0.1187,  0.1077,  0.1247,  0.1283,  0.1145,  0.0952,\n",
      "         0.1260,  0.1105,  0.1309,  0.0998,  0.1000,  0.1240,  0.1039,  0.1025,\n",
      "         0.0912,  0.0982,  0.0971,  0.1202,  0.1231,  0.1095,  0.1123,  0.0872,\n",
      "         0.0784,  0.1291,  0.1157,  0.1087,  0.1044,  0.0686,  0.1115,  0.1052,\n",
      "         0.0990,  0.1178,  0.1483,  0.1076,  0.0294,  0.1194,  0.1158,  0.1111,\n",
      "         0.1024,  0.1068,  0.1393,  0.1097,  0.1307,  0.1159,  0.1296,  0.1185,\n",
      "         0.1303,  0.1232,  0.1051,  0.1073,  0.0836,  0.0997,  0.0453,  0.1096,\n",
      "         0.1103,  0.1085,  0.0946,  0.1419,  0.1056,  0.1150,  0.1088,  0.0952,\n",
      "         0.0926,  0.1233,  0.1098,  0.1242,  0.1671,  0.1097,  0.1333,  0.1269,\n",
      "         0.1079,  0.1050,  0.1183,  0.1120,  0.0926,  0.1113,  0.1135,  0.0974,\n",
      "         0.0921,  0.1144,  0.1148,  0.0972,  0.1109,  0.1198,  0.0966,  0.1282,\n",
      "         0.1175,  0.0987,  0.0811,  0.1152,  0.1081,  0.0978,  0.1750,  0.0925,\n",
      "         0.0274,  0.1038,  0.0915,  0.0993,  0.1051,  0.1055,  0.1101,  0.0865,\n",
      "         0.0986,  0.1096,  0.0834,  0.1206,  0.1189,  0.1254,  0.1124,  0.1108,\n",
      "         0.1414,  0.0278,  0.0910,  0.1653,  0.1255,  0.1106,  0.0627,  0.0819,\n",
      "         0.1150,  0.1205,  0.1090,  0.1075,  0.0951,  0.1129,  0.1186,  0.1191,\n",
      "         0.1198,  0.1376,  0.0928,  0.0850,  0.1115,  0.1487,  0.1308,  0.1026,\n",
      "         0.1022,  0.1238,  0.1042,  0.1183,  0.1226,  0.1021,  0.1163,  0.0954,\n",
      "         0.1188,  0.1049,  0.1071,  0.1030,  0.1304,  0.1102,  0.0973, -0.0360],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0342,  0.0677, -0.4314,  ...,  0.1806,  0.5929,  0.2433],\n",
      "        [-0.2075, -0.4279, -0.0438,  ...,  0.6743, -0.1684,  0.2257],\n",
      "        [-0.6725, -0.2775, -0.0514,  ..., -0.2726,  0.1344,  0.6141],\n",
      "        ...,\n",
      "        [-0.0812,  0.0322, -0.2195,  ..., -0.4810, -0.4159,  0.0952],\n",
      "        [-0.3263,  0.2085, -0.4496,  ..., -0.6448, -0.1975,  0.1421],\n",
      "        [-0.1225,  0.4938,  0.5733,  ...,  0.3720,  0.9585,  0.3384]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0199, -0.0729,  0.9052,  ...,  0.5677,  0.0108, -0.2503],\n",
      "        [ 0.0866,  0.2804,  0.6544,  ...,  0.7494,  0.7263, -0.3285],\n",
      "        [ 0.0647,  0.4147,  0.2311,  ..., -0.9953, -0.2353, -0.0563],\n",
      "        ...,\n",
      "        [-0.5283,  0.8608, -1.5136,  ...,  0.3805,  0.3928,  0.4230],\n",
      "        [-0.9296,  0.5328,  0.2698,  ...,  0.3390,  0.8900,  0.4286],\n",
      "        [-0.3092,  0.6710, -0.8484,  ..., -1.1941,  0.6947,  0.4606]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-3.4455e-01,  3.7372e-01,  9.7840e-01,  ..., -6.4393e-03,\n",
      "         -2.3680e+00, -1.1464e+00],\n",
      "        [-2.9458e-01, -2.7562e-01, -8.8674e-02,  ..., -9.4078e-01,\n",
      "         -6.2594e-01, -1.4751e-01],\n",
      "        [-3.5662e-01, -9.6189e-01, -2.4631e-01,  ..., -2.2102e-01,\n",
      "          6.9351e-01,  8.6100e-02],\n",
      "        ...,\n",
      "        [ 8.4670e-02, -1.2637e-02,  6.4556e-01,  ...,  2.8635e-01,\n",
      "          3.8694e-01, -2.4215e-03],\n",
      "        [-2.7926e-01, -5.1606e-01, -7.5896e-02,  ...,  3.1792e-02,\n",
      "          5.0772e-02, -1.4620e+00],\n",
      "        [-1.6321e+00,  1.1461e+00,  8.9079e-01,  ..., -2.3632e+00,\n",
      "         -2.6924e+00, -1.2813e+00]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.1.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0196,  0.1304,  0.1060,  0.1190,  0.1237,  0.1096,  0.1062,  0.1078,\n",
      "         0.1022,  0.1264,  0.1188,  0.1089,  0.0854,  0.0846,  0.0917,  0.1101,\n",
      "         0.1002,  0.0888,  0.1057,  0.0796,  0.1052,  0.0948,  0.1019,  0.1076,\n",
      "         0.1080,  0.1021,  0.1012,  0.1157,  0.1122,  0.1057,  0.0916,  0.1146,\n",
      "         0.1112,  0.0878,  0.0972,  0.0857,  0.1164,  0.1228,  0.1060,  0.0838,\n",
      "         0.1184,  0.0955,  0.1134,  0.0854,  0.0874,  0.1396,  0.1319,  0.1117,\n",
      "         0.0800,  0.0953,  0.1004,  0.1414,  0.0956,  0.1201,  0.1040,  0.1078,\n",
      "         0.1233,  0.1234,  0.0937,  0.1074,  0.1054,  0.1004,  0.0945,  0.1101,\n",
      "         0.1162,  0.1086,  0.0893,  0.0791,  0.1093,  0.1271,  0.0517,  0.0931,\n",
      "         0.0265,  0.1067,  0.1255,  0.0866,  0.0974,  0.0967,  0.0882,  0.0905,\n",
      "         0.0887,  0.1127,  0.1102,  0.1201,  0.1041,  0.1113,  0.0966,  0.1173,\n",
      "         0.1072,  0.1168,  0.0998,  0.1063,  0.0907,  0.1180,  0.1139,  0.0940,\n",
      "         0.0938,  0.0558,  0.1100,  0.1075,  0.1252,  0.1078,  0.0804,  0.0789,\n",
      "         0.0907,  0.0963,  0.0987,  0.1176,  0.0776,  0.1026,  0.0996,  0.1203,\n",
      "         0.1246,  0.0874,  0.1055,  0.1005,  0.0834,  0.1107,  0.1212,  0.1121,\n",
      "         0.1038,  0.1214,  0.2469,  0.1252,  0.1027,  0.1059,  0.0930,  0.1134,\n",
      "         0.0769,  0.0823,  0.1107,  0.1180,  0.1015,  0.1043,  0.1120,  0.1125,\n",
      "         0.1105,  0.1012,  0.1100,  0.1113,  0.0891,  0.1186,  0.0958,  0.1155,\n",
      "         0.1047,  0.1005,  0.1206,  0.1005,  0.1101,  0.0958,  0.1126,  0.1309,\n",
      "         0.1024,  0.1333,  0.0848,  0.1142,  0.0812,  0.1218,  0.1136,  0.1171,\n",
      "         0.0939,  0.1069,  0.0915,  0.0739,  0.1089,  0.1154,  0.1114,  0.1115,\n",
      "         0.1070,  0.1115,  0.1038,  0.1048,  0.0904,  0.0950,  0.0988,  0.1166,\n",
      "         0.0928,  0.0772,  0.1069,  0.1043,  0.1119,  0.0916,  0.1293,  0.0954,\n",
      "         0.0938,  0.0800,  0.1164,  0.1515,  0.1177,  0.0743,  0.0990,  0.1163,\n",
      "         0.0797,  0.0801,  0.1159,  0.0922,  0.1229,  0.0991,  0.1202,  0.0979,\n",
      "         0.0964,  0.1295,  0.1034,  0.1155,  0.0731,  0.0858,  0.1095,  0.0872,\n",
      "         0.1185,  0.1311,  0.1053,  0.1031,  0.1056,  0.1083,  0.1078,  0.1344,\n",
      "         0.1043,  0.1135,  0.0856,  0.1062,  0.1022,  0.0818,  0.1167,  0.1107,\n",
      "         0.1128,  0.0963,  0.1232,  0.0865,  0.1182,  0.1294,  0.0993,  0.0795,\n",
      "         0.0972,  0.1884,  0.1073,  0.0844,  0.1088,  0.1257,  0.1062,  0.1226,\n",
      "         0.1100,  0.1007,  0.0431,  0.1009,  0.1184,  0.1122,  0.0932,  0.2612,\n",
      "         0.0970,  0.1201,  0.1413,  0.1063,  0.1093,  0.0974,  0.0802,  0.0842,\n",
      "         0.1171,  0.1229,  0.1074,  0.1009,  0.1067,  0.1095,  0.1009,  0.0903,\n",
      "         0.1270,  0.0875,  0.0996,  0.1154,  0.0751,  0.0953,  0.1016,  0.1059,\n",
      "         0.1124,  0.1213,  0.1041,  0.0235,  0.0838,  0.1174,  0.1168,  0.0918,\n",
      "         0.1100,  0.1011,  0.1027,  0.0989,  0.1161,  0.0984,  0.1107,  0.0907,\n",
      "         0.1242,  0.0931,  0.1095,  0.0963,  0.1120,  0.0989,  0.1105,  0.1222,\n",
      "         0.1046,  0.0922,  0.1185,  0.0973,  0.1035,  0.1045,  0.1232,  0.1172,\n",
      "         0.0854,  0.1541,  0.1157,  0.0929,  0.0827,  0.0818,  0.0753,  0.1150,\n",
      "         0.1124,  0.1060,  0.1107,  0.1153,  0.0909,  0.1002,  0.1315,  0.0997,\n",
      "         0.1118,  0.0933,  0.0941,  0.1364,  0.0919,  0.1076,  0.0945,  0.1060,\n",
      "         0.1280,  0.0836,  0.1001,  0.1058,  0.1181,  0.0750,  0.1105,  0.0828,\n",
      "         0.0885,  0.1226,  0.1007,  0.1319,  0.1222,  0.1043,  0.1040,  0.1145,\n",
      "         0.1008,  0.1077,  0.1103,  0.1133,  0.1224,  0.0948,  0.1075,  0.0995,\n",
      "         0.0836,  0.1097,  0.0961,  0.0998,  0.1162,  0.1123,  0.1172,  0.0963,\n",
      "         0.1215,  0.1221,  0.1151,  0.0879,  0.0853,  0.1136,  0.1104,  0.0441,\n",
      "         0.1058,  0.0945,  0.1350,  0.0951,  0.1143,  0.0875,  0.0861,  0.1232,\n",
      "         0.1064,  0.1092,  0.1052,  0.1123,  0.1270,  0.1058,  0.1000,  0.1060,\n",
      "         0.1091,  0.1040,  0.1155,  0.0981,  0.0848,  0.0554,  0.1082,  0.0957,\n",
      "         0.1243,  0.1196,  0.1304,  0.1104,  0.0103,  0.1035,  0.1108,  0.0945,\n",
      "         0.0870,  0.1025,  0.1325,  0.0928,  0.0916,  0.1019,  0.1163,  0.1255,\n",
      "         0.1151,  0.1154,  0.1019,  0.1014,  0.0655,  0.0929,  0.0487,  0.1079,\n",
      "         0.0940,  0.0947,  0.0862,  0.1388,  0.1102,  0.0959,  0.0919,  0.0853,\n",
      "         0.0771,  0.0959,  0.0880,  0.1051,  0.0401,  0.0933,  0.0790,  0.1261,\n",
      "         0.0919,  0.1126,  0.1095,  0.1135,  0.0944,  0.1090,  0.0883,  0.1045,\n",
      "         0.0911,  0.0980,  0.0828,  0.1045,  0.1137,  0.1140,  0.0933,  0.1463,\n",
      "         0.1167,  0.1000,  0.0873,  0.1221,  0.1265,  0.1007,  0.1373,  0.0983,\n",
      "         0.0279,  0.0848,  0.0942,  0.0801,  0.0937,  0.0924,  0.1069,  0.0827,\n",
      "         0.1031,  0.1028,  0.0790,  0.0865,  0.1130,  0.1320,  0.0824,  0.1169,\n",
      "         0.1150,  0.0254,  0.0836,  0.1272,  0.1208,  0.1040,  0.0837,  0.1175,\n",
      "         0.1085,  0.0927,  0.1200,  0.1026,  0.0911,  0.1090,  0.1086,  0.1114,\n",
      "         0.1275,  0.1191,  0.1078,  0.1070,  0.1015,  0.1256,  0.1042,  0.0962,\n",
      "         0.1170,  0.1041,  0.1042,  0.1156,  0.0916,  0.0969,  0.0967,  0.0992,\n",
      "         0.0822,  0.0998,  0.1053,  0.1007,  0.0950,  0.1136,  0.0960,  0.0209],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1051,  0.0111, -0.0064,  ...,  0.0412,  0.0543, -0.0575],\n",
      "        [-0.0687,  0.0302,  0.0174,  ...,  0.0327, -0.0510, -0.0182],\n",
      "        [-0.0369, -0.0458, -0.0083,  ...,  0.0136,  0.0262, -0.0187],\n",
      "        ...,\n",
      "        [-0.0257, -0.0203, -0.0350,  ...,  0.0165, -0.0187, -0.0458],\n",
      "        [ 0.0954,  0.0192,  0.0665,  ...,  0.0080,  0.0077,  0.0980],\n",
      "        [ 0.0502,  0.0210,  0.0046,  ..., -0.0159, -0.0244,  0.0096]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4725, -0.3481,  0.5863,  ...,  0.0101,  0.2988,  0.4306],\n",
      "        [ 0.3500, -0.0601,  0.1348,  ..., -0.0408, -0.8586,  0.1763],\n",
      "        [-0.0613,  0.0766,  0.1711,  ...,  0.3316, -0.1286, -0.1948],\n",
      "        ...,\n",
      "        [-0.0342, -0.1715,  0.1142,  ...,  0.0591, -0.2435, -0.1928],\n",
      "        [-0.0954, -0.2083, -0.4351,  ...,  0.1229, -0.2219, -0.3945],\n",
      "        [-0.3679, -0.0233,  0.3260,  ..., -0.2704,  0.0455, -0.2744]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0112,  0.8046,  0.2783,  ...,  0.3301, -0.0721, -0.0453],\n",
      "        [-0.2152, -0.6954,  0.2147,  ..., -0.1033,  0.2911,  0.0920],\n",
      "        [ 0.1170, -0.2848,  0.1252,  ...,  0.0501,  0.5439,  0.1612],\n",
      "        ...,\n",
      "        [ 0.2863,  0.0722, -0.0684,  ..., -0.1193,  0.3551, -0.0166],\n",
      "        [ 0.1255, -0.4023, -0.2570,  ...,  0.2943, -0.7623, -0.0343],\n",
      "        [ 0.2803, -0.3122,  0.3447,  ..., -0.1917,  0.2276,  0.0473]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.5569,  0.9335,  0.5559,  ...,  0.4378,  1.9519, -1.6991],\n",
      "        [ 0.4890,  0.2042, -0.7006,  ...,  0.4795,  0.1470, -0.2356],\n",
      "        [-0.1895, -0.3835, -0.2385,  ...,  0.9401, -0.4836, -0.0919],\n",
      "        ...,\n",
      "        [ 0.2422,  0.0736, -0.0545,  ...,  0.2548,  1.3404,  0.1889],\n",
      "        [ 0.2802,  0.4670, -0.2533,  ..., -0.0671, -0.3612,  0.0609],\n",
      "        [ 0.1441,  1.8439,  1.7690,  ...,  0.4237,  2.8674, -0.7012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0243,  0.2772,  0.2459,  0.2090,  0.2542,  0.2247,  0.2431,  0.1941,\n",
      "         0.1326,  0.1985,  0.1578,  0.1561,  0.1794,  0.1510,  0.1530,  0.2275,\n",
      "         0.1683,  0.1312,  0.1678,  0.1646,  0.2290,  0.1560,  0.1794,  0.3074,\n",
      "         0.1899,  0.1858,  0.1478,  0.1749,  0.1992,  0.2550,  0.2009,  0.2570,\n",
      "         0.2035,  0.1900,  0.2183,  0.2893,  0.2224,  0.1865,  0.1864,  0.1542,\n",
      "         0.2319,  0.1270,  0.1850,  0.1361,  0.1299,  0.2292,  0.2456,  0.2048,\n",
      "         0.1780,  0.1988,  0.1702,  0.2034,  0.2221,  0.1628,  0.1882,  0.1707,\n",
      "         0.1485,  0.2107,  0.1316,  0.2163,  0.1821,  0.1527,  0.1716,  0.2228,\n",
      "         0.2202,  0.2040,  0.1909,  0.1354,  0.1832,  0.1803,  0.0894,  0.1441,\n",
      "         0.0364,  0.1709,  0.1912,  0.1965,  0.2275,  0.2068,  0.5386,  0.1891,\n",
      "         0.1904,  0.1619,  0.2167,  0.1552,  0.1464,  0.1206,  0.1572,  0.1904,\n",
      "         0.1679,  0.1795,  0.1728,  0.2217,  0.1261,  0.1949,  0.1659,  0.1863,\n",
      "         0.1623,  0.0879,  0.1590,  0.2090,  0.2282,  0.3593,  0.1530,  0.1559,\n",
      "         0.1454,  0.1675,  0.1593,  0.1798,  0.1766,  0.1763,  0.1698,  0.1539,\n",
      "         0.2056,  0.1594,  0.2015,  0.1800,  0.1584,  0.1792,  0.1955,  0.1752,\n",
      "         0.1803,  0.2188,  0.0310,  0.2090,  0.1793,  0.1875,  0.1726,  0.1569,\n",
      "         0.1340,  0.1659,  0.1654,  0.2576,  0.1536,  0.1986,  0.1803,  0.1712,\n",
      "         0.2067,  0.1966,  0.2419,  0.1944,  0.1302,  0.1635,  0.1695,  0.2105,\n",
      "         0.1927,  0.1661,  0.1706,  0.1927,  0.2083,  0.1614,  0.2372,  0.2069,\n",
      "         0.2212,  0.2345,  0.1343,  0.1782,  0.2765,  0.3319,  0.2066,  0.2737,\n",
      "         0.1691,  0.1715,  0.1812,  0.1159,  0.2177,  0.4168,  0.1652,  0.1788,\n",
      "         0.1694,  0.2146,  0.1684,  0.2010,  0.1507,  0.1495,  0.1358,  0.1624,\n",
      "         0.1737,  0.1226,  0.1494,  0.2813,  0.1684,  0.1914,  0.2028,  0.1876,\n",
      "         0.1569,  0.1980,  0.1244,  0.2772,  0.2028,  0.1319,  0.1759,  0.1872,\n",
      "         0.1819,  0.1042,  0.1791,  0.1510,  0.2223,  0.2756,  0.2640,  0.1605,\n",
      "         0.1480,  0.1958,  0.1850,  0.1761,  0.1219,  0.1534,  0.1831,  0.2057,\n",
      "         0.1633,  0.4092,  0.3547,  0.2170,  0.1527,  0.2467,  0.2560,  0.2162,\n",
      "         0.2279,  0.2086,  0.3226,  0.2011,  0.2109,  0.2263,  0.2480,  0.1673,\n",
      "         0.1516,  0.1643,  0.2480,  0.1898,  0.3451,  0.2591,  0.1465,  0.1233,\n",
      "         0.1751,  0.0427,  0.1875,  0.1544,  0.1441,  0.1567,  0.2167,  0.1725,\n",
      "         0.3031,  0.1819,  0.0685,  0.1697,  0.2302,  0.2188,  0.1253, -0.0572,\n",
      "         0.1829,  0.1984,  0.3285,  0.1937,  0.1790,  0.1548,  0.1701,  0.1227,\n",
      "         0.2816,  0.2264,  0.1693,  0.1746,  0.2096,  0.1272,  0.1905,  0.1766,\n",
      "         0.1965,  0.1385,  0.2131,  0.2121,  0.1151,  0.1969,  0.1708,  0.2013,\n",
      "         0.1494,  0.1798,  0.1282,  0.0289,  0.1813,  0.1655,  0.1845,  0.1710,\n",
      "         0.0431,  0.2010,  0.1387,  0.2079,  0.2347,  0.1700,  0.1712,  0.2010,\n",
      "         0.2536,  0.1519,  0.2694,  0.1885,  0.1564,  0.1689,  0.1959,  0.1557,\n",
      "         0.1981,  0.2134,  0.2114,  0.1981,  0.3225,  0.2064,  0.2343,  0.2353,\n",
      "         0.1590,  0.2224,  0.1765,  0.1710,  0.1420,  0.1863,  0.1343,  0.1800,\n",
      "         0.1626,  0.1906,  0.1837,  0.2672,  0.1387,  0.1549,  0.1782,  0.1809,\n",
      "         0.1920,  0.1547,  0.2069,  0.2313,  0.1616,  0.1227,  0.1763,  0.1847,\n",
      "         0.2063,  0.1063,  0.1848,  0.2043,  0.1988,  0.1705,  0.2195,  0.1510,\n",
      "         0.1627,  0.2450,  0.1591,  0.1949,  0.1876,  0.2387,  0.1441,  0.2524,\n",
      "         0.1448,  0.1864,  0.2781,  0.2455,  0.2106,  0.1611,  0.1876,  0.1690,\n",
      "         0.1982,  0.1902,  0.1615,  0.2127,  0.1562,  0.2194,  0.1849,  0.1489,\n",
      "         0.2170,  0.1577,  0.2447,  0.1606,  0.1609,  0.1882,  0.1615,  0.0597,\n",
      "         0.1825,  0.1713,  0.2247,  0.1956,  0.1692,  0.1687,  0.1612,  0.1969,\n",
      "         0.1542,  0.1486,  0.1545,  0.1722,  0.1832,  0.1940,  0.1979,  0.1753,\n",
      "         0.1628,  0.1752,  0.2140,  0.1492,  0.1480, -0.0668,  0.2045,  0.1406,\n",
      "         0.2320,  0.2540,  0.2247,  0.1741, -0.0240,  0.1919,  0.2564,  0.1974,\n",
      "         0.1780,  0.2027,  0.1898,  0.1656,  0.1837,  0.1716,  0.2128,  0.2508,\n",
      "         0.1885,  0.2194,  0.1728,  0.1398,  0.1224,  0.1620,  0.1005,  0.2031,\n",
      "         0.1751,  0.1318,  0.1804,  0.1773,  0.1573,  0.1466,  0.1555,  0.1667,\n",
      "         0.1254,  0.2258,  0.1909,  0.1973,  0.0920,  0.2030,  0.1330,  0.2113,\n",
      "         0.1522,  0.1817,  0.2183,  0.1651,  0.1645,  0.1517,  0.1646,  0.1588,\n",
      "         0.1652,  0.2410,  0.1301,  0.1425,  0.1956,  0.1622,  0.2321,  0.2206,\n",
      "         0.1754,  0.1796,  0.1548,  0.1755,  0.1916,  0.2129,  0.2930,  0.1657,\n",
      "         0.0250,  0.2025,  0.1569,  0.1572,  0.2242,  0.1500,  0.1386,  0.2048,\n",
      "         0.1693,  0.1827,  0.1414,  0.1615,  0.2265,  0.2580,  0.1611,  0.1505,\n",
      "         0.2736, -0.0302,  0.1756,  0.2408,  0.3212,  0.1061,  0.1326,  0.0443,\n",
      "         0.2387,  0.1862,  0.2582,  0.1677,  0.1545,  0.2569,  0.1702,  0.1874,\n",
      "         0.2297,  0.1259,  0.1402,  0.1454,  0.1667,  0.2986,  0.1718,  0.1619,\n",
      "         0.1946,  0.1810,  0.1989,  0.1782,  0.1370,  0.1909,  0.2008,  0.1543,\n",
      "         0.1350,  0.1567,  0.1752,  0.1474,  0.1802,  0.2529,  0.1671,  0.0325],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0149, -0.0429, -0.0194,  ..., -0.1023, -0.0570,  0.0076],\n",
      "        [ 0.0449,  0.0917,  0.0728,  ..., -0.0666,  0.0573, -0.0132],\n",
      "        [ 0.0333,  0.0612,  0.0491,  ..., -0.0182, -0.0108, -0.0118],\n",
      "        ...,\n",
      "        [ 0.0049,  0.0081, -0.0397,  ..., -0.0285, -0.0751,  0.0003],\n",
      "        [ 0.0227,  0.0135,  0.0581,  ...,  0.1060, -0.0871,  0.0234],\n",
      "        [ 0.0056,  0.0658,  0.0086,  ...,  0.0214,  0.0582,  0.0505]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0107,  0.1704, -0.4995,  ...,  0.0599,  0.4537, -0.5359],\n",
      "        [-0.1231,  0.5215, -0.5414,  ...,  0.1952, -0.2001,  1.0093],\n",
      "        [-0.4800, -0.0913,  0.0607,  ...,  0.1154, -0.3528,  0.0777],\n",
      "        ...,\n",
      "        [ 0.0174, -0.6660,  0.1328,  ...,  0.1157,  0.2304,  0.0200],\n",
      "        [-0.0242, -0.2030,  0.2193,  ...,  0.2003,  0.2849, -0.1032],\n",
      "        [ 0.2863,  0.0426, -0.0945,  ...,  0.2758,  0.3288,  0.2966]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.7230, -0.7690,  0.5854,  ...,  0.9823,  0.5233, -0.0964],\n",
      "        [ 0.2554,  0.9219,  0.3053,  ..., -0.0916,  0.3193,  0.3021],\n",
      "        [ 0.6228, -0.7489, -0.2938,  ..., -0.3318, -0.4931,  0.6680],\n",
      "        ...,\n",
      "        [ 0.1437, -0.6566,  0.7580,  ..., -1.1699, -0.0385,  0.1128],\n",
      "        [-0.1192,  0.3826,  0.3679,  ..., -0.1666,  0.2991, -0.0671],\n",
      "        [ 0.5134, -0.0288, -0.8155,  ...,  0.1197,  0.9276, -0.1797]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.6826,  1.1084,  0.4797,  ...,  3.6476,  2.1582,  1.0528],\n",
      "        [ 0.7054,  0.2532, -0.4178,  ..., -0.4070, -0.3744, -0.1555],\n",
      "        [ 0.3783,  0.3664,  0.3484,  ..., -0.4856,  0.0109,  0.6548],\n",
      "        ...,\n",
      "        [ 0.3359, -0.5723,  0.1797,  ..., -0.1050, -0.9466,  0.3015],\n",
      "        [-0.0063,  1.0916, -0.3823,  ..., -0.6715, -0.1559,  0.5046],\n",
      "        [-0.5185, -0.3133,  1.4174,  ...,  3.8458,  3.7554,  2.4237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0661,  0.1633,  0.1355,  0.1366,  0.1683,  0.1596,  0.1481,  0.1176,\n",
      "         0.1287,  0.1618,  0.1389,  0.1414,  0.1091,  0.1127,  0.1350,  0.1847,\n",
      "         0.1514,  0.1152,  0.1702,  0.1574,  0.1461,  0.1043,  0.1317,  0.1374,\n",
      "         0.1218,  0.1178,  0.1296,  0.1217,  0.1674,  0.1412,  0.1259,  0.1744,\n",
      "         0.1525,  0.1290,  0.1626,  0.1521,  0.1412,  0.1440,  0.1458,  0.1281,\n",
      "         0.1359,  0.1108,  0.1565,  0.1280,  0.1257,  0.1438,  0.1282,  0.1403,\n",
      "         0.1241,  0.1262,  0.1113,  0.1663,  0.1562,  0.1381,  0.1328,  0.1357,\n",
      "         0.1548,  0.1660,  0.1203,  0.1422,  0.1354,  0.1079,  0.1184,  0.1240,\n",
      "         0.1895,  0.1511,  0.1087,  0.1661,  0.1860,  0.1325,  0.0965,  0.1119,\n",
      "        -0.0261,  0.1180,  0.1613,  0.1546,  0.1607,  0.1216,  0.1214,  0.1581,\n",
      "         0.1231,  0.1539,  0.1393,  0.1587,  0.1360,  0.1224,  0.1508,  0.1425,\n",
      "         0.1436,  0.1627,  0.1381,  0.1501,  0.1558,  0.1466,  0.1218,  0.1195,\n",
      "         0.1303,  0.0693,  0.1452,  0.1350,  0.1384,  0.1590,  0.0987,  0.1516,\n",
      "         0.1350,  0.1456,  0.1429,  0.1300,  0.1162,  0.1353,  0.1442,  0.1327,\n",
      "         0.1565,  0.1142,  0.1706,  0.1172,  0.1252,  0.1652,  0.1418,  0.1421,\n",
      "         0.1331,  0.1792,  0.3695,  0.2000,  0.1238,  0.1320,  0.1268,  0.1577,\n",
      "         0.1191,  0.1372,  0.1348,  0.1474,  0.1176,  0.1310,  0.1366,  0.1229,\n",
      "         0.1785,  0.1438,  0.1335,  0.1309,  0.1381,  0.1203,  0.1241,  0.1419,\n",
      "         0.1455,  0.1330,  0.1269,  0.1346,  0.1232,  0.1169,  0.1426,  0.1571,\n",
      "         0.1573,  0.1474,  0.1542,  0.1769,  0.1338,  0.0958,  0.1540,  0.1656,\n",
      "         0.1420,  0.1276,  0.1300,  0.1107,  0.1344,  0.1695,  0.1293,  0.1351,\n",
      "         0.1432,  0.1607,  0.1308,  0.1543,  0.1326,  0.1116,  0.1347,  0.1157,\n",
      "         0.1181,  0.1395,  0.1225,  0.1169,  0.1094,  0.1439,  0.1642,  0.1244,\n",
      "         0.1337,  0.1433,  0.1328,  0.1687,  0.1500,  0.1312,  0.1294,  0.1242,\n",
      "         0.1754,  0.1494,  0.1197,  0.1004,  0.1600,  0.1765,  0.1408,  0.1310,\n",
      "         0.1246,  0.1346,  0.1378,  0.1345,  0.1271,  0.1438,  0.1512,  0.1178,\n",
      "         0.1203,  0.1715,  0.1870,  0.1212,  0.1350,  0.1356,  0.1797,  0.1696,\n",
      "         0.1276,  0.1472,  0.1850,  0.1626,  0.1224,  0.1338,  0.1480,  0.1084,\n",
      "         0.1384,  0.1296,  0.1575,  0.1451,  0.1378,  0.1665,  0.1440, -0.1031,\n",
      "         0.1535,  0.2833,  0.1416,  0.1348,  0.1406,  0.1672,  0.1538,  0.1323,\n",
      "         0.2343,  0.1246, -0.0598,  0.1187,  0.1673,  0.1546,  0.1251,  0.3957,\n",
      "         0.1330,  0.1532,  0.1692,  0.1399,  0.1375,  0.1400,  0.1256,  0.1368,\n",
      "         0.1398,  0.1299,  0.1404,  0.1251,  0.1658,  0.1266,  0.1537,  0.1109,\n",
      "         0.1483,  0.1361,  0.1608,  0.1703,  0.1235,  0.1124,  0.1404,  0.1331,\n",
      "         0.1353,  0.1688,  0.1308,  0.0364,  0.1482,  0.1352,  0.1329,  0.1276,\n",
      "         0.1381,  0.1461,  0.1146,  0.1487,  0.1594,  0.1252,  0.1445,  0.1239,\n",
      "         0.1746,  0.1203,  0.1538,  0.1327,  0.1360,  0.1627,  0.1171,  0.1491,\n",
      "         0.1390,  0.1128,  0.1548,  0.1306,  0.1571,  0.1497,  0.1566,  0.1546,\n",
      "         0.1233,  0.1704,  0.1430,  0.1236,  0.1429,  0.1675,  0.0811,  0.1282,\n",
      "         0.1395,  0.1264,  0.1571,  0.1655,  0.1289,  0.1584,  0.1631,  0.1410,\n",
      "         0.1595,  0.1368,  0.1487,  0.1693,  0.1416,  0.1300,  0.1269,  0.1345,\n",
      "         0.1490,  0.1353,  0.1293,  0.1318,  0.1586,  0.1235,  0.1687,  0.1311,\n",
      "         0.1248,  0.1917,  0.1172,  0.1466,  0.1709,  0.1787,  0.1009,  0.1484,\n",
      "         0.1377,  0.1278,  0.1352,  0.1235,  0.1371,  0.1318,  0.1074,  0.1771,\n",
      "         0.1482,  0.1562,  0.1416,  0.1604,  0.1324,  0.1290,  0.1318,  0.1242,\n",
      "         0.1381,  0.1280,  0.1520,  0.0947,  0.1476,  0.1469,  0.1346,  0.1115,\n",
      "         0.1888,  0.1507,  0.1641,  0.1428,  0.1224,  0.1846,  0.1291,  0.1254,\n",
      "         0.1228,  0.1146,  0.1230,  0.1434,  0.1638,  0.1525,  0.1457,  0.1353,\n",
      "         0.1257,  0.1501,  0.1773,  0.1584,  0.1123,  0.0712,  0.1083,  0.1340,\n",
      "         0.1501,  0.1341,  0.1484,  0.1573, -0.0388,  0.1457,  0.1438,  0.1266,\n",
      "         0.1091,  0.1487,  0.1796,  0.1575,  0.1414,  0.1428,  0.1484,  0.1840,\n",
      "         0.1575,  0.1632,  0.1149,  0.1336,  0.0903,  0.1138,  0.0674,  0.1713,\n",
      "         0.1400,  0.1078,  0.1242,  0.1455,  0.1412,  0.1341,  0.1584,  0.1228,\n",
      "         0.1185,  0.1535,  0.1412,  0.1317,  0.2880,  0.1370,  0.1348,  0.1405,\n",
      "         0.1043,  0.1262,  0.1260,  0.1425,  0.1140,  0.1595,  0.1362,  0.1674,\n",
      "         0.1308,  0.1110,  0.1510,  0.1211,  0.1509,  0.1180,  0.1190,  0.1701,\n",
      "         0.1781,  0.1023,  0.1296,  0.1358,  0.1425,  0.1520,  0.1731,  0.1287,\n",
      "        -0.0223,  0.1734,  0.1293,  0.1383,  0.1447,  0.1233,  0.1122,  0.1162,\n",
      "         0.1458,  0.1388,  0.1083,  0.1510,  0.1524,  0.1234,  0.1274,  0.1178,\n",
      "         0.1819,  0.0747,  0.1142,  0.1694,  0.1673,  0.1353,  0.1054,  0.1018,\n",
      "         0.1198,  0.1187,  0.1782,  0.1192,  0.1135,  0.1390,  0.1252,  0.1374,\n",
      "         0.1540,  0.1796,  0.1165,  0.1342,  0.1562,  0.1588,  0.1202,  0.1080,\n",
      "         0.1573,  0.1679,  0.1340,  0.1946,  0.1561,  0.1407,  0.1382,  0.1233,\n",
      "         0.1191,  0.1483,  0.1421,  0.1091,  0.1296,  0.1500,  0.1454, -0.0298],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0021, -0.5612,  0.0651,  ...,  0.2572,  0.0033,  0.0724],\n",
      "        [ 0.1070,  0.1363, -0.5938,  ...,  0.1848,  0.1376,  0.1904],\n",
      "        [ 0.0940, -0.1267, -0.0105,  ...,  0.4098, -0.5058, -0.0678],\n",
      "        ...,\n",
      "        [ 0.0381,  0.1327,  0.4750,  ..., -0.5932, -0.3774,  0.1778],\n",
      "        [ 0.3949, -0.5607,  0.7681,  ..., -0.5381, -0.3658,  0.8843],\n",
      "        [-0.0181,  0.1461, -0.0537,  ...,  0.2734, -0.0798,  0.0816]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2040,  1.0685,  1.2571,  ..., -0.1193, -0.6731,  0.0026],\n",
      "        [-0.3658, -0.4903,  1.3604,  ...,  0.7533, -0.2092, -0.0851],\n",
      "        [ 0.9507, -0.6270,  0.1320,  ..., -1.4104, -0.3912,  0.9294],\n",
      "        ...,\n",
      "        [ 0.4135,  0.9214,  0.0414,  ..., -0.3895,  1.2277, -0.0456],\n",
      "        [-0.2052, -0.8346, -0.0558,  ..., -0.2646, -0.7067, -0.2433],\n",
      "        [ 0.1158,  0.1675,  1.4343,  ..., -0.5769,  0.8314,  0.3234]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.0313, -0.4197, -0.7844,  ..., -1.6020,  0.6251, -1.1336],\n",
      "        [ 0.5424, -0.3319,  0.0694,  ...,  0.6868,  0.4368, -0.7253],\n",
      "        [ 0.1179,  0.1475, -0.4730,  ...,  0.0746, -0.0291,  1.6210],\n",
      "        ...,\n",
      "        [ 0.7494,  0.7136, -0.5988,  ..., -0.3824,  0.3883,  0.3207],\n",
      "        [ 0.4685, -0.4564,  0.7756,  ...,  1.0226, -0.4559,  0.3407],\n",
      "        [-1.6502,  2.4404, -5.7322,  ..., -1.5146,  2.2480,  0.1036]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.2.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0232,  0.1354,  0.1395,  0.1215,  0.1469,  0.1443,  0.1275,  0.1194,\n",
      "         0.1249,  0.1328,  0.1222,  0.1165,  0.1135,  0.1323,  0.1123,  0.1175,\n",
      "         0.1098,  0.1240,  0.1277,  0.1196,  0.1388,  0.1126,  0.1264,  0.1302,\n",
      "         0.1266,  0.1228,  0.1243,  0.1432,  0.1086,  0.1249,  0.1232,  0.1483,\n",
      "         0.1378,  0.1300,  0.1240,  0.1162,  0.1238,  0.1330,  0.1256,  0.1143,\n",
      "         0.1338,  0.1103,  0.1283,  0.1040,  0.1299,  0.1519,  0.1413,  0.1485,\n",
      "         0.1266,  0.1240,  0.1179,  0.1395,  0.1288,  0.1224,  0.1286,  0.1219,\n",
      "         0.1376,  0.1360,  0.1128,  0.1519,  0.1123,  0.1131,  0.1261,  0.1414,\n",
      "         0.1493,  0.1389,  0.1361,  0.1090,  0.1435,  0.1219,  0.0585,  0.1256,\n",
      "         0.0171,  0.1315,  0.1478,  0.1171,  0.1208,  0.1369,  0.1133,  0.1208,\n",
      "         0.1056,  0.1226,  0.1368,  0.1418,  0.1308,  0.1101,  0.1408,  0.1477,\n",
      "         0.1334,  0.1319,  0.1256,  0.1403,  0.1092,  0.1412,  0.1189,  0.1362,\n",
      "         0.1143,  0.0540,  0.1257,  0.1298,  0.1713,  0.1322,  0.1180,  0.1055,\n",
      "         0.1246,  0.1189,  0.1311,  0.1234,  0.0969,  0.1061,  0.1296,  0.1298,\n",
      "         0.1370,  0.1163,  0.1330,  0.1494,  0.1114,  0.1380,  0.1253,  0.1374,\n",
      "         0.1484,  0.1591,  0.1981,  0.1496,  0.1393,  0.1105,  0.1283,  0.1275,\n",
      "         0.1256,  0.1016,  0.1416,  0.1284,  0.1220,  0.1299,  0.1410,  0.1315,\n",
      "         0.1304,  0.1313,  0.1244,  0.1271,  0.1170,  0.1191,  0.1339,  0.1351,\n",
      "         0.1260,  0.1163,  0.1236,  0.1377,  0.1276,  0.1247,  0.1443,  0.1454,\n",
      "         0.1369,  0.1293,  0.1130,  0.1538,  0.1263,  0.1035,  0.1312,  0.1273,\n",
      "         0.1143,  0.1305,  0.1078,  0.0978,  0.1423,  0.1386,  0.1330,  0.1380,\n",
      "         0.1362,  0.1416,  0.1330,  0.1228,  0.1224,  0.1152,  0.1132,  0.1245,\n",
      "         0.1272,  0.1097,  0.1306,  0.1081,  0.1298,  0.1126,  0.1584,  0.1045,\n",
      "         0.1157,  0.0960,  0.1164,  0.1397,  0.1475,  0.1028,  0.1372,  0.1243,\n",
      "         0.1231,  0.1290,  0.1300,  0.1234,  0.1452,  0.1311,  0.1292,  0.1181,\n",
      "         0.1073,  0.1516,  0.1176,  0.1389,  0.0972,  0.1339,  0.1280,  0.1228,\n",
      "         0.1217,  0.1323,  0.1379,  0.1217,  0.1263,  0.1575,  0.1539,  0.1365,\n",
      "         0.1284,  0.1203,  0.1300,  0.1440,  0.1094,  0.1113,  0.1445,  0.1369,\n",
      "         0.1247,  0.1218,  0.1376,  0.1185,  0.1459,  0.1556,  0.1346,  0.0946,\n",
      "         0.1306,  0.1795,  0.1169,  0.1121,  0.1253,  0.1299,  0.1540,  0.1387,\n",
      "         0.1584,  0.1334,  0.0445,  0.1331,  0.1434,  0.1268,  0.1075,  0.2885,\n",
      "         0.1242,  0.1623,  0.1484,  0.1158,  0.1253,  0.1280,  0.1221,  0.1132,\n",
      "         0.1296,  0.1467,  0.1281,  0.1318,  0.1274,  0.1314,  0.1164,  0.1248,\n",
      "         0.1449,  0.1063,  0.1298,  0.1594,  0.0908,  0.1279,  0.1179,  0.1400,\n",
      "         0.1257,  0.1459,  0.1273,  0.0092,  0.1218,  0.1333,  0.1366,  0.1244,\n",
      "         0.1073,  0.1291,  0.1234,  0.1180,  0.1529,  0.1132,  0.1265,  0.1180,\n",
      "         0.1515,  0.1196,  0.1339,  0.1260,  0.1320,  0.1201,  0.1331,  0.1340,\n",
      "         0.1267,  0.1176,  0.1346,  0.1266,  0.1450,  0.1312,  0.1341,  0.1435,\n",
      "         0.1204,  0.1685,  0.1263,  0.1286,  0.1115,  0.1155,  0.1033,  0.1287,\n",
      "         0.1279,  0.1302,  0.1319,  0.1321,  0.1174,  0.1222,  0.1256,  0.1199,\n",
      "         0.1379,  0.1407,  0.1238,  0.1599,  0.1342,  0.1292,  0.1051,  0.1344,\n",
      "         0.1454,  0.0844,  0.1354,  0.1465,  0.1446,  0.1080,  0.1329,  0.1142,\n",
      "         0.1159,  0.1543,  0.1258,  0.1417,  0.1518,  0.1225,  0.1175,  0.1319,\n",
      "         0.1235,  0.1293,  0.1418,  0.1435,  0.1284,  0.1201,  0.1316,  0.1364,\n",
      "         0.1010,  0.1339,  0.1278,  0.1267,  0.1328,  0.1273,  0.1385,  0.1144,\n",
      "         0.1386,  0.1256,  0.1335,  0.1196,  0.1140,  0.1290,  0.1361,  0.0702,\n",
      "         0.1368,  0.1230,  0.1379,  0.1327,  0.1246,  0.1187,  0.1180,  0.1255,\n",
      "         0.1148,  0.1140,  0.1217,  0.1308,  0.1527,  0.1471,  0.1188,  0.1271,\n",
      "         0.1301,  0.1308,  0.1361,  0.1100,  0.1159,  0.0614,  0.1326,  0.1115,\n",
      "         0.1237,  0.1320,  0.1481,  0.1235, -0.0159,  0.1496,  0.1406,  0.1219,\n",
      "         0.1250,  0.1150,  0.1442,  0.1211,  0.1191,  0.1168,  0.1335,  0.1455,\n",
      "         0.1284,  0.1360,  0.1272,  0.1288,  0.0885,  0.1168,  0.0669,  0.1139,\n",
      "         0.1258,  0.1084,  0.1162,  0.1536,  0.1176,  0.1252,  0.1358,  0.1100,\n",
      "         0.1056,  0.1262,  0.1297,  0.1330,  0.0790,  0.1324,  0.1188,  0.1444,\n",
      "         0.1152,  0.1299,  0.1248,  0.1345,  0.1049,  0.1334,  0.1278,  0.1268,\n",
      "         0.1253,  0.1188,  0.1150,  0.1210,  0.1351,  0.1258,  0.1044,  0.1530,\n",
      "         0.1416,  0.1287,  0.1009,  0.1203,  0.1307,  0.1269,  0.1541,  0.1224,\n",
      "         0.0153,  0.1306,  0.1139,  0.1100,  0.1120,  0.1100,  0.1290,  0.1180,\n",
      "         0.1304,  0.1203,  0.1064,  0.1186,  0.1198,  0.1509,  0.1151,  0.1363,\n",
      "         0.1244, -0.0214,  0.1188,  0.1499,  0.1696,  0.1239,  0.1107,  0.1193,\n",
      "         0.1462,  0.1175,  0.1384,  0.1084,  0.1160,  0.1284,  0.1220,  0.1291,\n",
      "         0.1460,  0.1088,  0.1249,  0.1252,  0.1320,  0.1362,  0.1195,  0.1180,\n",
      "         0.1548,  0.1166,  0.1160,  0.1469,  0.1101,  0.1298,  0.1153,  0.1210,\n",
      "         0.1138,  0.1311,  0.1183,  0.1174,  0.1178,  0.1371,  0.1267,  0.0158],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1000, -0.0552, -0.0044,  ..., -0.0244, -0.0069,  0.1018],\n",
      "        [-0.0758,  0.0380, -0.0539,  ..., -0.1267, -0.0204,  0.0300],\n",
      "        [-0.0167, -0.0261,  0.0309,  ..., -0.0451,  0.0234,  0.0156],\n",
      "        ...,\n",
      "        [-0.0337,  0.1234,  0.0028,  ...,  0.0149,  0.0258,  0.0166],\n",
      "        [-0.0491, -0.0500, -0.0228,  ...,  0.0584,  0.0063,  0.0997],\n",
      "        [ 0.0176, -0.0323, -0.0635,  ...,  0.0910,  0.0706,  0.0124]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4565,  0.2067,  0.0756,  ..., -0.3418, -0.1172, -0.2919],\n",
      "        [-0.0480, -0.3379, -0.1556,  ..., -0.3733, -0.3717, -0.3656],\n",
      "        [ 0.0121,  0.0297,  0.0734,  ..., -0.1326, -0.2754,  0.0438],\n",
      "        ...,\n",
      "        [-0.3864, -0.3948, -0.5201,  ...,  0.1177,  0.0983,  0.0984],\n",
      "        [ 0.1846,  0.0059,  0.2670,  ..., -0.3643, -0.0256, -0.4181],\n",
      "        [ 0.3330,  0.3467,  0.0306,  ...,  0.1206, -0.3223, -0.2119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2377,  0.1923,  0.5082,  ...,  0.2495,  1.0831,  0.0254],\n",
      "        [-0.0552,  0.3046,  0.5859,  ..., -0.8091, -0.0733,  0.0494],\n",
      "        [-0.2588,  0.7092, -0.0227,  ...,  0.3861, -0.3815,  0.0604],\n",
      "        ...,\n",
      "        [ 0.1340,  0.1323, -0.1854,  ..., -0.8133,  0.7976,  0.0779],\n",
      "        [-0.0417, -0.8983,  0.1535,  ...,  0.2420,  0.9707, -0.0505],\n",
      "        [ 0.2919,  0.5125,  0.3532,  ...,  0.6387,  0.8505,  0.0962]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0600,  0.4669, -0.1883,  ..., -1.6997,  0.8765,  0.2415],\n",
      "        [-1.0602, -0.5382, -0.4423,  ..., -0.2793, -0.4617,  0.4693],\n",
      "        [ 0.6527, -0.7283, -0.3322,  ...,  0.2832,  0.7370,  0.0882],\n",
      "        ...,\n",
      "        [ 0.1260, -0.8535, -0.5675,  ...,  0.6142,  0.9973,  0.4454],\n",
      "        [ 0.6116, -0.6377, -0.6705,  ..., -0.3894,  0.4614, -0.7919],\n",
      "        [-0.1476,  0.2204,  2.2552,  ..., -1.7113,  0.4689, -4.9546]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0248,  0.2261,  0.2053,  0.2401,  0.2869,  0.2480,  0.2589,  0.2451,\n",
      "         0.2157,  0.2332,  0.2139,  0.1987,  0.2210,  0.2160,  0.1855,  0.1936,\n",
      "         0.1916,  0.2090,  0.2355,  0.2341,  0.2851,  0.2121,  0.2262,  0.2462,\n",
      "         0.2294,  0.2686,  0.1961,  0.2308,  0.2195,  0.2620,  0.2407,  0.2710,\n",
      "         0.2680,  0.2138,  0.2628,  0.2334,  0.2258,  0.2050,  0.2551,  0.1927,\n",
      "         0.2292,  0.2286,  0.2604,  0.2085,  0.2234,  0.2008,  0.2498,  0.2370,\n",
      "         0.2483,  0.2227,  0.2053,  0.2436,  0.2581,  0.2230,  0.2426,  0.2352,\n",
      "         0.2136,  0.2390,  0.2019,  0.2104,  0.2388,  0.2357,  0.2300,  0.2773,\n",
      "         0.2824,  0.2186,  0.2495,  0.2169,  0.2329,  0.2270,  0.2578,  0.2336,\n",
      "         0.0366,  0.1845,  0.2691,  0.2309,  0.2425,  0.2333,  0.2104,  0.2367,\n",
      "         0.2355,  0.2333,  0.3410,  0.2557,  0.2207,  0.2068,  0.2314,  0.2434,\n",
      "         0.2139,  0.2422,  0.2251,  0.2602,  0.1927,  0.2246,  0.2135,  0.2288,\n",
      "         0.1980,  0.1042,  0.2177,  0.2302,  0.3410,  0.2503,  0.1898,  0.2717,\n",
      "         0.2284,  0.2338,  0.2234,  0.2060,  0.1993,  0.2296,  0.2121,  0.2153,\n",
      "         0.2819,  0.2063,  0.2168,  0.3040,  0.1939,  0.2519,  0.2261,  0.2405,\n",
      "         0.2145,  0.2530,  0.0413,  0.3592,  0.2184,  0.2393,  0.2484,  0.2300,\n",
      "         0.2190,  0.2116,  0.2396,  0.2693,  0.1829,  0.2262,  0.2332,  0.2297,\n",
      "         0.2295,  0.2168,  0.2162,  0.1967,  0.2114,  0.2249,  0.2312,  0.2253,\n",
      "         0.2342,  0.1776,  0.2503,  0.2467,  0.2032,  0.2045,  0.2386,  0.2943,\n",
      "         0.2529,  0.2587,  0.2044,  0.2773,  0.2276,  0.3019,  0.2336,  0.2395,\n",
      "         0.1952,  0.2403,  0.2765,  0.1785,  0.2734,  0.2534,  0.2345,  0.2279,\n",
      "         0.2074,  0.2756,  0.2513,  0.2327,  0.2214,  0.1869,  0.1988,  0.2297,\n",
      "         0.2219,  0.1854,  0.2755,  0.1780,  0.1929,  0.2328,  0.2431,  0.2093,\n",
      "         0.2245,  0.2401,  0.1426,  0.2782,  0.2456,  0.1628,  0.2319,  0.1577,\n",
      "         0.2249,  0.1985,  0.2486,  0.2154,  0.2900,  0.3296,  0.2058,  0.2052,\n",
      "         0.2080,  0.2972,  0.1722,  0.2494,  0.1681,  0.2271,  0.2426,  0.1950,\n",
      "         0.2262,  0.2850,  0.2696,  0.2611,  0.2095,  0.2632,  0.1900,  0.2160,\n",
      "         0.2621,  0.2143,  0.2494,  0.2837,  0.2334,  0.2141,  0.2519,  0.2160,\n",
      "         0.2193,  0.2228,  0.2875,  0.1966,  0.2509,  0.2393,  0.2275,  0.1050,\n",
      "         0.2077,  0.0500,  0.2320,  0.1922,  0.2074,  0.2085,  0.3066,  0.2397,\n",
      "         0.1408,  0.2204,  0.1278,  0.2383,  0.2690,  0.2113,  0.1700,  0.0426,\n",
      "         0.2165,  0.2608,  0.2878,  0.2270,  0.2113,  0.2281,  0.2402,  0.1920,\n",
      "         0.2379,  0.2989,  0.2421,  0.2255,  0.2530,  0.2278,  0.2554,  0.1987,\n",
      "         0.2667,  0.1939,  0.2237,  0.3252,  0.1810,  0.2323,  0.2122,  0.2563,\n",
      "         0.2287,  0.2525,  0.1958, -0.0352,  0.2411,  0.2238,  0.1939,  0.2394,\n",
      "         0.0973,  0.2631,  0.2165,  0.2181,  0.4044,  0.1999,  0.2375,  0.1913,\n",
      "         0.2662,  0.2005,  0.1949,  0.2384,  0.2242,  0.2178,  0.2385,  0.2426,\n",
      "         0.1910,  0.2063,  0.2245,  0.2108,  0.2571,  0.2260,  0.2121,  0.2447,\n",
      "         0.2191,  0.2327,  0.2284,  0.2481,  0.2038,  0.2161,  0.1665,  0.2197,\n",
      "         0.2419,  0.1954,  0.2839,  0.2588,  0.1754,  0.2154,  0.2235,  0.2492,\n",
      "         0.2720,  0.2366,  0.2288,  0.2840,  0.2118,  0.1830,  0.2247,  0.3306,\n",
      "         0.2467,  0.1117,  0.2517,  0.2704,  0.2531,  0.2167,  0.2877,  0.1858,\n",
      "         0.1722,  0.2555,  0.2529,  0.1966,  0.2695,  0.2381,  0.2154,  0.2165,\n",
      "         0.2260,  0.2001,  0.2182,  0.2213,  0.2321,  0.2620,  0.2348,  0.2388,\n",
      "         0.3353,  0.2652,  0.1997,  0.2645,  0.2627,  0.2422,  0.2435,  0.2356,\n",
      "         0.2516,  0.2065,  0.2603,  0.2159,  0.2038,  0.2251,  0.2653,  0.1671,\n",
      "         0.2565,  0.2558,  0.2808,  0.2210,  0.1926,  0.2375,  0.2377,  0.2246,\n",
      "         0.1956,  0.1933,  0.2149,  0.2255,  0.2179,  0.2353,  0.2232,  0.2297,\n",
      "         0.1777,  0.2143,  0.2573,  0.2069,  0.2147, -0.0927,  0.2086,  0.2019,\n",
      "         0.2390,  0.2321,  0.2253,  0.2161, -0.0413,  0.2363,  0.2805,  0.2391,\n",
      "         0.2049,  0.2170,  0.2312,  0.2181,  0.2320,  0.2234,  0.2209,  0.2735,\n",
      "         0.2411,  0.2232,  0.2054,  0.1988,  0.1535,  0.2049,  0.1114,  0.2496,\n",
      "         0.2224,  0.2087,  0.2044,  0.2674,  0.2070,  0.1962,  0.2512,  0.2302,\n",
      "         0.2033,  0.2560,  0.2226,  0.2442,  0.2557,  0.2312,  0.2028,  0.2571,\n",
      "         0.2158,  0.2080,  0.2206,  0.2165,  0.1994,  0.2430,  0.2191,  0.2353,\n",
      "         0.1940,  0.2220,  0.2043,  0.2130,  0.2183,  0.2008,  0.1760,  0.2436,\n",
      "         0.2255,  0.2311,  0.1488,  0.2227,  0.2064,  0.2236,  0.3614,  0.2353,\n",
      "         0.0339,  0.3001,  0.2029,  0.2116,  0.2094,  0.2248,  0.2100,  0.1982,\n",
      "         0.2246,  0.2409,  0.1342,  0.2136,  0.2489,  0.2344,  0.2160,  0.2346,\n",
      "         0.2620,  0.0347,  0.1973,  0.2617,  0.2764,  0.2326,  0.1972,  0.0997,\n",
      "         0.2805,  0.2096,  0.2277,  0.1911,  0.2052,  0.2767,  0.1904,  0.2502,\n",
      "         0.2484,  0.1915,  0.2327,  0.1850,  0.2557,  0.2393,  0.2224,  0.2357,\n",
      "         0.3021,  0.2044,  0.2164,  0.2217,  0.1883,  0.2120,  0.2012,  0.2067,\n",
      "         0.1976,  0.2301,  0.2450,  0.2151,  0.2178,  0.2476,  0.2212,  0.0390],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0286, -0.1024,  0.0076,  ..., -0.0042, -0.0231,  0.0096],\n",
      "        [ 0.0196,  0.0747, -0.0859,  ...,  0.0919, -0.0790,  0.0295],\n",
      "        [ 0.0378, -0.0883,  0.0613,  ...,  0.0264, -0.0579,  0.0070],\n",
      "        ...,\n",
      "        [ 0.0134, -0.0566,  0.0238,  ...,  0.0011,  0.1180,  0.0270],\n",
      "        [ 0.0246, -0.0100,  0.1550,  ..., -0.0932, -0.0135, -0.0217],\n",
      "        [ 0.0059, -0.0049,  0.0187,  ...,  0.0897,  0.0932, -0.0180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-7.9659e-02, -7.5658e-01,  1.9226e-01,  ..., -3.3576e-01,\n",
      "         -1.0933e-01,  2.1341e-01],\n",
      "        [ 6.6231e-01, -1.3580e-05,  3.3040e-01,  ..., -3.2323e-01,\n",
      "          5.1967e-01,  1.4344e-01],\n",
      "        [-2.7367e-01,  3.9377e-01,  3.9249e-01,  ..., -9.9524e-02,\n",
      "          4.5609e-01,  1.7737e-01],\n",
      "        ...,\n",
      "        [-1.6710e-01,  2.7230e-01,  1.3614e-01,  ..., -2.3010e-01,\n",
      "          1.2341e-01,  2.2922e-01],\n",
      "        [ 2.2027e-01,  5.0831e-03, -8.6705e-02,  ..., -7.9902e-01,\n",
      "          4.2796e-01, -4.3853e-02],\n",
      "        [ 6.1937e-01,  6.1563e-01,  1.2349e+00,  ...,  3.7378e-01,\n",
      "         -7.2741e-01, -4.4217e-02]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-4.5117e-01,  4.5060e-02,  6.1772e-01,  ...,  8.0364e-01,\n",
      "         -3.5555e-01, -5.9366e-02],\n",
      "        [ 7.9837e-02, -7.4020e-01, -3.4188e-01,  ...,  4.3477e-01,\n",
      "         -2.8038e-01,  1.2707e-01],\n",
      "        [ 4.8731e-02, -7.9880e-01, -1.1685e-01,  ...,  4.0995e-01,\n",
      "         -6.0567e-01,  5.8207e-02],\n",
      "        ...,\n",
      "        [-5.8182e-01,  2.0856e-01,  6.1540e-01,  ..., -8.3163e-02,\n",
      "          2.5947e-01, -3.1009e-01],\n",
      "        [ 5.9804e-01, -5.4725e-01,  2.6107e-01,  ..., -4.8735e-01,\n",
      "          1.1750e-04,  1.6675e-02],\n",
      "        [ 7.0859e-01,  6.6664e-01, -5.7312e-01,  ..., -4.1292e-01,\n",
      "         -5.9399e-01,  3.1905e-01]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.9856, -0.5765,  0.0042,  ...,  0.2281, -2.2584, -1.6841],\n",
      "        [-0.4666, -0.8370,  0.0690,  ..., -0.4784, -0.5984,  0.0858],\n",
      "        [-0.0806,  0.1556, -0.5127,  ..., -0.6968, -0.4669,  0.0967],\n",
      "        ...,\n",
      "        [-0.0307,  0.0749,  0.3316,  ...,  0.0190,  0.2021, -0.0766],\n",
      "        [ 0.3254,  0.7203,  0.3161,  ..., -0.3147, -0.2819, -0.2427],\n",
      "        [ 1.3469, -0.6952,  1.8800,  ...,  0.6707, -2.7867, -3.6516]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0443,  0.1456,  0.1402,  0.1609,  0.1506,  0.1590,  0.1286,  0.1302,\n",
      "         0.1085,  0.1205,  0.1422,  0.1160,  0.1145,  0.1219,  0.1166,  0.1507,\n",
      "         0.1535,  0.1294,  0.1635,  0.1534,  0.2231,  0.1523,  0.1161,  0.1777,\n",
      "         0.1312,  0.1447,  0.1439,  0.1285,  0.1207,  0.1272,  0.1663,  0.1548,\n",
      "         0.1813,  0.1250,  0.1390,  0.1745,  0.1269,  0.1258,  0.1393,  0.1477,\n",
      "         0.1583,  0.1165,  0.1481,  0.1318,  0.1265,  0.1668,  0.1470,  0.1496,\n",
      "         0.1749,  0.1614,  0.1339,  0.1456,  0.1495,  0.1487,  0.1307,  0.1072,\n",
      "         0.1493,  0.1469,  0.1180,  0.1213,  0.1759,  0.1444,  0.1144,  0.1724,\n",
      "         0.1843,  0.1615,  0.1712,  0.1921,  0.1594,  0.1215,  0.1896,  0.1233,\n",
      "        -0.0505,  0.1072,  0.1565,  0.1395,  0.1601,  0.1474,  0.1584,  0.1667,\n",
      "         0.1469,  0.1234,  0.1533,  0.1359,  0.1486,  0.1133,  0.1928,  0.1705,\n",
      "         0.1520,  0.1306,  0.1517,  0.1801,  0.1450,  0.1609,  0.1343,  0.1227,\n",
      "         0.1027,  0.0747,  0.1389,  0.1201,  0.1913,  0.1427,  0.1121,  0.1594,\n",
      "         0.1459,  0.1506,  0.1526,  0.1794,  0.1321,  0.1553,  0.1177,  0.1887,\n",
      "         0.1494,  0.1254,  0.1119,  0.1719,  0.1149,  0.1595,  0.1158,  0.1546,\n",
      "         0.1718,  0.1646,  0.3198,  0.1744,  0.1514,  0.1852,  0.1358,  0.1264,\n",
      "         0.1231,  0.1585,  0.1675,  0.1806,  0.1481,  0.1680,  0.1929,  0.1354,\n",
      "         0.1312,  0.1255,  0.1460,  0.1299,  0.1717,  0.1348,  0.1601,  0.1674,\n",
      "         0.1313,  0.1245,  0.1569,  0.1410,  0.1334,  0.1355,  0.1418,  0.1867,\n",
      "         0.1809,  0.1895,  0.1623,  0.1510,  0.1279,  0.1628,  0.1371,  0.1184,\n",
      "         0.1222,  0.1484,  0.1549,  0.1012,  0.1489,  0.1365,  0.1991,  0.1564,\n",
      "         0.1208,  0.1469,  0.1966,  0.1481,  0.1565, -0.1180,  0.1241,  0.1347,\n",
      "         0.1530,  0.0996,  0.1509,  0.1202,  0.1411,  0.1446,  0.1651,  0.1386,\n",
      "         0.1237,  0.1568,  0.1264,  0.1725,  0.1636,  0.1143,  0.1544,  0.1469,\n",
      "         0.1315, -0.1816,  0.1301,  0.1195,  0.1546,  0.1289,  0.1495,  0.1223,\n",
      "         0.1444,  0.1589,  0.1295,  0.1203,  0.1146,  0.1633,  0.1481,  0.1348,\n",
      "         0.1629,  0.1857,  0.1539,  0.1591,  0.1214,  0.1577,  0.1392,  0.1491,\n",
      "         0.1316,  0.1600,  0.1590,  0.1729,  0.1353,  0.1550,  0.1792,  0.1306,\n",
      "         0.1601,  0.1346,  0.1677,  0.1337,  0.1709,  0.1691,  0.1438, -0.0951,\n",
      "         0.1632,  0.2463,  0.1290,  0.1360,  0.1336,  0.1196,  0.1622,  0.1365,\n",
      "         0.1653,  0.1372,  0.0836,  0.1564,  0.1595,  0.1477,  0.1212,  0.4215,\n",
      "         0.1454,  0.1850,  0.1502,  0.1609,  0.1475,  0.1287,  0.1481,  0.1316,\n",
      "         0.1660,  0.1720,  0.1234,  0.1507,  0.1430,  0.1511,  0.1492,  0.1288,\n",
      "         0.1638,  0.1297,  0.1645,  0.1345,  0.1559,  0.1444,  0.1309,  0.1589,\n",
      "         0.1610,  0.2010,  0.1244,  0.1001,  0.1537,  0.1327,  0.1162,  0.1560,\n",
      "         0.1194,  0.1391,  0.1038,  0.1689,  0.1991,  0.1339,  0.1552,  0.1478,\n",
      "         0.1797,  0.1527,  0.1296,  0.1507,  0.1470,  0.1703,  0.1373,  0.1234,\n",
      "         0.1871,  0.1216,  0.1705,  0.1508,  0.1029,  0.1278,  0.1357,  0.1774,\n",
      "         0.1156,  0.1300,  0.1575,  0.1414,  0.1327,  0.1191,  0.1210,  0.1762,\n",
      "         0.1292,  0.1365,  0.1629,  0.1521,  0.1214,  0.1361,  0.1419,  0.1250,\n",
      "         0.1703,  0.1507,  0.1531,  0.1755,  0.1649,  0.1655,  0.1534,  0.1565,\n",
      "         0.1730,  0.1484,  0.1419,  0.1633,  0.1547,  0.1415,  0.1635,  0.1266,\n",
      "         0.1204,  0.1673,  0.1605,  0.1302,  0.1609,  0.1622,  0.1419,  0.1270,\n",
      "         0.1423,  0.1578,  0.1393,  0.1241,  0.1711,  0.1527,  0.1071,  0.1549,\n",
      "         0.1528,  0.1830,  0.1712,  0.1755,  0.1674,  0.1139,  0.1685,  0.1150,\n",
      "         0.1393,  0.1395,  0.1499,  0.1309,  0.1755,  0.1512,  0.1717,  0.1621,\n",
      "         0.1886,  0.1468,  0.1925,  0.1357,  0.1680,  0.1298,  0.1362,  0.1534,\n",
      "         0.1186,  0.1248,  0.1357,  0.1425,  0.1420,  0.1694,  0.1220,  0.1372,\n",
      "         0.1089,  0.1811,  0.1555,  0.1509,  0.1124, -0.0865,  0.1308,  0.1464,\n",
      "         0.1244,  0.1606,  0.1288,  0.1584, -0.0148,  0.1306,  0.1598,  0.1249,\n",
      "         0.1351,  0.1187,  0.1757,  0.1780,  0.1166,  0.1713,  0.1531,  0.1727,\n",
      "         0.1624,  0.1652,  0.1912,  0.1439,  0.1267,  0.0945,  0.0678,  0.1715,\n",
      "         0.1453,  0.1296,  0.1102,  0.1945,  0.1473,  0.1343,  0.1521,  0.1688,\n",
      "         0.1281,  0.1510,  0.1168,  0.1432,  0.3618,  0.1618,  0.1630,  0.1572,\n",
      "         0.1328,  0.1297,  0.1424,  0.1556,  0.1585,  0.1697,  0.1415,  0.1468,\n",
      "         0.1234,  0.1230,  0.1310,  0.1223,  0.1113,  0.1398,  0.1213,  0.1363,\n",
      "         0.1419,  0.1475,  0.1333,  0.1752,  0.1670,  0.1295,  0.2102,  0.1353,\n",
      "         0.0526,  0.1475,  0.1584,  0.1392,  0.1352,  0.1232,  0.1828,  0.1224,\n",
      "         0.1296,  0.1483,  0.0978,  0.1362,  0.1619,  0.1453,  0.1246,  0.1138,\n",
      "         0.1611, -0.0167,  0.1069,  0.1580,  0.1305,  0.1332,  0.1275,  0.0686,\n",
      "         0.1277,  0.1535,  0.1760,  0.1025,  0.1290,  0.1435,  0.1253,  0.1635,\n",
      "         0.1643,  0.1930,  0.1632,  0.1143,  0.1580,  0.1536,  0.1479,  0.1511,\n",
      "         0.1747,  0.1544,  0.1464,  0.1382,  0.1321,  0.1441,  0.1278,  0.1213,\n",
      "         0.1695,  0.1511,  0.1757,  0.1461,  0.2010,  0.1336,  0.1433, -0.0491],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.2828e-01, -2.2425e-01, -1.4714e-01,  ...,  3.8075e-02,\n",
      "          3.6676e-03,  2.3829e-01],\n",
      "        [-2.8649e-01, -1.8812e-01, -3.8203e-01,  ..., -6.1824e-03,\n",
      "         -1.0036e-01, -6.9703e-02],\n",
      "        [-2.4833e-01,  3.3704e-02,  4.4403e-01,  ..., -4.1141e-01,\n",
      "         -7.1355e-02, -2.7565e-02],\n",
      "        ...,\n",
      "        [-3.4316e-04,  8.1889e-02,  1.2396e-01,  ...,  8.5951e-02,\n",
      "          9.0492e-02,  5.8697e-02],\n",
      "        [ 1.9134e-01, -3.7948e-02, -1.9174e-01,  ..., -1.6684e-01,\n",
      "         -2.6264e-01,  6.0556e-02],\n",
      "        [-8.0265e-02, -4.7652e-02,  6.8633e-02,  ...,  6.5629e-02,\n",
      "         -4.1368e-02,  7.7885e-02]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.0709,  0.8075, -0.1383,  ...,  0.3453,  0.6574, -0.2699],\n",
      "        [ 0.0141, -0.4258, -1.3339,  ..., -0.1667,  0.7210,  0.3037],\n",
      "        [ 0.2641,  1.2312,  0.9153,  ..., -0.8167,  0.6205, -0.1681],\n",
      "        ...,\n",
      "        [ 0.5760,  1.2016,  0.0305,  ...,  0.1317, -1.9521,  0.1692],\n",
      "        [-1.0917, -1.4675, -2.2479,  ...,  0.9208,  0.9247, -0.3851],\n",
      "        [ 0.6676,  0.4108, -0.2121,  ..., -0.0792,  0.3654,  0.1198]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 4.3137, -1.4001, -0.4721,  ..., -0.5161,  1.9251, -0.1235],\n",
      "        [ 0.1919,  0.4403, -0.3344,  ..., -0.6147,  0.3512, -0.1825],\n",
      "        [ 0.3214,  0.3861, -0.1067,  ..., -0.2288,  0.8269,  0.2198],\n",
      "        ...,\n",
      "        [-0.3963,  0.2031,  0.0472,  ...,  0.0429, -1.0028,  0.3347],\n",
      "        [ 0.0396, -0.3645, -0.4093,  ..., -0.3889, -1.1723,  0.1047],\n",
      "        [ 2.9343,  0.4104,  0.0477,  ..., -1.1176,  2.3372, -0.5630]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.3.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0307,  0.1924,  0.1777,  0.1888,  0.2072,  0.2011,  0.2039,  0.2004,\n",
      "         0.2041,  0.1847,  0.1656,  0.1662,  0.1720,  0.1603,  0.1530,  0.1673,\n",
      "         0.1659,  0.1531,  0.1875,  0.1833,  0.1709,  0.1775,  0.1789,  0.2017,\n",
      "         0.1792,  0.1733,  0.1808,  0.1905,  0.1651,  0.1622,  0.1631,  0.1799,\n",
      "         0.2083,  0.1890,  0.1844,  0.1907,  0.1844,  0.1898,  0.1897,  0.1752,\n",
      "         0.1996,  0.1764,  0.1871,  0.1648,  0.1819,  0.1904,  0.1884,  0.1804,\n",
      "         0.1890,  0.1773,  0.1763,  0.1974,  0.1843,  0.1729,  0.1696,  0.1816,\n",
      "         0.1984,  0.1802,  0.1711,  0.1950,  0.1811,  0.1778,  0.1755,  0.2052,\n",
      "         0.2213,  0.1908,  0.1770,  0.1874,  0.2054,  0.1696,  0.1339,  0.1692,\n",
      "        -0.0328,  0.1757,  0.2126,  0.1674,  0.1862,  0.1917,  0.1763,  0.1822,\n",
      "         0.1601,  0.1790,  0.1809,  0.1916,  0.1924,  0.1626,  0.1989,  0.1876,\n",
      "         0.1843,  0.1792,  0.1910,  0.1775,  0.1608,  0.1880,  0.1650,  0.1829,\n",
      "         0.1728,  0.1073,  0.1741,  0.1810,  0.2093,  0.1822,  0.1630,  0.1785,\n",
      "         0.1936,  0.1884,  0.1822,  0.1604,  0.1778,  0.1694,  0.1665,  0.1798,\n",
      "         0.1772,  0.1767,  0.1826,  0.1711,  0.1704,  0.1985,  0.1760,  0.1824,\n",
      "         0.1882,  0.1882,  0.3079,  0.2020,  0.1983,  0.1868,  0.1737,  0.1700,\n",
      "         0.1778,  0.1759,  0.1951,  0.1839,  0.1704,  0.1806,  0.2125,  0.1899,\n",
      "         0.1748,  0.1624,  0.1870,  0.1792,  0.1813,  0.1733,  0.1729,  0.2052,\n",
      "         0.1766,  0.1614,  0.1945,  0.2022,  0.1841,  0.1767,  0.1711,  0.2009,\n",
      "         0.1782,  0.1984,  0.1718,  0.2104,  0.1632,  0.1722,  0.2066,  0.1743,\n",
      "         0.1766,  0.1861,  0.1692,  0.1416,  0.1936,  0.1790,  0.1903,  0.2114,\n",
      "         0.1969,  0.1847,  0.2000,  0.1942,  0.1844,  0.1659,  0.1707,  0.1793,\n",
      "         0.1719,  0.1449,  0.1801,  0.1608,  0.1770,  0.1547,  0.1786,  0.1694,\n",
      "         0.1622,  0.1763,  0.1612,  0.2123,  0.1992,  0.1606,  0.1856,  0.1583,\n",
      "         0.1681,  0.2316,  0.1880,  0.1665,  0.2001,  0.1821,  0.1820,  0.1851,\n",
      "         0.1610,  0.2034,  0.1474,  0.1932,  0.1318,  0.1911,  0.1843,  0.1710,\n",
      "         0.1754,  0.1913,  0.1731,  0.1793,  0.1800,  0.1841,  0.1701,  0.1819,\n",
      "         0.1860,  0.2014,  0.1742,  0.1958,  0.1727,  0.1731,  0.1740,  0.1801,\n",
      "         0.1829,  0.1764,  0.1882,  0.1728,  0.2029,  0.1932,  0.1624,  0.1116,\n",
      "         0.1828,  0.2618,  0.1683,  0.1702,  0.1803,  0.1911,  0.1957,  0.1808,\n",
      "         0.1910,  0.1608, -0.0114,  0.1761,  0.1776,  0.1885,  0.1518,  0.5345,\n",
      "         0.1585,  0.2077,  0.1853,  0.1714,  0.1842,  0.1849,  0.1663,  0.1714,\n",
      "         0.1704,  0.1941,  0.1959,  0.1730,  0.1821,  0.1847,  0.1664,  0.1516,\n",
      "         0.1935,  0.1748,  0.1813,  0.1867,  0.1640,  0.1728,  0.1852,  0.1957,\n",
      "         0.1679,  0.1985,  0.1726, -0.0295,  0.1786,  0.1791,  0.1679,  0.1776,\n",
      "         0.1418,  0.1722,  0.1720,  0.1958,  0.2066,  0.1668,  0.1876,  0.1919,\n",
      "         0.1885,  0.1617,  0.1880,  0.1846,  0.1688,  0.1864,  0.1834,  0.1916,\n",
      "         0.1759,  0.1753,  0.1793,  0.1725,  0.1764,  0.1778,  0.1821,  0.1923,\n",
      "         0.1787,  0.1639,  0.1903,  0.1735,  0.1742,  0.1862,  0.1444,  0.1984,\n",
      "         0.1872,  0.1702,  0.1877,  0.1964,  0.1614,  0.1809,  0.1898,  0.1993,\n",
      "         0.1998,  0.1873,  0.1777,  0.1996,  0.1868,  0.1728,  0.1554,  0.1849,\n",
      "         0.1821, -0.1781,  0.1876,  0.2017,  0.1927,  0.1661,  0.1944,  0.1558,\n",
      "         0.1600,  0.2065,  0.1990,  0.2074,  0.2063,  0.1855,  0.1831,  0.1860,\n",
      "         0.1696,  0.1777,  0.1866,  0.1829,  0.1822,  0.1757,  0.1851,  0.1705,\n",
      "         0.0984,  0.1711,  0.1779,  0.1891,  0.1847,  0.1737,  0.1809,  0.1593,\n",
      "         0.2080,  0.1916,  0.1786,  0.1816,  0.1695,  0.1831,  0.1917,  0.1157,\n",
      "         0.1859,  0.1711,  0.1941,  0.1930,  0.1673,  0.1874,  0.1799,  0.1584,\n",
      "         0.1664,  0.1593,  0.1969,  0.1854,  0.1825,  0.1959,  0.1723,  0.1802,\n",
      "         0.1758,  0.1774,  0.1890,  0.1802,  0.1560,  0.0979,  0.1957,  0.1800,\n",
      "         0.1638,  0.1746,  0.1714,  0.1897, -0.0282,  0.1988,  0.1953,  0.1679,\n",
      "         0.1735,  0.1605,  0.1824,  0.1811,  0.1749,  0.1800,  0.1829,  0.1887,\n",
      "         0.2041,  0.1936,  0.1693,  0.1750,  0.1534,  0.1791,  0.1101,  0.1742,\n",
      "         0.1751,  0.1574,  0.1664,  0.2082,  0.1618,  0.1939,  0.1852,  0.1679,\n",
      "         0.1689,  0.1869,  0.1551,  0.1824,  0.1686,  0.1950,  0.1866,  0.1918,\n",
      "         0.1672,  0.1841,  0.1779,  0.1892,  0.1788,  0.1847,  0.1796,  0.1892,\n",
      "         0.1561,  0.1809,  0.1776,  0.1715,  0.1721,  0.1681,  0.1692,  0.1879,\n",
      "         0.1716,  0.1796,  0.1470,  0.1698,  0.1591,  0.1797,  0.2267,  0.1856,\n",
      "         0.0423,  0.1800,  0.1761,  0.1658,  0.1718,  0.1669,  0.1812,  0.1559,\n",
      "         0.1782,  0.1798,  0.1373,  0.1809,  0.1826,  0.1982,  0.1823,  0.1957,\n",
      "         0.1987,  0.0162,  0.1664,  0.2064,  0.1824,  0.1474,  0.1670,  0.1268,\n",
      "         0.2008,  0.1456,  0.1701,  0.1860,  0.1564,  0.1911,  0.1676,  0.1859,\n",
      "         0.2085,  0.1425,  0.1809,  0.1656,  0.2008,  0.2126,  0.1850,  0.1725,\n",
      "         0.1987,  0.1960,  0.1627,  0.1756,  0.1790,  0.1908,  0.1613,  0.1750,\n",
      "         0.1702,  0.1832,  0.1834,  0.1613,  0.1749,  0.1913,  0.1718,  0.0391],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0053,  0.0190, -0.0432,  ...,  0.0122,  0.0795,  0.0042],\n",
      "        [ 0.0043, -0.0450, -0.0089,  ..., -0.0198,  0.0096,  0.0136],\n",
      "        [ 0.0251,  0.0022, -0.0083,  ...,  0.0260,  0.0189,  0.0032],\n",
      "        ...,\n",
      "        [ 0.0020, -0.0259, -0.0160,  ...,  0.0942, -0.0130,  0.0035],\n",
      "        [ 0.0073, -0.0218, -0.0443,  ...,  0.0155,  0.0229,  0.0062],\n",
      "        [-0.0197, -0.0601,  0.0377,  ..., -0.0225, -0.0017, -0.0026]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1152,  0.1194, -0.4095,  ...,  0.4302,  0.3281, -0.1178],\n",
      "        [ 0.0241,  0.3907, -0.1943,  ...,  0.9320,  0.4452,  0.0382],\n",
      "        [ 0.0440,  0.6027, -0.9282,  ..., -0.1513, -0.2204, -0.0974],\n",
      "        ...,\n",
      "        [-0.1305,  0.5063, -0.2446,  ...,  0.0529, -0.0294,  0.0123],\n",
      "        [-0.1095,  0.2976, -0.0447,  ..., -0.6227,  0.3770,  0.0734],\n",
      "        [ 0.3309, -0.5698, -0.3348,  ..., -0.1585, -0.3545, -0.0375]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4110,  1.3117, -1.9252,  ..., -0.9921,  1.3838, -0.0286],\n",
      "        [ 0.1943,  0.0546, -0.9799,  ..., -0.1860,  0.3479, -0.0055],\n",
      "        [ 0.0679,  0.3958, -1.3110,  ..., -0.6382, -1.4138,  0.0462],\n",
      "        ...,\n",
      "        [-0.2352, -0.3766, -0.0996,  ...,  1.2400,  0.1351,  0.0219],\n",
      "        [ 0.0050, -0.6156,  0.9200,  ...,  0.3051, -0.0337,  0.0197],\n",
      "        [-0.2226, -0.0118, -0.8812,  ..., -0.4928, -0.2372, -0.1084]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2749, -1.3021,  2.1406,  ...,  0.0547,  0.7226, -2.3239],\n",
      "        [-0.3887, -1.0965, -0.9537,  ...,  0.3138,  0.1937, -1.6788],\n",
      "        [ 0.1370,  0.4384,  0.9398,  ...,  1.0398,  0.5520, -1.1538],\n",
      "        ...,\n",
      "        [-0.1523,  0.8128,  0.9090,  ...,  0.6088, -0.2708, -0.7848],\n",
      "        [ 1.6667, -0.2466, -0.3365,  ..., -0.9528, -0.0164, -0.4728],\n",
      "        [-2.3336, -4.2269,  5.4934,  ..., -1.5325,  3.5862, -2.0881]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0688,  0.2450,  0.2446,  0.2592,  0.3017,  0.2243,  0.2935,  0.2519,\n",
      "         0.2361,  0.2683,  0.1845,  0.2171,  0.2400,  0.2611,  0.2286,  0.2236,\n",
      "         0.2376,  0.2122,  0.2763,  0.2628,  0.2635,  0.2284,  0.2749,  0.2508,\n",
      "         0.2459,  0.2340,  0.2029,  0.1968,  0.2324,  0.2559,  0.2420,  0.2520,\n",
      "         0.3033,  0.2335,  0.3005,  0.3031,  0.2433,  0.2247,  0.2650,  0.2383,\n",
      "         0.2961,  0.2121,  0.2666,  0.2790,  0.2579,  0.2512,  0.2661,  0.2238,\n",
      "         0.2688,  0.2229,  0.2616,  0.2771,  0.2440,  0.2363,  0.2572,  0.2362,\n",
      "         0.2542,  0.2661,  0.2450,  0.2149,  0.2732,  0.2354,  0.2821,  0.2789,\n",
      "         0.2824,  0.2313,  0.2421,  0.3278,  0.2680,  0.2227,  0.4705,  0.2105,\n",
      "         0.0573,  0.1879,  0.2945,  0.2413,  0.2803,  0.2785,  0.2141,  0.3005,\n",
      "         0.2337,  0.2989,  0.2954,  0.2789,  0.2435,  0.2233,  0.2718,  0.2607,\n",
      "         0.2430,  0.2387,  0.2149,  0.2248,  0.2508,  0.2528,  0.2180,  0.2577,\n",
      "         0.2517,  0.1297,  0.2409,  0.2014,  0.3219,  0.2402,  0.2202,  0.3025,\n",
      "         0.2602,  0.2639,  0.2421,  0.2205,  0.2605,  0.2262,  0.2139,  0.2332,\n",
      "         0.2444,  0.2381,  0.2415,  0.2339,  0.2282,  0.2911,  0.2584,  0.2576,\n",
      "         0.2486,  0.2556,  0.0703,  0.2694,  0.2567,  0.2402,  0.2574,  0.2365,\n",
      "         0.2328,  0.2566,  0.2724,  0.2427,  0.1927,  0.2406,  0.2611,  0.2544,\n",
      "         0.2664,  0.2362,  0.2440,  0.2094,  0.2396,  0.2166,  0.2503,  0.2572,\n",
      "         0.2563,  0.2353,  0.2573,  0.2770,  0.2606,  0.2549,  0.2283,  0.2866,\n",
      "         0.2530,  0.2500,  0.2537,  0.2764,  0.2282,  0.3699,  0.2826,  0.2286,\n",
      "         0.2399,  0.2347,  0.2964,  0.1759,  0.2567,  0.2841,  0.2893,  0.2607,\n",
      "         0.2150,  0.2895,  0.2476,  0.2468,  0.2213,  0.1893,  0.2126,  0.2381,\n",
      "         0.2439,  0.2102,  0.2339,  0.2280,  0.2246,  0.2388,  0.2676,  0.2612,\n",
      "         0.2097,  0.2903,  0.1711,  0.2885,  0.2643,  0.2214,  0.2443,  0.2008,\n",
      "         0.2073,  0.2869,  0.2567,  0.2446,  0.3396,  0.3755,  0.2428,  0.2423,\n",
      "         0.2315,  0.2557,  0.2067,  0.2169,  0.1907,  0.2516,  0.2817,  0.2402,\n",
      "         0.1992,  0.2864,  0.3206,  0.3100,  0.2053,  0.2950,  0.2581,  0.1988,\n",
      "         0.2877,  0.2562,  0.2926,  0.3066,  0.2663,  0.2715,  0.2003,  0.2119,\n",
      "         0.2325,  0.2307,  0.2516,  0.2749,  0.2778,  0.2601,  0.2013,  0.1560,\n",
      "         0.2597,  0.0645,  0.2467,  0.2499,  0.2597,  0.2441,  0.2992,  0.2685,\n",
      "         0.2028,  0.2590, -0.0680,  0.2619,  0.2922,  0.2410,  0.2078,  0.0816,\n",
      "         0.2263,  0.2680,  0.2738,  0.2447,  0.2364,  0.2608,  0.2527,  0.2110,\n",
      "         0.2551,  0.2586,  0.2892,  0.2201,  0.2741,  0.2651,  0.2403,  0.2232,\n",
      "         0.2766,  0.2228,  0.2344,  0.2600,  0.2844,  0.3152,  0.2513,  0.2590,\n",
      "         0.2203,  0.2537,  0.2217, -0.0495,  0.2904,  0.2308,  0.2233,  0.2617,\n",
      "         0.1241,  0.2369,  0.2461,  0.2762,  0.2995,  0.2363,  0.2597,  0.2018,\n",
      "         0.2461,  0.2641,  0.2057,  0.2581,  0.2343,  0.2752,  0.2169,  0.2448,\n",
      "         0.2559,  0.2792,  0.2526,  0.2399,  0.2451,  0.2224,  0.2399,  0.2278,\n",
      "         0.2636,  0.2368,  0.2299,  0.2407,  0.2813,  0.2647,  0.2014,  0.2211,\n",
      "         0.2520,  0.2597,  0.2835,  0.2802,  0.2103,  0.2317,  0.2713,  0.2661,\n",
      "         0.2726,  0.2668,  0.2449,  0.3131,  0.2424,  0.2198,  0.2450,  0.2838,\n",
      "         0.2174,  0.2199,  0.2712,  0.2967,  0.2457,  0.2489,  0.2730,  0.1955,\n",
      "         0.2481,  0.2832,  0.2691,  0.1943,  0.2786,  0.2302,  0.2498,  0.2442,\n",
      "         0.2869,  0.2424,  0.2716,  0.2307,  0.2609,  0.2638,  0.2824,  0.2371,\n",
      "         0.3164,  0.2519,  0.2454,  0.2742,  0.2503,  0.2428,  0.2365,  0.2403,\n",
      "         0.2182,  0.2424,  0.2201,  0.2426,  0.2207,  0.2360,  0.2560,  0.1568,\n",
      "         0.2748,  0.2469,  0.2313,  0.2111,  0.2080,  0.3041,  0.2414,  0.2492,\n",
      "         0.2340,  0.2066,  0.2520,  0.2524,  0.2239,  0.2963,  0.2337,  0.2388,\n",
      "         0.1922,  0.2706,  0.2544,  0.2701,  0.2325, -0.1159,  0.1994,  0.2642,\n",
      "         0.2492,  0.2545,  0.2055,  0.2364, -0.0667,  0.2754,  0.2441,  0.2384,\n",
      "         0.2262,  0.2897,  0.2190,  0.2727,  0.2437,  0.2581,  0.2324,  0.2247,\n",
      "         0.2631,  0.2220,  0.2143,  0.2367,  0.2163,  0.2186,  0.1534,  0.2893,\n",
      "         0.2645,  0.2199,  0.2606,  0.2853,  0.2379,  0.2665,  0.2713,  0.2166,\n",
      "         0.2777,  0.2571,  0.2649,  0.2640,  0.4113,  0.2796,  0.2781,  0.2801,\n",
      "         0.2069,  0.2071,  0.2869,  0.2431,  0.2520,  0.2552,  0.2544,  0.2282,\n",
      "         0.2139,  0.2575,  0.2770,  0.2251,  0.2325,  0.1950,  0.2293,  0.2108,\n",
      "         0.2275,  0.2482,  0.1843,  0.2313,  0.2028,  0.2361,  0.2982,  0.2520,\n",
      "         0.0616,  0.3323,  0.2933,  0.2178,  0.2671,  0.2327,  0.2450,  0.2209,\n",
      "         0.2106,  0.2513,  0.1797,  0.2468,  0.2601,  0.2265,  0.2423,  0.2444,\n",
      "         0.2642,  0.0500,  0.2436,  0.2535,  0.2584,  0.2785,  0.2103, -0.1249,\n",
      "         0.2356,  0.2707,  0.2582,  0.2550,  0.2215,  0.2172,  0.2326,  0.2495,\n",
      "         0.2831,  0.2957,  0.2350,  0.1996,  0.2725,  0.2595,  0.2452,  0.2256,\n",
      "         0.3006,  0.2918,  0.2115,  0.2550,  0.2607,  0.2622,  0.2423,  0.2349,\n",
      "         0.2448,  0.2956,  0.2645,  0.2144,  0.2437,  0.2634,  0.3101,  0.0966],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0344,  0.0157,  0.0940,  ...,  0.0043, -0.0264, -0.0065],\n",
      "        [ 0.0053, -0.0321, -0.0212,  ..., -0.1435,  0.0249, -0.0325],\n",
      "        [ 0.0527,  0.0087, -0.0114,  ...,  0.0599,  0.0640,  0.0886],\n",
      "        ...,\n",
      "        [-0.0519,  0.0116, -0.0562,  ..., -0.0330,  0.0380, -0.0227],\n",
      "        [ 0.0389,  0.0220,  0.0788,  ...,  0.0811,  0.0299,  0.0057],\n",
      "        [-0.0474,  0.0024, -0.0228,  ...,  0.0738, -0.0514,  0.0008]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4353,  0.0082, -0.3272,  ...,  0.4424, -0.4943, -1.4986],\n",
      "        [ 0.6832, -0.7217, -0.1410,  ..., -0.1391,  0.8109,  0.8471],\n",
      "        [ 0.8102,  0.6234,  0.0458,  ...,  0.7420, -1.2544,  0.5627],\n",
      "        ...,\n",
      "        [-0.0173,  0.2937, -0.3208,  ...,  0.1753,  0.0491,  0.9397],\n",
      "        [ 0.0294,  0.3945,  0.2652,  ..., -0.4781, -0.5796, -0.7708],\n",
      "        [-0.0678,  0.1649, -0.0606,  ...,  0.2399, -0.1554,  0.1351]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1046, -0.1714, -0.7579,  ..., -0.9196,  0.2596,  0.1581],\n",
      "        [ 1.8140,  1.0544, -0.5320,  ...,  0.4151, -0.7512, -0.1065],\n",
      "        [-0.9589, -1.5606, -0.8460,  ..., -0.8361, -1.0427, -0.2846],\n",
      "        ...,\n",
      "        [-0.9000, -0.2540,  0.7042,  ...,  1.2488, -1.4386,  0.2911],\n",
      "        [-0.2875, -0.3048, -0.3557,  ..., -0.8754,  2.1391, -0.2843],\n",
      "        [-0.0648, -0.5204, -0.6590,  ...,  0.9724,  0.3325, -0.4216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2306,  0.6223, -1.2608,  ..., -0.1903, -1.3074, -1.1089],\n",
      "        [-0.3310, -0.3060,  0.1647,  ..., -1.3347,  0.8052, -0.7446],\n",
      "        [ 0.3104, -0.9165,  0.1125,  ...,  0.1419,  2.0464,  0.1675],\n",
      "        ...,\n",
      "        [-0.4106, -0.0693, -0.3670,  ...,  1.0941,  0.6814, -1.0260],\n",
      "        [ 0.1781, -0.6882,  0.5808,  ..., -0.5123, -0.6809,  0.1982],\n",
      "        [ 1.4941,  4.0058,  0.3006,  ..., -0.7202,  1.7434, -2.0050]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0349,  0.1064,  0.1110,  0.1198,  0.1420,  0.1384,  0.1132,  0.1443,\n",
      "         0.1124,  0.0852,  0.1297,  0.1215,  0.1513,  0.1409,  0.1163,  0.1506,\n",
      "         0.1423,  0.0990,  0.1388,  0.1221,  0.1517,  0.1395,  0.1296,  0.1322,\n",
      "         0.1222,  0.1201,  0.1024,  0.1013,  0.1227,  0.1372,  0.1347,  0.1464,\n",
      "         0.1238,  0.1341,  0.1615,  0.2078,  0.1400,  0.1419,  0.1324,  0.1495,\n",
      "         0.1513,  0.1116,  0.1289,  0.1266,  0.1324,  0.1369,  0.1273,  0.1294,\n",
      "         0.1420,  0.1210,  0.1319,  0.1389,  0.1547,  0.1206,  0.1122,  0.1127,\n",
      "         0.1508,  0.1321,  0.1470,  0.1054,  0.1215,  0.1204,  0.1237,  0.1619,\n",
      "         0.1402,  0.1142,  0.1190,  0.2874,  0.1638,  0.0972,  0.3884,  0.1494,\n",
      "        -0.0210,  0.1157,  0.1350,  0.1487,  0.1377,  0.1259,  0.1128,  0.1621,\n",
      "         0.1409,  0.1449,  0.1611,  0.1164,  0.1416,  0.1342,  0.1609,  0.1306,\n",
      "         0.1331,  0.1430,  0.1346,  0.1491,  0.1362,  0.1157,  0.1043,  0.1151,\n",
      "         0.1345, -0.0716,  0.1265,  0.1115,  0.1524,  0.1239,  0.1059,  0.1547,\n",
      "         0.1287,  0.1627,  0.1247,  0.1015,  0.1074,  0.1327,  0.0986,  0.1325,\n",
      "         0.1116,  0.1188,  0.1305,  0.1346,  0.1133,  0.1362,  0.0961,  0.1391,\n",
      "         0.1161,  0.1470,  0.3253,  0.1673,  0.1149,  0.1368,  0.1296,  0.1031,\n",
      "         0.1152,  0.1441,  0.1639,  0.1290,  0.1137,  0.1478,  0.1319,  0.1211,\n",
      "         0.1283,  0.1242,  0.1321,  0.1163,  0.1654,  0.1394,  0.1116,  0.1347,\n",
      "         0.1562,  0.1285,  0.1131,  0.1392,  0.1239,  0.0947,  0.1344,  0.1427,\n",
      "         0.1409,  0.1304,  0.1281,  0.1287,  0.1362,  0.1968,  0.1510,  0.1159,\n",
      "         0.1257,  0.1258,  0.1366,  0.0777,  0.1118,  0.1308,  0.1706,  0.1430,\n",
      "         0.1179,  0.1601,  0.1523,  0.1256,  0.1375,  0.1152,  0.1394,  0.1094,\n",
      "         0.1113,  0.1230,  0.1155,  0.1555,  0.0989,  0.1424,  0.1073,  0.1351,\n",
      "         0.1137,  0.1468,  0.1070,  0.1166,  0.1157,  0.1490,  0.1242,  0.1091,\n",
      "         0.1491,  0.2473,  0.1037,  0.0992,  0.1261,  0.1434,  0.1328,  0.1053,\n",
      "         0.1104,  0.1309,  0.0920,  0.1097,  0.1055,  0.1190,  0.1379,  0.1144,\n",
      "         0.1344,  0.1639,  0.1757,  0.1498,  0.1408,  0.1484,  0.1417,  0.1271,\n",
      "         0.1322,  0.1255,  0.1579,  0.1380,  0.1433,  0.1418,  0.1295,  0.1202,\n",
      "         0.1263,  0.1186,  0.1228,  0.1218,  0.1555,  0.1420,  0.1398,  0.0679,\n",
      "         0.1208,  0.2284,  0.1323,  0.1548,  0.1286,  0.1090,  0.1528,  0.1342,\n",
      "         0.1605,  0.1308, -0.0033,  0.1388,  0.1344,  0.1093,  0.1001,  0.4722,\n",
      "         0.1404,  0.1319,  0.1399,  0.1570,  0.1035,  0.1369,  0.1287,  0.1585,\n",
      "         0.1331,  0.1218,  0.1452,  0.0992,  0.1211,  0.1495,  0.1412,  0.1148,\n",
      "         0.1269,  0.1000,  0.1248,  0.1389,  0.2047,  0.1286,  0.1211,  0.1481,\n",
      "         0.0904,  0.1370,  0.1074,  0.0132,  0.1574,  0.1200,  0.0977,  0.1513,\n",
      "        -0.0986,  0.1391,  0.1295,  0.1431,  0.1674,  0.1364,  0.1252,  0.1071,\n",
      "         0.1506,  0.1211,  0.1093,  0.1199,  0.1319,  0.1403,  0.1128,  0.0908,\n",
      "         0.1817,  0.1282,  0.1476,  0.1273,  0.1360,  0.1482,  0.1076,  0.1532,\n",
      "         0.1253,  0.0936,  0.1179,  0.1138,  0.1245,  0.1067,  0.1332,  0.1122,\n",
      "         0.1311,  0.1301,  0.1617,  0.1410,  0.1555,  0.1150,  0.1624,  0.1083,\n",
      "         0.1369,  0.1347,  0.1435,  0.1078,  0.1123,  0.0945,  0.1437,  0.1510,\n",
      "         0.1127,  0.1503,  0.1386,  0.1318,  0.1411,  0.1465,  0.1463,  0.1229,\n",
      "         0.1146,  0.1401,  0.0981,  0.1177,  0.1483,  0.1386,  0.1380,  0.1331,\n",
      "         0.1312,  0.1227,  0.0863,  0.1334,  0.1208,  0.1468,  0.1544,  0.1251,\n",
      "         0.0836,  0.1161,  0.1406,  0.1329,  0.1333,  0.1314,  0.1577,  0.1135,\n",
      "         0.1262,  0.0992,  0.1321,  0.1077,  0.1161,  0.1551,  0.1386,  0.2180,\n",
      "         0.1672,  0.1088,  0.1315,  0.1105,  0.1099,  0.1499,  0.1307,  0.1283,\n",
      "         0.1190,  0.1227,  0.1368,  0.1218,  0.0944,  0.1539,  0.1536,  0.1082,\n",
      "         0.1038,  0.1904,  0.1257,  0.1435,  0.1361,  0.0729,  0.1095,  0.1414,\n",
      "         0.1124, -0.1683,  0.1196,  0.1226, -0.0040,  0.1296,  0.1366,  0.1150,\n",
      "         0.1123,  0.1541,  0.1222,  0.1787,  0.1302,  0.1216,  0.0881,  0.1341,\n",
      "         0.1253,  0.1231,  0.1404,  0.1486,  0.1264,  0.1150,  0.0710,  0.1757,\n",
      "         0.1236,  0.1111,  0.1258,  0.1333,  0.1450,  0.1241,  0.1554,  0.1447,\n",
      "         0.1089,  0.1580,  0.1370,  0.1397,  0.4617,  0.1414,  0.1236,  0.1472,\n",
      "         0.1201,  0.1272,  0.1257,  0.1319,  0.1171,  0.1318,  0.1425,  0.1020,\n",
      "         0.1367,  0.1106,  0.1698,  0.1532,  0.1186,  0.0954,  0.1123,  0.1133,\n",
      "         0.1104,  0.1063,  0.1114,  0.1265,  0.1533,  0.1132,  0.1308,  0.1175,\n",
      "         0.0242,  0.1467,  0.1806,  0.1197,  0.1432,  0.1174,  0.1307,  0.1303,\n",
      "         0.1005,  0.1373,  0.1042,  0.1319,  0.1526,  0.1176,  0.0911,  0.1204,\n",
      "         0.1226, -0.0038,  0.1000,  0.1204,  0.1375,  0.1544,  0.1114,  0.0737,\n",
      "         0.1318,  0.1446,  0.1461,  0.1260,  0.1206,  0.1276,  0.1374,  0.1323,\n",
      "         0.1450,  0.2142,  0.1220,  0.0982,  0.1594,  0.1425,  0.1242,  0.1019,\n",
      "         0.1528,  0.1745,  0.1197,  0.1263,  0.1765,  0.1437,  0.0922,  0.1183,\n",
      "         0.1121,  0.1193,  0.1308,  0.1160,  0.1441,  0.1431,  0.1090, -0.0175],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1360, -0.4492,  0.1561,  ...,  0.2565, -0.0680,  0.0391],\n",
      "        [ 0.0553,  0.5908, -0.0470,  ...,  0.2715, -0.2772,  0.0044],\n",
      "        [ 0.0807,  0.2037, -0.2314,  ..., -0.3568,  0.0851,  0.0029],\n",
      "        ...,\n",
      "        [ 0.2623, -0.2130, -0.6767,  ...,  0.2571, -0.3973,  0.1240],\n",
      "        [ 0.0214, -0.0268, -0.1685,  ..., -0.1071, -0.0563,  0.0917],\n",
      "        [ 0.1536,  0.0683, -0.0427,  ...,  0.0154,  0.0526,  0.0637]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.1921e-01,  8.0964e-01, -1.0905e+00,  ...,  1.8403e+00,\n",
      "          7.4232e-01, -1.1123e-02],\n",
      "        [-3.4539e-02,  6.7129e-01, -1.2385e+00,  ..., -2.0712e+00,\n",
      "         -5.9272e-01,  8.9100e-05],\n",
      "        [-4.6923e-01,  7.4884e-01, -3.3048e-01,  ...,  7.1550e-01,\n",
      "          2.3549e+00, -2.0725e-01],\n",
      "        ...,\n",
      "        [-3.5314e-01,  6.3811e-01, -1.0400e+00,  ..., -2.5411e-01,\n",
      "         -1.3542e+00, -8.2401e-01],\n",
      "        [ 1.0640e-01, -1.0370e+00, -6.6508e-01,  ...,  8.1560e-01,\n",
      "          6.9617e-01,  1.9648e-01],\n",
      "        [ 2.6111e-01,  7.7217e-02,  9.2548e-01,  ...,  8.4315e-01,\n",
      "         -1.8406e+00, -4.6525e-01]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.3147,  0.2509, -0.5352,  ..., -0.3644,  0.6453,  2.2034],\n",
      "        [-1.1685,  0.1038,  0.9928,  ...,  1.4672,  1.7536,  0.6203],\n",
      "        [ 0.0201, -0.6886,  0.7475,  ...,  0.3372,  0.4305,  0.1649],\n",
      "        ...,\n",
      "        [ 0.7235,  0.7825, -0.2665,  ..., -0.0819, -0.6986, -0.1362],\n",
      "        [ 0.3114,  0.6293, -1.1692,  ...,  0.7390,  0.2563, -0.4834],\n",
      "        [ 2.6470, -0.3793,  0.3395,  ...,  3.1630,  0.9604,  0.3713]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.4.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0492,  0.2185,  0.2057,  0.2375,  0.2059,  0.2057,  0.2188,  0.2252,\n",
      "         0.1958,  0.2122,  0.2200,  0.1824,  0.2196,  0.2299,  0.2083,  0.1926,\n",
      "         0.2036,  0.2173,  0.2318,  0.2234,  0.2081,  0.2289,  0.2267,  0.2207,\n",
      "         0.2200,  0.1987,  0.2076,  0.1984,  0.2146,  0.2020,  0.2141,  0.2005,\n",
      "         0.2467,  0.2057,  0.2379,  0.2694,  0.2186,  0.2102,  0.2285,  0.2017,\n",
      "         0.2377,  0.2129,  0.2270,  0.2149,  0.2190,  0.2398,  0.2162,  0.2097,\n",
      "         0.2337,  0.2066,  0.2173,  0.2568,  0.2249,  0.2092,  0.2125,  0.2133,\n",
      "         0.2100,  0.2077,  0.2212,  0.1810,  0.2160,  0.2071,  0.2406,  0.2087,\n",
      "         0.2536,  0.2132,  0.2162,  0.2558,  0.2213,  0.2042,  0.1697,  0.2137,\n",
      "         0.0289,  0.2101,  0.2558,  0.2144,  0.2436,  0.2333,  0.2211,  0.2264,\n",
      "         0.1945,  0.2185,  0.2149,  0.2103,  0.2134,  0.2171,  0.2700,  0.2247,\n",
      "         0.2273,  0.2326,  0.2194,  0.2144,  0.2014,  0.2302,  0.2051,  0.2304,\n",
      "         0.2222,  0.1105,  0.2150,  0.1998,  0.2414,  0.2120,  0.1970,  0.2192,\n",
      "         0.2301,  0.2185,  0.2194,  0.2073,  0.2363,  0.2160,  0.1966,  0.2020,\n",
      "         0.2214,  0.2219,  0.2162,  0.2076,  0.1954,  0.2403,  0.2307,  0.2231,\n",
      "         0.2363,  0.2428,  0.2960,  0.2218,  0.2413,  0.2224,  0.2204,  0.2136,\n",
      "         0.2180,  0.2231,  0.2272,  0.2231,  0.1823,  0.2333,  0.2332,  0.2159,\n",
      "         0.2105,  0.2080,  0.2279,  0.1977,  0.2117,  0.1992,  0.2272,  0.2438,\n",
      "         0.2384,  0.2348,  0.2137,  0.2553,  0.2075,  0.2120,  0.2114,  0.2329,\n",
      "         0.2154,  0.2304,  0.2101,  0.2319,  0.2304,  0.2310,  0.2361,  0.2300,\n",
      "         0.2267,  0.2220,  0.2270,  0.1764,  0.2222,  0.2349,  0.2285,  0.2438,\n",
      "         0.2276,  0.2137,  0.2285,  0.2143,  0.2262,  0.2174,  0.2273,  0.2221,\n",
      "         0.2248,  0.2199,  0.2083,  0.2147,  0.1999,  0.2106,  0.2073,  0.2316,\n",
      "         0.2198,  0.2307,  0.1839,  0.2321,  0.2301,  0.2208,  0.2152,  0.1940,\n",
      "         0.2077,  0.3189,  0.2303,  0.2155,  0.2552,  0.2218,  0.1974,  0.2501,\n",
      "         0.2207,  0.2193,  0.1903,  0.2176,  0.1866,  0.2445,  0.2303,  0.2375,\n",
      "         0.2012,  0.2211,  0.2163,  0.2372,  0.2025,  0.2200,  0.2138,  0.2138,\n",
      "         0.2335,  0.2354,  0.2220,  0.2520,  0.2176,  0.2273,  0.2119,  0.2102,\n",
      "         0.2345,  0.2116,  0.2208,  0.2036,  0.2304,  0.2451,  0.1871,  0.1381,\n",
      "         0.2294,  0.2236,  0.2182,  0.2124,  0.2068,  0.2105,  0.2450,  0.2156,\n",
      "         0.2254,  0.2110,  0.0594,  0.2331,  0.2284,  0.2168,  0.2034,  0.4069,\n",
      "         0.2079,  0.2337,  0.2148,  0.2134,  0.2037,  0.2326,  0.2122,  0.2237,\n",
      "         0.1989,  0.2314,  0.2517,  0.2182,  0.2230,  0.2127,  0.2129,  0.1966,\n",
      "         0.2325,  0.2185,  0.2388,  0.2119,  0.2180,  0.2349,  0.2123,  0.2416,\n",
      "         0.2201,  0.2185,  0.2206, -0.0311,  0.2156,  0.2272,  0.2073,  0.2170,\n",
      "         0.1520,  0.2288,  0.2199,  0.2325,  0.2150,  0.1956,  0.2150,  0.2129,\n",
      "         0.2017,  0.2003,  0.2096,  0.2347,  0.1995,  0.2158,  0.2304,  0.2108,\n",
      "         0.2094,  0.2293,  0.2089,  0.2146,  0.2085,  0.2046,  0.2144,  0.2338,\n",
      "         0.2302,  0.2010,  0.2181,  0.2298,  0.2231,  0.2435,  0.2150,  0.2400,\n",
      "         0.2356,  0.1862,  0.2386,  0.2226,  0.2044,  0.2071,  0.2175,  0.2082,\n",
      "         0.2551,  0.2282,  0.2310,  0.2261,  0.2234,  0.2108,  0.2174,  0.2032,\n",
      "         0.2073,  0.1873,  0.2192,  0.2276,  0.2312,  0.2028,  0.2347,  0.1871,\n",
      "         0.2235,  0.2294,  0.2346,  0.2137,  0.2418,  0.2360,  0.2177,  0.2160,\n",
      "         0.2182,  0.2032,  0.2096,  0.2140,  0.2070,  0.2251,  0.2231,  0.2190,\n",
      "         0.1729,  0.2195,  0.2221,  0.2484,  0.2056,  0.2079,  0.2438,  0.2147,\n",
      "         0.2213,  0.2144,  0.2070,  0.2288,  0.1992,  0.2065,  0.2335,  0.1537,\n",
      "         0.2253,  0.2200,  0.2232,  0.2079,  0.2074,  0.2275,  0.2200,  0.2121,\n",
      "         0.1962,  0.2035,  0.2308,  0.2471,  0.2039,  0.2296,  0.2119,  0.2160,\n",
      "         0.1935,  0.2285,  0.2210,  0.2118,  0.2252, -0.1238,  0.2151,  0.2421,\n",
      "         0.2068,  0.1983,  0.2083,  0.2081, -0.0342,  0.2390,  0.2211,  0.1885,\n",
      "         0.2247,  0.2329,  0.2242,  0.2090,  0.2038,  0.2350,  0.2355,  0.2401,\n",
      "         0.2175,  0.2076,  0.2027,  0.2292,  0.2040,  0.2042,  0.1443,  0.2421,\n",
      "         0.2072,  0.2115,  0.2190,  0.2332,  0.2164,  0.2595,  0.2549,  0.2273,\n",
      "         0.2145,  0.2335,  0.2197,  0.2391,  0.2468,  0.2515,  0.2274,  0.2201,\n",
      "         0.2047,  0.1844,  0.2323,  0.2099,  0.2389,  0.2370,  0.2528,  0.2110,\n",
      "         0.2123,  0.2070,  0.2270,  0.2359,  0.2111,  0.1849,  0.2166,  0.2197,\n",
      "         0.2101,  0.2247,  0.1800,  0.1986,  0.1938,  0.2062,  0.2265,  0.2140,\n",
      "        -0.0309,  0.2329,  0.2234,  0.2076,  0.2302,  0.2239,  0.2392,  0.2115,\n",
      "         0.2202,  0.2309,  0.1893,  0.2152,  0.2124,  0.2184,  0.2251,  0.2471,\n",
      "         0.2287, -0.0351,  0.1993,  0.2347,  0.2095,  0.2129,  0.2016,  0.1335,\n",
      "         0.2321,  0.2145,  0.2153,  0.2254,  0.2124,  0.1966,  0.2137,  0.2172,\n",
      "         0.2233,  0.1748,  0.2137,  0.1889,  0.2238,  0.2562,  0.2389,  0.2188,\n",
      "         0.2410,  0.2498,  0.2178,  0.2273,  0.2270,  0.2502,  0.2367,  0.2144,\n",
      "         0.2204,  0.2133,  0.2183,  0.2171,  0.2263,  0.2162,  0.2329, -0.0274],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0142, -0.0155,  0.1014,  ...,  0.0122,  0.0449,  0.0004],\n",
      "        [-0.0046,  0.0692, -0.0622,  ..., -0.0636,  0.0076,  0.0225],\n",
      "        [ 0.0077,  0.0172, -0.0301,  ..., -0.0320, -0.0121,  0.0017],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0236, -0.0495,  ...,  0.0917,  0.0740, -0.0256],\n",
      "        [-0.0040, -0.0320, -0.0054,  ...,  0.0436,  0.0096,  0.0270],\n",
      "        [ 0.0067, -0.0031,  0.0327,  ..., -0.0139, -0.0369,  0.0367]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0859, -0.6323, -0.0586,  ...,  0.1270, -0.5186, -0.0385],\n",
      "        [-0.0532,  0.3476,  0.4312,  ..., -0.3939, -0.0811, -0.1000],\n",
      "        [ 0.1273, -0.0219,  0.2336,  ..., -0.0800, -0.1077,  0.0445],\n",
      "        ...,\n",
      "        [ 0.0185,  0.1561, -0.4941,  ...,  0.4805,  0.0230, -0.0013],\n",
      "        [-0.0028,  0.2135,  0.2040,  ..., -0.1292, -0.1465, -0.1239],\n",
      "        [-0.0010, -0.1440, -0.0591,  ..., -0.2214,  0.1459, -0.0919]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 7.2115e-02,  9.8282e-01,  4.3474e-01,  ...,  3.2897e-01,\n",
      "         -4.2180e-01,  5.0347e-02],\n",
      "        [-8.1816e-02, -9.9089e-01,  7.3909e-01,  ...,  3.5938e-01,\n",
      "          2.6455e-01, -3.6792e-04],\n",
      "        [-1.4551e-01, -2.0840e+00, -1.0918e+00,  ..., -5.0687e-01,\n",
      "         -4.4679e-01,  1.6426e-02],\n",
      "        ...,\n",
      "        [-1.8528e-01, -7.7376e-01,  4.6258e-01,  ...,  2.2884e-01,\n",
      "         -2.6753e-01,  6.8005e-02],\n",
      "        [ 9.7337e-02, -3.0741e-01,  1.8941e+00,  ...,  7.8867e-01,\n",
      "         -1.5648e-01,  1.4533e-01],\n",
      "        [ 6.5757e-02, -1.2354e-01,  4.3894e-01,  ...,  4.4933e-02,\n",
      "         -9.5160e-02, -8.1430e-03]], device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.4877, -0.3662,  0.6230,  ...,  4.5171, -4.6757,  0.3504],\n",
      "        [ 1.0765, -0.4308, -1.5541,  ..., -0.4380,  0.8485, -0.1288],\n",
      "        [ 0.8740,  0.6156, -0.1265,  ..., -1.4460, -0.0690, -0.0564],\n",
      "        ...,\n",
      "        [ 0.8193, -0.6798,  0.4388,  ..., -0.1981,  1.3601, -0.1115],\n",
      "        [ 0.5204,  0.5093,  1.0031,  ..., -0.6302,  0.3810,  0.1173],\n",
      "        [ 1.2173,  0.6602,  0.6513,  ...,  2.5725, -4.6122, -4.5731]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.2401,  0.2271,  0.2578,  0.3132,  0.3437,  0.2862,  0.3218,  0.3152,\n",
      "         0.2846,  0.3081,  0.2411,  0.2439,  0.3025,  0.3647,  0.2687,  0.2694,\n",
      "         0.3241,  0.2422,  0.3238,  0.3139,  0.2528,  0.2999,  0.3062,  0.2828,\n",
      "         0.2645,  0.2537,  0.2318,  0.2550,  0.2782,  0.2972,  0.2725,  0.2713,\n",
      "         0.2987,  0.2299,  0.3330,  0.4363,  0.2891,  0.2558,  0.2990,  0.2732,\n",
      "         0.3015,  0.3000,  0.3259,  0.3003,  0.2864,  0.3141,  0.2814,  0.2906,\n",
      "         0.3321,  0.2902,  0.2853,  0.3090,  0.3230,  0.2804,  0.2729,  0.3371,\n",
      "         0.2809,  0.3003,  0.2894,  0.2350,  0.3009,  0.3204,  0.2951,  0.3088,\n",
      "         0.3726,  0.2861,  0.3000,  0.6088,  0.2811,  0.2412,  1.1768,  0.2702,\n",
      "         0.0744,  0.3181,  0.3045,  0.2839,  0.3329,  0.2733,  0.2667,  0.3967,\n",
      "         0.3430,  0.3421,  0.3640,  0.2948,  0.2937,  0.2623,  0.3424,  0.2988,\n",
      "         0.3059,  0.2825,  0.2879,  0.2744,  0.3239,  0.3139,  0.2748,  0.3197,\n",
      "         0.2708, -0.1917,  0.2932,  0.2266,  0.2829,  0.2896,  0.2743,  0.3345,\n",
      "         0.3402,  0.3206,  0.2511,  0.2699,  0.3218,  0.3205,  0.2033,  0.2707,\n",
      "         0.2873,  0.2682,  0.3070,  0.2971,  0.2720,  0.3120,  0.3063,  0.2469,\n",
      "         0.2904,  0.3018,  0.0398,  0.3265,  0.2904,  0.2946,  0.3081,  0.2662,\n",
      "         0.3040,  0.3128,  0.3410,  0.2944,  0.2160,  0.2842,  0.3142,  0.2964,\n",
      "         0.2945,  0.3222,  0.3045,  0.2437,  0.2504,  0.2903,  0.3159,  0.3444,\n",
      "         0.3109,  0.3284,  0.2842,  0.3695,  0.2693,  0.2952,  0.2807,  0.4012,\n",
      "         0.2902,  0.2789,  0.2979,  0.2571,  0.3097,  0.6697,  0.2995,  0.2972,\n",
      "         0.3029,  0.2829,  0.3878,  0.2238,  0.2792,  0.3280,  0.2671,  0.2749,\n",
      "         0.3001,  0.3065,  0.3006,  0.3269,  0.2933,  0.2494,  0.2891,  0.2762,\n",
      "         0.2836,  0.3295,  0.2777,  0.2440,  0.2436,  0.2893,  0.3071,  0.3265,\n",
      "         0.2930,  0.3205,  0.2120,  0.3263,  0.2927,  0.3481,  0.3293,  0.2775,\n",
      "         0.2509,  0.4271,  0.2783,  0.2491,  0.3733,  0.4801,  0.2974,  0.2963,\n",
      "         0.2609,  0.3276,  0.2534,  0.2395,  0.2059,  0.2865,  0.2975,  0.2946,\n",
      "         0.2424,  0.3545,  0.3177,  0.3075,  0.2603,  0.3129,  0.2950,  0.2321,\n",
      "         0.3262,  0.3069,  0.4626,  0.3435,  0.3004,  0.2936,  0.2520,  0.2160,\n",
      "         0.2881,  0.2776,  0.2625,  0.3627,  0.2841,  0.2949,  0.2691, -0.1919,\n",
      "         0.2986,  0.0369,  0.2932,  0.3208,  0.2714,  0.2391,  0.3203,  0.3295,\n",
      "         0.2180,  0.2730, -0.0779,  0.3264,  0.3526,  0.3118,  0.2568,  0.0291,\n",
      "         0.2527,  0.2971,  0.2931,  0.2798,  0.2626,  0.3180,  0.3096,  0.3612,\n",
      "         0.2882,  0.2827,  0.3184,  0.3052,  0.3389,  0.2829,  0.2906,  0.2417,\n",
      "         0.2956,  0.2752,  0.3049,  0.2993,  0.3545,  0.4089,  0.2611,  0.3003,\n",
      "         0.2557,  0.2612,  0.2551, -0.0701,  0.3442,  0.2563,  0.2711,  0.2935,\n",
      "         0.1242,  0.2674,  0.2969,  0.3357,  0.3191,  0.2752,  0.3014,  0.2665,\n",
      "         0.3299,  0.3173,  0.2359,  0.2860,  0.2637,  0.3028,  0.3071,  0.2571,\n",
      "         0.2888,  0.3618,  0.2853,  0.2650,  0.3232,  0.3072,  0.2480,  0.2679,\n",
      "         0.3065,  0.2480,  0.2738,  0.2770,  0.3187,  0.2923,  0.2674,  0.2959,\n",
      "         0.3124,  0.2721,  0.3286,  0.3607,  0.2694,  0.2739,  0.2743,  0.3501,\n",
      "         0.2834,  0.3255,  0.3498,  0.3625,  0.2446,  0.2565,  0.3888,  0.2973,\n",
      "         0.2583,  0.3767,  0.3082,  0.3279,  0.3171,  0.3099,  0.2907,  0.2671,\n",
      "         0.2908,  0.3502,  0.3030,  0.2786,  0.3209,  0.2991,  0.3115,  0.3114,\n",
      "         0.3163,  0.2579,  0.2621,  0.2291,  0.3228,  0.2694,  0.2854,  0.2836,\n",
      "         0.3724,  0.3285,  0.2883,  0.3878,  0.2779,  0.2828,  0.2892,  0.2785,\n",
      "         0.2788,  0.2574,  0.2390,  0.2998,  0.2623,  0.2890,  0.2904,  0.2397,\n",
      "         0.3109,  0.2754,  0.2912,  0.2834,  0.2589,  0.3106,  0.2957,  0.2735,\n",
      "         0.2793,  0.2547,  0.2889,  0.3017,  0.2583,  0.3036,  0.3165,  0.2846,\n",
      "         0.2544,  0.3820,  0.3213,  0.2961,  0.3127, -0.1713,  0.2745,  0.3203,\n",
      "         0.2623,  0.2930,  0.2395,  0.2688,  0.0900,  0.3336,  0.3237,  0.2562,\n",
      "         0.2952,  0.3804,  0.2763,  0.3138,  0.3058,  0.2749,  0.3050,  0.2906,\n",
      "         0.3103,  0.2739,  0.3554,  0.3501,  0.2729,  0.2864,  0.2051,  0.3928,\n",
      "         0.3208,  0.2397,  0.3091,  0.3015,  0.3001,  0.3280,  0.3187,  0.3243,\n",
      "         0.2957,  0.2935,  0.2813,  0.3171,  0.9052,  0.3102,  0.3190,  0.2678,\n",
      "         0.2608,  0.2473,  0.3192,  0.2856,  0.3771,  0.2930,  0.3128,  0.2462,\n",
      "         0.2940,  0.2631,  0.3149,  0.3080,  0.2541, -0.2341,  0.2843,  0.2828,\n",
      "         0.2603,  0.2838,  0.3187,  0.3135,  0.2377,  0.3201,  0.3546,  0.2625,\n",
      "        -0.0732,  0.4379,  0.4211,  0.2757,  0.3521,  0.2836,  0.2698,  0.3248,\n",
      "         0.2774,  0.3397,  0.2489,  0.3063,  0.2825,  0.2505,  0.3069,  0.2956,\n",
      "         0.3153, -0.0546,  0.2804,  0.3154,  0.2809,  0.4041,  0.2320,  0.1163,\n",
      "         0.3054,  0.3487,  0.2749,  0.2696,  0.2646,  0.2848,  0.2394,  0.2882,\n",
      "         0.2990,  0.4843,  0.2618,  0.2274,  0.3076,  0.3024,  0.3450,  0.2807,\n",
      "         0.2941,  0.4342,  0.2986,  0.2656,  0.2681,  0.3206,  0.2704,  0.2537,\n",
      "         0.2791,  0.3219,  0.2579,  0.2581,  0.2868,  0.2721,  0.3495, -0.0777],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0011,  0.0356,  0.0034,  ..., -0.0756, -0.0052,  0.0062],\n",
      "        [ 0.0271,  0.0014,  0.0208,  ..., -0.0429, -0.0194, -0.0029],\n",
      "        [-0.0328,  0.0218,  0.0664,  ..., -0.0217, -0.0860,  0.0156],\n",
      "        ...,\n",
      "        [ 0.0094, -0.0762, -0.0136,  ...,  0.0731, -0.0037, -0.0080],\n",
      "        [ 0.0096, -0.0321,  0.0300,  ...,  0.0264, -0.0476, -0.0084],\n",
      "        [ 0.0035,  0.0433, -0.0706,  ..., -0.0675,  0.1115,  0.0167]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0067, -0.6084,  0.3450,  ...,  0.1032,  0.7951, -0.0432],\n",
      "        [-0.1934,  0.3308, -0.2577,  ...,  0.0547,  0.0640,  0.2861],\n",
      "        [-0.1576,  0.1965,  0.3307,  ..., -0.3832, -0.7593, -0.7611],\n",
      "        ...,\n",
      "        [-0.4235,  1.0737,  1.1550,  ..., -0.6092,  0.7672,  0.5938],\n",
      "        [ 0.8081, -0.0947,  0.7728,  ...,  0.6154, -0.6012,  0.3054],\n",
      "        [-0.3670,  0.6676,  0.0751,  ...,  0.4512,  1.1797,  0.5504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.6875,  0.6120, -0.1333,  ..., -0.2895,  0.9760, -0.4392],\n",
      "        [ 1.3703,  0.0054,  0.5406,  ..., -0.0444, -0.3856,  0.0394],\n",
      "        [-0.7098, -0.6353,  0.7109,  ...,  1.6320,  0.7110, -0.0701],\n",
      "        ...,\n",
      "        [ 1.6226, -0.0672,  0.6300,  ...,  0.3309, -0.7841, -0.4617],\n",
      "        [-0.8234,  0.3911, -0.2670,  ..., -0.0045,  0.2907, -0.2537],\n",
      "        [-0.9741, -0.3242,  1.1478,  ..., -0.6322,  0.1773,  0.4077]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-4.0232,  2.6883, -0.8643,  ..., -0.7422, -1.6349,  0.2404],\n",
      "        [ 1.4583,  0.0907, -0.5265,  ...,  0.0898, -0.2168, -0.4743],\n",
      "        [ 0.2326, -0.1834,  1.7876,  ..., -0.9012, -0.1089,  0.3724],\n",
      "        ...,\n",
      "        [-0.3447, -0.2393, -0.1474,  ..., -0.5869,  1.3157, -0.7142],\n",
      "        [ 1.7621,  0.6286, -0.2088,  ...,  0.8076,  0.2927,  0.3773],\n",
      "        [ 1.1375, -1.5300, -2.0850,  ...,  3.1829,  3.7368, -1.9046]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0446,  0.1595,  0.1583,  0.1697,  0.1977,  0.1444,  0.1830,  0.1549,\n",
      "         0.1479,  0.1444,  0.1587,  0.1314,  0.1670,  0.2017,  0.1968,  0.2206,\n",
      "         0.1809,  0.1412,  0.1812,  0.1940,  0.1607,  0.1795,  0.1962,  0.1767,\n",
      "         0.1382,  0.1765,  0.1301,  0.1394,  0.1748,  0.1993,  0.1812,  0.1573,\n",
      "         0.1813,  0.1515,  0.2155,  0.2536,  0.1981,  0.1729,  0.1702,  0.1918,\n",
      "         0.1746,  0.1788,  0.1767,  0.1842,  0.1891,  0.1546,  0.1576,  0.1665,\n",
      "         0.1699,  0.1613,  0.1684,  0.1936,  0.2194,  0.1782,  0.1864,  0.1693,\n",
      "         0.1919,  0.1704,  0.1841,  0.1348,  0.1864,  0.1541,  0.1909,  0.1769,\n",
      "         0.1696,  0.1781,  0.1730,  0.3451,  0.1877,  0.1394,  0.3521,  0.1711,\n",
      "         0.0310,  0.1385,  0.1891,  0.2213,  0.2357,  0.1783,  0.1645,  0.2002,\n",
      "         0.2216,  0.1888,  0.2228,  0.1368,  0.1837,  0.1709,  0.1856,  0.1535,\n",
      "         0.1818,  0.1751,  0.1962,  0.1689,  0.1928,  0.1639,  0.1312,  0.1355,\n",
      "         0.1633,  0.1150,  0.1565,  0.1311,  0.1704,  0.2018,  0.1654,  0.2136,\n",
      "         0.1684,  0.1886,  0.1690,  0.1547,  0.1679,  0.1628,  0.1150,  0.1692,\n",
      "         0.1655,  0.1706,  0.1642,  0.1819,  0.1630,  0.1974,  0.1140,  0.1610,\n",
      "         0.1701,  0.1853,  0.3663,  0.1854,  0.1823,  0.1657,  0.1893,  0.1611,\n",
      "         0.1841,  0.1902,  0.1703,  0.1765,  0.1317,  0.1897,  0.1720,  0.1398,\n",
      "         0.2000,  0.1607,  0.1749,  0.1395,  0.1751,  0.1842,  0.1548,  0.1761,\n",
      "         0.1864,  0.1673,  0.1489,  0.2339,  0.1508,  0.1458,  0.1952,  0.1875,\n",
      "         0.1699,  0.1418,  0.1797,  0.1431,  0.1993,  0.2038,  0.1808,  0.1636,\n",
      "         0.1672,  0.1749,  0.1941,  0.1188,  0.1542,  0.1900,  0.1811,  0.1842,\n",
      "         0.1701,  0.1805,  0.1760,  0.1409,  0.1891,  0.1576,  0.1737,  0.1532,\n",
      "         0.1513,  0.1832,  0.1533,  0.1694,  0.1402,  0.1767,  0.2051,  0.1971,\n",
      "         0.1517,  0.1907,  0.1609,  0.1937,  0.1586,  0.1828,  0.1895,  0.1460,\n",
      "         0.1783,  0.2594,  0.1791,  0.1689,  0.1902,  0.1613,  0.1671,  0.1780,\n",
      "         0.1821,  0.1610,  0.1349,  0.1471,  0.1371,  0.1720,  0.1903,  0.1775,\n",
      "         0.1554,  0.2219,  0.2232,  0.2189,  0.1582,  0.1702,  0.1717,  0.1289,\n",
      "         0.1888,  0.1890,  0.1756,  0.2093,  0.1695,  0.2057,  0.1428,  0.1526,\n",
      "         0.2074,  0.1831,  0.1457,  0.1614,  0.1847,  0.1897,  0.1421, -0.1008,\n",
      "         0.1747,  0.2632,  0.1516,  0.2139,  0.2052,  0.1617,  0.1789,  0.1595,\n",
      "         0.1102,  0.1619, -0.0335,  0.2067,  0.2018,  0.1699,  0.1380,  0.5751,\n",
      "         0.2008,  0.1702,  0.1587,  0.1858,  0.1386,  0.1868,  0.1622,  0.1877,\n",
      "         0.1781,  0.1789,  0.1805,  0.1662,  0.1629,  0.1708,  0.1562,  0.1526,\n",
      "         0.1668,  0.1511,  0.1750,  0.1738,  0.2076,  0.1948,  0.1640,  0.1764,\n",
      "         0.1413,  0.1586,  0.1448,  0.0018,  0.2389,  0.1624,  0.1563,  0.1857,\n",
      "        -0.0979,  0.1619,  0.1560,  0.1876,  0.2054,  0.1213,  0.1464,  0.1393,\n",
      "         0.2146,  0.1684,  0.1323,  0.1830,  0.1717,  0.2049,  0.1654,  0.1522,\n",
      "         0.1970,  0.1925,  0.1791,  0.1758,  0.1950,  0.1724,  0.1243,  0.1788,\n",
      "         0.1983,  0.1409,  0.1300,  0.1630,  0.1778,  0.1862,  0.1461,  0.1544,\n",
      "         0.1613,  0.1430,  0.2252,  0.1760,  0.1856,  0.1503,  0.1946,  0.1929,\n",
      "         0.1665,  0.1906,  0.1871,  0.1835,  0.1605,  0.1376,  0.1780,  0.1581,\n",
      "         0.1329,  0.1358,  0.1751,  0.1614,  0.1774,  0.1761,  0.1801,  0.1605,\n",
      "         0.2017,  0.1625,  0.1571,  0.1757,  0.1958,  0.1664,  0.1648,  0.1840,\n",
      "         0.2018,  0.1752,  0.1383,  0.1272,  0.1511,  0.1979,  0.1935,  0.2089,\n",
      "         0.1230,  0.1912,  0.1797,  0.2044,  0.1551,  0.1781,  0.1817,  0.1401,\n",
      "         0.1505,  0.1494,  0.1603,  0.1754,  0.1603,  0.1677,  0.1754,  0.1437,\n",
      "         0.2074,  0.1794,  0.1416,  0.1698,  0.1375,  0.2083,  0.1794,  0.1600,\n",
      "         0.1336,  0.1682,  0.1897,  0.1715,  0.1396,  0.2013,  0.1961,  0.1564,\n",
      "         0.1368,  0.2701,  0.1605,  0.1867,  0.1865,  0.0851,  0.1403,  0.2075,\n",
      "         0.1682,  0.1695,  0.1472,  0.1611, -0.0018,  0.1754,  0.1899,  0.1479,\n",
      "         0.1491,  0.1865,  0.1319,  0.2043,  0.1760,  0.1809,  0.1403,  0.1568,\n",
      "         0.1822,  0.1436,  0.1478,  0.2242,  0.1942,  0.1702,  0.1250,  0.2594,\n",
      "         0.1760, -0.1513,  0.1755,  0.1752,  0.1516,  0.1828,  0.2064,  0.1747,\n",
      "         0.1985,  0.1782,  0.2138,  0.1752,  0.4518,  0.1996,  0.1951,  0.2057,\n",
      "         0.1638,  0.1584,  0.1541,  0.1682,  0.2105,  0.1781,  0.2048,  0.1400,\n",
      "         0.1780,  0.1647,  0.1913,  0.1798,  0.1439,  0.1402,  0.1657,  0.1590,\n",
      "         0.1628,  0.1615,  0.1709,  0.1739,  0.1481,  0.1446,  0.1926,  0.1799,\n",
      "         0.0430,  0.2126,  0.2493,  0.1579,  0.1806,  0.1784,  0.1702,  0.1922,\n",
      "         0.1556,  0.1963,  0.1068,  0.1505,  0.1833,  0.1604,  0.1801,  0.1839,\n",
      "         0.1838, -0.0016,  0.1696,  0.1719,  0.1513,  0.1736,  0.1622, -0.0947,\n",
      "         0.1438,  0.1678,  0.1972,  0.1442,  0.1700,  0.1635,  0.1786,  0.1613,\n",
      "         0.1919,  0.1685,  0.1719,  0.1395,  0.1982,  0.1768,  0.1990,  0.1663,\n",
      "         0.1848,  0.2374,  0.1793,  0.1655,  0.2463,  0.1780,  0.1599,  0.1562,\n",
      "         0.1666,  0.1696,  0.1563,  0.1837,  0.1682,  0.1898,  0.1657, -0.0285],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0477, -0.2612,  0.3161,  ...,  0.0795, -0.1505, -0.0213],\n",
      "        [ 0.0281,  0.0536, -0.0082,  ...,  0.0599, -0.3908, -0.0868],\n",
      "        [ 0.1008,  0.1549,  0.0605,  ..., -0.1133, -0.2514, -0.0424],\n",
      "        ...,\n",
      "        [ 0.0207, -0.0371, -0.3941,  ...,  0.0288,  0.1542,  0.0467],\n",
      "        [ 0.4550, -0.5606, -0.4506,  ..., -0.4043, -0.0400, -0.1232],\n",
      "        [ 0.1474,  0.1971, -0.1548,  ...,  0.4521,  0.0968, -0.0228]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2997, -0.3971, -0.7291,  ..., -0.6412, -0.3527,  0.0172],\n",
      "        [-0.1600, -0.4832,  0.1175,  ...,  0.9180, -0.5584,  0.3602],\n",
      "        [-0.0493,  0.5930,  1.7753,  ..., -0.0309, -1.0294,  0.1314],\n",
      "        ...,\n",
      "        [ 0.5303, -0.9888,  0.6684,  ..., -0.4228, -1.6073,  0.0306],\n",
      "        [-1.1707,  0.5206,  1.7486,  ...,  0.1711, -1.2120,  0.2249],\n",
      "        [ 0.0672, -2.8723,  1.1669,  ..., -0.2031, -0.8697, -0.6891]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.5249, -1.3097,  0.9689,  ...,  1.3594,  0.3366, -0.5766],\n",
      "        [ 0.2148,  2.0314, -0.3629,  ...,  1.5684, -1.8007,  0.4058],\n",
      "        [ 0.3108,  0.3311, -0.2689,  ...,  0.8560, -0.0125,  0.3340],\n",
      "        ...,\n",
      "        [ 0.7941, -0.6714,  0.5754,  ...,  0.2482, -0.7224,  0.3317],\n",
      "        [ 0.5010,  0.4175,  0.0348,  ..., -0.8459,  0.4821, -0.1239],\n",
      "        [ 1.3517, -2.5703,  1.3805,  ..., -1.4900,  3.8585, -0.4495]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.5.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0334,  0.2401,  0.2375,  0.2621,  0.2430,  0.2404,  0.2598,  0.2608,\n",
      "         0.2553,  0.2284,  0.2261,  0.2176,  0.2702,  0.3034,  0.2377,  0.2489,\n",
      "         0.2216,  0.2460,  0.2788,  0.2807,  0.2505,  0.2968,  0.2616,  0.2728,\n",
      "         0.2610,  0.2309,  0.2418,  0.2197,  0.2553,  0.2539,  0.2567,  0.2397,\n",
      "         0.2919,  0.2162,  0.3023,  0.3534,  0.2814,  0.2613,  0.2601,  0.2669,\n",
      "         0.2711,  0.2722,  0.2757,  0.2467,  0.2847,  0.2680,  0.2303,  0.2510,\n",
      "         0.2936,  0.2489,  0.2541,  0.2718,  0.2521,  0.2717,  0.2413,  0.2549,\n",
      "         0.2366,  0.2487,  0.2877,  0.2398,  0.2597,  0.2396,  0.2585,  0.2604,\n",
      "         0.2855,  0.2441,  0.2715,  0.3271,  0.2427,  0.2301,  0.2288,  0.2599,\n",
      "         0.0497,  0.2160,  0.2583,  0.2663,  0.2902,  0.2808,  0.2650,  0.2666,\n",
      "         0.2609,  0.2526,  0.2433,  0.2458,  0.2649,  0.2431,  0.2869,  0.2520,\n",
      "         0.2581,  0.2640,  0.2560,  0.2732,  0.2371,  0.2558,  0.2227,  0.2507,\n",
      "         0.2656,  0.1591,  0.2318,  0.2113,  0.2450,  0.2518,  0.2618,  0.2859,\n",
      "         0.2649,  0.2700,  0.2542,  0.2415,  0.2742,  0.2515,  0.2308,  0.2235,\n",
      "         0.2551,  0.2446,  0.2394,  0.2155,  0.2562,  0.2879,  0.2389,  0.2416,\n",
      "         0.2678,  0.2469,  0.2742,  0.2471,  0.2830,  0.2811,  0.2900,  0.2501,\n",
      "         0.2826,  0.2593,  0.2752,  0.2601,  0.1980,  0.2523,  0.2908,  0.2605,\n",
      "         0.2588,  0.2767,  0.2877,  0.2312,  0.2608,  0.2505,  0.2477,  0.2928,\n",
      "         0.2743,  0.3004,  0.2396,  0.2905,  0.2282,  0.2359,  0.2604,  0.2542,\n",
      "         0.2245,  0.2496,  0.2550,  0.2829,  0.2740,  0.2525,  0.2696,  0.2413,\n",
      "         0.2648,  0.2798,  0.2680,  0.1983,  0.2615,  0.2600,  0.2357,  0.2586,\n",
      "         0.2396,  0.2581,  0.2613,  0.2653,  0.2698,  0.2240,  0.2891,  0.2430,\n",
      "         0.2621,  0.2960,  0.2220,  0.2718,  0.2287,  0.2415,  0.2433,  0.2890,\n",
      "         0.2181,  0.2742,  0.2245,  0.2373,  0.2559,  0.2634,  0.2758,  0.2603,\n",
      "         0.2475,  0.4369,  0.2437,  0.2301,  0.2820,  0.2921,  0.2294,  0.2690,\n",
      "         0.2454,  0.2442,  0.2176,  0.2257,  0.2240,  0.2663,  0.2862,  0.2849,\n",
      "         0.2356,  0.2874,  0.2122,  0.2693,  0.2455,  0.2452,  0.2442,  0.2331,\n",
      "         0.2646,  0.2590,  0.2520,  0.3042,  0.2809,  0.2486,  0.2254,  0.2294,\n",
      "         0.2585,  0.2553,  0.2340,  0.2379,  0.2726,  0.2725,  0.2314, -0.1686,\n",
      "         0.2656,  0.2230,  0.2492,  0.2591,  0.2576,  0.2257,  0.2921,  0.2768,\n",
      "         0.2378,  0.2558,  0.1116,  0.2727,  0.2652,  0.2437,  0.2412,  0.4290,\n",
      "         0.2500,  0.2653,  0.2561,  0.2482,  0.2192,  0.2689,  0.2567,  0.2861,\n",
      "         0.2345,  0.2640,  0.2783,  0.2715,  0.2602,  0.2277,  0.2306,  0.2381,\n",
      "         0.2527,  0.3024,  0.2640,  0.2481,  0.2943,  0.2724,  0.2279,  0.2683,\n",
      "         0.2239,  0.2587,  0.2290,  0.0387,  0.2433,  0.2691,  0.2336,  0.2712,\n",
      "         0.1821,  0.2398,  0.2556,  0.2810,  0.2698,  0.2356,  0.2286,  0.2518,\n",
      "         0.2307,  0.2514,  0.2307,  0.2629,  0.2213,  0.2531,  0.2707,  0.2674,\n",
      "         0.2450,  0.2949,  0.2544,  0.2420,  0.2491,  0.2349,  0.2190,  0.2570,\n",
      "         0.3033,  0.2165,  0.2320,  0.2460,  0.2777,  0.2811,  0.2464,  0.2749,\n",
      "         0.2575,  0.2319,  0.2877,  0.2827,  0.2613,  0.2504,  0.2391,  0.2873,\n",
      "         0.2721,  0.2822,  0.2393,  0.2717,  0.2566,  0.2237,  0.2443,  0.2484,\n",
      "         0.2367, -0.2099,  0.2598,  0.2559,  0.2692,  0.2535,  0.2837,  0.2452,\n",
      "         0.2727,  0.2706,  0.2696,  0.2573,  0.2725,  0.2617,  0.2538,  0.2606,\n",
      "         0.2691,  0.2181,  0.2516,  0.2291,  0.2438,  0.2509,  0.2334,  0.2699,\n",
      "         0.2148,  0.2696,  0.2743,  0.2744,  0.2362,  0.2400,  0.2861,  0.2244,\n",
      "         0.2401,  0.2485,  0.2219,  0.2583,  0.2432,  0.2636,  0.2446,  0.2038,\n",
      "         0.2574,  0.2471,  0.2345,  0.2542,  0.2512,  0.2485,  0.2646,  0.2202,\n",
      "         0.2238,  0.2375,  0.2806,  0.2586,  0.2161,  0.2773,  0.2658,  0.2294,\n",
      "         0.2194,  0.2585,  0.2566,  0.2365,  0.2722,  0.1571,  0.2594,  0.2993,\n",
      "         0.2523,  0.2237,  0.2163,  0.2577, -0.0430,  0.2784,  0.2674,  0.2229,\n",
      "         0.2793,  0.3141,  0.2431,  0.2424,  0.2486,  0.2695,  0.2588,  0.2589,\n",
      "         0.2574,  0.2542,  0.2356,  0.3066,  0.2949,  0.2419,  0.1997,  0.2470,\n",
      "         0.2768,  0.2530,  0.2469,  0.2406,  0.2677,  0.3183,  0.2991,  0.2715,\n",
      "         0.2766,  0.2638,  0.2554,  0.2628,  0.3249,  0.2869,  0.2739,  0.2450,\n",
      "         0.2495,  0.2319,  0.2592,  0.2656,  0.3110,  0.2978,  0.2701,  0.2241,\n",
      "         0.2570,  0.2457,  0.2925,  0.2667,  0.2486,  0.2104,  0.2771,  0.2076,\n",
      "         0.2459,  0.2635,  0.2452,  0.2287,  0.2385,  0.2314,  0.2799,  0.2450,\n",
      "        -0.0460,  0.2840,  0.3361,  0.2565,  0.2444,  0.2552,  0.2635,  0.2506,\n",
      "         0.2409,  0.2846,  0.2315,  0.2553,  0.2499,  0.2206,  0.2757,  0.2745,\n",
      "         0.2623,  0.0467,  0.2484,  0.2764,  0.2482,  0.2433,  0.2407,  0.1383,\n",
      "         0.2348,  0.2319,  0.2609,  0.2615,  0.2544,  0.2282,  0.2331,  0.2222,\n",
      "         0.2775,  0.1816,  0.2405,  0.2391,  0.2831,  0.2679,  0.2992,  0.2472,\n",
      "         0.2608,  0.3243,  0.2473,  0.2638,  0.2554,  0.3194,  0.2599,  0.2470,\n",
      "         0.2462,  0.2671,  0.2462,  0.2512,  0.2652,  0.2634,  0.2429,  0.0530],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0025,  0.0261,  0.0672,  ...,  0.0122, -0.0686, -0.0224],\n",
      "        [ 0.0292,  0.0255,  0.0113,  ..., -0.0506,  0.0248, -0.0283],\n",
      "        [-0.0207,  0.0046, -0.0303,  ...,  0.0010,  0.0353,  0.0229],\n",
      "        ...,\n",
      "        [ 0.0281, -0.0398, -0.0189,  ...,  0.0678,  0.0103, -0.0482],\n",
      "        [ 0.0099, -0.0811, -0.0250,  ...,  0.0161,  0.0136, -0.0174],\n",
      "        [-0.0374,  0.0069, -0.0125,  ..., -0.0278, -0.0207,  0.0504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0186, -0.1988,  0.1249,  ...,  0.0335, -0.3309,  0.1038],\n",
      "        [-0.1561,  0.1261,  0.0427,  ...,  0.2304,  0.0711,  0.2134],\n",
      "        [ 0.1775,  0.2772, -0.1124,  ...,  0.1452, -0.0856, -0.1183],\n",
      "        ...,\n",
      "        [-0.1505,  0.4048, -0.0272,  ..., -0.0087, -0.0564,  0.2281],\n",
      "        [-0.1610, -0.2601, -0.0941,  ...,  0.1155,  0.3794,  0.1627],\n",
      "        [ 0.2268, -0.3273, -0.2673,  ..., -0.2369,  0.1784, -0.2047]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2537,  0.4711,  0.3241,  ..., -0.7542,  0.8400,  0.0479],\n",
      "        [ 0.1113, -1.0605,  0.0272,  ...,  0.3805,  0.4335, -0.0586],\n",
      "        [-0.3338,  1.3751,  0.4800,  ...,  0.5083, -0.2165,  0.0838],\n",
      "        ...,\n",
      "        [ 0.0707, -0.6164, -0.7053,  ...,  1.0009, -0.6625, -0.0490],\n",
      "        [ 0.0214,  0.3287, -1.4039,  ...,  1.6678, -0.1461,  0.0592],\n",
      "        [-0.3007,  0.4501,  0.5444,  ..., -0.2096, -0.8562,  0.0695]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 1.3156,  0.7991,  0.7725,  ..., -0.1747, -5.9321, -4.7151],\n",
      "        [-0.5217, -0.4774, -1.7143,  ..., -1.6476,  1.6017, -0.2421],\n",
      "        [-0.1701,  0.1373, -0.4448,  ..., -0.3824, -0.2982,  1.6234],\n",
      "        ...,\n",
      "        [ 0.3282, -0.3537, -0.5097,  ..., -0.2447, -0.9679, -0.4419],\n",
      "        [ 0.5792, -0.5066, -0.2615,  ..., -1.2069,  0.8647,  0.3902],\n",
      "        [ 5.5577, -9.0856,  0.0658,  ...,  0.9107, -8.4388, -1.1560]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0649,  0.2547,  0.2984,  0.3290,  0.3150,  0.2430,  0.3351,  0.3165,\n",
      "         0.3294,  0.2876,  0.2553,  0.2382,  0.3603,  0.4004,  0.3379,  0.2993,\n",
      "         0.3221,  0.3102,  0.3568,  0.3920,  0.3052,  0.3965,  0.3067,  0.2946,\n",
      "         0.2883,  0.2495,  0.2848,  0.2456,  0.3120,  0.3470,  0.3009,  0.2497,\n",
      "         0.3230, -0.2964,  0.3724,  0.4616,  0.3790,  0.2822,  0.3267,  0.3007,\n",
      "         0.3031,  0.3110,  0.3187,  0.3494,  0.3305,  0.2723,  0.2601,  0.2973,\n",
      "         0.3403,  0.2984,  0.2861,  0.3485,  0.3801,  0.3015,  0.3012,  0.3320,\n",
      "         0.2589,  0.3000,  0.3970,  0.2588,  0.3253,  0.2976,  0.3434,  0.3391,\n",
      "         0.3010,  0.2867,  0.3403,  0.5727,  0.2828,  0.2828,  1.0471,  0.3528,\n",
      "         0.0617,  0.2690,  0.3055,  0.3294,  0.3577,  0.3277,  0.3301,  0.3694,\n",
      "         0.3647,  0.3230,  0.3493,  0.2794,  0.3224,  0.3276,  0.3701,  0.3093,\n",
      "         0.3370,  0.3100,  0.3147,  0.3021,  0.2887,  0.2726,  0.2705,  0.2880,\n",
      "         0.2975,  0.2198,  0.2461,  0.2559,  0.3040,  0.2988,  0.3411,  0.3952,\n",
      "         0.3511,  0.3237,  0.2972,  0.2703,  0.3628,  0.2982,  0.2749,  0.2400,\n",
      "         0.3036,  0.3272,  0.2876,  0.2969,  0.3475,  0.3048,  0.2675,  0.2854,\n",
      "         0.3100,  0.3109,  0.0791,  0.3020,  0.3243,  0.2935,  0.3722,  0.3129,\n",
      "         0.3601,  0.3466,  0.2974,  0.3484,  0.2515,  0.3004,  0.3290,  0.3082,\n",
      "         0.3159,  0.3382,  0.3492,  0.2733,  0.2903,  0.2877,  0.3288,  0.3487,\n",
      "         0.3244,  0.3243,  0.3227,  0.3886,  0.2446,  0.3504,  0.3068,  0.3093,\n",
      "         0.3001,  0.2963,  0.3717,  0.2896,  0.3730,  0.3522,  0.2991,  0.3261,\n",
      "         0.3137,  0.3435,  0.3350,  0.2278,  0.3238,  0.3292,  0.2846,  0.3068,\n",
      "         0.2837,  0.2935,  0.3262,  0.3083,  0.3321,  0.2700,  0.3252,  0.3000,\n",
      "         0.3281,  0.3724,  0.2721,  0.3322,  0.2935,  0.2937,  0.2752,  0.3731,\n",
      "         0.2968,  0.4119,  0.2487,  0.3074,  0.2811,  0.3631,  0.3425,  0.2650,\n",
      "         0.2856,  0.5068,  0.3330,  0.3001,  0.3815,  0.4601,  0.2793,  0.3098,\n",
      "         0.3404,  0.2799,  0.2561,  0.2665,  0.2428,  0.3620,  0.3015,  0.3672,\n",
      "         0.3084,  0.3584,  0.2893,  0.3655,  0.3274,  0.3059,  0.3382,  0.2778,\n",
      "         0.3265,  0.3296,  0.3558,  0.3750,  0.3672,  0.3296,  0.2537,  0.2716,\n",
      "         0.3060,  0.2969,  0.2735,  0.3128,  0.3139,  0.2958,  0.3512,  0.2090,\n",
      "         0.3425,  0.0774,  0.3131,  0.4086,  0.3063,  0.2709,  0.3499,  0.3563,\n",
      "         0.2434,  0.2888, -0.1087,  0.3850,  0.3181,  0.3320,  0.2781,  0.1385,\n",
      "         0.2675,  0.2572,  0.2906,  0.3104,  0.2576,  0.3348,  0.3350,  0.3276,\n",
      "         0.2870,  0.2976,  0.3939,  0.3162,  0.3486,  0.2616,  0.2639,  0.2840,\n",
      "         0.2876,  0.2993,  0.3199,  0.2731,  0.4136,  0.3188,  0.2886,  0.3055,\n",
      "         0.2749,  0.2967,  0.2912,  0.0547,  0.3219,  0.3115,  0.3031,  0.3635,\n",
      "         0.1421,  0.2728,  0.3146,  0.3970,  0.3174,  0.3024,  0.2689,  0.2763,\n",
      "         0.2918,  0.3194,  0.2518,  0.3277,  0.2583,  0.3518,  0.3329,  0.2925,\n",
      "         0.2588,  0.4044,  0.2593,  0.3247,  0.3265,  0.2959,  0.2459,  0.3299,\n",
      "         0.3362,  0.2519,  0.2906,  0.2916,  0.3764,  0.3500,  0.3166,  0.3224,\n",
      "         0.3042,  0.3082,  0.3313,  0.3620,  0.3457,  0.2939,  0.2669,  0.3207,\n",
      "         0.3037,  0.3763,  0.3331,  0.3725,  0.2941,  0.2549,  0.3484,  0.2869,\n",
      "         0.2551,  0.2809,  0.3171,  0.3031,  0.2710,  0.2950,  0.3250,  0.3019,\n",
      "         0.3331,  0.3203,  0.3329,  0.2975,  0.2885,  0.3067,  0.2902,  0.3002,\n",
      "         0.3801,  0.2526,  0.3085,  0.2451,  0.2360,  0.3055,  0.3095,  0.3046,\n",
      "         0.5557,  0.3089,  0.3326,  0.4192,  0.2863,  0.2791,  0.3268,  0.2574,\n",
      "         0.2568,  0.3013,  0.2788,  0.3314,  0.3133,  0.3315,  0.3098,  0.2404,\n",
      "         0.3351,  0.3122,  0.2487,  0.3120,  0.3169,  0.3216,  0.2991,  0.2550,\n",
      "         0.2884,  0.2726,  0.3310,  0.3102,  0.2561,  0.3281,  0.3671,  0.2918,\n",
      "         0.2627,  0.3570,  0.2840,  0.3136,  0.3711,  0.1834,  0.3015,  0.4059,\n",
      "         0.2852,  0.2884,  0.2821,  0.3102,  0.1347,  0.3144,  0.3022,  0.2744,\n",
      "         0.3585,  0.3982,  0.2537,  0.3631,  0.3039,  0.3450,  0.3088,  0.2740,\n",
      "         0.3085,  0.3120,  0.2919,  0.3763,  0.3534,  0.2899,  0.2644,  0.3584,\n",
      "         0.3368,  0.2763,  0.3224,  0.2681,  0.3206,  0.4421,  0.3789,  0.3738,\n",
      "         0.3854,  0.3282,  0.3644,  0.3305,  0.8929,  0.4002,  0.3797,  0.2549,\n",
      "         0.3430,  0.2449,  0.3240,  0.2902,  0.3994,  0.3735,  0.3930,  0.2348,\n",
      "         0.3283,  0.2873,  0.3693,  0.3620,  0.2547, -0.2345,  0.3507,  0.2510,\n",
      "         0.2871,  0.3321,  0.3031,  0.3054,  0.2610,  0.2709,  0.3002,  0.3228,\n",
      "        -0.0698,  0.4158,  0.4055,  0.3347,  0.3869,  0.3064,  0.2996,  0.3694,\n",
      "         0.2914,  0.3373, -0.3004,  0.3126,  0.3209,  0.2416,  0.3776,  0.3300,\n",
      "         0.3056,  0.0524,  0.3139,  0.3456,  0.2919,  0.3452,  0.3038,  0.1142,\n",
      "         0.2694,  0.3490,  0.3035,  0.3253,  0.2896,  0.2643,  0.2978,  0.2892,\n",
      "         0.2873,  0.3879,  0.3152,  0.2835,  0.3497,  0.3016,  0.3927,  0.2734,\n",
      "         0.2856,  0.4828,  0.3054,  0.3107,  0.3086,  0.3702,  0.3647,  0.3190,\n",
      "         0.3315,  0.3085,  0.3008,  0.2991,  0.3354,  0.3049,  0.3658,  0.0637],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0350, -0.0987, -0.0735,  ..., -0.0055,  0.0310, -0.0115],\n",
      "        [-0.0635, -0.0409,  0.0549,  ...,  0.0142, -0.0177, -0.0077],\n",
      "        [ 0.0200,  0.0713, -0.0033,  ..., -0.0089,  0.0258, -0.0005],\n",
      "        ...,\n",
      "        [ 0.0282, -0.0187, -0.0067,  ..., -0.0710, -0.0240, -0.0127],\n",
      "        [ 0.0056, -0.0353,  0.0015,  ...,  0.0739, -0.0005, -0.0040],\n",
      "        [ 0.0180, -0.0647, -0.1197,  ..., -0.0539, -0.0639, -0.0043]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2759, -0.5049, -0.4387,  ...,  0.8241,  0.0081, -0.2513],\n",
      "        [ 0.1385, -0.0341, -0.0956,  ...,  0.2843, -0.3302, -0.0109],\n",
      "        [ 0.3414, -0.5534,  0.7530,  ..., -0.1994,  0.8605,  0.2345],\n",
      "        ...,\n",
      "        [-0.2186, -0.4575, -0.3194,  ..., -0.4309, -0.0770,  0.1949],\n",
      "        [ 0.6835,  0.4827,  0.6119,  ...,  0.1382, -0.3306,  0.1547],\n",
      "        [ 0.2452,  0.5298, -0.3042,  ..., -0.0043,  0.3931, -0.4436]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.8857, -0.9897,  2.2195,  ..., -0.3266, -1.5144,  0.0879],\n",
      "        [ 0.2537, -0.4623, -0.5221,  ...,  1.5049, -0.6102, -0.0182],\n",
      "        [-0.9159,  0.6226,  1.5505,  ..., -1.9245,  1.8122, -0.3237],\n",
      "        ...,\n",
      "        [-1.1564, -0.2632,  1.2380,  ..., -0.9941, -0.4050, -0.9101],\n",
      "        [-1.3100,  0.4713, -0.8075,  ...,  0.0322, -0.2405, -0.7313],\n",
      "        [ 1.0224,  1.5325,  1.1055,  ..., -1.1705,  1.9173,  0.5734]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.0685, -1.5179,  8.7582,  ...,  4.5742,  4.2692,  4.0716],\n",
      "        [-0.3642, -0.6255, -0.6983,  ...,  0.8033, -0.8488,  0.4445],\n",
      "        [ 2.0949, -0.8757,  0.8619,  ...,  0.2913,  0.4047,  0.2615],\n",
      "        ...,\n",
      "        [ 0.0362,  0.9741, -0.9505,  ...,  0.7510,  1.3187, -1.2122],\n",
      "        [-0.6067, -2.6597,  0.0726,  ...,  1.7030,  0.6653,  1.0150],\n",
      "        [ 6.3859,  0.6919, -1.4057,  ..., -2.3897, -2.7933,  4.6922]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0511,  0.1145,  0.1407,  0.1408,  0.1365,  0.1169,  0.1475,  0.1464,\n",
      "         0.1138,  0.1104,  0.1085,  0.1040,  0.1585,  0.1925,  0.1561,  0.1560,\n",
      "         0.1466,  0.1489,  0.1758,  0.1721,  0.1353,  0.1890,  0.1381,  0.1482,\n",
      "         0.1390,  0.1449,  0.1163,  0.1205,  0.1428,  0.1602,  0.1447,  0.1247,\n",
      "         0.1301,  0.1007,  0.1919,  0.2833,  0.1528,  0.1355,  0.1471,  0.1827,\n",
      "         0.1388,  0.1520,  0.1390,  0.1803,  0.1805,  0.1438,  0.1062,  0.1398,\n",
      "         0.1430,  0.1506,  0.1361,  0.1916,  0.1809,  0.1410,  0.1675,  0.1213,\n",
      "         0.1324,  0.1450,  0.1858,  0.1292,  0.1263,  0.1473,  0.1683,  0.1563,\n",
      "         0.1336,  0.1490,  0.1370,  0.4072,  0.1842,  0.1061,  0.2663,  0.1429,\n",
      "         0.0316,  0.1165,  0.1331,  0.1978,  0.2138,  0.1493,  0.1414,  0.1774,\n",
      "         0.2330,  0.1887,  0.2039,  0.1133,  0.1480,  0.1178,  0.1863,  0.1161,\n",
      "         0.1449,  0.1681,  0.1680,  0.1596,  0.1568,  0.1222,  0.1182,  0.1277,\n",
      "         0.1328,  0.1218,  0.1413,  0.1149,  0.1225,  0.1421,  0.1484,  0.1680,\n",
      "         0.1456,  0.1544,  0.1462,  0.1183,  0.1407,  0.1340,  0.1177,  0.1211,\n",
      "         0.1333,  0.1306,  0.1097,  0.1435,  0.1525,  0.1284,  0.0971,  0.1290,\n",
      "         0.1401,  0.1733, -0.2363,  0.1430,  0.1408,  0.1269,  0.1750,  0.1290,\n",
      "         0.1453,  0.1322,  0.1389,  0.1670, -0.1153,  0.1293,  0.1572,  0.1201,\n",
      "         0.1762,  0.1487,  0.1499,  0.1221,  0.1540,  0.1518,  0.1200,  0.1579,\n",
      "         0.1872,  0.1544,  0.1185,  0.1821,  0.1258,  0.1086,  0.1412,  0.1529,\n",
      "         0.1288,  0.1213,  0.1734,  0.1449,  0.1740,  0.1470,  0.1546,  0.1246,\n",
      "         0.1730,  0.1425,  0.1627, -0.0814,  0.1062,  0.1563,  0.1043,  0.1457,\n",
      "         0.1068,  0.1335,  0.1673,  0.1196,  0.1364, -0.1002,  0.1615, -0.1188,\n",
      "         0.1571,  0.1851,  0.1212,  0.1811,  0.1261,  0.1517,  0.1484,  0.2111,\n",
      "         0.1170,  0.1821,  0.1037,  0.1219,  0.1213,  0.1737,  0.1597,  0.1519,\n",
      "         0.1272,  0.2442,  0.1304,  0.1257,  0.1690,  0.1819,  0.1343,  0.1641,\n",
      "         0.1369,  0.1270,  0.1060,  0.1334,  0.1210,  0.1569,  0.1634,  0.1498,\n",
      "         0.1160,  0.1910,  0.1652,  0.1653,  0.1258,  0.1322,  0.1594,  0.1327,\n",
      "         0.1678,  0.1698,  0.1557,  0.1791,  0.2181,  0.1672,  0.0828,  0.1137,\n",
      "         0.1437,  0.1171,  0.1167,  0.1345,  0.1476,  0.1447,  0.1406, -0.0911,\n",
      "         0.1475,  0.1660,  0.1617,  0.2032,  0.1699,  0.1126,  0.1401,  0.1863,\n",
      "         0.1085,  0.1360, -0.0840,  0.1930,  0.1915,  0.1412,  0.0899,  0.3573,\n",
      "         0.1883,  0.1287,  0.1335,  0.1435,  0.1064,  0.1723,  0.1540,  0.1888,\n",
      "         0.1533,  0.1351,  0.1752,  0.1431,  0.1395,  0.1445,  0.1105,  0.1079,\n",
      "         0.1421,  0.1489,  0.1360,  0.1338,  0.2772,  0.1539,  0.1369,  0.1405,\n",
      "         0.0992,  0.1320,  0.1126,  0.0182,  0.2524,  0.1325,  0.1459,  0.1836,\n",
      "         0.0836,  0.1354,  0.1713,  0.1931,  0.1328,  0.0979,  0.1218,  0.1331,\n",
      "         0.1820,  0.1629,  0.0929,  0.1258,  0.1292,  0.1869,  0.1242,  0.1349,\n",
      "         0.1480,  0.1799,  0.1607,  0.1390,  0.1630,  0.1310,  0.1047,  0.1370,\n",
      "         0.1607,  0.0884,  0.1036,  0.1391,  0.1370,  0.1358,  0.1742,  0.1372,\n",
      "         0.1435,  0.1311,  0.1649,  0.1537,  0.2138,  0.1397,  0.1715,  0.1330,\n",
      "         0.1250,  0.1555,  0.1770,  0.1458,  0.1391,  0.0949,  0.1648,  0.1089,\n",
      "         0.0955, -0.1454,  0.1287,  0.1297,  0.1465,  0.1328,  0.1507,  0.1737,\n",
      "         0.1578,  0.1233,  0.1359,  0.1343,  0.1480,  0.1598,  0.1597,  0.1486,\n",
      "         0.1788,  0.1134,  0.1126,  0.1297,  0.1107,  0.1690,  0.1455,  0.1557,\n",
      "         0.1732,  0.1185,  0.1431,  0.1683,  0.1307,  0.1240,  0.1812,  0.1168,\n",
      "         0.1423,  0.1381,  0.1333,  0.1414,  0.1396,  0.1243,  0.1525,  0.1264,\n",
      "         0.1668,  0.1247,  0.1044,  0.1241,  0.1606,  0.1513,  0.1379,  0.1155,\n",
      "         0.1147,  0.1351,  0.1555,  0.1548,  0.1197,  0.1350,  0.2012,  0.1347,\n",
      "         0.1055,  0.2896,  0.1729,  0.1283,  0.1713, -0.0941,  0.1134,  0.1947,\n",
      "         0.1194,  0.1336,  0.1189,  0.1571,  0.0678,  0.1254,  0.1276,  0.1310,\n",
      "         0.1650,  0.2090,  0.1131,  0.1728,  0.1586,  0.1409,  0.1503,  0.1469,\n",
      "         0.1327,  0.1081,  0.1301,  0.1918,  0.1770,  0.1214,  0.0895,  0.2168,\n",
      "         0.1473,  0.1537,  0.1686,  0.1214,  0.1126,  0.2154,  0.1902,  0.1777,\n",
      "         0.1676,  0.1536,  0.1642,  0.1586,  0.3615,  0.1658,  0.1996,  0.1298,\n",
      "         0.1486,  0.1074,  0.1431,  0.1372,  0.1947,  0.1403,  0.1921,  0.1220,\n",
      "         0.1386,  0.1168,  0.1792,  0.1782,  0.0917,  0.0986,  0.1515,  0.1242,\n",
      "         0.1206,  0.1299,  0.1534,  0.1138,  0.1157,  0.1074,  0.1710,  0.1328,\n",
      "        -0.0272,  0.1780,  0.2171,  0.1381,  0.1827,  0.1289,  0.1499,  0.1750,\n",
      "         0.1530,  0.1707,  0.1028,  0.1791,  0.1624,  0.1117,  0.1443,  0.1405,\n",
      "         0.1496, -0.0603,  0.1247,  0.1550,  0.1115,  0.1649,  0.1337,  0.0781,\n",
      "         0.1296,  0.1937,  0.1799,  0.1406,  0.1352,  0.1266,  0.1449,  0.1325,\n",
      "         0.1535,  0.1362,  0.1227,  0.1182,  0.2076,  0.1359,  0.1951,  0.1309,\n",
      "         0.1576,  0.1959,  0.1423,  0.1441,  0.1854,  0.2045,  0.1500,  0.1500,\n",
      "         0.1439,  0.1423,  0.1052,  0.1397,  0.1675,  0.1542,  0.1472, -0.0299],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1274, -0.1538, -0.0459,  ...,  0.0622,  0.0795,  0.0467],\n",
      "        [-0.0401, -0.3418,  0.0617,  ..., -0.2914, -0.2234,  0.0443],\n",
      "        [-0.2242, -0.1235,  0.0887,  ...,  0.2207, -0.1684,  0.0694],\n",
      "        ...,\n",
      "        [-0.0613,  0.2960,  0.1318,  ..., -0.5436, -0.0880, -0.0357],\n",
      "        [-0.0722, -0.1836,  0.0629,  ...,  0.1863, -0.0072,  0.0590],\n",
      "        [-0.0534,  0.2550, -0.2677,  ...,  0.1805, -0.0323,  0.0134]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.2322,  0.1803, -1.2028,  ...,  1.2325,  0.0455, -0.2118],\n",
      "        [-0.0528, -0.3771, -0.1523,  ...,  1.1650,  2.3242,  0.7617],\n",
      "        [-0.2377, -2.1630, -2.6977,  ...,  0.0436,  0.1526,  0.3480],\n",
      "        ...,\n",
      "        [-0.2120,  0.1307,  0.6281,  ...,  0.2635, -0.1713,  0.1741],\n",
      "        [ 0.0626,  2.5704, -2.2567,  ..., -0.2778,  1.6184, -0.1515],\n",
      "        [-0.2590, -1.9699, -0.1217,  ...,  1.0506, -1.5264, -0.4175]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.6345,  2.8193,  3.3773,  ...,  1.7076, -1.7113, -2.1177],\n",
      "        [-0.4207,  0.5357, -0.8217,  ..., -0.6233,  2.1911,  0.5932],\n",
      "        [-1.4063, -0.5045,  2.4021,  ...,  1.2620,  0.3714, -0.7891],\n",
      "        ...,\n",
      "        [-1.0588, -0.1221, -0.3657,  ..., -0.1058,  2.1298, -2.1074],\n",
      "        [-0.3031,  0.5080,  0.3522,  ...,  0.6569, -0.7590, -1.1836],\n",
      "        [ 0.4820,  3.8298,  0.2668,  ..., -3.5743,  0.2615,  0.8965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.6.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0261,  0.2577,  0.2862,  0.3029,  0.2543,  0.2264,  0.2788,  0.2522,\n",
      "         0.2253,  0.2753,  0.2726,  0.2076,  0.3119,  0.3673,  0.2732,  0.3215,\n",
      "         0.2598,  0.2306,  0.3050,  0.3105,  0.2469,  0.3407,  0.2543,  0.2960,\n",
      "         0.2633,  0.2221,  0.2571,  0.2445,  0.2490,  0.2962,  0.2408,  0.2389,\n",
      "         0.2728,  0.2559,  0.3301,  0.4543,  0.3270,  0.2664,  0.2740,  0.3212,\n",
      "         0.2725,  0.2988,  0.2767,  0.2543,  0.3089,  0.3031,  0.2540,  0.2652,\n",
      "         0.3241,  0.2594,  0.2345,  0.3096,  0.2782,  0.2339,  0.2745,  0.2617,\n",
      "         0.2402,  0.2751,  0.3178,  0.2251,  0.2688,  0.2755,  0.2869,  0.2501,\n",
      "         0.2873,  0.2599,  0.2917,  0.4512,  0.2815,  0.2430,  0.3696,  0.2656,\n",
      "        -0.0813,  0.2486,  0.2569,  0.3447,  0.2915,  0.3090,  0.3208,  0.2935,\n",
      "         0.3641,  0.2810,  0.2939,  0.2507,  0.2702,  0.2956,  0.3277,  0.2672,\n",
      "         0.2621,  0.2877,  0.2771,  0.2686,  0.2454,  0.2623,  0.2489,  0.2966,\n",
      "         0.2858,  0.2182,  0.2481,  0.2432,  0.2534,  0.2611,  0.3172,  0.3002,\n",
      "         0.2590,  0.2929,  0.2698,  0.2495,  0.2677,  0.2523,  0.2788,  0.2399,\n",
      "         0.2540,  0.2733,  0.2600,  0.2537,  0.2910,  0.2402,  0.2184,  0.2610,\n",
      "         0.2769,  0.2810,  0.3018,  0.2614,  0.2888,  0.2900,  0.3482,  0.2683,\n",
      "         0.2878,  0.2848,  0.2590,  0.2879,  0.2158,  0.2631,  0.2692,  0.2778,\n",
      "         0.2895,  0.3001,  0.3144,  0.2459,  0.2834,  0.2566,  0.2659,  0.3472,\n",
      "         0.3106,  0.3397,  0.2532,  0.3233,  0.2202,  0.2525,  0.2568,  0.2526,\n",
      "         0.2477,  0.2647,  0.2997,  0.2944,  0.3226,  0.2657,  0.2676,  0.2868,\n",
      "         0.3179,  0.3017,  0.2685,  0.2453,  0.2700,  0.3089,  0.2581,  0.2648,\n",
      "         0.2363,  0.2545,  0.2607,  0.2592,  0.3115,  0.2428,  0.3419,  0.2799,\n",
      "         0.3179,  0.3852,  0.2328,  0.3148,  0.2549,  0.2483,  0.2278,  0.3401,\n",
      "         0.2514,  0.3393,  0.2699,  0.2359,  0.2413,  0.3495,  0.2928,  0.2883,\n",
      "         0.2916,  0.6726,  0.2521,  0.2390,  0.2741,  0.3184,  0.2452,  0.3223,\n",
      "         0.2756,  0.2415,  0.2852,  0.2303,  0.2731,  0.3076,  0.3017,  0.3027,\n",
      "         0.2851,  0.2672,  0.2692,  0.3053,  0.2528,  0.2512,  0.2711,  0.2810,\n",
      "         0.2777,  0.2623,  0.2809,  0.3456,  0.3017,  0.2958,  0.2301,  0.2533,\n",
      "         0.2499,  0.2664,  0.2075,  0.2324,  0.2661,  0.2706,  0.2691,  0.2665,\n",
      "         0.2881,  0.2624,  0.2756,  0.3014,  0.2655,  0.2616,  0.2813,  0.2943,\n",
      "         0.2454,  0.2436, -0.1471,  0.3391,  0.2781,  0.2349,  0.2705,  0.5823,\n",
      "         0.2633,  0.2605,  0.2244,  0.2462,  0.2331,  0.2920,  0.2587,  0.3855,\n",
      "         0.2785,  0.2246,  0.3000,  0.2931,  0.2631,  0.2411,  0.2419,  0.2734,\n",
      "         0.2577,  0.3224,  0.2844,  0.2514,  0.3626,  0.2867,  0.2461,  0.2622,\n",
      "         0.2186,  0.2688,  0.2531,  0.0626,  0.3019,  0.2676,  0.2449,  0.2865,\n",
      "         0.2077,  0.2567,  0.2731,  0.3167,  0.2997,  0.2360,  0.2439,  0.2645,\n",
      "         0.2347,  0.2564,  0.2556,  0.2815,  0.2100,  0.2901,  0.2726,  0.2712,\n",
      "         0.2843,  0.3473,  0.2565,  0.2838,  0.3014,  0.2479,  0.2405,  0.2828,\n",
      "         0.3622,  0.2587,  0.2259,  0.2537,  0.2780,  0.3162,  0.3019,  0.2683,\n",
      "         0.2749,  0.2553,  0.3069,  0.3199,  0.3286,  0.2632,  0.2815,  0.2672,\n",
      "         0.2504,  0.2916,  0.3054,  0.2938,  0.2814,  0.2200,  0.2681,  0.2420,\n",
      "         0.2260,  0.3200,  0.2657,  0.2551,  0.2783,  0.2379,  0.2977,  0.2953,\n",
      "         0.2840,  0.2641,  0.2863,  0.2858,  0.2573,  0.2873,  0.3053,  0.2642,\n",
      "         0.3050,  0.2393,  0.2559,  0.2603,  0.2393,  0.2483,  0.2729,  0.2765,\n",
      "         0.3202,  0.2594,  0.3038,  0.2795,  0.2422,  0.2581,  0.3059,  0.2599,\n",
      "         0.2222,  0.2385,  0.2466,  0.3036,  0.2575,  0.2737,  0.2679,  0.2442,\n",
      "         0.2708,  0.2616,  0.2081,  0.2663,  0.3267,  0.2827,  0.2623,  0.2263,\n",
      "         0.2595,  0.2629,  0.2837,  0.2648,  0.2063,  0.2754,  0.3278,  0.2613,\n",
      "         0.2421,  0.3004,  0.2807,  0.2535,  0.3010, -0.2413,  0.2775,  0.4032,\n",
      "         0.2662,  0.2485,  0.2677,  0.2649,  0.0877,  0.2942,  0.2502,  0.2363,\n",
      "         0.3255,  0.3023,  0.2625,  0.2836,  0.2571,  0.2795,  0.2305,  0.2550,\n",
      "         0.2631,  0.2423,  0.2563,  0.3429,  0.3528,  0.2353,  0.2373,  0.2823,\n",
      "         0.2604,  0.2539,  0.2725,  0.2360,  0.2931,  0.3895,  0.3215,  0.3431,\n",
      "         0.3265,  0.2792,  0.3039,  0.2885,  0.4833,  0.3279,  0.3255,  0.2266,\n",
      "         0.2732,  0.2515,  0.2752,  0.2544,  0.3447,  0.2983,  0.2922,  0.2267,\n",
      "         0.2957,  0.2511,  0.3470,  0.3225,  0.2536,  0.2390,  0.3097,  0.2726,\n",
      "         0.2504,  0.2722,  0.3044,  0.2526,  0.2626,  0.2353,  0.2537,  0.2548,\n",
      "         0.0781,  0.2866,  0.3973,  0.2840,  0.2884,  0.2867,  0.2718,  0.2864,\n",
      "         0.2682,  0.3266,  0.3307,  0.2640,  0.2355,  0.2497,  0.2870,  0.2911,\n",
      "         0.2943,  0.0995,  0.2905,  0.2741,  0.2228,  0.3045,  0.2607,  0.1680,\n",
      "         0.2450,  0.2608,  0.2793,  0.2989,  0.3065,  0.2297,  0.2758,  0.2477,\n",
      "         0.2757,  0.2142,  0.2677,  0.2432,  0.3204,  0.2870,  0.3272,  0.2766,\n",
      "         0.2831,  0.3863,  0.2841,  0.2629,  0.2808,  0.3561,  0.3101,  0.2653,\n",
      "         0.2840,  0.2602,  0.2447,  0.2634,  0.3008,  0.2666,  0.2521,  0.0483],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0003, -0.0233, -0.0213,  ...,  0.0186,  0.0333,  0.0187],\n",
      "        [ 0.0033,  0.0280, -0.0684,  ...,  0.0425,  0.1061, -0.0204],\n",
      "        [-0.0280,  0.0216,  0.0283,  ...,  0.0062,  0.0336,  0.0170],\n",
      "        ...,\n",
      "        [-0.0099, -0.0494, -0.0336,  ..., -0.0019,  0.0136, -0.0245],\n",
      "        [-0.0137, -0.0289, -0.0074,  ...,  0.0233, -0.0429, -0.0079],\n",
      "        [ 0.0182,  0.0145,  0.0067,  ..., -0.0201, -0.0103, -0.0335]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0674, -0.4957,  0.6120,  ..., -0.3020, -0.2030, -0.0399],\n",
      "        [ 0.0810, -0.1054, -0.4142,  ...,  0.7785,  0.7184,  0.0798],\n",
      "        [ 0.0470,  0.2864,  0.5545,  ..., -0.5170,  0.5925, -0.2027],\n",
      "        ...,\n",
      "        [-0.2829, -0.6648,  0.2398,  ..., -0.2159,  0.1985, -0.0486],\n",
      "        [-0.0555,  0.3136, -0.2200,  ...,  0.5394, -0.4254, -0.1106],\n",
      "        [-0.0224, -0.0437,  0.2362,  ..., -0.4399,  0.1368,  0.1474]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0705, -0.3366,  0.6073,  ..., -0.2276,  1.1151, -0.1877],\n",
      "        [-0.0254, -1.1013, -0.7850,  ..., -0.2152,  0.4681,  0.0514],\n",
      "        [ 0.1340,  0.7770,  0.2927,  ...,  2.0004,  0.3122, -0.3459],\n",
      "        ...,\n",
      "        [-0.0538,  0.5230, -0.6940,  ..., -0.3545, -0.8182,  0.0437],\n",
      "        [ 1.1130,  0.3765, -0.4924,  ...,  0.8542,  1.2835,  0.0844],\n",
      "        [-0.5927, -0.5760, -0.6710,  ..., -0.5877, -0.5085,  0.1830]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.9181, -1.2419, -0.2916,  ..., -4.6358,  6.3111, -1.9544],\n",
      "        [ 0.7815, -0.9282,  0.5148,  ...,  1.2200, -0.1917, -0.8751],\n",
      "        [ 0.3038,  0.6428,  1.0262,  ..., -0.9120, -1.2400,  0.6325],\n",
      "        ...,\n",
      "        [ 0.2936, -0.6750,  0.0621,  ..., -1.7974,  2.1781, -0.0894],\n",
      "        [-0.9854, -0.0115,  0.6530,  ..., -0.4319,  0.2091, -0.6288],\n",
      "        [ 6.4718, -1.1837, -4.3371,  ..., -2.8781, -0.7048, -5.4496]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0508,  0.2549,  0.2608,  0.2477,  0.2414,  0.2041,  0.2557,  0.2337,\n",
      "         0.2184,  0.2305,  0.2061,  0.2154,  0.2459,  0.4198,  0.2511,  0.2784,\n",
      "         0.2665,  0.2248,  0.3052,  0.2999,  0.2222,  0.3373,  0.2939,  0.1860,\n",
      "         0.3011,  0.1886,  0.1932,  0.2099,  0.2776,  0.2623,  0.2132,  0.2179,\n",
      "         0.2426,  0.2343,  0.3637,  0.4682,  0.3030,  0.2294,  0.2411,  0.3416,\n",
      "         0.2719,  0.2820,  0.2117,  0.2891,  0.2950,  0.2975,  0.2544,  0.1926,\n",
      "         0.2649,  0.2804,  0.3373,  0.2142,  0.2525,  0.2208,  0.2867,  0.2452,\n",
      "         0.2065,  0.2518,  0.3370,  0.2040,  0.2545,  0.2576,  0.3266,  0.2538,\n",
      "         0.2696,  0.2105,  0.2745,  0.7592,  0.2955,  0.2606,  1.3712,  0.1830,\n",
      "        -0.0523,  0.3007,  0.2738,  0.3231,  0.2650,  0.3748,  0.3958,  0.2887,\n",
      "         0.4312,  0.2855,  0.3909,  0.1905,  0.2397,  0.2495,  0.4091,  0.2930,\n",
      "         0.2055,  0.3175,  0.2586,  0.2131,  0.2688,  0.2140,  0.1913,  0.2990,\n",
      "         0.2393, -0.1685,  0.1981,  0.2192,  0.2232,  0.2467,  0.4214,  0.3089,\n",
      "         0.2548,  0.2758,  0.2589,  0.2111,  0.2682,  0.2779,  0.2165,  0.2376,\n",
      "         0.2074,  0.2493,  0.2247,  0.2323,  0.2608,  0.2475,  0.1662,  0.2087,\n",
      "         0.2599,  0.2512, -0.0074,  0.2350,  0.3062,  0.2300,  0.2709,  0.2340,\n",
      "         0.2837,  0.2886,  0.2110,  0.2858,  0.1649,  0.2612,  0.2088,  0.2333,\n",
      "         0.3140,  0.2853,  0.3343,  0.2554,  0.3233,  0.1948,  0.2120,  0.4198,\n",
      "         0.3234,  0.3710,  0.2317,  0.2946,  0.1760,  0.2558,  0.2565,  0.2099,\n",
      "         0.2422,  0.2286,  0.3008,  0.2246,  0.3663,  0.4005,  0.2544,  0.2777,\n",
      "         0.3054,  0.3370,  0.3219,  0.1835,  0.2363,  0.2546,  0.2619,  0.2248,\n",
      "         0.1896,  0.2176,  0.2886,  0.2163,  0.3242,  0.2549,  0.3624,  0.2898,\n",
      "         0.3942,  0.5336,  0.2309,  0.2897,  0.2312,  0.2494,  0.2289,  0.4111,\n",
      "         0.2309,  0.3442,  0.2207,  0.2029,  0.1958,  0.3492,  0.3349,  0.3428,\n",
      "         0.2351,  0.8190,  0.2887,  0.2090,  0.3137,  0.5398,  0.2511,  0.2954,\n",
      "         0.2696,  0.2358,  0.2506,  0.2152,  0.2037,  0.3230,  0.2535,  0.2992,\n",
      "         0.2588,  0.3561,  0.2664,  0.2635,  0.2633,  0.2540,  0.3534,  0.3307,\n",
      "         0.2334,  0.2890,  0.4068,  0.2967,  0.3416,  0.2843,  0.2185, -0.2733,\n",
      "         0.2468,  0.2922,  0.2023,  0.2545,  0.2195,  0.2296,  0.2507,  0.3037,\n",
      "         0.2627,  0.0111,  0.2645,  0.3641,  0.2264,  0.2066,  0.2568,  0.3865,\n",
      "         0.2118,  0.2306,  0.1305,  0.3688,  0.3309,  0.2302,  0.2633, -0.0060,\n",
      "         0.2345,  0.2084,  0.2437,  0.2342,  0.2035,  0.3076,  0.3053,  0.3710,\n",
      "         0.2657,  0.2201,  0.2990,  0.3149,  0.2714,  0.2278,  0.1941,  0.2190,\n",
      "         0.2526,  0.2954,  0.2457,  0.2059,  0.4245,  0.3934,  0.2301,  0.2662,\n",
      "         0.2012,  0.2380,  0.2822,  0.0522,  0.3647,  0.3376,  0.2115,  0.2650,\n",
      "         0.1888,  0.2434,  0.2955,  0.2791,  0.2555,  0.2037,  0.2592,  0.2152,\n",
      "         0.1939,  0.2839,  0.2101,  0.3087,  0.1944,  0.3730,  0.2965,  0.2329,\n",
      "         0.3115,  0.3252,  0.2553,  0.2772,  0.3777,  0.2139,  0.2091,  0.2450,\n",
      "         0.2683,  0.2078,  0.2388,  0.3201,  0.3148,  0.2560,  0.2536,  0.3012,\n",
      "         0.2419,  0.2797,  0.2537,  0.3655,  0.3019,  0.1807,  0.2347,  0.2889,\n",
      "         0.2019,  0.2851,  0.3020,  0.2406,  0.3204,  0.1947,  0.3988,  0.3010,\n",
      "         0.1925, -0.2454,  0.2324,  0.2025,  0.2191,  0.2218,  0.2645,  0.2883,\n",
      "         0.3882,  0.2177,  0.2234,  0.2836,  0.2008,  0.2803,  0.2539,  0.2355,\n",
      "         0.2796,  0.1911,  0.1820, -0.2071,  0.2096,  0.2735,  0.3216,  0.2816,\n",
      "         1.1901,  0.2812,  0.2976,  0.3235,  0.2018,  0.2348,  0.3006,  0.1925,\n",
      "         0.1932,  0.2286,  0.1992,  0.2798,  0.2645,  0.1976,  0.2521,  0.2579,\n",
      "         0.2981,  0.2786,  0.1881,  0.2169,  0.3734,  0.2563,  0.2364,  0.2152,\n",
      "         0.2441,  0.2714,  0.2341,  0.2445,  0.1867,  0.2918,  0.3196,  0.1831,\n",
      "         0.2084,  0.3369,  0.2308,  0.2618,  0.2827,  0.2905,  0.2626,  0.4169,\n",
      "         0.2541,  0.2522,  0.1880,  0.2844,  0.0746,  0.3044,  0.2369,  0.2330,\n",
      "         0.2832,  0.3140,  0.2122,  0.2913,  0.2661,  0.2485,  0.2210,  0.2498,\n",
      "         0.2490,  0.2472,  0.2363,  0.5732,  0.3581,  0.1818,  0.1953,  0.3650,\n",
      "         0.3336,  0.2788,  0.2232,  0.2107,  0.2971,  0.3722,  0.3587,  0.3427,\n",
      "         0.3199,  0.3407,  0.2782,  0.3355,  0.8597,  0.2791,  0.3311,  0.1549,\n",
      "         0.2961,  0.2164,  0.2829,  0.2704,  0.4098,  0.3384,  0.2997,  0.2048,\n",
      "         0.2785,  0.2686,  0.3116,  0.3691,  0.1878,  0.2154,  0.3121,  0.2139,\n",
      "         0.2148,  0.2655,  0.4526,  0.2917,  0.2021,  0.2582,  0.2557,  0.2010,\n",
      "         0.0660,  0.3224,  0.3794,  0.3219,  0.3890,  0.3240,  0.2424,  0.2978,\n",
      "         0.2539,  0.3882,  0.3355,  0.2615,  0.2504,  0.1943,  0.3057,  0.2532,\n",
      "         0.2816,  0.0534,  0.2531,  0.2762,  0.2033,  0.3835,  0.2349,  0.1440,\n",
      "         0.2884,  0.3346,  0.2288,  0.2864,  0.2663,  0.2265,  0.2677,  0.2536,\n",
      "         0.2340,  0.4472,  0.2557,  0.2908,  0.3575,  0.2764,  0.2686,  0.2399,\n",
      "         0.2709,  0.3772,  0.3089,  0.3156,  0.2956,  0.3299,  0.2490,  0.3376,\n",
      "         0.3113,  0.2886,  0.2336,  0.2906,  0.2293,  0.2109,  0.2643,  0.0557],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027,  0.0796, -0.0362,  ..., -0.0194,  0.1424, -0.0204],\n",
      "        [-0.0196, -0.0068, -0.0675,  ...,  0.0259,  0.0108,  0.0239],\n",
      "        [ 0.0905, -0.0438,  0.0631,  ...,  0.1060,  0.1272, -0.0415],\n",
      "        ...,\n",
      "        [-0.0096,  0.1248,  0.0498,  ..., -0.0689,  0.0286,  0.0139],\n",
      "        [ 0.0401,  0.0604,  0.0260,  ...,  0.1019, -0.0697, -0.0358],\n",
      "        [ 0.0442,  0.0534,  0.0478,  ...,  0.0357, -0.0369,  0.0019]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.8793,  1.2517, -0.6215,  ...,  0.9163, -0.9853, -0.3697],\n",
      "        [ 0.9223, -0.2156,  0.5059,  ..., -0.1307, -0.7456,  0.8759],\n",
      "        [-0.0422,  0.0059, -0.0801,  ...,  0.0116, -0.1104,  0.1308],\n",
      "        ...,\n",
      "        [-0.4249, -0.2961, -0.1861,  ...,  1.2410, -0.3344,  0.0281],\n",
      "        [ 0.9401,  0.9616,  0.3307,  ..., -0.3274,  0.1608, -0.1878],\n",
      "        [-0.3374,  0.7594, -0.2358,  ...,  0.0082,  0.2379, -0.7007]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.2977, -0.7967,  2.9589,  ...,  0.2676, -1.6951,  0.1967],\n",
      "        [ 1.7570, -1.0796,  1.2748,  ...,  0.5896, -0.2467, -0.4081],\n",
      "        [ 0.2501, -0.6636,  1.9899,  ..., -0.8311,  4.0676, -0.1675],\n",
      "        ...,\n",
      "        [-1.3048,  1.0361, -0.9191,  ...,  1.0258,  3.1353, -0.8118],\n",
      "        [-0.0530, -1.0714, -1.3765,  ..., -0.8615, -0.6752, -0.0975],\n",
      "        [ 1.4191,  0.5613,  2.1180,  ...,  1.6124,  0.6801,  0.0453]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3803,  5.4647, -3.9925,  ..., -0.0511,  1.8467,  1.2594],\n",
      "        [ 4.6416,  1.0051, -2.2293,  ...,  1.3834,  2.3092, -0.9992],\n",
      "        [-1.1529, -2.6478,  0.8137,  ...,  1.7614, -0.8922,  1.6339],\n",
      "        ...,\n",
      "        [ 0.4250,  0.7376,  0.3812,  ...,  1.6989,  2.3225,  1.3706],\n",
      "        [ 2.5427,  0.8943,  0.7147,  ...,  1.6638,  1.7781,  0.1217],\n",
      "        [-5.8057, -3.9156,  2.6240,  ..., -6.8709, -0.4872, -4.3320]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([-0.0322,  0.0837,  0.0930,  0.0980,  0.0878,  0.0964,  0.1143,  0.0895,\n",
      "         0.0853,  0.0887,  0.0795,  0.0607,  0.1040,  0.1315,  0.1215,  0.1260,\n",
      "         0.1012,  0.1155,  0.0957,  0.1234,  0.0860,  0.1263,  0.0846,  0.1121,\n",
      "         0.0901,  0.0959,  0.0951,  0.0823,  0.0852,  0.1366,  0.1010,  0.0717,\n",
      "         0.0935, -0.0656,  0.2121,  0.1907,  0.1298,  0.0905,  0.0921,  0.1855,\n",
      "         0.0873,  0.1100,  0.0985,  0.1611,  0.1754,  0.0821,  0.0844,  0.0826,\n",
      "         0.1260,  0.1194,  0.0830,  0.1340,  0.1646,  0.0933,  0.1021,  0.0870,\n",
      "         0.1075,  0.1252,  0.1458,  0.0950,  0.1042,  0.1013,  0.0953,  0.1138,\n",
      "         0.0882,  0.0991,  0.1020,  0.2903,  0.1243,  0.0718,  0.0979,  0.1121,\n",
      "         0.0182,  0.1042,  0.1079,  0.1617,  0.1716,  0.1176,  0.1110,  0.1312,\n",
      "         0.2128,  0.1422,  0.1474,  0.0802,  0.1075,  0.0830,  0.1340,  0.0824,\n",
      "         0.0976,  0.1638,  0.1291,  0.0967,  0.1384,  0.0969,  0.0919,  0.0983,\n",
      "         0.1025, -0.0861,  0.0953,  0.0704,  0.1029,  0.0904,  0.1272,  0.1175,\n",
      "         0.1138,  0.1175,  0.1079,  0.0901,  0.1123,  0.0884,  0.0849,  0.0911,\n",
      "         0.0942,  0.1056,  0.0887,  0.0918,  0.1077,  0.0955,  0.0760,  0.0972,\n",
      "         0.0843,  0.1107,  0.1701,  0.1035,  0.0853,  0.0903,  0.1311,  0.0986,\n",
      "         0.1229,  0.0938,  0.0819,  0.1176,  0.0717,  0.0916,  0.1221,  0.0956,\n",
      "         0.1244,  0.1164,  0.1413,  0.0947,  0.1232,  0.0985,  0.1055,  0.1101,\n",
      "         0.1446,  0.0954,  0.0913,  0.1344,  0.0778,  0.0892,  0.1017,  0.1613,\n",
      "         0.0870,  0.0752,  0.2100,  0.1020,  0.1294,  0.0854,  0.0913,  0.0965,\n",
      "         0.1332,  0.0879,  0.1076,  0.0813,  0.0787,  0.1156,  0.0853,  0.1074,\n",
      "         0.0753,  0.1113,  0.1126,  0.0871,  0.1173,  0.0756,  0.1016,  0.0851,\n",
      "         0.1015,  0.1077,  0.1017,  0.1188,  0.0958,  0.0852,  0.1076,  0.1815,\n",
      "         0.0876,  0.1302,  0.0992,  0.0912,  0.0997,  0.1365,  0.1250,  0.0900,\n",
      "         0.1043,  0.1526,  0.1106,  0.1012,  0.1181,  0.1232,  0.0954,  0.1163,\n",
      "         0.1126,  0.1126,  0.0615,  0.0989,  0.0913,  0.1043,  0.1547,  0.0978,\n",
      "         0.0783,  0.1774,  0.0985,  0.1209,  0.1042,  0.0812,  0.1046,  0.0821,\n",
      "         0.1223,  0.1242,  0.0931,  0.1535,  0.1711,  0.1303,  0.0808,  0.0739,\n",
      "         0.1108,  0.0926,  0.0718,  0.0767,  0.1115,  0.1022,  0.0861, -0.0566,\n",
      "         0.1100,  0.0987,  0.1009,  0.1410,  0.1085,  0.0745,  0.0890,  0.1039,\n",
      "         0.0793,  0.0813,  0.0756,  0.1016,  0.1278,  0.0975, -0.0739,  0.2341,\n",
      "         0.0991,  0.0989,  0.0887,  0.1123,  0.0743,  0.1165,  0.1084,  0.1184,\n",
      "         0.0951,  0.1089,  0.1245,  0.1190,  0.1143,  0.0949,  0.0807,  0.0826,\n",
      "         0.1004,  0.1045,  0.1052,  0.1149,  0.2321,  0.1273,  0.1320,  0.0983,\n",
      "         0.0870,  0.1086,  0.0779,  0.0106,  0.3380,  0.1053,  0.1092,  0.1377,\n",
      "         0.0546,  0.0990,  0.2014,  0.1230,  0.0990,  0.0813,  0.0919,  0.0884,\n",
      "         0.1191,  0.1059,  0.0768,  0.0998,  0.0814,  0.2005,  0.0953,  0.1029,\n",
      "         0.0979,  0.1229,  0.1148,  0.0977,  0.1187,  0.1031,  0.0657,  0.1085,\n",
      "         0.1042, -0.0746,  0.1007,  0.0973,  0.1148,  0.1013,  0.1735,  0.0828,\n",
      "         0.0827,  0.0708,  0.1149,  0.0986,  0.1362,  0.0894,  0.1240,  0.1045,\n",
      "         0.1097,  0.1434,  0.1358,  0.1035,  0.0921,  0.0773,  0.1237,  0.0974,\n",
      "         0.0761,  0.1317,  0.0980,  0.0860,  0.0921,  0.1013,  0.0976,  0.1202,\n",
      "         0.1546,  0.0963,  0.0870,  0.1048,  0.1111,  0.1083,  0.1163,  0.1029,\n",
      "         0.1123,  0.1130,  0.0849,  0.0748,  0.0849,  0.1298,  0.1315,  0.1224,\n",
      "         0.0922,  0.0932,  0.1274,  0.1045,  0.0954,  0.0890,  0.1430,  0.1007,\n",
      "         0.1188,  0.0809,  0.0928,  0.1119,  0.1142,  0.1056,  0.1046,  0.0772,\n",
      "         0.1272,  0.0899,  0.0934,  0.0998,  0.1045,  0.1021,  0.1142,  0.0669,\n",
      "         0.0943,  0.1016,  0.0968,  0.1093,  0.1066,  0.1356,  0.1455,  0.0943,\n",
      "         0.0832,  0.3318,  0.1170,  0.0950,  0.1000, -0.0766,  0.0902,  0.1238,\n",
      "         0.0875,  0.0902,  0.0894,  0.1031, -0.0180,  0.0922,  0.0881,  0.0888,\n",
      "         0.1061,  0.2544,  0.0792,  0.1303,  0.0894,  0.1005,  0.0912,  0.1014,\n",
      "         0.0955,  0.0754,  0.0969,  0.1899,  0.1259,  0.1149,  0.0733,  0.2545,\n",
      "         0.1061,  0.1168,  0.1165,  0.0953,  0.0750,  0.1946,  0.1088,  0.1332,\n",
      "         0.1272,  0.1055,  0.1115,  0.1042,  0.1609,  0.1164,  0.1395,  0.0899,\n",
      "         0.0986,  0.0743,  0.0890,  0.0947,  0.1318,  0.0996,  0.1423,  0.0705,\n",
      "         0.1059,  0.0875,  0.1197,  0.1424,  0.0854,  0.0641,  0.1034,  0.0903,\n",
      "         0.0718,  0.1147,  0.0936,  0.1164,  0.0948,  0.0821,  0.1026,  0.0910,\n",
      "         0.0159,  0.1249,  0.1324,  0.1112,  0.1210,  0.1066,  0.0893,  0.1166,\n",
      "         0.1483,  0.1178,  0.0740,  0.1140,  0.1062,  0.0758,  0.1114,  0.0917,\n",
      "         0.0926,  0.0371,  0.0937,  0.1095,  0.0645,  0.0898,  0.1296, -0.0505,\n",
      "         0.0906,  0.1196,  0.1251,  0.0950,  0.1056,  0.0834,  0.0932,  0.0844,\n",
      "         0.1004,  0.1020,  0.0829,  0.0820,  0.1956,  0.1009,  0.1025,  0.0926,\n",
      "         0.1085,  0.1919,  0.1029,  0.0876,  0.1450,  0.1290,  0.1145,  0.1315,\n",
      "         0.1146,  0.1032,  0.1111,  0.0969,  0.1200,  0.1101,  0.1036,  0.0088],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2768, -0.1221, -0.1549,  ...,  0.2882, -0.2140,  0.0976],\n",
      "        [-0.2030,  0.2999,  0.1728,  ..., -0.2155,  0.0232, -0.0643],\n",
      "        [-0.0768, -0.0037, -0.1792,  ..., -0.2307, -0.1937, -0.0036],\n",
      "        ...,\n",
      "        [ 0.0030,  0.2153,  0.2135,  ...,  0.0668,  0.2352, -0.0051],\n",
      "        [-0.0952, -0.1288,  0.1874,  ...,  0.1391, -0.2868,  0.0288],\n",
      "        [-0.0772,  0.1356,  0.0693,  ..., -0.0293, -0.0626,  0.0265]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.7220, -5.3325,  0.0697,  ..., -3.4960, -1.3261,  0.3790],\n",
      "        [ 0.1499, -2.7216, -0.6729,  ...,  3.7211, -0.8029,  0.9634],\n",
      "        [-0.1196,  1.8406,  0.3775,  ..., -1.3349,  5.7774, -0.1442],\n",
      "        ...,\n",
      "        [ 0.0601,  2.9227,  0.5725,  ...,  3.2954,  1.1892, -0.5361],\n",
      "        [-0.2745, -1.0730,  0.7769,  ..., -0.7220,  0.3643,  0.2007],\n",
      "        [-0.4000,  1.8644, -3.6015,  ...,  2.5265,  1.3370,  0.1752]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.2541, -1.1711, -1.8791,  ...,  3.0386, -0.9018, -0.7967],\n",
      "        [-1.3407,  0.7066,  2.9633,  ..., -0.7052,  1.0746, -1.6748],\n",
      "        [ 0.6466, -1.1741, -0.5569,  ..., -0.7537, -0.0247,  0.7925],\n",
      "        ...,\n",
      "        [-0.8835, -0.7759, -0.5194,  ...,  2.2994,  0.1032, -0.4352],\n",
      "        [-1.8132,  1.0443,  0.6460,  ...,  1.0884,  1.5017,  1.0776],\n",
      "        [ 2.3696, -4.5068, -9.2105,  ..., -3.2438, -1.2726, -6.2836]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.block.7.layer.2.layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0700,  0.2059,  0.2523,  0.2422,  0.2293,  0.2014,  0.2155,  0.1966,\n",
      "         0.1592,  0.2007,  0.2148,  0.1933,  0.2745,  0.3740,  0.2418,  0.3268,\n",
      "         0.2425,  0.2458,  0.2536,  0.2697,  0.2173,  0.2876,  0.2280,  0.2128,\n",
      "         0.2667,  0.1629,  0.1923,  0.2385,  0.2233,  0.2491,  0.2093,  0.1926,\n",
      "         0.2037,  0.2308,  0.2546,  0.5759,  0.2449,  0.1811,  0.2205,  0.2728,\n",
      "         0.2102,  0.2824,  0.2482,  0.2355,  0.2492,  0.3207,  0.2171,  0.1988,\n",
      "         0.2885,  0.2395,  0.2381,  0.2720,  0.2236,  0.1895,  0.2321,  0.2087,\n",
      "         0.1970,  0.2249,  0.2972,  0.1933,  0.2326,  0.2356,  0.2643,  0.1923,\n",
      "         0.2058,  0.2193,  0.2548,  0.5446,  0.2332,  0.2067,  0.6452,  0.2069,\n",
      "         0.0608,  0.2149,  0.1975,  0.3298,  0.2651,  0.2742,  0.2823,  0.2476,\n",
      "         0.3222,  0.2185,  0.2235,  0.1878,  0.2128,  0.2456,  0.2613,  0.2512,\n",
      "         0.2263,  0.2023,  0.2301,  0.1993,  0.2087,  0.1821,  0.1872,  0.2444,\n",
      "         0.2213,  0.2091,  0.1919,  0.1905,  0.1644,  0.1943,  0.2749,  0.2218,\n",
      "         0.2175,  0.2364,  0.2300,  0.2018,  0.2013,  0.1863,  0.2412,  0.2044,\n",
      "         0.2188,  0.2481,  0.2129,  0.2158,  0.2266,  0.1775,  0.1719,  0.2150,\n",
      "         0.2031,  0.2000,  0.2722,  0.2068,  0.2486,  0.2182,  0.2590,  0.2058,\n",
      "         0.2496,  0.3010,  0.1773,  0.2467,  0.1806,  0.2468,  0.2055,  0.2344,\n",
      "         0.2458,  0.2623,  0.3043,  0.1888,  0.2197,  0.2256,  0.2286,  0.3558,\n",
      "         0.2412,  0.3716,  0.2456,  0.2683,  0.1774,  0.2297,  0.2334,  0.2268,\n",
      "         0.1941,  0.2123,  0.2517,  0.1908,  0.2870,  0.3184,  0.2528,  0.2632,\n",
      "         0.3683,  0.2725,  0.2297,  0.2014,  0.2225,  0.2456,  0.2026,  0.1840,\n",
      "         0.1773,  0.1816,  0.2327,  0.2203,  0.2825,  0.2244,  0.3180,  0.2067,\n",
      "         0.2893,  0.3487,  0.1818,  0.3361,  0.1931,  0.2253,  0.2102,  0.3184,\n",
      "         0.1888,  0.3094,  0.2357,  0.1708,  0.1866,  0.3457,  0.2376,  0.3867,\n",
      "         0.2320,  0.7108,  0.2089,  0.1979,  0.2228,  0.2873,  0.1867,  0.2652,\n",
      "         0.2522,  0.1904,  0.2096,  0.1859,  0.2254,  0.2740,  0.2490,  0.2597,\n",
      "         0.2442,  0.2133,  0.2464,  0.2302,  0.2144,  0.2056,  0.2726,  0.2574,\n",
      "         0.2088,  0.2360,  0.2346,  0.3122,  0.2718,  0.2548,  0.1657,  0.2289,\n",
      "         0.2249,  0.1920,  0.1702,  0.1849,  0.2047,  0.2319,  0.1998,  0.3111,\n",
      "         0.2586,  0.1805,  0.2306,  0.2968,  0.2332,  0.1812,  0.2221,  0.2796,\n",
      "         0.2611,  0.2262, -0.2316,  0.3095,  0.2278,  0.2005,  0.2511,  0.4966,\n",
      "         0.2183,  0.2251,  0.2107,  0.2039,  0.2048,  0.2364,  0.2244,  0.3860,\n",
      "         0.2168,  0.1660,  0.2626,  0.2482,  0.2045,  0.2039,  0.1808,  0.2316,\n",
      "         0.1785,  0.2541,  0.2304,  0.2091,  0.3728,  0.2358,  0.1874,  0.2096,\n",
      "         0.1863,  0.2192,  0.2073,  0.0496,  0.2357,  0.2051,  0.2203,  0.2619,\n",
      "         0.1963,  0.1849,  0.2444,  0.2807,  0.2501,  0.2110,  0.1916,  0.2591,\n",
      "         0.2124,  0.2409,  0.1922,  0.2342,  0.1657,  0.2669,  0.2009,  0.2070,\n",
      "         0.2533,  0.2597,  0.2131,  0.2296,  0.2853,  0.2050,  0.1819,  0.2213,\n",
      "         0.3002,  0.2141,  0.2189,  0.2169,  0.2579,  0.2232,  0.2970,  0.2369,\n",
      "         0.2147,  0.2157,  0.2390,  0.2690,  0.3494,  0.1933,  0.2690,  0.2044,\n",
      "         0.2075,  0.2703,  0.2251,  0.1838,  0.2602,  0.1760,  0.2311,  0.2322,\n",
      "         0.1477,  0.2716,  0.2247,  0.2114,  0.2439,  0.1816,  0.2464,  0.2530,\n",
      "         0.2536,  0.1948,  0.1966,  0.2357,  0.1986,  0.2503,  0.2663,  0.2044,\n",
      "         0.2733,  0.2251,  0.2043,  0.1918,  0.1833,  0.2226,  0.2346,  0.2231,\n",
      "         0.3585,  0.2114,  0.2828,  0.2574,  0.1889,  0.1960,  0.2407,  0.2109,\n",
      "         0.1940,  0.1916,  0.2009,  0.2327,  0.2353,  0.2434,  0.2047,  0.1944,\n",
      "         0.2257,  0.2227,  0.1520,  0.2333,  0.2918,  0.2534,  0.2198,  0.1835,\n",
      "         0.2309,  0.2098,  0.2238,  0.2086,  0.1646,  0.2231,  0.3115,  0.2020,\n",
      "         0.1953,  0.2414,  0.2595,  0.2058,  0.2657,  0.2627,  0.2539,  0.3742,\n",
      "         0.2559,  0.2162,  0.2192,  0.2474,  0.1274,  0.2586,  0.1772,  0.2112,\n",
      "         0.2543,  0.2548,  0.1706,  0.2195,  0.2006,  0.2317,  0.1875,  0.2148,\n",
      "         0.1977,  0.2001,  0.2041,  0.2522,  0.3015,  0.2469,  0.1868,  0.2384,\n",
      "         0.2204,  0.2260,  0.2206,  0.1817,  0.2915,  0.3155,  0.2893,  0.3277,\n",
      "         0.2636,  0.2598,  0.2615,  0.2517,  0.6144,  0.3000,  0.3107,  0.1592,\n",
      "         0.2437,  0.2154,  0.2280,  0.2054,  0.3055,  0.2701,  0.2788,  0.1798,\n",
      "         0.2772,  0.1952,  0.2631,  0.2983,  0.1749,  0.2045,  0.2827,  0.2546,\n",
      "         0.2262,  0.2380,  0.3115,  0.2256,  0.2106,  0.1900,  0.2196,  0.2173,\n",
      "         0.0625,  0.2211,  0.3614,  0.2376,  0.2558,  0.2090,  0.2251,  0.2715,\n",
      "         0.1893,  0.2844,  0.3493,  0.2300,  0.2127,  0.1761,  0.2062,  0.2348,\n",
      "         0.2482,  0.0783,  0.2712,  0.2165,  0.1650,  0.2490,  0.2363,  0.1578,\n",
      "         0.2160,  0.2910,  0.2554,  0.2566,  0.2579,  0.2076,  0.2399,  0.2001,\n",
      "         0.2052,  0.2538,  0.2021,  0.2092,  0.2279,  0.2376,  0.2657,  0.2084,\n",
      "         0.2236,  0.4253,  0.2764,  0.2554,  0.2242,  0.3241,  0.2715,  0.2514,\n",
      "         0.2408,  0.2035,  0.2060,  0.2327,  0.2351,  0.1845,  0.2196,  0.0638],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.final_layer_norm.weight\n",
      "Parameter containing:\n",
      "tensor([ 1.5428e-01,  1.6580e-01,  1.8390e-01,  2.0196e-01,  1.5292e-01,\n",
      "         1.4062e-01,  1.5603e-01,  1.4413e-01,  1.3941e-01,  1.5669e-01,\n",
      "         1.6217e-01,  1.2746e-01,  1.7348e-01,  3.2839e-01,  2.0159e-01,\n",
      "         2.5381e-01,  1.7708e-01,  1.7534e-01,  1.8065e-01,  1.9004e-01,\n",
      "         1.5443e-01,  2.0761e-01,  1.4638e-01,  1.6434e-01,  1.8032e-01,\n",
      "         1.4203e-01,  1.7595e-01,  2.0901e-01,  1.7603e-01,  2.0156e-01,\n",
      "         1.5880e-01,  1.4772e-01,  1.3892e-01,  2.2541e-01,  1.6859e-01,\n",
      "         6.0860e-01,  1.7491e-01,  1.6748e-01,  1.5830e-01,  2.3320e-01,\n",
      "         1.5898e-01,  2.2538e-01,  1.5989e-01,  1.8242e-01,  2.1605e-01,\n",
      "         2.5948e-01,  1.8831e-01,  1.5753e-01,  2.4264e-01,  1.9416e-01,\n",
      "         1.5439e-01,  2.0168e-01,  1.6479e-01,  1.3876e-01,  1.9867e-01,\n",
      "         1.4830e-01,  1.4344e-01,  1.4786e-01,  2.5705e-01,  1.5636e-01,\n",
      "         1.4886e-01,  1.9478e-01,  1.9669e-01,  1.4896e-01,  1.5391e-01,\n",
      "         1.5702e-01,  1.6349e-01,  7.4499e-01,  1.5938e-01,  1.5597e-01,\n",
      "         4.8398e-02,  1.6555e-01, -3.6483e-03,  1.6795e-01,  1.5508e-01,\n",
      "         2.5185e-01,  1.9184e-01,  2.0683e-01,  1.9831e-01,  2.0185e-01,\n",
      "         2.1455e-01,  1.6466e-01,  1.6015e-01,  1.2144e-01,  1.3879e-01,\n",
      "         1.6669e-01,  1.8500e-01,  1.6826e-01,  1.5720e-01,  2.1525e-01,\n",
      "         1.7327e-01,  1.3737e-01,  1.4837e-01,  1.4145e-01,  1.4504e-01,\n",
      "         2.1185e-01,  1.6616e-01,  3.6710e-01,  1.3229e-01,  1.5268e-01,\n",
      "         1.5739e-01,  1.4867e-01,  1.9728e-01,  1.5432e-01,  1.5796e-01,\n",
      "         1.7679e-01,  1.5266e-01,  1.5644e-01,  1.6941e-01,  1.4913e-01,\n",
      "         1.9368e-01,  1.5248e-01,  1.8168e-01,  1.9704e-01,  1.3695e-01,\n",
      "         1.6965e-01,  1.6020e-01,  1.5827e-01,  1.3783e-01,  1.7088e-01,\n",
      "         1.5007e-01,  1.5637e-01,  1.6037e-01,  1.5736e-01,  1.7927e-01,\n",
      "         1.5250e-01,  2.0108e-01,  1.4593e-01,  1.7613e-01,  1.8476e-01,\n",
      "         1.4116e-01,  1.9786e-01,  1.6087e-01,  1.4439e-01,  1.5620e-01,\n",
      "         1.5880e-01,  2.0178e-01,  1.8536e-01,  2.0259e-01,  1.5243e-01,\n",
      "         1.8375e-01,  2.0980e-01,  1.6561e-01,  2.3978e-01,  1.8946e-01,\n",
      "         2.7929e-01,  1.8359e-01,  2.3554e-01,  1.4161e-01,  1.6349e-01,\n",
      "         1.6346e-01,  1.7108e-01,  1.4910e-01,  1.4323e-01,  2.3061e-01,\n",
      "         1.3725e-01,  2.2235e-01,  1.7030e-01,  1.8228e-01,  2.3654e-01,\n",
      "         2.8292e-01,  1.8604e-01,  2.0001e-01,  1.4943e-01,  1.7417e-01,\n",
      "         1.6911e-01,  1.4474e-01,  1.2838e-01,  1.2866e-01,  1.5583e-01,\n",
      "         1.8649e-01,  1.6786e-01,  2.0810e-01,  2.0861e-01,  2.3502e-01,\n",
      "         1.4731e-01,  2.0010e-01,  2.2241e-01,  1.4517e-01,  3.5154e-01,\n",
      "         1.3899e-01,  1.7241e-01,  1.4774e-01,  1.9689e-01,  1.4832e-01,\n",
      "         2.5267e-01,  2.2726e-01,  1.2864e-01,  1.3741e-01,  2.7182e-01,\n",
      "         1.7824e-01,  2.6026e-01,  1.4258e-01,  2.1084e+00,  1.5352e-01,\n",
      "         1.5415e-01,  1.9603e-01,  1.8705e-01,  1.4338e-01,  1.9241e-01,\n",
      "         2.0436e-01,  1.5511e-01,  2.0068e-01,  1.4012e-01,  1.9793e-01,\n",
      "         1.8518e-01,  1.9048e-01,  1.6498e-01,  2.3594e-01,  1.4824e-01,\n",
      "         1.8452e-01,  1.6163e-01,  1.4800e-01,  1.4316e-01,  2.1670e-01,\n",
      "         1.9253e-01,  1.7294e-01,  1.7391e-01,  2.0627e-01,  2.3162e-01,\n",
      "         1.8926e-01,  1.7186e-01,  1.3188e-01,  2.0612e-01,  1.6638e-01,\n",
      "         1.5749e-01,  1.3861e-01,  1.6019e-01,  1.5593e-01,  2.0391e-01,\n",
      "         1.5253e-01,  2.5040e+00,  2.1494e-01,  1.2388e-01,  1.8467e-01,\n",
      "         1.9669e-01,  1.7117e-01,  1.4379e-01,  1.4216e-01,  1.7209e-01,\n",
      "         2.3063e-01,  1.7178e-01,  2.4501e+00,  2.5258e-01,  1.4257e-01,\n",
      "         1.6473e-01,  2.5463e-01,  1.5792e-01,  1.5682e-01,  1.6122e-01,\n",
      "         1.4014e-01,  1.5796e-01,  1.3883e-01,  1.7126e-01,  1.6701e-01,\n",
      "         2.6390e-01,  1.7095e-01,  1.2387e-01,  1.8450e-01,  1.6331e-01,\n",
      "         1.5500e-01,  1.6275e-01,  1.7276e-01,  1.7407e-01,  1.3898e-01,\n",
      "         2.1593e-01,  1.8073e-01,  1.8006e-01,  3.9922e-01,  1.5888e-01,\n",
      "         1.5892e-01,  1.6028e-01,  1.4273e-01,  1.6057e-01,  1.5702e-01,\n",
      "         6.9870e-01,  1.6808e-01,  1.5900e-01,  1.7077e-01,  1.6555e-01,\n",
      "         1.9458e-01,  1.5715e-01,  1.6526e-01,  1.8114e-01,  1.4955e-01,\n",
      "         1.7098e-01,  1.3227e-01,  1.5378e-01,  1.4742e-01,  1.6871e-01,\n",
      "         1.4466e-01,  1.7797e-01,  1.3558e-01,  1.9265e-01,  1.5131e-01,\n",
      "         1.5311e-01,  1.9501e-01,  1.9177e-01,  1.4788e-01,  2.3728e-01,\n",
      "         1.8343e-01,  1.4890e-01,  1.9792e-01,  1.6489e-01,  2.8205e-01,\n",
      "         1.8053e-01,  1.6291e-01,  1.7481e-01,  1.9073e-01,  1.8273e-01,\n",
      "         2.1921e-01,  1.5600e-01,  1.4665e-01,  1.6313e-01,  1.7744e-01,\n",
      "         2.4105e-01,  2.5726e-01,  1.7500e-01,  1.6240e-01,  1.5233e-01,\n",
      "         1.5184e-01,  1.9139e-01,  1.9473e-01,  1.2499e-01,  1.8324e-01,\n",
      "         1.4527e-01,  1.7477e-01,  1.6574e-01,  1.3498e-01,  5.4390e+00,\n",
      "         1.5739e-01,  1.4211e-01,  1.5879e-01,  1.4973e-01,  1.7203e-01,\n",
      "         1.9696e-01,  1.9750e-01,  1.2717e-01,  1.5496e-01,  2.1744e-01,\n",
      "         1.3808e-01,  1.8136e-01,  2.4352e-01,  1.6660e-01,  2.1081e-01,\n",
      "         1.6157e-01,  1.6853e-01,  1.7727e-01,  1.3652e-01,  1.5106e-01,\n",
      "         1.7342e-01,  1.7285e-01,  2.8185e-01,  1.5292e-01,  2.2168e-01,\n",
      "         2.0266e-01,  1.2626e-01,  1.4080e-01,  1.7260e-01,  1.5412e-01,\n",
      "         1.5168e-01,  1.5250e-01,  1.4569e-01,  1.5295e-01,  1.5625e-01,\n",
      "         1.7063e-01,  1.5681e-01,  1.4130e-01,  1.7794e-01,  1.8610e-01,\n",
      "         1.2409e-01,  1.8253e-01,  1.9686e-01,  1.5819e-01,  1.5835e-01,\n",
      "         1.5229e-01,  1.4897e-01,  1.6719e-01,  1.6565e-01,  1.5636e-01,\n",
      "         1.3640e-01,  1.5484e-01,  2.5213e-01,  1.5852e-01,  1.4539e-01,\n",
      "         1.6755e-01,  1.9851e-01,  1.6713e-01,  1.9777e-01,  1.2940e+00,\n",
      "         1.9238e-01,  2.6624e-01,  1.9581e-01,  1.5609e-01,  1.7050e-01,\n",
      "         1.5728e-01,  3.9944e+00,  1.7087e-01,  1.3065e-01,  1.7254e-01,\n",
      "         1.9815e-01,  2.0276e-01,  1.1857e-01,  1.8610e-01,  1.6435e-01,\n",
      "         1.6359e-01,  1.3627e-01,  1.5523e-01,  1.3961e-01,  1.5140e-01,\n",
      "         1.7710e-01,  2.1071e-01,  1.8755e-01,  2.3526e-01,  1.4051e-01,\n",
      "         1.7259e-01,  1.8101e-01,  2.2694e-01,  1.7512e-01,  1.4926e-01,\n",
      "         2.4794e-01,  2.8355e-01,  2.0459e-01,  2.4805e-01,  1.8722e-01,\n",
      "         1.6034e-01,  1.6610e-01,  1.6755e-01,  8.0132e+00,  2.0856e-01,\n",
      "         2.2119e-01,  1.4603e-01,  1.8128e-01,  1.8170e-01,  2.2521e-01,\n",
      "         1.7628e-01,  2.2907e-01,  1.7959e-01,  2.1356e-01,  1.3983e-01,\n",
      "         2.1420e-01,  1.3331e-01,  2.1937e-01,  2.1758e-01,  1.4983e-01,\n",
      "         1.6765e-01,  2.0716e-01,  1.8475e-01,  1.3935e-01,  1.8093e-01,\n",
      "         3.8670e-01,  2.0759e-01,  1.6322e-01,  1.6921e-01,  1.5053e-01,\n",
      "         1.5763e-01,  1.0413e-02,  1.9564e-01,  2.7498e-01,  1.9915e-01,\n",
      "         2.0926e-01,  1.5328e-01,  1.7920e-01,  1.7348e-01,  1.4264e-01,\n",
      "         2.4903e-01,  1.1966e+00,  1.5906e-01,  1.6776e-01,  1.4058e-01,\n",
      "         1.6228e-01,  1.7973e-01,  1.5291e-01,  2.2698e+00,  2.4809e-01,\n",
      "         1.6936e-01,  1.5123e-01,  1.7908e-01,  1.5151e-01,  1.8336e-01,\n",
      "         1.4101e-01,  2.2338e-01,  2.2480e-01,  1.9182e-01,  2.2327e-01,\n",
      "         1.5978e-01,  1.7747e-01,  1.4748e-01,  1.4775e-01,  1.4353e-01,\n",
      "         1.4338e-01,  1.7076e-01,  1.8825e-01,  1.7172e-01,  2.0305e-01,\n",
      "         1.6967e-01,  1.5598e-01,  4.8382e-01,  1.9437e-01,  1.7443e-01,\n",
      "         1.8528e-01,  2.8140e-01,  2.0020e-01,  1.8004e-01,  2.1061e-01,\n",
      "         1.4837e-01,  1.6229e-01,  1.8264e-01,  1.7787e-01,  1.5788e-01,\n",
      "         1.9193e-01,  2.6843e-02], device='cuda:0', requires_grad=True)\n",
      "lm_head.weight\n",
      "Parameter containing:\n",
      "tensor([[-2.2655,  0.9261, -1.0169,  ...,  0.2281, -2.2498, -3.6881],\n",
      "        [-0.2549,  0.0415,  1.3473,  ..., -0.7947, -0.1016, -0.0230],\n",
      "        [-0.2262,  0.0264,  0.8229,  ..., -1.3851,  0.9653,  0.0202],\n",
      "        ...,\n",
      "        [-2.2648,  0.9232, -1.0102,  ...,  0.3030, -2.2359, -3.7000],\n",
      "        [-2.2729,  0.9637, -0.9862,  ...,  0.2807, -2.2149, -3.6453],\n",
      "        [-2.2872,  0.8938, -1.0061,  ...,  0.2389, -2.1855, -3.6960]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ba7c4401-c036-4fd3-a755-dd474319440b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[143.62669491767883,\n",
       " 142.5286384820938,\n",
       " 143.50277489423752,\n",
       " 146.23862969875336,\n",
       " 140.5147095322609,\n",
       " 141.00257223844528,\n",
       " 139.3914748430252,\n",
       " 136.42909741401672,\n",
       " 141.59473425149918,\n",
       " 141.94694358110428,\n",
       " 140.41304218769073,\n",
       " 139.03717106580734,\n",
       " 140.07656514644623,\n",
       " 139.0496900677681,\n",
       " 133.51497408747673,\n",
       " 143.52370661497116,\n",
       " 135.01737874746323,\n",
       " 144.30945867300034,\n",
       " 137.03003215789795,\n",
       " 132.50970220565796,\n",
       " 134.4957340657711,\n",
       " 137.367799192667,\n",
       " 136.06550937891006,\n",
       " 130.4050893187523,\n",
       " 121.49772857874632,\n",
       " 129.01674707233906,\n",
       " 127.08680990338326,\n",
       " 125.33649933338165,\n",
       " 119.89576543867588,\n",
       " 125.69496434926987,\n",
       " 121.67850044369698,\n",
       " 138.26129053533077,\n",
       " 131.07128305733204,\n",
       " 127.71054154634476,\n",
       " 124.89710092544556,\n",
       " 129.9786283671856,\n",
       " 121.57243762910366,\n",
       " 119.14458272606134,\n",
       " 126.05276826024055,\n",
       " 133.2459368109703,\n",
       " 122.86035543680191,\n",
       " 123.20752841234207,\n",
       " 106.55835398286581,\n",
       " 115.82176097482443,\n",
       " 114.60257506370544,\n",
       " 114.78088916838169,\n",
       " 107.76556182652712,\n",
       " 113.857197413221,\n",
       " 96.57973081246018,\n",
       " 106.73437930736691,\n",
       " 97.5641563758254,\n",
       " 95.66529198735952,\n",
       " 93.67192112002522,\n",
       " 111.64739824831486,\n",
       " 116.35455247201025,\n",
       " 103.08623736910522,\n",
       " 100.48711034096777,\n",
       " 114.12756237387657,\n",
       " 106.53347855061293,\n",
       " 104.6505661890842,\n",
       " 110.28618248924613,\n",
       " 116.79008141160011,\n",
       " 121.74708787724376,\n",
       " 104.89155095815659,\n",
       " 107.77591145224869,\n",
       " 94.34633373469114,\n",
       " 88.32246338948607]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "47c8dbe4-d025-4dae-bdbf-b436952fa3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "steps = [100*i for i in range(len(loss_array))]\n",
    "plt.plot(steps, loss_array, label='train_loss')\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"Total loss per 100 steps\")\n",
    "\n",
    "# plt.plot(validationEpoch_loss,label='val_loss')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5439bc-b998-4282-ae14-f2ee1b5de18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2f5c2d98-3b82-405f-ae32-fc92938756ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "input_ids = tokenized_datasets[\"test\"][\"input_ids\"]\n",
    "labels = tokenized_datasets[\"test\"]['answerKey']\n",
    "model_outputs = []\n",
    "probability_output = []\n",
    "for i in range(0, len(input_ids)):\n",
    "        # print(input_ids[i])\n",
    "    \n",
    "        test_tensor = torch.unsqueeze(torch.tensor(input_ids[i]), 0).to(device)\n",
    "        preds = model(input_ids=test_tensor, decoder_input_ids=torch.tensor([[model1.config.decoder_start_token_id,]]).to(device))      \n",
    "        preds_prob = []\n",
    "        for t in ans_id_dict.keys():\n",
    "            preds_prob.append(torch.nn.functional.softmax(preds.logits, dim=-1)[...,t][0][0].item())\n",
    "            \n",
    "        model_outputs.append(index_to_ans[np.argmax(preds_prob)])\n",
    "        probability_output.append(preds_prob)\n",
    "        \n",
    "result = 0\n",
    "for i in range(min(len(model_outputs), len(labels))):\n",
    "    if model_outputs[i] == labels[i] or ans_to_index[model_outputs[i]] == labels[i]:\n",
    "        result += 1\n",
    "result\n",
    "#946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4cd989ae-c1c5-4625-8fd7-940f6be5b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model/flant5_small_lr_10-5_qa_finetuning\")\n",
    "# model.save_pretrained(\"model/flant5_small_lr_10-5_qa_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c2b90585-1c8d-4dfd-8ba2-e7ff038a17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distill = AutoModelForSeq2SeqLM.from_pretrained(model_small).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d5c6bec1-0eb7-42e7-8806-c6f56c5ef253",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_distill = AdamW(model_distill.parameters(), lr=1e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler_distill = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer_distill, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9d5e71ac-4554-4b31-8646-2353055e4b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c618be40ea4aa69ab613c0d4f1a24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11025/3073067617.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"labels\"], requires_grad=True).to(\"cuda\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.86449912190437\n",
      "138.91517215967178\n",
      "150.93376556038857\n",
      "142.8078818321228\n",
      "145.0705853998661\n",
      "146.0051245689392\n",
      "148.82969915866852\n",
      "140.3003169298172\n",
      "138.95034384727478\n",
      "144.84409523010254\n",
      "139.80027836561203\n",
      "141.53859668970108\n",
      "136.7689799964428\n",
      "142.9713208079338\n",
      "141.84799379110336\n",
      "139.20672261714935\n",
      "134.2067288160324\n",
      "135.48868530988693\n",
      "138.37610417604446\n",
      "138.37235140800476\n",
      "142.19211095571518\n",
      "138.58078506588936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd4eeea6ba64d5081df2b7bfc69739e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.95535212755203\n",
      "138.28546410799026\n",
      "137.0655271410942\n",
      "142.52820453047752\n",
      "135.14123183488846\n",
      "137.1697365641594\n",
      "139.90739941596985\n",
      "136.22340935468674\n",
      "132.2825597524643\n",
      "142.2263279557228\n",
      "134.84686416387558\n",
      "135.2166632115841\n",
      "132.9179409444332\n",
      "144.51396715641022\n",
      "135.69644170999527\n",
      "138.56980293989182\n",
      "140.60684955120087\n",
      "143.94923427700996\n",
      "132.93076008558273\n",
      "135.64128440618515\n",
      "134.14329493045807\n",
      "135.85812044143677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad4b4b161b343e6a6a939e1bab3dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.07756239175797\n",
      "138.7563318312168\n",
      "137.5031026005745\n",
      "133.75583243370056\n",
      "137.45620507001877\n",
      "133.31829644739628\n",
      "134.49946635961533\n",
      "134.74108582735062\n",
      "135.99394124746323\n",
      "141.08088967204094\n",
      "135.43927624821663\n",
      "136.34262463450432\n",
      "134.81897020339966\n",
      "134.7232624590397\n",
      "135.43172988295555\n",
      "140.29313603043556\n",
      "137.75714778900146\n",
      "140.7169880270958\n",
      "136.1780550479889\n",
      "136.05849945545197\n",
      "135.57680395245552\n",
      "134.34211671352386\n",
      "133.26164504885674\n"
     ]
    }
   ],
   "source": [
    "model_distill.train()\n",
    "loss_array_distill = []\n",
    "loss_per_100 = 0\n",
    "c = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # print(batch)\n",
    "        labels = torch.tensor(batch[\"labels\"], requires_grad=True).to(\"cuda\")\n",
    "        # labels.retain_grad()\n",
    "        labels = labels.squeeze()\n",
    "        labels = torch.nn.functional.softmax(labels, dim=-1)\n",
    "        # print(labels)\n",
    "        # labels = batch['labels'][0].to(device)\n",
    "        batch = batch['input_ids'].to(device)\n",
    "        outputs = model_distill(batch, decoder_input_ids=torch.tensor([[0]]).to(device))\n",
    "        # print(outputs)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        preds = logits[0, 0, tuple(list(ans_id_dict.keys()))]\n",
    "        # print(preds)\n",
    "        if len(preds) != len(labels):\n",
    "            continue\n",
    "            \n",
    "        loss = loss_fct(preds, labels)\n",
    "        # loss.retain_grad()\n",
    "        loss_per_100 += loss.item()\n",
    "        if c%100==99:\n",
    "            print(loss_per_100)\n",
    "            loss_array_distill.append(loss_per_100)\n",
    "            loss_per_100 = 0\n",
    "        \n",
    "        optimizer_distill.zero_grad()\n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer_distill.step()\n",
    "        lr_scheduler_distill.step()\n",
    "        # progress_bar.update(1)\n",
    "        c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "349f146c-6b22-4b25-8551-5f6547b3d7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "steps = [100*i for i in range(len(loss_array_distill))]\n",
    "plt.plot(steps, loss_array_distill, label='train_loss')\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"Total loss per 100 steps\")\n",
    "\n",
    "# plt.plot(validationEpoch_loss,label='val_loss')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9899d4dd-d023-4ffc-87c8-d156ef64fbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_distill.eval()\n",
    "model_distill_outputs = []\n",
    "labels = tokenized_datasets[\"test\"]['answerKey']\n",
    "\n",
    "for i in range(0, len(input_ids)):\n",
    "        # print(input_ids[i])\n",
    "    \n",
    "        test_tensor = torch.unsqueeze(torch.tensor(input_ids[i]), 0).to(device)\n",
    "        preds = model_distill(input_ids=test_tensor, decoder_input_ids=torch.tensor([[model1.config.decoder_start_token_id,]]).to(device))      \n",
    "        preds_prob = []\n",
    "        for t in ans_id_dict.keys():\n",
    "            preds_prob.append(torch.nn.functional.softmax(preds.logits, dim=-1)[...,t][0][0].item())\n",
    "            \n",
    "        model_distill_outputs.append(index_to_ans[np.argmax(preds_prob)])\n",
    "        \n",
    "result_distill = 0\n",
    "for i in range(min(len(model_distill_outputs), len(labels))):\n",
    "    if model_distill_outputs[i] == labels[i] or ans_to_index[model_distill_outputs[i]] == labels[i]:\n",
    "        result_distill += 1\n",
    "result_distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6c76ae6d-1acc-4c73-9420-232ce08a1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model/flant5_small_lr_10-5_qa_distill_match_large_output_abcd\")\n",
    "# model_distill.save_pretrained(\"model/flant5_small_lr_10-5_qa_distill_match_large_output_abcd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39543e3c-a6fc-4ae2-8692-9320d6a706e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EWC, ewc_train\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import EWC, ewc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f423608-cee8-4471-8d4f-cb03fd523dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_small).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b3e9a-963c-4326-9f95-7cc99f1d74b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b89ed-9a1b-4196-bf8b-0c8fcf113b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_best = AutoModelForSeq2SeqLM.from_pretrained(model_small).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2b932-f6c5-44bf-8bf3-81e9e80c5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model_1_best.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        highest_prob_index = np.argmax(batch['labels'][0])\n",
    "        labels = [0.0] * len(batch['labels'][0])\n",
    "        labels[highest_prob_index] = 1.0\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "        batch = batch['input_ids'].to(device)\n",
    "        outputs = model_1_best(batch, decoder_input_ids=torch.tensor([[0]]).to(device))\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        logits = outputs.get(\"logits\")\n",
    "        preds_prod = []\n",
    "        for i in range(1):\n",
    "            pred_prob = []\n",
    "            for t in ans_id_dict.keys():\n",
    "                pred_prob.append(logits[..., t][0][0].item())\n",
    "            preds_prod.append(pred_prob)\n",
    "        preds = torch.tensor(preds_prod[0]).to(\"cuda\")\n",
    "        loss = loss_fct(preds, labels)\n",
    "        loss.requires_grad_()\n",
    "        \n",
    "        # loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3055b7-2457-46d5-82c1-16e28a6c388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_best.eval()\n",
    "input_ids = tokenized_datasets[\"test\"][\"input_ids\"]\n",
    "labels = tokenized_datasets[\"test\"]['answerKey']\n",
    "model_1_best_outputs = []\n",
    "probability_1_best_output = []\n",
    "for i in range(0, len(input_ids)):\n",
    "        # print(input_ids[i])\n",
    "    \n",
    "        test_tensor = torch.unsqueeze(torch.tensor(input_ids[i]), 0).to(device)\n",
    "        preds = model_1_best(input_ids=test_tensor, decoder_input_ids=torch.tensor([[model1.config.decoder_start_token_id,]]).to(device))      \n",
    "        preds_prob = []\n",
    "        for t in ans_id_dict.keys():\n",
    "            preds_prob.append(torch.nn.functional.softmax(preds.logits, dim=-1)[...,t][0][0].item())\n",
    "            \n",
    "        model_1_best_outputs.append(index_to_ans[np.argmax(preds_prob)])\n",
    "        probability_1_best_output.append(preds_prob)\n",
    "        \n",
    "result = 0\n",
    "for i in range(min(len(model_1_best_outputs), len(labels))):\n",
    "    if model_1_best_outputs[i] == labels[i] or ans_to_index[model_1_best_outputs[i]] == labels[i]:\n",
    "        result += 1\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebfebb-677e-4807-a8a7-dfce266ef5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_1_best_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
