{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522b964a-e40c-4e4a-9ef1-a4e964cd8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = \"/scratches/dialfs/alta/hln35/.cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratches/dialfs/alta/hln35/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe62f14-bf16-40bc-bae7-c71d425b9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\", cache_dir=cache_dir)\n",
    "metric = load(\"rouge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05e5eb7-4654-4a37-a7f5-5f985725d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_checkpoint = \"google/flan-t5-small\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87d9635-4673-4322-9432-9c71af35b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a451a56-215e-4d2b-bc82-972870284701",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b369f40-057b-487f-93d0-6d4128196de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ef7aab-4a21-434a-b71d-75f693390465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\\nAs they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames.\\nOne of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\\nPolice have appealed for information about the attack.\\nInsp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.\\n\"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"',\n",
       " 'summary': 'Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.',\n",
       " 'id': '40143035'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4348977c-724d-4e2f-b93d-091fb16d4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad90ac1-5d1e-4d36-b306-66e0acc22a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'Simone Favaro got the crucial try with the last move of the game, following earlier touchdowns by Chris Fusaro, Zander Fagerson and Junior Bulumakau.\\nRynard Landman and Ashton Hewitt got a try in either half for the Dragons.\\nGlasgow showed far superior strength in depth as they took control of a messy match in the second period.\\nHome coach Gregor Townsend gave a debut to powerhouse Fijian-born Wallaby wing Taqele Naiyaravoro, and centre Alex Dunbar returned from long-term injury, while the Dragons gave first starts of the season to wing Aled Brew and hooker Elliot Dee.\\nGlasgow lost hooker Pat McArthur to an early shoulder injury but took advantage of their first pressure when Rory Clegg slotted over a penalty on 12 minutes.\\nIt took 24 minutes for a disjointed game to produce a try as Sarel Pretorius sniped from close range and Landman forced his way over for Jason Tovey to convert - although it was the lock\\'s last contribution as he departed with a chest injury shortly afterwards.\\nGlasgow struck back when Fusaro drove over from a rolling maul on 35 minutes for Clegg to convert.\\nBut the Dragons levelled at 10-10 before half-time when Naiyaravoro was yellow-carded for an aerial tackle on Brew and Tovey slotted the easy goal.\\nThe visitors could not make the most of their one-man advantage after the break as their error count cost them dearly.\\nIt was Glasgow\\'s bench experience that showed when Mike Blair\\'s break led to a short-range score from teenage prop Fagerson, converted by Clegg.\\nDebutant Favaro was the second home player to be sin-binned, on 63 minutes, but again the Warriors made light of it as replacement wing Bulumakau, a recruit from the Army, pounced to deftly hack through a bouncing ball for an opportunist try.\\nThe Dragons got back within striking range with some excellent combined handling putting Hewitt over unopposed after 72 minutes.\\nHowever, Favaro became sinner-turned-saint as he got on the end of another effective rolling maul to earn his side the extra point with the last move of the game, Clegg converting.\\nDragons director of rugby Lyn Jones said: \"We\\'re disappointed to have lost but our performance was a lot better [than against Leinster] and the game could have gone either way.\\n\"Unfortunately too many errors behind the scrum cost us a great deal, though from where we were a fortnight ago in Dublin our workrate and desire was excellent.\\n\"It was simply error count from individuals behind the scrum that cost us field position, it\\'s not rocket science - they were correct in how they played and we had a few errors, that was the difference.\"\\nGlasgow Warriors: Rory Hughes, Taqele Naiyaravoro, Alex Dunbar, Fraser Lyle, Lee Jones, Rory Clegg, Grayson Hart; Alex Allan, Pat MacArthur, Zander Fagerson, Rob Harley (capt), Scott Cummings, Hugh Blake, Chris Fusaro, Adam Ashe.\\nReplacements: Fergus Scott, Jerry Yanuyanutawa, Mike Cusack, Greg Peterson, Simone Favaro, Mike Blair, Gregor Hunter, Junior Bulumakau.\\nDragons: Carl Meyer, Ashton Hewitt, Ross Wardle, Adam Warren, Aled Brew, Jason Tovey, Sarel Pretorius; Boris Stankovich, Elliot Dee, Brok Harris, Nick Crosswell, Rynard Landman (capt), Lewis Evans, Nic Cudd, Ed Jackson.\\nReplacements: Rhys Buckley, Phil Price, Shaun Knight, Matthew Screech, Ollie Griffiths, Luc Jones, Charlie Davies, Nick Scott.',\n",
       " 'summary': 'Defending Pro12 champions Glasgow Warriors bagged a late bonus-point victory over the Dragons despite a host of absentees and two yellow cards.',\n",
       " 'id': '34540833',\n",
       " 'input_ids': [21603,\n",
       "  10,\n",
       "  6308,\n",
       "  15,\n",
       "  1699,\n",
       "  4331,\n",
       "  32,\n",
       "  530,\n",
       "  8,\n",
       "  4462,\n",
       "  653,\n",
       "  28,\n",
       "  8,\n",
       "  336,\n",
       "  888,\n",
       "  13,\n",
       "  8,\n",
       "  467,\n",
       "  6,\n",
       "  826,\n",
       "  2283,\n",
       "  19396,\n",
       "  7,\n",
       "  57,\n",
       "  4409,\n",
       "  6343,\n",
       "  7,\n",
       "  291,\n",
       "  32,\n",
       "  6,\n",
       "  1027,\n",
       "  11849,\n",
       "  1699,\n",
       "  1304,\n",
       "  739,\n",
       "  11,\n",
       "  10243,\n",
       "  12455,\n",
       "  440,\n",
       "  5667,\n",
       "  76,\n",
       "  5,\n",
       "  12749,\n",
       "  29,\n",
       "  986,\n",
       "  2216,\n",
       "  348,\n",
       "  11,\n",
       "  7153,\n",
       "  17,\n",
       "  106,\n",
       "  216,\n",
       "  7820,\n",
       "  17,\n",
       "  530,\n",
       "  3,\n",
       "  9,\n",
       "  653,\n",
       "  16,\n",
       "  893,\n",
       "  985,\n",
       "  21,\n",
       "  8,\n",
       "  10282,\n",
       "  7,\n",
       "  5,\n",
       "  18588,\n",
       "  3217,\n",
       "  623,\n",
       "  4784,\n",
       "  2793,\n",
       "  16,\n",
       "  4963,\n",
       "  38,\n",
       "  79,\n",
       "  808,\n",
       "  610,\n",
       "  13,\n",
       "  3,\n",
       "  9,\n",
       "  21124,\n",
       "  1588,\n",
       "  16,\n",
       "  8,\n",
       "  511,\n",
       "  1059,\n",
       "  5,\n",
       "  1210,\n",
       "  3763,\n",
       "  3,\n",
       "  26705,\n",
       "  4463,\n",
       "  7,\n",
       "  989,\n",
       "  1891,\n",
       "  3,\n",
       "  9,\n",
       "  5695,\n",
       "  12,\n",
       "  579,\n",
       "  1840,\n",
       "  377,\n",
       "  17279,\n",
       "  152,\n",
       "  18,\n",
       "  7473,\n",
       "  3556,\n",
       "  9,\n",
       "  969,\n",
       "  3,\n",
       "  3108,\n",
       "  2067,\n",
       "  1824,\n",
       "  400,\n",
       "  1823,\n",
       "  23,\n",
       "  63,\n",
       "  2551,\n",
       "  1967,\n",
       "  32,\n",
       "  6,\n",
       "  11,\n",
       "  2050,\n",
       "  5104,\n",
       "  6393,\n",
       "  1047,\n",
       "  3666,\n",
       "  45,\n",
       "  307,\n",
       "  18,\n",
       "  1987,\n",
       "  2871,\n",
       "  6,\n",
       "  298,\n",
       "  8,\n",
       "  10282,\n",
       "  7,\n",
       "  1891,\n",
       "  166,\n",
       "  3511,\n",
       "  13,\n",
       "  8,\n",
       "  774,\n",
       "  12,\n",
       "  3,\n",
       "  3108,\n",
       "  71,\n",
       "  1361,\n",
       "  3004,\n",
       "  210,\n",
       "  11,\n",
       "  7970,\n",
       "  49,\n",
       "  15334,\n",
       "  23,\n",
       "  32,\n",
       "  17,\n",
       "  374,\n",
       "  15,\n",
       "  5,\n",
       "  18588,\n",
       "  1513,\n",
       "  7970,\n",
       "  49,\n",
       "  5192,\n",
       "  3038,\n",
       "  7754,\n",
       "  10666,\n",
       "  12,\n",
       "  46,\n",
       "  778,\n",
       "  8173,\n",
       "  2871,\n",
       "  68,\n",
       "  808,\n",
       "  2337,\n",
       "  13,\n",
       "  70,\n",
       "  166,\n",
       "  1666,\n",
       "  116,\n",
       "  2158,\n",
       "  651,\n",
       "  205,\n",
       "  109,\n",
       "  4102,\n",
       "  4918,\n",
       "  1054,\n",
       "  147,\n",
       "  3,\n",
       "  9,\n",
       "  10736,\n",
       "  30,\n",
       "  586,\n",
       "  676,\n",
       "  5,\n",
       "  94,\n",
       "  808,\n",
       "  997,\n",
       "  676,\n",
       "  21,\n",
       "  3,\n",
       "  9,\n",
       "  1028,\n",
       "  1927,\n",
       "  2429,\n",
       "  26,\n",
       "  467,\n",
       "  12,\n",
       "  1759,\n",
       "  3,\n",
       "  9,\n",
       "  653,\n",
       "  38,\n",
       "  1138,\n",
       "  60,\n",
       "  40,\n",
       "  1266,\n",
       "  3600,\n",
       "  302,\n",
       "  3,\n",
       "  20317,\n",
       "  15,\n",
       "  26,\n",
       "  45,\n",
       "  885,\n",
       "  620,\n",
       "  11,\n",
       "  2216,\n",
       "  348,\n",
       "  5241,\n",
       "  112,\n",
       "  194,\n",
       "  147,\n",
       "  21,\n",
       "  9637,\n",
       "  304,\n",
       "  162,\n",
       "  63,\n",
       "  12,\n",
       "  5755,\n",
       "  3,\n",
       "  18,\n",
       "  2199,\n",
       "  34,\n",
       "  47,\n",
       "  8,\n",
       "  6081,\n",
       "  31,\n",
       "  7,\n",
       "  336,\n",
       "  6275,\n",
       "  38,\n",
       "  3,\n",
       "  88,\n",
       "  3,\n",
       "  31779,\n",
       "  28,\n",
       "  3,\n",
       "  9,\n",
       "  5738,\n",
       "  2871,\n",
       "  10545,\n",
       "  15627,\n",
       "  5,\n",
       "  18588,\n",
       "  10056,\n",
       "  223,\n",
       "  116,\n",
       "  6343,\n",
       "  7,\n",
       "  291,\n",
       "  32,\n",
       "  10719,\n",
       "  147,\n",
       "  45,\n",
       "  3,\n",
       "  9,\n",
       "  8394,\n",
       "  954,\n",
       "  83,\n",
       "  30,\n",
       "  3097,\n",
       "  676,\n",
       "  21,\n",
       "  205,\n",
       "  109,\n",
       "  4102,\n",
       "  12,\n",
       "  5755,\n",
       "  5,\n",
       "  299,\n",
       "  8,\n",
       "  10282,\n",
       "  7,\n",
       "  593,\n",
       "  1361,\n",
       "  44,\n",
       "  335,\n",
       "  4536,\n",
       "  274,\n",
       "  985,\n",
       "  18,\n",
       "  715,\n",
       "  116,\n",
       "  1823,\n",
       "  23,\n",
       "  63,\n",
       "  2551,\n",
       "  1967,\n",
       "  32,\n",
       "  47,\n",
       "  4459,\n",
       "  18,\n",
       "  6043,\n",
       "  15,\n",
       "  26,\n",
       "  21,\n",
       "  46,\n",
       "  22142,\n",
       "  8000,\n",
       "  30,\n",
       "  3004,\n",
       "  210,\n",
       "  11,\n",
       "  304,\n",
       "  162,\n",
       "  63,\n",
       "  4918,\n",
       "  1054,\n",
       "  8,\n",
       "  514,\n",
       "  1288,\n",
       "  5,\n",
       "  37,\n",
       "  2692,\n",
       "  228,\n",
       "  59,\n",
       "  143,\n",
       "  8,\n",
       "  167,\n",
       "  13,\n",
       "  70,\n",
       "  80,\n",
       "  18,\n",
       "  348,\n",
       "  2337,\n",
       "  227,\n",
       "  8,\n",
       "  1733,\n",
       "  38,\n",
       "  70,\n",
       "  3505,\n",
       "  3476,\n",
       "  583,\n",
       "  135,\n",
       "  12537,\n",
       "  120,\n",
       "  5,\n",
       "  94,\n",
       "  47,\n",
       "  18588,\n",
       "  31,\n",
       "  7,\n",
       "  8453,\n",
       "  351,\n",
       "  24,\n",
       "  3217,\n",
       "  116,\n",
       "  4794,\n",
       "  25594,\n",
       "  31,\n",
       "  7,\n",
       "  1733,\n",
       "  2237,\n",
       "  12,\n",
       "  3,\n",
       "  9,\n",
       "  710,\n",
       "  18,\n",
       "  5517,\n",
       "  2604,\n",
       "  45,\n",
       "  21615,\n",
       "  6377,\n",
       "  1699,\n",
       "  1304,\n",
       "  739,\n",
       "  6,\n",
       "  12069,\n",
       "  57,\n",
       "  205,\n",
       "  109,\n",
       "  4102,\n",
       "  5,\n",
       "  374,\n",
       "  2780,\n",
       "  288,\n",
       "  1699,\n",
       "  4331,\n",
       "  32,\n",
       "  47,\n",
       "  8,\n",
       "  511,\n",
       "  234,\n",
       "  1959,\n",
       "  12,\n",
       "  36,\n",
       "  3731,\n",
       "  18,\n",
       "  115,\n",
       "  14029,\n",
       "  26,\n",
       "  6,\n",
       "  30,\n",
       "  3,\n",
       "  3891,\n",
       "  676,\n",
       "  6,\n",
       "  68,\n",
       "  541,\n",
       "  8,\n",
       "  24499,\n",
       "  263,\n",
       "  659,\n",
       "  13,\n",
       "  34,\n",
       "  38,\n",
       "  3709,\n",
       "  3,\n",
       "  3108,\n",
       "  12455,\n",
       "  440,\n",
       "  5667,\n",
       "  76,\n",
       "  6,\n",
       "  3,\n",
       "  9,\n",
       "  11474,\n",
       "  45,\n",
       "  8,\n",
       "  6788,\n",
       "  6,\n",
       "  3,\n",
       "  102,\n",
       "  7906,\n",
       "  26,\n",
       "  12,\n",
       "  20,\n",
       "  89,\n",
       "  17,\n",
       "  120,\n",
       "  8093,\n",
       "  190,\n",
       "  3,\n",
       "  9,\n",
       "  3,\n",
       "  4076,\n",
       "  4733,\n",
       "  1996,\n",
       "  21,\n",
       "  46,\n",
       "  3,\n",
       "  32,\n",
       "  102,\n",
       "  1493,\n",
       "  202,\n",
       "  343,\n",
       "  653,\n",
       "  5,\n",
       "  37,\n",
       "  10282,\n",
       "  7,\n",
       "  530,\n",
       "  223,\n",
       "  441,\n",
       "  11214,\n",
       "  620,\n",
       "  28,\n",
       "  128,\n",
       "  1287,\n",
       "  3334,\n",
       "  5834,\n",
       "  3,\n",
       "  3131,\n",
       "  216,\n",
       "  7820,\n",
       "  17,\n",
       "  147,\n",
       "  73,\n",
       "  28236,\n",
       "  3843,\n",
       "  227,\n",
       "  9455,\n",
       "  676,\n",
       "  5,\n",
       "  611,\n",
       "  6,\n",
       "  1699,\n",
       "  4331,\n",
       "  32,\n",
       "  1632,\n",
       "  3731,\n",
       "  687,\n",
       "  18,\n",
       "  24346,\n",
       "  18,\n",
       "  7,\n",
       "  9,\n",
       "  77,\n",
       "  17,\n",
       "  38,\n",
       "  3,\n",
       "  88,\n",
       "  530,\n",
       "  30,\n",
       "  8,\n",
       "  414,\n",
       "  13,\n",
       "  430,\n",
       "  1231,\n",
       "  8394,\n",
       "  954,\n",
       "  83,\n",
       "  12,\n",
       "  3807,\n",
       "  112,\n",
       "  596,\n",
       "  8,\n",
       "  996,\n",
       "  500,\n",
       "  28,\n",
       "  8,\n",
       "  336,\n",
       "  888,\n",
       "  13,\n",
       "  8,\n",
       "  467,\n",
       "  6,\n",
       "  205,\n",
       "  109,\n",
       "  4102,\n",
       "  3,\n",
       "  21049,\n",
       "  5,\n",
       "  10282,\n",
       "  7,\n",
       "  2090,\n",
       "  13,\n",
       "  22209,\n",
       "  5225,\n",
       "  29,\n",
       "  6193,\n",
       "  243,\n",
       "  10,\n",
       "  96,\n",
       "  1326,\n",
       "  31,\n",
       "  60,\n",
       "  10978,\n",
       "  12,\n",
       "  43,\n",
       "  1513,\n",
       "  68,\n",
       "  69,\n",
       "  821,\n",
       "  47,\n",
       "  3,\n",
       "  9,\n",
       "  418,\n",
       "  394,\n",
       "  784,\n",
       "  6736,\n",
       "  581,\n",
       "  312,\n",
       "  77,\n",
       "  1370,\n",
       "  908,\n",
       "  11,\n",
       "  8,\n",
       "  467,\n",
       "  228,\n",
       "  43,\n",
       "  2767,\n",
       "  893,\n",
       "  194,\n",
       "  5,\n",
       "  96,\n",
       "  5110,\n",
       "  29178,\n",
       "  396,\n",
       "  186,\n",
       "  6854,\n",
       "  1187,\n",
       "  8,\n",
       "  14667,\n",
       "  440,\n",
       "  583,\n",
       "  178,\n",
       "  3,\n",
       "  9,\n",
       "  248,\n",
       "  1154,\n",
       "  6,\n",
       "  713,\n",
       "  45,\n",
       "  213,\n",
       "  62,\n",
       "  130,\n",
       "  3,\n",
       "  9,\n",
       "  21,\n",
       "  17,\n",
       "  7602,\n",
       "  977,\n",
       "  16,\n",
       "  14112,\n",
       "  69,\n",
       "  161,\n",
       "  2206,\n",
       "  11,\n",
       "  3667,\n",
       "  47,\n",
       "  1287,\n",
       "  5,\n",
       "  96,\n",
       "  196,\n",
       "  17,\n",
       "  47,\n",
       "  914,\n",
       "  3505,\n",
       "  3476,\n",
       "  45,\n",
       "  1742,\n",
       "  1187,\n",
       "  8,\n",
       "  14667,\n",
       "  440,\n",
       "  24,\n",
       "  583,\n",
       "  178,\n",
       "  1057,\n",
       "  1102,\n",
       "  6,\n",
       "  34,\n",
       "  31,\n",
       "  7,\n",
       "  59,\n",
       "  15721,\n",
       "  2056,\n",
       "  3,\n",
       "  18,\n",
       "  79,\n",
       "  130,\n",
       "  2024,\n",
       "  16,\n",
       "  149,\n",
       "  79,\n",
       "  1944,\n",
       "  11,\n",
       "  62,\n",
       "  141,\n",
       "  3,\n",
       "  9,\n",
       "  360,\n",
       "  6854,\n",
       "  6,\n",
       "  24,\n",
       "  47,\n",
       "  8,\n",
       "  1750,\n",
       "  535,\n",
       "  18588,\n",
       "  24499,\n",
       "  10,\n",
       "  2158,\n",
       "  651,\n",
       "  21512,\n",
       "  6,\n",
       "  2067,\n",
       "  1824,\n",
       "  400,\n",
       "  1823,\n",
       "  23,\n",
       "  63,\n",
       "  2551,\n",
       "  1967,\n",
       "  32,\n",
       "  6,\n",
       "  5104,\n",
       "  6393,\n",
       "  1047,\n",
       "  6,\n",
       "  27495,\n",
       "  5225,\n",
       "  109,\n",
       "  6,\n",
       "  5531,\n",
       "  6193,\n",
       "  6,\n",
       "  2158,\n",
       "  651,\n",
       "  205,\n",
       "  109,\n",
       "  4102,\n",
       "  6,\n",
       "  13375,\n",
       "  739,\n",
       "  10498,\n",
       "  117,\n",
       "  5104,\n",
       "  432,\n",
       "  152,\n",
       "  6,\n",
       "  5192,\n",
       "  2143,\n",
       "  7754,\n",
       "  10666,\n",
       "  6,\n",
       "  1027,\n",
       "  11849,\n",
       "  1699,\n",
       "  1304,\n",
       "  739,\n",
       "  6,\n",
       "  5376,\n",
       "  26085,\n",
       "  41,\n",
       "  4010,\n",
       "  17,\n",
       "  201,\n",
       "  4972,\n",
       "  205,\n",
       "  23434,\n",
       "  7,\n",
       "  6,\n",
       "  11560,\n",
       "  107,\n",
       "  21581,\n",
       "  6,\n",
       "  4409,\n",
       "  6343,\n",
       "  7,\n",
       "  291,\n",
       "  32,\n",
       "  6,\n",
       "  7124,\n",
       "  282,\n",
       "  88,\n",
       "  5,\n",
       "  21438,\n",
       "  7,\n",
       "  10,\n",
       "  7566,\n",
       "  1744,\n",
       "  7,\n",
       "  4972,\n",
       "  6,\n",
       "  16637,\n",
       "  18707,\n",
       "  76,\n",
       "  63,\n",
       "  9,\n",
       "  4796,\n",
       "  7396,\n",
       "  6,\n",
       "  4794,\n",
       "  1839,\n",
       "  15525,\n",
       "  6,\n",
       "  11859,\n",
       "  2737,\n",
       "  739,\n",
       "  6,\n",
       "  6308,\n",
       "  15,\n",
       "  1699,\n",
       "  4331,\n",
       "  32,\n",
       "  6,\n",
       "  4794,\n",
       "  25594,\n",
       "  6,\n",
       "  3,\n",
       "  26705,\n",
       "  14046,\n",
       "  6,\n",
       "  10243,\n",
       "  12455,\n",
       "  440,\n",
       "  5667,\n",
       "  76,\n",
       "  5,\n",
       "  10282,\n",
       "  7,\n",
       "  10,\n",
       "  7291,\n",
       "  19191,\n",
       "  6,\n",
       "  7153,\n",
       "  17,\n",
       "  106,\n",
       "  216,\n",
       "  7820,\n",
       "  17,\n",
       "  6,\n",
       "  9616,\n",
       "  15811,\n",
       "  109,\n",
       "  6,\n",
       "  7124,\n",
       "  16700,\n",
       "  6,\n",
       "  71,\n",
       "  1361,\n",
       "  3004,\n",
       "  210,\n",
       "  6,\n",
       "  9637,\n",
       "  304,\n",
       "  162,\n",
       "  63,\n",
       "  6,\n",
       "  1138,\n",
       "  60,\n",
       "  40,\n",
       "  1266,\n",
       "  3600,\n",
       "  302,\n",
       "  117,\n",
       "  7254,\n",
       "  159,\n",
       "  11469,\n",
       "  9789,\n",
       "  362,\n",
       "  6,\n",
       "  15334,\n",
       "  23,\n",
       "  32,\n",
       "  17,\n",
       "  374,\n",
       "  15,\n",
       "  6,\n",
       "  4027,\n",
       "  157,\n",
       "  12551,\n",
       "  6,\n",
       "  7486,\n",
       "  4737,\n",
       "  2091,\n",
       "  6,\n",
       "  12749,\n",
       "  29,\n",
       "  986,\n",
       "  2216,\n",
       "  348,\n",
       "  41,\n",
       "  4010,\n",
       "  17,\n",
       "  201,\n",
       "  9765,\n",
       "  18305,\n",
       "  6,\n",
       "  20310,\n",
       "  1839,\n",
       "  26,\n",
       "  26,\n",
       "  6,\n",
       "  4857,\n",
       "  7714,\n",
       "  5,\n",
       "  21438,\n",
       "  7,\n",
       "  10,\n",
       "  11092,\n",
       "  63,\n",
       "  7,\n",
       "  10295,\n",
       "  1306,\n",
       "  6,\n",
       "  8188,\n",
       "  5312,\n",
       "  6,\n",
       "  3926,\n",
       "  202,\n",
       "  11595,\n",
       "  6,\n",
       "  9771,\n",
       "  4712,\n",
       "  60,\n",
       "  10217,\n",
       "  6,\n",
       "  5424,\n",
       "  1896,\n",
       "  29345,\n",
       "  7,\n",
       "  6,\n",
       "  3,\n",
       "  11748,\n",
       "  6193,\n",
       "  6,\n",
       "  12707,\n",
       "  28571,\n",
       "  6,\n",
       "  7486,\n",
       "  4972,\n",
       "  5,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3,\n",
       "  16196,\n",
       "  9303,\n",
       "  749,\n",
       "  2122,\n",
       "  6336,\n",
       "  7,\n",
       "  18588,\n",
       "  24499,\n",
       "  2182,\n",
       "  5402,\n",
       "  3,\n",
       "  9,\n",
       "  1480,\n",
       "  4023,\n",
       "  18,\n",
       "  2700,\n",
       "  6224,\n",
       "  147,\n",
       "  8,\n",
       "  10282,\n",
       "  7,\n",
       "  3,\n",
       "  3565,\n",
       "  3,\n",
       "  9,\n",
       "  2290,\n",
       "  13,\n",
       "  14101,\n",
       "  15,\n",
       "  15,\n",
       "  7,\n",
       "  11,\n",
       "  192,\n",
       "  4459,\n",
       "  2190,\n",
       "  5,\n",
       "  1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee67b4d7-e91e-4cff-85d2-c25bf448b044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb77c8a-73b3-490d-879e-2d6bc0bb802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = tokenized_datasets[\"train\"][\"input_ids\"]\n",
    "labels = tokenizer.batch_decode(tokenized_datasets[\"train\"][\"labels\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b814cd0c-5f81-422b-8fab-75b725de2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc14d32-cf8d-42a3-bddd-a7e20e2c912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm  fire alarm alarm alarm alarm alarm alarm alarm alarm alarm alarm alarm alarm alarm alarm alarm'] 128\n",
      "['A group of tourists has been attacked by a fire in a bus park in Northern Ireland.']\n",
      "the average score is: \n",
      "{'rouge1': 0.01869158878504673, 'rouge2': 0.0, 'rougeL': 0.01869158878504673, 'rougeLsum': 0.01869158878504673}\n"
     ]
    }
   ],
   "source": [
    "model_small_distill = f\"model/flant5_small_lr_10-4_race_distill_epoch2\"\n",
    "model_small_distill = AutoModelForSeq2SeqLM.from_pretrained(model_small_distill, local_files_only=True).to(device)\n",
    "results_small_distill = {}\n",
    "for i in range(1, 2, 1):\n",
    "        test_tensor = torch.tensor([test_input_ids[i]]).to(device)\n",
    "        preds_distill = model_small_distill.generate(test_tensor, max_new_tokens=max_target_length, do_sample=False)  \n",
    "        preds = model.generate(test_tensor, max_new_tokens=max_target_length, do_sample=False)  \n",
    "        \n",
    "        preds_distill = tokenizer.batch_decode(preds_distill, skip_special_tokens=True)\n",
    "        preds = tokenizer.batch_decode(preds, skip_special_tokens=True)  \n",
    "    \n",
    "        print(preds_distill, len(preds_distill[0].split(\" \")))\n",
    "        print(preds)\n",
    "        result = metric.compute(predictions=preds_distill, references=[labels[i]], use_stemmer=True, use_aggregator=False)\n",
    "        for key, value in result.items():\n",
    "            if key not in results_small_distill:\n",
    "                results_small_distill[key] = value\n",
    "            else:\n",
    "                results_small_distill[key] += value\n",
    "results_small_distill_agg = {}\n",
    "\n",
    "for k, v in results_small_distill.items():\n",
    "    results_small_distill_agg[k] = np.average(v)\n",
    "print(f\"the average score is: \")\n",
    "print(results_small_distill_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911db0f6-65ae-456a-9ae8-f86e76bdeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bddd5bf-af2e-4daf-8fb0-a5108f0dcaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a46c71-506b-4fc5-9de3-7d985fd9bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42514e41-1965-4b9e-863d-ce450d6829b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54fb01e3-1b5e-4258-b5e8-ca4f8127bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25508' max='25508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25508/25508 5:00:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.551300</td>\n",
       "      <td>2.275294</td>\n",
       "      <td>0.292600</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>18.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.517700</td>\n",
       "      <td>2.255920</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>18.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.503100</td>\n",
       "      <td>2.247270</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>18.831100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.491600</td>\n",
       "      <td>2.244451</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>18.826900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratches/dialfs/alta/hln35/miniconda/envs/distillation2/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25508, training_loss=2.5254513894724626, metrics={'train_runtime': 18017.8211, 'train_samples_per_second': 45.298, 'train_steps_per_second': 1.416, 'total_flos': 3.02870562806956e+17, 'train_loss': 2.5254513894724626, 'epoch': 4.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model/flant5_small_lr_10-5_wd_10-2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aecd9a-21e5-469a-8d70-3d70a8a0c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer2 = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# inputs = tokenizer2(raw_datasets[\"test\"][\"document\"], return_tensors = \"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf72e6-bf93-49f0-bcc1-036aab448b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tensor = torch.tensor(tokenized_datasets[\"test\"][\"input_ids\"])[:1000]\n",
    "# preds = model.generate(test_tensor, max_new_tokens=max_target_length, do_sample=False)  \n",
    "# # preds = torch.tensor(preds)                                                                  \n",
    "# preds = tokenizer.batch_decode(preds, skip_special_tokens=True)        \n",
    "# print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f448c-dc77-4393-9895-b2e7c5fb89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result = metric.compute(predictions=preds, references=labels, use_stemmer=True, use_aggregator=False)\n",
    "# result = {key: value for key, value in result.items()}\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898bd6f6-5f4d-4b6c-be1e-d6c7e5756f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_id in tokenized_datasets[\"test\"][\"input_ids\"]:\n",
    "#     print(input_id)\n",
    "#     output = model.generate(input_id, max_new_tokens=max_target_length, do_sample=False)\n",
    "test_input_ids = tokenized_datasets[\"test\"][\"input_ids\"]\n",
    "results = {}\n",
    "group_len = 20\n",
    "for i in range(0, len(test_input_ids)):\n",
    "        test_tensor = torch.tensor([test_input_ids[i]])\n",
    "        preds = model.generate(test_tensor, max_new_tokens=max_target_length, do_sample=False)                                                               \n",
    "        preds = tokenizer.batch_decode(preds, skip_special_tokens=True)                          \n",
    "        result = metric.compute(predictions=preds, references=[labels[i]], use_stemmer=True, use_aggregator=False)\n",
    "        for key, value in result.items():\n",
    "            if key not in results:\n",
    "                results[key] = value\n",
    "            else:\n",
    "                results[key] += value\n",
    "\n",
    "import json\n",
    "with open(\"rouge_small_fine_tuned_.txt\", \"w\") as fp:\n",
    "    json.dump(results, fp)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9acbde-39bd-47ce-bbcf-f197152dd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82018fe1-1beb-4d95-ad83-83c896bee209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = metric.compute(predictions=preds, references=labels, use_stemmer=True, use_aggregator=False)\n",
    "# # Extract a few results\n",
    "# result = {key: value for key, value in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d805d-dcbc-45b9-99a6-59716b1d5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c49f8-16d9-45da-90a0-dc68b65cca6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0e7a0-6116-4d16-aea7-3348ac47b4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ceaa0-e7a4-454f-be74-3e1cd6baf4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_small_agg = {}\n",
    "\n",
    "for k, v in results.items():\n",
    "    results_small_agg[k] = np.average(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1765ab-15cb-4cff-aa74-566cfb85d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_large_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cd254-8659-4579-96fc-0f0b782864fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_small_agg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
