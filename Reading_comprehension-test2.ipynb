{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b000096c-df2b-4e57-a2c3-aea36de1f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = \"/scratches/dialfs/alta/hln35/.cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratches/dialfs/alta/hln35/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b72a828-17e0-4250-975a-54818ea1b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = \"google/flan-t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae8529b-f7cc-4257-ac87-63306f38e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_points = load_dataset(\"boolq\", split='validation', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e774c2c-e10f-45fe-9a46-646d70ddb6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'passage'],\n",
       "    num_rows: 3270\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67a9537-626d-413a-8dbf-6edb9c2240c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'does ethanol take more energy make that produces',\n",
       " 'answer': False,\n",
       " 'passage': \"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643fd143-bed0-4dc2-8e0e-4dcbdb7203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"rule: for each question there is a passage in context. answer the question by saying the statement is True or False. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3bccc6f-6a68-425e-9a8d-8946eda42e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3550aadd-fef1-4ba0-aa3c-387d4535966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_small)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_small)\n",
    "model1 = AutoModel.from_pretrained(model_small)\n",
    "model2 = AutoModelForQuestionAnswering.from_pretrained(model_small)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a182b7a5-4ee8-41d8-b373-7c1c9c09b93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4f614b1-f1a6-43b0-a008-76fae7e71ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786d67c9-a693-4a12-b553-51c5a287583e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForQuestionAnswering(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f6e200-04c6-4327-acec-bf13b93ac77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"google/flan-t5-small\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 1024,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"gelu_new\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 8,\n",
       "  \"num_heads\": 6,\n",
       "  \"num_layers\": 8,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f0e44f-4187-4437-9425-d5d5c641fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebcd7019-7ca3-47c0-a3f0-20eff9491191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_right(input_ids, config):\n",
    "        decoder_start_token_id = config.decoder_start_token_id\n",
    "        pad_token_id = config.pad_token_id\n",
    "\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\n",
    "                \"self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id.\"\n",
    "                \"See T5 docs for more information.\"\n",
    "            )\n",
    "\n",
    "        # shift inputs to the right\n",
    "       \n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n",
    "        shifted_input_ids[..., 0] = decoder_start_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "        return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "056918e4-a45b-4740-8abe-d2afff0393b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = data_points[0][\"question\"]\n",
    "passage = data_points[0][\"passage\"]\n",
    "ans = \"\"\n",
    "text = prefix + \"context: \" + passage + \". question: \" + question\n",
    "ref = str(data_points[0][\"answer\"])\n",
    "inputs = tokenizer(text, max_length=max_input_length, truncation=True, return_tensors=\"pt\").input_ids\n",
    "preds_tokenized = model(inputs, decoder_input_ids=torch.tensor([[model.config.decoder_start_token_id, model.config.decoder_start_token_id]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3da5f325-6794-4680-805c-0785af045d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 323])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "279c6199-a0be-470e-9a7f-c379cf12b384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=None, logits=tensor([[[-53.1579,  -5.9468, -12.0900,  ..., -53.2189, -53.1536, -53.2408],\n",
       "         [-53.1579,  -5.9468, -12.0900,  ..., -53.2189, -53.1536, -53.2407]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-0.3246, -0.4472,  2.7203,  0.5027, -1.2017, -0.0500, -0.2239,\n",
       "           -0.8631,  0.4351, -1.9833, -0.0921,  0.9110,  2.4308,  0.5330,\n",
       "            0.5235,  0.8297, -0.4288, -0.5965, -0.4551,  0.9857,  0.2452,\n",
       "            0.8802,  1.4902,  0.2550,  1.1417,  0.5892,  0.5407,  0.1096,\n",
       "            0.3798,  0.9067,  0.8513, -3.0221, -0.2641, -0.1702, -0.6509,\n",
       "           -0.6096, -2.6451,  1.2174, -0.4624,  0.8654, -0.4411, -0.5461,\n",
       "           -0.3555,  0.3705, -0.3718,  0.8499, -1.7910,  0.4685,  0.0680,\n",
       "            0.1161, -0.0328,  0.2432,  1.0836, -0.3219,  0.6269, -0.0300,\n",
       "           -0.3030,  0.5362,  0.3154, -0.2068,  0.6602,  0.5856, -0.2654,\n",
       "            0.3955],\n",
       "          [-0.3246, -0.4472,  2.7203,  0.5027, -1.2017, -0.0500, -0.2239,\n",
       "           -0.8631,  0.4351, -1.9833, -0.0921,  0.9110,  2.4308,  0.5330,\n",
       "            0.5235,  0.8297, -0.4288, -0.5965, -0.4551,  0.9857,  0.2452,\n",
       "            0.8802,  1.4902,  0.2550,  1.1417,  0.5892,  0.5407,  0.1096,\n",
       "            0.3798,  0.9067,  0.8513, -3.0221, -0.2641, -0.1702, -0.6509,\n",
       "           -0.6096, -2.6451,  1.2174, -0.4624,  0.8654, -0.4411, -0.5461,\n",
       "           -0.3555,  0.3705, -0.3718,  0.8499, -1.7910,  0.4685,  0.0680,\n",
       "            0.1161, -0.0328,  0.2432,  1.0836, -0.3219,  0.6269, -0.0300,\n",
       "           -0.3030,  0.5362,  0.3154, -0.2068,  0.6602,  0.5856, -0.2654,\n",
       "            0.3955]],\n",
       "\n",
       "         [[ 1.7940,  3.4396,  2.5323, -3.8065,  0.5211, -0.5802, -0.4031,\n",
       "           -0.0840, -0.9955,  0.0640,  0.1742,  1.3589, -1.6423, -0.3898,\n",
       "            1.0158, -0.9212,  0.6736,  0.1128, -1.6973, -0.8054,  0.1269,\n",
       "           -0.2162,  1.0050, -0.5986,  1.9310, -3.1894,  0.0105, -0.6978,\n",
       "            1.3520, -0.0273,  0.7278,  0.4088, -0.0494, -1.0580, -0.8139,\n",
       "            0.7552,  0.2954, -0.7907,  0.3049, -0.0492, -3.3327, -0.2540,\n",
       "            0.6256,  1.5695,  0.0258, -0.7101, -1.7692,  0.9313, -3.4544,\n",
       "            0.0319, -0.2225, -0.8928,  0.9622,  0.2919, -0.9329,  2.9196,\n",
       "           -0.5204, -0.4929, -2.1080, -0.6635, -0.1174, -0.6185,  1.0407,\n",
       "            0.1142],\n",
       "          [ 1.7940,  3.4396,  2.5323, -3.8065,  0.5211, -0.5802, -0.4031,\n",
       "           -0.0840, -0.9955,  0.0640,  0.1742,  1.3589, -1.6423, -0.3898,\n",
       "            1.0158, -0.9212,  0.6736,  0.1128, -1.6973, -0.8054,  0.1269,\n",
       "           -0.2162,  1.0050, -0.5986,  1.9310, -3.1894,  0.0105, -0.6978,\n",
       "            1.3520, -0.0273,  0.7278,  0.4088, -0.0494, -1.0580, -0.8139,\n",
       "            0.7552,  0.2954, -0.7907,  0.3049, -0.0492, -3.3327, -0.2540,\n",
       "            0.6256,  1.5695,  0.0258, -0.7101, -1.7692,  0.9313, -3.4544,\n",
       "            0.0319, -0.2225, -0.8928,  0.9622,  0.2919, -0.9329,  2.9196,\n",
       "           -0.5204, -0.4929, -2.1080, -0.6635, -0.1174, -0.6185,  1.0407,\n",
       "            0.1142]],\n",
       "\n",
       "         [[-1.0024,  0.2849,  3.3928,  2.1399,  2.3892,  3.7610,  1.3921,\n",
       "            1.4513,  0.6572, -1.3842, -0.5293,  0.2958,  0.2217, -1.3988,\n",
       "           -1.2938,  0.1832,  0.3043, -0.5078, -1.6318,  0.1279,  1.5395,\n",
       "            0.1603,  3.3130, -0.6971,  0.9714, -1.6142,  1.0346,  0.4241,\n",
       "            0.3745,  1.7686, -1.1782, -2.4231, -1.7018, -1.1335, -0.2667,\n",
       "           -1.8313, -0.8979,  0.6376, -0.4386, -1.1621,  1.7887,  1.6119,\n",
       "            1.6943,  2.3260, -1.5136,  1.8502, -5.6108,  0.3269,  1.9077,\n",
       "           -0.0895, -0.1142, -1.2461, -0.5355,  1.9808, -0.6652, -0.3081,\n",
       "           -0.4544,  1.8522,  0.5637,  1.6063,  0.2405,  1.8777,  0.4605,\n",
       "           -0.8583],\n",
       "          [-1.0024,  0.2849,  3.3928,  2.1399,  2.3892,  3.7610,  1.3921,\n",
       "            1.4513,  0.6572, -1.3842, -0.5293,  0.2958,  0.2217, -1.3988,\n",
       "           -1.2938,  0.1832,  0.3043, -0.5078, -1.6318,  0.1279,  1.5395,\n",
       "            0.1603,  3.3130, -0.6971,  0.9714, -1.6142,  1.0346,  0.4241,\n",
       "            0.3745,  1.7686, -1.1782, -2.4231, -1.7018, -1.1335, -0.2667,\n",
       "           -1.8313, -0.8979,  0.6376, -0.4386, -1.1621,  1.7887,  1.6119,\n",
       "            1.6943,  2.3260, -1.5136,  1.8502, -5.6108,  0.3269,  1.9077,\n",
       "           -0.0895, -0.1142, -1.2461, -0.5355,  1.9808, -0.6652, -0.3081,\n",
       "           -0.4544,  1.8522,  0.5637,  1.6063,  0.2405,  1.8777,  0.4605,\n",
       "           -0.8583]],\n",
       "\n",
       "         [[-0.5735,  0.1323, -0.5761, -0.0898,  0.3345, -0.1876, -0.3911,\n",
       "            0.3169,  0.0114,  0.0955, -0.2468,  0.3325, -0.3596, -0.0613,\n",
       "            0.5860,  0.3586, -0.2335, -0.0656,  0.2875, -0.1697,  0.1448,\n",
       "            0.2461,  0.1811, -0.5666,  0.0915, -0.3139, -0.4238, -0.1308,\n",
       "            0.0885,  0.9670,  0.1156,  0.0738,  0.0506, -0.0500, -0.1556,\n",
       "           -0.5776, -0.1908,  0.7708, -0.9822, -0.8105, -0.3232, -0.7163,\n",
       "           -1.7973, -0.2083,  0.4273, -0.3119,  0.2105,  0.9463, -0.1546,\n",
       "            0.3375, -0.9528,  0.4822, -0.3644, -0.4830,  0.7044, -0.1103,\n",
       "           -0.4532,  0.2493, -0.2640,  0.3957,  0.0119,  0.3854, -0.6545,\n",
       "            1.5567],\n",
       "          [-0.5735,  0.1323, -0.5761, -0.0898,  0.3345, -0.1876, -0.3911,\n",
       "            0.3169,  0.0114,  0.0955, -0.2468,  0.3325, -0.3596, -0.0613,\n",
       "            0.5860,  0.3586, -0.2335, -0.0656,  0.2875, -0.1697,  0.1448,\n",
       "            0.2461,  0.1811, -0.5666,  0.0915, -0.3139, -0.4238, -0.1308,\n",
       "            0.0885,  0.9670,  0.1156,  0.0738,  0.0506, -0.0500, -0.1556,\n",
       "           -0.5776, -0.1908,  0.7708, -0.9822, -0.8105, -0.3232, -0.7163,\n",
       "           -1.7973, -0.2083,  0.4273, -0.3119,  0.2105,  0.9463, -0.1546,\n",
       "            0.3375, -0.9528,  0.4822, -0.3644, -0.4830,  0.7044, -0.1103,\n",
       "           -0.4532,  0.2493, -0.2640,  0.3957,  0.0119,  0.3854, -0.6545,\n",
       "            1.5567]],\n",
       "\n",
       "         [[ 0.6756, -0.1112, -0.0380, -0.0346,  0.9380,  1.5116, -0.0259,\n",
       "           -0.4907, -1.4945,  0.3333, -0.8646,  2.3173, -0.3749,  0.5611,\n",
       "           -0.3081,  0.6046, -0.0487, -0.5112, -0.0088,  0.0703,  1.4214,\n",
       "           -0.6741,  0.5313, -0.8915, -0.7273,  0.2326, -0.3916, -0.9822,\n",
       "           -0.2490,  0.3908, -0.5017, -0.1373, -0.9381, -0.2147, -0.5308,\n",
       "           -1.0357, -0.6126, -0.0509,  0.2863, -0.6941, -1.6422, -0.5873,\n",
       "            0.3969,  0.4470, -0.0675, -0.2113,  0.1656,  1.0848,  0.0369,\n",
       "           -0.5575,  0.6730, -0.6065,  0.3745,  0.0663,  1.1187, -0.3228,\n",
       "           -0.1324,  0.4137, -0.9805,  0.7807,  0.8052,  0.6142, -0.4693,\n",
       "            1.2707],\n",
       "          [ 0.6756, -0.1112, -0.0380, -0.0346,  0.9380,  1.5116, -0.0259,\n",
       "           -0.4907, -1.4945,  0.3333, -0.8646,  2.3173, -0.3749,  0.5611,\n",
       "           -0.3081,  0.6046, -0.0487, -0.5112, -0.0088,  0.0703,  1.4214,\n",
       "           -0.6741,  0.5313, -0.8915, -0.7273,  0.2326, -0.3916, -0.9822,\n",
       "           -0.2490,  0.3908, -0.5017, -0.1373, -0.9381, -0.2147, -0.5308,\n",
       "           -1.0357, -0.6126, -0.0509,  0.2863, -0.6941, -1.6422, -0.5873,\n",
       "            0.3969,  0.4470, -0.0675, -0.2113,  0.1656,  1.0848,  0.0369,\n",
       "           -0.5575,  0.6730, -0.6065,  0.3745,  0.0663,  1.1187, -0.3228,\n",
       "           -0.1324,  0.4137, -0.9805,  0.7807,  0.8052,  0.6142, -0.4693,\n",
       "            1.2707]],\n",
       "\n",
       "         [[ 3.2050, -0.2294,  0.0626, -2.4144, -1.0359, -0.5418,  0.0816,\n",
       "           -0.6485,  0.1262,  1.6957,  0.9766,  0.3147, -0.7715, -0.3507,\n",
       "           -0.2327,  0.9321,  0.6606, -1.6642, -0.6282,  1.8335,  1.9626,\n",
       "            0.9116,  0.1530, -0.0758,  0.2507, -0.3374,  1.0307, -3.4551,\n",
       "            0.0184,  0.1369, -1.7274, -0.7156,  1.3922,  0.3316,  1.8077,\n",
       "           -1.3152,  0.1062,  1.1151, -0.2882,  0.5495, -1.6037,  1.0997,\n",
       "            0.4848,  0.0616,  0.6270,  0.1277, -0.6391,  3.9684,  0.8059,\n",
       "           -0.1786,  0.2016,  2.0816,  0.6669, -2.5043, -0.2702,  1.2837,\n",
       "           -1.8220, -0.1749, -1.4111, -0.0287,  0.5189,  0.9404, -1.3660,\n",
       "            0.3040],\n",
       "          [ 3.2050, -0.2294,  0.0626, -2.4144, -1.0359, -0.5418,  0.0816,\n",
       "           -0.6485,  0.1262,  1.6957,  0.9766,  0.3147, -0.7715, -0.3507,\n",
       "           -0.2327,  0.9321,  0.6606, -1.6642, -0.6282,  1.8335,  1.9626,\n",
       "            0.9116,  0.1530, -0.0758,  0.2507, -0.3374,  1.0307, -3.4551,\n",
       "            0.0184,  0.1369, -1.7274, -0.7156,  1.3922,  0.3316,  1.8077,\n",
       "           -1.3152,  0.1062,  1.1151, -0.2882,  0.5495, -1.6037,  1.0997,\n",
       "            0.4848,  0.0616,  0.6270,  0.1277, -0.6391,  3.9684,  0.8059,\n",
       "           -0.1786,  0.2016,  2.0816,  0.6669, -2.5043, -0.2702,  1.2837,\n",
       "           -1.8220, -0.1749, -1.4111, -0.0287,  0.5189,  0.9404, -1.3660,\n",
       "            0.3040]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 2.1958e-01, -1.7011e-01, -2.6294e-01, -2.4049e-01, -4.9828e-03,\n",
       "           -1.2525e-01,  3.3598e-02,  9.5157e-03,  2.2565e-01,  9.2678e-02,\n",
       "            1.4103e-01, -6.4643e-03,  9.1129e-03, -4.2295e-02, -8.2258e-03,\n",
       "            2.0954e-02,  1.1105e-02,  1.1207e-01,  9.8265e-02,  1.1115e-01,\n",
       "           -3.5272e-02, -2.7524e-02, -5.5589e-02, -8.0482e-02, -1.8962e-01,\n",
       "           -4.0825e-02, -1.7354e-02,  1.1769e-01,  3.2031e-02,  4.8372e-04,\n",
       "            1.8169e-02,  2.0241e-01,  4.0996e-02, -4.0733e-02, -1.8231e-01,\n",
       "           -4.5302e-02,  2.2473e-01, -7.0577e-02,  4.5935e-02, -1.5453e-01,\n",
       "           -1.5262e-01,  1.8051e-01,  8.6664e-02, -5.9104e-03,  4.1934e-02,\n",
       "           -2.6907e-02,  6.1931e-02, -1.9023e-01,  1.7378e-01, -2.1772e-02,\n",
       "           -2.5190e-03,  1.2195e-01, -1.6491e-02,  2.3100e-01, -5.3832e-02,\n",
       "           -3.8661e-02, -6.3840e-02, -1.5077e-01,  3.3937e-01, -7.3196e-03,\n",
       "           -3.1595e-02, -1.8429e-02,  1.8027e-01, -4.8478e-02],\n",
       "          [ 2.1958e-01, -1.7011e-01, -2.6294e-01, -2.4049e-01, -4.9828e-03,\n",
       "           -1.2525e-01,  3.3598e-02,  9.5157e-03,  2.2565e-01,  9.2678e-02,\n",
       "            1.4103e-01, -6.4643e-03,  9.1129e-03, -4.2295e-02, -8.2258e-03,\n",
       "            2.0954e-02,  1.1105e-02,  1.1207e-01,  9.8265e-02,  1.1115e-01,\n",
       "           -3.5272e-02, -2.7524e-02, -5.5589e-02, -8.0482e-02, -1.8962e-01,\n",
       "           -4.0825e-02, -1.7354e-02,  1.1769e-01,  3.2031e-02,  4.8372e-04,\n",
       "            1.8169e-02,  2.0241e-01,  4.0996e-02, -4.0733e-02, -1.8231e-01,\n",
       "           -4.5302e-02,  2.2473e-01, -7.0577e-02,  4.5935e-02, -1.5453e-01,\n",
       "           -1.5262e-01,  1.8051e-01,  8.6664e-02, -5.9104e-03,  4.1934e-02,\n",
       "           -2.6907e-02,  6.1931e-02, -1.9023e-01,  1.7378e-01, -2.1772e-02,\n",
       "           -2.5190e-03,  1.2195e-01, -1.6491e-02,  2.3100e-01, -5.3832e-02,\n",
       "           -3.8661e-02, -6.3840e-02, -1.5077e-01,  3.3937e-01, -7.3196e-03,\n",
       "           -3.1595e-02, -1.8429e-02,  1.8027e-01, -4.8478e-02]],\n",
       "\n",
       "         [[-2.7322e-02, -5.1276e-02, -1.8018e-01,  2.3354e-01,  7.0110e-02,\n",
       "            1.3566e-01, -1.4869e-01,  5.1855e-02, -3.5065e-02, -1.2094e-02,\n",
       "           -1.2978e-01,  2.3859e-01, -9.2128e-03, -4.7366e-02, -1.2962e-01,\n",
       "            6.1978e-01, -1.0242e-01,  7.7652e-03, -1.5782e-01,  2.7901e-01,\n",
       "            7.3192e-03, -8.4250e-02, -1.4403e-03, -2.1260e-01, -4.9019e-02,\n",
       "           -8.6598e-02,  2.6275e-01, -2.1350e-02, -1.9727e-01, -1.2448e-02,\n",
       "            8.6310e-02, -1.0212e-02, -9.5992e-02,  8.1138e-02, -5.1485e-02,\n",
       "           -2.8666e-02, -4.6693e-02,  1.5774e-01,  6.1265e-02, -1.9394e-01,\n",
       "           -5.5863e-02,  5.1956e-02,  2.8264e-02,  6.4586e-02,  2.0451e-02,\n",
       "            4.5251e-02,  1.4673e-02, -1.8939e-01, -1.0174e-02, -1.1417e-01,\n",
       "            5.5890e-02,  3.1477e-01, -8.0180e-02, -3.9746e-02, -2.0402e-01,\n",
       "           -2.8908e-02,  5.9302e-02,  1.5482e-02,  2.0751e-01, -8.4006e-03,\n",
       "           -2.1856e-02,  1.9695e-01, -7.0471e-02,  6.9417e-02],\n",
       "          [-2.7322e-02, -5.1276e-02, -1.8018e-01,  2.3354e-01,  7.0110e-02,\n",
       "            1.3566e-01, -1.4869e-01,  5.1855e-02, -3.5065e-02, -1.2094e-02,\n",
       "           -1.2978e-01,  2.3859e-01, -9.2128e-03, -4.7366e-02, -1.2962e-01,\n",
       "            6.1978e-01, -1.0242e-01,  7.7652e-03, -1.5782e-01,  2.7901e-01,\n",
       "            7.3192e-03, -8.4250e-02, -1.4403e-03, -2.1260e-01, -4.9019e-02,\n",
       "           -8.6598e-02,  2.6275e-01, -2.1350e-02, -1.9727e-01, -1.2448e-02,\n",
       "            8.6310e-02, -1.0212e-02, -9.5992e-02,  8.1138e-02, -5.1485e-02,\n",
       "           -2.8666e-02, -4.6693e-02,  1.5774e-01,  6.1265e-02, -1.9394e-01,\n",
       "           -5.5863e-02,  5.1956e-02,  2.8264e-02,  6.4586e-02,  2.0451e-02,\n",
       "            4.5251e-02,  1.4673e-02, -1.8939e-01, -1.0174e-02, -1.1417e-01,\n",
       "            5.5890e-02,  3.1477e-01, -8.0180e-02, -3.9746e-02, -2.0402e-01,\n",
       "           -2.8908e-02,  5.9302e-02,  1.5482e-02,  2.0751e-01, -8.4006e-03,\n",
       "           -2.1856e-02,  1.9695e-01, -7.0471e-02,  6.9417e-02]],\n",
       "\n",
       "         [[-1.6162e-02,  1.1747e-01, -3.6647e-02, -1.6168e-02, -1.0973e-01,\n",
       "           -1.0933e-01,  1.7901e-01, -1.3146e-01,  1.2204e-01,  3.7292e-02,\n",
       "           -1.0871e-01,  3.7688e-02,  1.9268e-02,  7.1907e-02, -1.2550e-01,\n",
       "            1.1821e-01,  1.3909e-01, -1.9142e-02, -7.7033e-02,  2.4760e-02,\n",
       "           -8.3882e-02, -1.2735e-02,  1.0599e-01, -2.3832e-01, -3.3450e-02,\n",
       "            2.6648e-02,  1.7687e-01, -7.1091e-02, -9.3065e-02,  7.3148e-02,\n",
       "            2.1341e-01,  4.2844e-02, -2.3442e-02,  4.0387e-02,  1.8705e-01,\n",
       "            2.3893e-01, -3.8390e-02,  1.5139e-01,  2.1343e-01, -2.2498e-01,\n",
       "            3.2671e-02,  1.3170e-01, -1.1146e-01, -7.3228e-02, -1.4896e-01,\n",
       "            1.6645e-02,  1.9027e-01, -2.1043e-02, -1.0192e-01, -1.5966e-01,\n",
       "            2.2352e-01,  6.8636e-02, -1.4147e-01, -2.1583e-03, -1.3451e-01,\n",
       "           -5.8433e-02, -6.4204e-03,  1.8267e-01, -6.0213e-02,  5.3085e-02,\n",
       "           -5.9311e-02,  1.1459e-01, -7.5510e-02, -1.9113e-01],\n",
       "          [-1.6162e-02,  1.1747e-01, -3.6647e-02, -1.6168e-02, -1.0973e-01,\n",
       "           -1.0933e-01,  1.7901e-01, -1.3146e-01,  1.2204e-01,  3.7292e-02,\n",
       "           -1.0871e-01,  3.7688e-02,  1.9268e-02,  7.1907e-02, -1.2550e-01,\n",
       "            1.1821e-01,  1.3909e-01, -1.9142e-02, -7.7033e-02,  2.4760e-02,\n",
       "           -8.3882e-02, -1.2735e-02,  1.0599e-01, -2.3832e-01, -3.3450e-02,\n",
       "            2.6648e-02,  1.7687e-01, -7.1091e-02, -9.3065e-02,  7.3148e-02,\n",
       "            2.1341e-01,  4.2844e-02, -2.3442e-02,  4.0387e-02,  1.8705e-01,\n",
       "            2.3893e-01, -3.8390e-02,  1.5139e-01,  2.1343e-01, -2.2498e-01,\n",
       "            3.2671e-02,  1.3170e-01, -1.1146e-01, -7.3228e-02, -1.4896e-01,\n",
       "            1.6645e-02,  1.9027e-01, -2.1043e-02, -1.0192e-01, -1.5966e-01,\n",
       "            2.2352e-01,  6.8636e-02, -1.4147e-01, -2.1583e-03, -1.3451e-01,\n",
       "           -5.8433e-02, -6.4204e-03,  1.8267e-01, -6.0213e-02,  5.3085e-02,\n",
       "           -5.9311e-02,  1.1459e-01, -7.5510e-02, -1.9113e-01]],\n",
       "\n",
       "         [[-4.6946e-01, -1.9415e-01, -2.9450e-01,  1.2248e-01, -1.3669e-02,\n",
       "            4.4072e-01,  1.8613e-01, -1.7919e-01,  1.0360e-01, -5.2744e-01,\n",
       "           -7.2384e-01,  1.4806e-01,  4.5292e-01,  2.0063e-01,  3.0107e-01,\n",
       "           -2.2751e-01,  4.7949e-01,  2.2749e-01,  2.3872e-02,  1.2951e-01,\n",
       "           -5.3064e-01,  3.3716e-02, -2.6352e-01,  3.8725e-01,  2.7743e-01,\n",
       "           -5.7863e-02,  1.0998e-02,  3.1228e-01,  3.1324e-01, -6.5082e-01,\n",
       "            1.9706e-01,  6.7643e-03,  9.5229e-02, -3.9655e-01, -3.0566e-01,\n",
       "           -4.6201e-01,  4.4252e-01, -3.3148e-01, -6.3273e-02,  8.3461e-01,\n",
       "           -2.3809e-01, -6.5207e-02, -2.8409e-02, -8.6271e-02,  3.6755e-01,\n",
       "           -4.6654e-01, -2.7550e-02, -8.7285e-02, -7.2351e-02,  1.3752e-01,\n",
       "           -2.3101e-01, -2.1725e-01, -5.5341e-02, -3.9264e-01, -4.7466e-01,\n",
       "            1.9322e-01, -3.6306e-02, -1.0816e-01, -2.5755e-01,  3.5348e-01,\n",
       "            1.7464e-01, -2.4148e-01, -8.2528e-01,  3.0508e-02],\n",
       "          [-4.6946e-01, -1.9415e-01, -2.9450e-01,  1.2248e-01, -1.3669e-02,\n",
       "            4.4072e-01,  1.8613e-01, -1.7919e-01,  1.0360e-01, -5.2744e-01,\n",
       "           -7.2384e-01,  1.4806e-01,  4.5292e-01,  2.0063e-01,  3.0107e-01,\n",
       "           -2.2751e-01,  4.7949e-01,  2.2749e-01,  2.3872e-02,  1.2951e-01,\n",
       "           -5.3064e-01,  3.3716e-02, -2.6352e-01,  3.8725e-01,  2.7743e-01,\n",
       "           -5.7863e-02,  1.0998e-02,  3.1228e-01,  3.1324e-01, -6.5082e-01,\n",
       "            1.9706e-01,  6.7643e-03,  9.5229e-02, -3.9655e-01, -3.0566e-01,\n",
       "           -4.6201e-01,  4.4252e-01, -3.3148e-01, -6.3273e-02,  8.3461e-01,\n",
       "           -2.3809e-01, -6.5207e-02, -2.8409e-02, -8.6271e-02,  3.6755e-01,\n",
       "           -4.6654e-01, -2.7550e-02, -8.7285e-02, -7.2351e-02,  1.3752e-01,\n",
       "           -2.3101e-01, -2.1725e-01, -5.5341e-02, -3.9264e-01, -4.7466e-01,\n",
       "            1.9322e-01, -3.6306e-02, -1.0816e-01, -2.5755e-01,  3.5348e-01,\n",
       "            1.7464e-01, -2.4148e-01, -8.2528e-01,  3.0508e-02]],\n",
       "\n",
       "         [[-5.3423e-01, -1.7002e-01,  1.2753e-01, -5.8113e-01, -1.0750e-02,\n",
       "           -4.3099e-01, -4.5976e-01,  1.0451e-01, -1.4804e-02, -2.0715e-01,\n",
       "           -4.7965e-01,  4.7444e-01,  7.4396e-01, -4.9733e-01, -3.8015e-02,\n",
       "           -5.9638e-01,  7.8221e-02,  3.4680e-01,  4.4866e-02, -1.6992e-01,\n",
       "            5.4343e-02,  2.7237e-01,  1.0183e-01, -1.2996e-01, -4.6016e-01,\n",
       "           -3.4960e-01,  1.0293e-01,  1.6619e-01,  5.0241e-01, -7.2063e-02,\n",
       "           -1.2205e-01,  1.4827e-01, -1.0830e+00, -1.4735e-01, -1.1291e-01,\n",
       "            2.3449e-01, -1.8280e-01, -2.0237e-01,  7.7345e-01, -8.0872e-01,\n",
       "           -9.1295e-02,  1.8482e-02,  1.3434e-01, -3.5186e-01, -6.4619e-02,\n",
       "           -1.4866e-01, -1.3210e-01, -1.0860e-01,  3.3949e-01, -8.5704e-02,\n",
       "           -1.2350e-01,  1.2492e+00,  4.2472e-01,  2.4947e-01,  2.1004e-01,\n",
       "           -2.7007e-01, -3.6349e-01, -8.8679e-02,  3.3781e-01,  1.8994e-02,\n",
       "           -2.6022e-01,  2.6583e-02,  4.5797e-01,  1.2104e-01],\n",
       "          [-5.3423e-01, -1.7002e-01,  1.2753e-01, -5.8113e-01, -1.0750e-02,\n",
       "           -4.3099e-01, -4.5976e-01,  1.0451e-01, -1.4804e-02, -2.0715e-01,\n",
       "           -4.7965e-01,  4.7444e-01,  7.4396e-01, -4.9733e-01, -3.8015e-02,\n",
       "           -5.9638e-01,  7.8221e-02,  3.4680e-01,  4.4866e-02, -1.6992e-01,\n",
       "            5.4343e-02,  2.7237e-01,  1.0183e-01, -1.2996e-01, -4.6016e-01,\n",
       "           -3.4960e-01,  1.0293e-01,  1.6619e-01,  5.0241e-01, -7.2063e-02,\n",
       "           -1.2205e-01,  1.4827e-01, -1.0830e+00, -1.4735e-01, -1.1291e-01,\n",
       "            2.3449e-01, -1.8280e-01, -2.0237e-01,  7.7345e-01, -8.0872e-01,\n",
       "           -9.1295e-02,  1.8482e-02,  1.3434e-01, -3.5186e-01, -6.4619e-02,\n",
       "           -1.4866e-01, -1.3210e-01, -1.0860e-01,  3.3949e-01, -8.5704e-02,\n",
       "           -1.2350e-01,  1.2492e+00,  4.2472e-01,  2.4947e-01,  2.1004e-01,\n",
       "           -2.7007e-01, -3.6349e-01, -8.8679e-02,  3.3781e-01,  1.8994e-02,\n",
       "           -2.6022e-01,  2.6583e-02,  4.5797e-01,  1.2104e-01]],\n",
       "\n",
       "         [[ 3.6880e-02, -5.2449e-02, -1.2241e-01,  1.0731e-01, -8.5449e-02,\n",
       "           -2.2003e-01, -3.5495e-01, -5.0868e-01, -1.1845e-01,  4.9382e-01,\n",
       "           -1.8595e-01, -2.8593e-01, -2.6435e-01,  1.3214e-01,  1.0929e-01,\n",
       "            1.8525e-02, -3.1369e-02,  2.0011e-01,  1.2489e-01,  3.3244e-01,\n",
       "            1.5847e-01,  2.1202e-01,  4.1648e-01, -2.1404e-01, -3.6304e-01,\n",
       "            1.1607e-01, -3.4853e-02,  2.1132e-02, -1.3115e-02,  1.3639e-01,\n",
       "            9.7268e-02,  1.1957e-01,  8.1182e-02, -6.0265e-02, -7.3256e-03,\n",
       "            3.5794e-01, -1.2102e-01,  2.0002e-01, -6.2676e-01, -9.2556e-02,\n",
       "            6.8429e-02, -3.8805e-01,  4.9307e-01,  8.0502e-02, -2.4613e-01,\n",
       "           -1.2944e-01,  1.9093e-01, -2.2714e-01,  3.1962e-01,  6.0214e-02,\n",
       "           -1.1978e-01, -4.5091e-02,  1.4420e-01,  2.1505e-01,  1.3663e-01,\n",
       "            2.9976e-02,  5.3032e-02, -2.5806e-01, -1.5480e-01, -2.2982e-01,\n",
       "           -4.3043e-01,  7.0036e-02,  3.0345e-01,  7.9527e-02],\n",
       "          [ 3.6880e-02, -5.2449e-02, -1.2241e-01,  1.0731e-01, -8.5449e-02,\n",
       "           -2.2003e-01, -3.5495e-01, -5.0868e-01, -1.1845e-01,  4.9382e-01,\n",
       "           -1.8595e-01, -2.8593e-01, -2.6435e-01,  1.3214e-01,  1.0929e-01,\n",
       "            1.8525e-02, -3.1369e-02,  2.0011e-01,  1.2489e-01,  3.3244e-01,\n",
       "            1.5847e-01,  2.1202e-01,  4.1648e-01, -2.1404e-01, -3.6304e-01,\n",
       "            1.1607e-01, -3.4853e-02,  2.1132e-02, -1.3115e-02,  1.3639e-01,\n",
       "            9.7268e-02,  1.1957e-01,  8.1182e-02, -6.0265e-02, -7.3256e-03,\n",
       "            3.5794e-01, -1.2102e-01,  2.0002e-01, -6.2676e-01, -9.2556e-02,\n",
       "            6.8429e-02, -3.8805e-01,  4.9307e-01,  8.0502e-02, -2.4613e-01,\n",
       "           -1.2944e-01,  1.9093e-01, -2.2714e-01,  3.1962e-01,  6.0214e-02,\n",
       "           -1.1978e-01, -4.5091e-02,  1.4420e-01,  2.1505e-01,  1.3663e-01,\n",
       "            2.9976e-02,  5.3032e-02, -2.5806e-01, -1.5480e-01, -2.2982e-01,\n",
       "           -4.3043e-01,  7.0036e-02,  3.0345e-01,  7.9527e-02]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 2.6098, -0.8737, -0.1386,  ...,  2.5829,  1.3195, -1.2119],\n",
       "          [ 0.0924,  2.3606, -0.8606,  ...,  0.0472, -1.0429,  1.1658],\n",
       "          [ 1.2339,  2.1273,  0.2277,  ...,  0.3364, -0.3502,  2.2906],\n",
       "          ...,\n",
       "          [-0.2513, -0.1337, -0.3198,  ..., -1.3663, -0.4982,  1.0709],\n",
       "          [-0.0741, -0.1597, -0.5983,  ..., -1.8909, -1.2481,  0.6339],\n",
       "          [-0.0550, -0.1323, -0.3104,  ..., -0.1529, -0.1026, -0.2600]],\n",
       "\n",
       "         [[-1.0659,  2.8615,  3.2723,  ..., -1.8712,  1.0196, -1.4468],\n",
       "          [-0.2003,  2.4289,  3.4815,  ..., -0.4060,  0.6322, -1.3990],\n",
       "          [-1.3662,  0.5845,  3.9931,  ...,  0.0278,  0.2851, -0.2689],\n",
       "          ...,\n",
       "          [ 1.2202, -1.5738,  3.4746,  ..., -0.2827, -2.9887,  0.1603],\n",
       "          [ 1.1145, -0.2182,  0.4383,  ..., -0.3009, -2.5790,  1.3157],\n",
       "          [ 0.3552,  0.2303, -0.7742,  ..., -0.1399, -0.4126,  0.2003]],\n",
       "\n",
       "         [[-1.1248, -1.8294, -2.6503,  ...,  3.1928,  0.0267,  2.3728],\n",
       "          [ 0.7511,  0.7116,  1.2161,  ...,  0.7780, -1.9883,  0.3623],\n",
       "          [ 2.8634, -0.0174,  1.4677,  ..., -2.2345, -1.9899, -1.0678],\n",
       "          ...,\n",
       "          [ 1.9823,  2.0229,  0.1454,  ..., -0.6618, -1.8226, -0.1803],\n",
       "          [-0.2685,  0.2682, -1.7654,  ..., -2.2225,  0.2620,  0.4349],\n",
       "          [ 0.1152,  0.1541,  0.1964,  ...,  0.1865, -0.1332, -0.0529]],\n",
       "\n",
       "         [[-2.4924, -0.2472, -1.4766,  ..., -0.1396, -0.2304, -0.2160],\n",
       "          [-0.7450,  0.7337,  0.1190,  ...,  0.4157, -0.6710,  0.7735],\n",
       "          [-0.1098,  0.7872,  0.6169,  ...,  1.2638,  0.4944, -1.3132],\n",
       "          ...,\n",
       "          [-0.4770, -0.9953,  0.7930,  ..., -0.4716, -0.7395,  0.3116],\n",
       "          [ 1.3201, -0.7382,  1.0169,  ...,  0.5641, -1.4416,  0.0371],\n",
       "          [-0.2969, -0.2775, -0.0969,  ..., -0.0367,  0.0267, -0.2523]],\n",
       "\n",
       "         [[ 1.4074,  0.2762, -0.2981,  ...,  0.1790,  2.0073,  0.5783],\n",
       "          [ 1.7507,  1.3919,  4.1846,  ...,  0.7433,  2.5401,  0.3709],\n",
       "          [ 0.0429,  1.2032,  2.1281,  ...,  0.2320,  1.6995,  0.5714],\n",
       "          ...,\n",
       "          [ 0.9451,  0.5103,  2.2757,  ..., -4.1175, -0.3083, -0.0847],\n",
       "          [-0.3808, -1.1952,  4.5359,  ..., -3.2353,  0.7894,  0.9594],\n",
       "          [-0.0321, -0.3180, -0.3928,  ..., -0.1297, -0.3176,  0.0925]],\n",
       "\n",
       "         [[-0.9568,  1.6108,  1.8628,  ..., -0.8545,  3.1236,  0.7623],\n",
       "          [-1.9490,  1.1084,  1.5226,  ..., -0.0103,  1.1153, -0.3322],\n",
       "          [-1.8990,  0.4687,  0.7045,  ...,  0.0833,  1.9977,  0.9703],\n",
       "          ...,\n",
       "          [-2.2986,  0.4046, -0.1767,  ..., -0.3182,  0.2215,  0.9036],\n",
       "          [-1.4409,  0.0800,  0.1864,  ...,  0.6671,  0.4886,  1.7599],\n",
       "          [ 0.1345, -0.5135, -0.0823,  ...,  0.1828, -0.1791, -0.3050]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 7.3555e-01,  1.0041e+00, -7.8711e-01,  ..., -8.7236e-01,\n",
       "            2.3521e+00,  8.3965e-02],\n",
       "          [ 1.2536e+00, -7.4338e-01, -2.1818e-01,  ..., -2.5113e-01,\n",
       "            2.0683e+00, -6.1343e-01],\n",
       "          [ 1.6075e+00, -4.3147e-01, -2.6455e-01,  ..., -8.4679e-01,\n",
       "            1.0059e+00,  4.7713e-01],\n",
       "          ...,\n",
       "          [ 2.2125e+00, -1.9394e-03,  9.8689e-01,  ..., -2.0842e+00,\n",
       "           -1.9856e-01,  3.4854e-01],\n",
       "          [-1.2362e+00,  4.2987e-01,  8.6372e-01,  ..., -1.9793e+00,\n",
       "           -3.2069e-01,  7.3141e-01],\n",
       "          [ 1.2405e-01,  3.2714e-02,  1.3890e-01,  ...,  5.1952e-02,\n",
       "           -5.8639e-02, -8.4271e-02]],\n",
       "\n",
       "         [[-1.5159e+00, -1.3564e+00,  2.1399e+00,  ...,  5.0916e-01,\n",
       "           -3.1968e-01, -7.0725e-01],\n",
       "          [-7.4540e-01,  1.0940e-01,  3.2791e+00,  ...,  1.6597e-02,\n",
       "            5.9681e-01, -9.7504e-01],\n",
       "          [-6.5847e-01, -4.5532e-01,  2.7582e+00,  ..., -4.3175e-02,\n",
       "           -5.2742e-01, -1.1991e+00],\n",
       "          ...,\n",
       "          [ 2.1597e-01, -1.1603e+00,  8.3252e-01,  ..., -1.1669e+00,\n",
       "            5.4151e-01, -6.5989e-01],\n",
       "          [ 2.1089e+00, -3.8452e-01,  2.5784e+00,  ..., -6.8648e-01,\n",
       "            3.1850e-01, -1.3521e+00],\n",
       "          [-7.2441e-02,  2.0761e-01, -2.0945e-01,  ..., -1.5434e-01,\n",
       "            3.8864e-02,  2.1398e-01]],\n",
       "\n",
       "         [[ 1.2790e+00, -1.5646e+00, -4.2719e-03,  ...,  8.5564e-01,\n",
       "           -2.3524e-01, -1.4547e+00],\n",
       "          [ 1.6442e-01, -2.3147e-01, -7.0356e-01,  ...,  1.2079e+00,\n",
       "            1.6479e+00, -5.4780e-01],\n",
       "          [ 2.7770e-02, -2.4318e-01, -5.6507e-01,  ...,  1.3178e-01,\n",
       "            7.4118e-01, -7.8917e-01],\n",
       "          ...,\n",
       "          [-7.4311e-01, -5.9009e-02, -7.0967e-01,  ..., -3.6058e-01,\n",
       "           -2.3471e-01, -1.5551e+00],\n",
       "          [ 3.4838e-01, -1.5596e-01,  1.3360e+00,  ..., -2.0268e+00,\n",
       "            1.9929e+00, -1.0964e+00],\n",
       "          [-5.4587e-02, -2.2482e-02,  3.3090e-02,  ..., -6.3935e-02,\n",
       "            1.9678e-01, -8.9003e-02]],\n",
       "\n",
       "         [[-1.3200e+00, -3.9994e-01, -1.0139e+00,  ..., -1.9132e+00,\n",
       "            1.7586e+00, -1.4070e-01],\n",
       "          [-3.0271e-01, -1.1783e+00,  4.3669e-01,  ...,  1.2362e+00,\n",
       "           -4.2322e-01,  2.6661e-02],\n",
       "          [ 1.7279e+00, -7.5868e-01, -7.9883e-01,  ...,  1.3519e+00,\n",
       "           -5.2508e-01, -4.4949e-01],\n",
       "          ...,\n",
       "          [-2.1373e-01, -7.3129e-01,  1.5927e-01,  ...,  1.8972e-01,\n",
       "           -9.1948e-01,  5.4238e-01],\n",
       "          [-3.5849e-01, -5.4636e-01, -7.9576e-01,  ..., -8.4020e-01,\n",
       "            1.7218e+00, -9.8159e-01],\n",
       "          [-5.9207e-02,  2.1989e-02, -6.7085e-02,  ...,  2.1515e-01,\n",
       "           -1.9088e-01,  4.9420e-02]],\n",
       "\n",
       "         [[-8.4722e-01, -7.7108e-01,  2.1572e+00,  ..., -1.9540e+00,\n",
       "           -1.0267e+00, -1.8731e+00],\n",
       "          [-9.1575e-02,  2.6777e-01,  8.0013e-01,  ..., -2.3446e-01,\n",
       "           -3.2595e-01,  4.1047e-02],\n",
       "          [-8.0338e-01,  8.5072e-01, -7.0057e-01,  ...,  1.3558e-01,\n",
       "           -1.7705e+00, -1.6635e+00],\n",
       "          ...,\n",
       "          [ 1.9310e+00,  5.5285e-01,  1.6462e+00,  ...,  1.9048e+00,\n",
       "            1.3659e-01,  1.2047e+00],\n",
       "          [-7.0218e-01,  2.3160e+00,  8.9134e-01,  ...,  4.6585e-01,\n",
       "            2.0640e+00,  3.0354e-01],\n",
       "          [-1.8221e-02,  6.6725e-02,  3.1118e-02,  ...,  1.3294e-02,\n",
       "            1.3338e-02, -6.6606e-02]],\n",
       "\n",
       "         [[-1.7129e+00, -1.5060e-01,  1.1518e-01,  ..., -3.9093e-01,\n",
       "            9.2839e-01,  1.0215e+00],\n",
       "          [ 3.9962e-01, -3.2658e-01,  2.3499e-01,  ..., -8.4723e-02,\n",
       "            1.2921e+00,  2.3730e-01],\n",
       "          [-1.6507e+00,  3.9327e-01, -8.7114e-03,  ..., -1.0877e-01,\n",
       "            2.2351e-01,  8.8878e-01],\n",
       "          ...,\n",
       "          [-6.9189e-01, -4.2464e-01,  2.8251e-01,  ...,  1.0019e+00,\n",
       "           -5.3648e-02,  1.1180e+00],\n",
       "          [ 7.5527e-01,  9.9451e-01, -7.1037e-01,  ...,  2.3904e+00,\n",
       "           -1.6583e+00,  8.1721e-02],\n",
       "          [-2.6630e-02, -8.1376e-03,  5.7391e-02,  ...,  5.0803e-02,\n",
       "            3.6095e-01, -3.8617e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.8853e-01,  5.2214e-01, -6.8147e-01,  2.5426e-01, -6.1023e-01,\n",
       "            3.1257e-01,  6.3274e-02,  4.5816e-01, -2.9016e-01, -1.1062e-01,\n",
       "            3.5397e-02,  1.2020e-01,  2.9260e-01, -1.0787e-01, -8.1662e-01,\n",
       "            7.7995e-01, -4.6260e-01,  1.0074e+00,  1.0293e+00,  1.4994e-01,\n",
       "           -3.7064e-01, -2.5465e-04, -2.4791e-01,  2.2161e-01, -4.9824e-01,\n",
       "           -9.4153e-01, -1.3805e-01, -8.5721e-02, -1.0912e+00, -2.1952e-01,\n",
       "           -8.3314e-02, -3.2156e-01, -3.5229e-01,  7.2678e-01, -2.4872e-01,\n",
       "            2.8805e-01,  1.4078e+00,  1.3160e+00,  3.2739e-01,  8.0820e-02,\n",
       "           -2.8107e-01,  1.2428e+00, -1.7243e-01, -1.2390e-01, -1.2697e-01,\n",
       "            2.8102e-01,  1.7755e-01,  4.6345e-01, -5.9436e-01, -1.1964e-01,\n",
       "            7.1959e-02,  1.3343e-01, -4.8851e-02,  4.3673e-01, -4.7871e-01,\n",
       "            7.3165e-01, -4.5741e-01,  5.1645e-01, -3.3022e-01, -3.7082e-01,\n",
       "           -1.5566e-01,  4.0896e-01,  1.1079e-01, -1.5034e+00],\n",
       "          [ 1.8853e-01,  5.2214e-01, -6.8147e-01,  2.5426e-01, -6.1023e-01,\n",
       "            3.1257e-01,  6.3274e-02,  4.5816e-01, -2.9016e-01, -1.1062e-01,\n",
       "            3.5397e-02,  1.2020e-01,  2.9260e-01, -1.0787e-01, -8.1662e-01,\n",
       "            7.7995e-01, -4.6260e-01,  1.0074e+00,  1.0293e+00,  1.4994e-01,\n",
       "           -3.7064e-01, -2.5464e-04, -2.4791e-01,  2.2161e-01, -4.9824e-01,\n",
       "           -9.4153e-01, -1.3805e-01, -8.5721e-02, -1.0912e+00, -2.1952e-01,\n",
       "           -8.3314e-02, -3.2156e-01, -3.5229e-01,  7.2678e-01, -2.4872e-01,\n",
       "            2.8805e-01,  1.4078e+00,  1.3160e+00,  3.2739e-01,  8.0820e-02,\n",
       "           -2.8107e-01,  1.2428e+00, -1.7243e-01, -1.2390e-01, -1.2697e-01,\n",
       "            2.8102e-01,  1.7755e-01,  4.6345e-01, -5.9436e-01, -1.1964e-01,\n",
       "            7.1959e-02,  1.3343e-01, -4.8851e-02,  4.3673e-01, -4.7871e-01,\n",
       "            7.3165e-01, -4.5741e-01,  5.1645e-01, -3.3022e-01, -3.7082e-01,\n",
       "           -1.5566e-01,  4.0896e-01,  1.1079e-01, -1.5034e+00]],\n",
       "\n",
       "         [[ 1.2269e+00,  1.5384e-01,  5.0477e-02, -4.2591e-01,  1.1485e-01,\n",
       "           -3.5910e-01, -5.2594e-01,  5.2912e-01,  2.7173e-01,  3.3209e-01,\n",
       "           -5.0818e-01,  6.1385e-01, -9.2385e-02, -2.4906e-01, -2.9607e-01,\n",
       "           -1.2667e+00, -1.2753e-01,  3.0116e-01,  5.0761e-02,  1.1775e-02,\n",
       "            1.7841e-01, -2.9607e-01,  2.3434e-01,  3.8523e-01, -1.6994e-01,\n",
       "           -5.2938e-01, -1.0753e+00,  7.7872e-01, -2.6460e-01,  3.7477e-02,\n",
       "           -4.2261e-01,  8.0999e-01, -3.1661e-01, -6.2811e-01, -2.7442e-01,\n",
       "            2.3592e-02, -1.2734e-01, -1.9544e-01,  3.8094e-01, -6.3454e-02,\n",
       "            7.8148e-01,  2.2817e-01,  2.2718e-01,  5.5741e-02,  5.3309e-02,\n",
       "           -3.9008e-01, -1.0101e+00, -1.3697e-02,  5.3454e-01, -8.4421e-01,\n",
       "           -3.4690e-01, -6.9813e-01, -1.0830e+00,  1.0986e+00,  3.7688e-02,\n",
       "           -2.9959e-01,  5.0553e-01, -3.6366e-01,  4.4768e-01,  1.5970e-01,\n",
       "            1.1734e-01,  2.4993e-01,  1.4114e-01,  1.4064e-01],\n",
       "          [ 1.2269e+00,  1.5384e-01,  5.0477e-02, -4.2591e-01,  1.1485e-01,\n",
       "           -3.5910e-01, -5.2594e-01,  5.2912e-01,  2.7173e-01,  3.3209e-01,\n",
       "           -5.0818e-01,  6.1385e-01, -9.2385e-02, -2.4906e-01, -2.9607e-01,\n",
       "           -1.2667e+00, -1.2753e-01,  3.0116e-01,  5.0761e-02,  1.1775e-02,\n",
       "            1.7841e-01, -2.9607e-01,  2.3434e-01,  3.8523e-01, -1.6994e-01,\n",
       "           -5.2938e-01, -1.0753e+00,  7.7872e-01, -2.6460e-01,  3.7477e-02,\n",
       "           -4.2261e-01,  8.0999e-01, -3.1661e-01, -6.2811e-01, -2.7442e-01,\n",
       "            2.3592e-02, -1.2734e-01, -1.9544e-01,  3.8094e-01, -6.3454e-02,\n",
       "            7.8148e-01,  2.2817e-01,  2.2718e-01,  5.5741e-02,  5.3309e-02,\n",
       "           -3.9008e-01, -1.0101e+00, -1.3697e-02,  5.3454e-01, -8.4421e-01,\n",
       "           -3.4690e-01, -6.9813e-01, -1.0830e+00,  1.0986e+00,  3.7688e-02,\n",
       "           -2.9959e-01,  5.0553e-01, -3.6366e-01,  4.4768e-01,  1.5970e-01,\n",
       "            1.1734e-01,  2.4993e-01,  1.4114e-01,  1.4064e-01]],\n",
       "\n",
       "         [[ 8.2628e-01,  4.1142e-01, -5.6425e-01, -1.0982e+00,  4.0328e-02,\n",
       "            3.7163e-01,  8.5920e-01, -3.8783e-01, -1.7708e+00, -6.6892e-01,\n",
       "            1.6122e+00,  8.6357e-01, -9.0922e-01, -1.7722e-01,  1.7336e-01,\n",
       "           -9.5757e-02,  1.5617e-01,  1.1529e+00, -6.0161e-02,  1.6843e-01,\n",
       "            8.1768e-01, -2.7896e-01, -1.0713e+00, -1.8068e-01,  6.9782e-01,\n",
       "            5.5508e-02,  1.6176e+00,  1.9029e+00, -1.2058e+00, -8.1707e-01,\n",
       "           -3.8878e-01, -1.3236e+00, -5.1514e-01, -9.2891e-01,  2.5418e-01,\n",
       "           -4.9743e-02, -7.9396e-01,  4.9868e-01,  4.0585e-01, -1.8729e-01,\n",
       "            1.8782e-01, -9.7056e-01,  4.4984e-01, -2.4871e-01, -2.7704e-01,\n",
       "           -1.9447e-01,  4.3875e-01, -2.6549e-01,  1.3001e-01,  5.2513e-01,\n",
       "           -3.1329e-02,  7.1451e-01,  6.4623e-01, -6.4753e-02,  1.0855e+00,\n",
       "            3.9122e-02,  1.3668e+00,  6.6285e-01,  8.6888e-01, -1.3297e+00,\n",
       "           -6.3220e-01, -7.4722e-01,  1.7570e+00, -2.5809e-01],\n",
       "          [ 8.2628e-01,  4.1142e-01, -5.6425e-01, -1.0982e+00,  4.0328e-02,\n",
       "            3.7163e-01,  8.5920e-01, -3.8783e-01, -1.7708e+00, -6.6892e-01,\n",
       "            1.6122e+00,  8.6357e-01, -9.0922e-01, -1.7722e-01,  1.7336e-01,\n",
       "           -9.5757e-02,  1.5617e-01,  1.1529e+00, -6.0161e-02,  1.6843e-01,\n",
       "            8.1768e-01, -2.7896e-01, -1.0713e+00, -1.8068e-01,  6.9782e-01,\n",
       "            5.5508e-02,  1.6176e+00,  1.9029e+00, -1.2058e+00, -8.1707e-01,\n",
       "           -3.8878e-01, -1.3236e+00, -5.1514e-01, -9.2891e-01,  2.5418e-01,\n",
       "           -4.9743e-02, -7.9396e-01,  4.9868e-01,  4.0585e-01, -1.8729e-01,\n",
       "            1.8782e-01, -9.7056e-01,  4.4984e-01, -2.4871e-01, -2.7704e-01,\n",
       "           -1.9447e-01,  4.3875e-01, -2.6549e-01,  1.3001e-01,  5.2513e-01,\n",
       "           -3.1329e-02,  7.1451e-01,  6.4623e-01, -6.4753e-02,  1.0855e+00,\n",
       "            3.9122e-02,  1.3668e+00,  6.6285e-01,  8.6888e-01, -1.3297e+00,\n",
       "           -6.3220e-01, -7.4722e-01,  1.7570e+00, -2.5809e-01]],\n",
       "\n",
       "         [[ 7.9142e-01,  1.4674e+00, -7.2448e-01,  6.3015e-01,  1.0020e-01,\n",
       "            2.3758e-02,  5.1480e-02, -1.8547e-01,  3.9026e-01,  2.0347e-01,\n",
       "           -7.4337e-02, -2.2743e-01,  1.1173e+00,  1.4547e-01, -3.7484e-01,\n",
       "           -3.2423e-01,  2.2071e-01, -2.7587e-01, -9.0280e-01,  8.6946e-02,\n",
       "           -3.9148e-01,  7.1893e-01,  5.9955e-01, -9.3348e-01, -7.5243e-01,\n",
       "           -1.5265e-01,  4.6024e-01,  1.2918e-01,  3.6952e-01,  4.6855e-01,\n",
       "            8.2064e-02, -3.1547e-02, -2.3474e-01, -7.7468e-01,  5.7462e-01,\n",
       "            4.3827e-01,  3.2703e-01, -3.6279e-02, -6.9552e-01, -2.2048e-01,\n",
       "           -3.1501e-01,  6.6227e-01,  8.9386e-01, -6.9075e-01,  1.9320e-01,\n",
       "           -5.7559e-01,  4.6515e-01, -3.3302e-01,  2.9123e-01, -3.0970e-01,\n",
       "           -3.7564e-01, -2.0406e-01, -1.4532e+00,  5.0301e-01,  6.2178e-01,\n",
       "           -2.0789e-01, -1.1446e+00,  4.5478e-02, -4.5047e-01, -3.4761e-02,\n",
       "           -3.8237e-01, -3.3576e-01, -4.1737e-01,  1.6891e-01],\n",
       "          [ 7.9142e-01,  1.4674e+00, -7.2448e-01,  6.3015e-01,  1.0020e-01,\n",
       "            2.3758e-02,  5.1480e-02, -1.8547e-01,  3.9026e-01,  2.0347e-01,\n",
       "           -7.4337e-02, -2.2743e-01,  1.1173e+00,  1.4547e-01, -3.7484e-01,\n",
       "           -3.2423e-01,  2.2071e-01, -2.7587e-01, -9.0280e-01,  8.6946e-02,\n",
       "           -3.9148e-01,  7.1893e-01,  5.9955e-01, -9.3348e-01, -7.5243e-01,\n",
       "           -1.5265e-01,  4.6024e-01,  1.2918e-01,  3.6952e-01,  4.6855e-01,\n",
       "            8.2064e-02, -3.1547e-02, -2.3474e-01, -7.7468e-01,  5.7462e-01,\n",
       "            4.3827e-01,  3.2703e-01, -3.6279e-02, -6.9552e-01, -2.2048e-01,\n",
       "           -3.1501e-01,  6.6227e-01,  8.9386e-01, -6.9075e-01,  1.9320e-01,\n",
       "           -5.7559e-01,  4.6515e-01, -3.3302e-01,  2.9123e-01, -3.0970e-01,\n",
       "           -3.7564e-01, -2.0406e-01, -1.4532e+00,  5.0301e-01,  6.2178e-01,\n",
       "           -2.0789e-01, -1.1446e+00,  4.5478e-02, -4.5047e-01, -3.4761e-02,\n",
       "           -3.8237e-01, -3.3576e-01, -4.1737e-01,  1.6891e-01]],\n",
       "\n",
       "         [[-1.3408e-01, -2.1635e-01,  1.1315e-01, -4.6392e-01, -6.3468e-01,\n",
       "            5.4546e-01, -2.9374e-01,  4.9900e-01, -5.7695e-02,  1.3603e-01,\n",
       "            6.5166e-01, -1.7180e-01,  8.1734e-02, -1.2510e-01, -2.8957e-01,\n",
       "            2.6210e-01,  2.4296e-01, -1.5045e-02, -8.8138e-02,  1.6085e-01,\n",
       "           -2.3706e-01,  5.3785e-02, -5.4942e-01, -3.1501e-01, -5.8201e-01,\n",
       "            1.6606e-01,  4.0617e-01, -5.3007e-01, -2.5626e-01, -6.8881e-02,\n",
       "           -1.3734e-01,  9.4075e-01, -6.7235e-02,  2.9405e-01,  1.2777e-01,\n",
       "           -7.5570e-02,  4.5733e-01,  5.0780e-02, -4.8222e-01,  9.8709e-01,\n",
       "            4.9780e-02,  2.5794e-01, -1.9192e-01, -1.8380e-01, -6.0910e-02,\n",
       "           -4.0574e-01, -1.1680e+00, -1.0845e-01, -2.8882e-01,  1.1112e-01,\n",
       "            7.2620e-01, -1.6066e-01,  2.3499e-01,  2.0233e-01,  2.0977e-01,\n",
       "            1.0971e-01, -5.2585e-01,  4.7669e-01,  1.5275e-01, -6.2664e-02,\n",
       "            7.6645e-01, -7.3803e-02, -4.5201e-02, -5.1982e-01],\n",
       "          [-1.3408e-01, -2.1635e-01,  1.1315e-01, -4.6392e-01, -6.3468e-01,\n",
       "            5.4546e-01, -2.9374e-01,  4.9900e-01, -5.7695e-02,  1.3603e-01,\n",
       "            6.5166e-01, -1.7180e-01,  8.1734e-02, -1.2510e-01, -2.8957e-01,\n",
       "            2.6210e-01,  2.4296e-01, -1.5045e-02, -8.8138e-02,  1.6085e-01,\n",
       "           -2.3706e-01,  5.3785e-02, -5.4942e-01, -3.1501e-01, -5.8201e-01,\n",
       "            1.6606e-01,  4.0617e-01, -5.3007e-01, -2.5626e-01, -6.8882e-02,\n",
       "           -1.3734e-01,  9.4075e-01, -6.7235e-02,  2.9405e-01,  1.2777e-01,\n",
       "           -7.5570e-02,  4.5733e-01,  5.0780e-02, -4.8222e-01,  9.8709e-01,\n",
       "            4.9780e-02,  2.5794e-01, -1.9192e-01, -1.8380e-01, -6.0910e-02,\n",
       "           -4.0574e-01, -1.1680e+00, -1.0845e-01, -2.8882e-01,  1.1112e-01,\n",
       "            7.2620e-01, -1.6066e-01,  2.3499e-01,  2.0233e-01,  2.0977e-01,\n",
       "            1.0971e-01, -5.2585e-01,  4.7669e-01,  1.5275e-01, -6.2664e-02,\n",
       "            7.6645e-01, -7.3803e-02, -4.5201e-02, -5.1982e-01]],\n",
       "\n",
       "         [[-1.5910e+00,  1.3813e+00, -1.1386e+00,  3.2326e-01, -2.9177e-01,\n",
       "            2.5927e-01,  2.1643e+00, -6.3060e-01,  6.6102e-01, -4.9283e-02,\n",
       "            5.2229e-01, -3.5242e-01, -3.4325e-01,  2.1758e-01, -4.9097e-01,\n",
       "            3.0588e-01, -9.6834e-01, -3.0139e-01,  5.4144e-01,  9.2539e-01,\n",
       "           -3.5343e-01,  1.6538e+00, -5.5650e-01, -3.2298e-01, -2.4360e-01,\n",
       "            5.8017e-01, -1.9242e+00, -4.0589e-01,  6.7513e-01,  5.9890e-01,\n",
       "            1.0095e+00, -1.5050e-01,  3.6047e-01,  1.5420e+00, -1.4315e+00,\n",
       "           -4.8783e-01,  3.0489e-02, -8.6196e-01, -3.9288e-01, -2.4811e-01,\n",
       "           -1.8607e-01,  2.9918e-01, -7.7727e-01, -8.5746e-01,  4.0915e-01,\n",
       "           -1.9503e+00, -8.0256e-01,  1.0397e+00, -1.8670e+00, -3.7910e-01,\n",
       "            2.5910e-02,  1.2427e+00, -3.3208e-01, -1.3949e+00,  6.3819e-01,\n",
       "            5.5870e-01, -1.1675e-01,  6.8015e-01, -2.5021e-01,  2.0900e-01,\n",
       "            2.7006e-01,  5.2247e-01, -2.2378e-01,  7.9998e-01],\n",
       "          [-1.5910e+00,  1.3813e+00, -1.1386e+00,  3.2326e-01, -2.9177e-01,\n",
       "            2.5927e-01,  2.1643e+00, -6.3060e-01,  6.6102e-01, -4.9283e-02,\n",
       "            5.2229e-01, -3.5242e-01, -3.4325e-01,  2.1758e-01, -4.9097e-01,\n",
       "            3.0588e-01, -9.6834e-01, -3.0139e-01,  5.4144e-01,  9.2539e-01,\n",
       "           -3.5343e-01,  1.6538e+00, -5.5650e-01, -3.2298e-01, -2.4360e-01,\n",
       "            5.8017e-01, -1.9242e+00, -4.0589e-01,  6.7513e-01,  5.9890e-01,\n",
       "            1.0095e+00, -1.5050e-01,  3.6047e-01,  1.5420e+00, -1.4315e+00,\n",
       "           -4.8783e-01,  3.0489e-02, -8.6196e-01, -3.9288e-01, -2.4811e-01,\n",
       "           -1.8607e-01,  2.9918e-01, -7.7727e-01, -8.5746e-01,  4.0915e-01,\n",
       "           -1.9503e+00, -8.0256e-01,  1.0397e+00, -1.8670e+00, -3.7910e-01,\n",
       "            2.5910e-02,  1.2427e+00, -3.3208e-01, -1.3949e+00,  6.3819e-01,\n",
       "            5.5870e-01, -1.1675e-01,  6.8015e-01, -2.5021e-01,  2.0900e-01,\n",
       "            2.7006e-01,  5.2247e-01, -2.2378e-01,  7.9998e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 0.1741,  0.0112,  0.0658,  0.0491,  0.1338, -0.2664,  0.0499,\n",
       "           -0.1333,  0.1851, -0.1351,  0.0075, -0.0731, -0.0545,  0.1224,\n",
       "           -0.0291,  0.0898, -0.1123, -0.1087,  0.0392,  0.0594,  0.1623,\n",
       "            0.1168,  0.0213, -0.0544, -0.0102,  0.0943,  0.0507, -0.0387,\n",
       "            0.0714, -0.1998, -0.0359, -0.1286, -0.0970,  0.0965, -0.0225,\n",
       "           -0.2665, -0.0256,  0.1612, -0.0204, -0.0193, -0.1242,  0.1126,\n",
       "            0.0714, -0.0442, -0.0089,  0.3231,  0.0840,  0.0708,  0.1916,\n",
       "           -0.0231, -0.0013, -0.1459, -0.0856, -0.0543,  0.0889,  0.0243,\n",
       "            0.0151, -0.1484, -0.0850,  0.1997, -0.2608, -0.1919, -0.0331,\n",
       "            0.1139],\n",
       "          [ 0.1741,  0.0112,  0.0658,  0.0491,  0.1338, -0.2664,  0.0499,\n",
       "           -0.1333,  0.1851, -0.1351,  0.0075, -0.0731, -0.0545,  0.1224,\n",
       "           -0.0291,  0.0898, -0.1123, -0.1087,  0.0392,  0.0594,  0.1623,\n",
       "            0.1168,  0.0213, -0.0544, -0.0102,  0.0943,  0.0507, -0.0387,\n",
       "            0.0714, -0.1998, -0.0359, -0.1286, -0.0970,  0.0965, -0.0225,\n",
       "           -0.2665, -0.0256,  0.1612, -0.0204, -0.0193, -0.1242,  0.1126,\n",
       "            0.0714, -0.0442, -0.0089,  0.3231,  0.0840,  0.0708,  0.1916,\n",
       "           -0.0231, -0.0013, -0.1459, -0.0856, -0.0543,  0.0889,  0.0243,\n",
       "            0.0151, -0.1484, -0.0850,  0.1997, -0.2608, -0.1919, -0.0331,\n",
       "            0.1139]],\n",
       "\n",
       "         [[ 0.0927,  0.0643, -0.3610,  0.0462,  0.0197, -0.0871, -0.0235,\n",
       "           -0.0954, -0.0963, -0.0483,  0.0874,  0.0095,  0.0175,  0.1748,\n",
       "           -0.0769,  0.0957,  0.2076,  0.0818, -0.0552, -0.1236, -0.0881,\n",
       "           -0.0959, -0.1585, -0.0111, -0.0028,  0.0088,  0.1067, -0.1772,\n",
       "           -0.0219,  0.0371, -0.0572, -0.0257,  0.0954,  0.0478, -0.0008,\n",
       "            0.3351, -0.0918,  0.0314,  0.0380,  0.0640, -0.0146, -0.2611,\n",
       "            0.0807, -0.0244, -0.1574, -0.0634,  0.0360, -0.0035,  0.2086,\n",
       "            0.0583, -0.0186,  0.0543, -0.1334,  0.0263,  0.0446, -0.0735,\n",
       "            0.0114, -0.0330,  0.1071, -0.0467, -0.0336,  0.1373,  0.0065,\n",
       "            0.0126],\n",
       "          [ 0.0927,  0.0643, -0.3610,  0.0462,  0.0197, -0.0871, -0.0235,\n",
       "           -0.0954, -0.0963, -0.0483,  0.0874,  0.0095,  0.0175,  0.1748,\n",
       "           -0.0769,  0.0957,  0.2076,  0.0818, -0.0552, -0.1236, -0.0881,\n",
       "           -0.0959, -0.1585, -0.0111, -0.0028,  0.0088,  0.1067, -0.1772,\n",
       "           -0.0219,  0.0371, -0.0572, -0.0257,  0.0954,  0.0478, -0.0008,\n",
       "            0.3351, -0.0918,  0.0314,  0.0380,  0.0640, -0.0146, -0.2611,\n",
       "            0.0807, -0.0244, -0.1574, -0.0634,  0.0360, -0.0035,  0.2086,\n",
       "            0.0583, -0.0186,  0.0543, -0.1334,  0.0263,  0.0446, -0.0735,\n",
       "            0.0114, -0.0330,  0.1071, -0.0467, -0.0336,  0.1373,  0.0065,\n",
       "            0.0126]],\n",
       "\n",
       "         [[ 0.0646,  0.1694,  0.1419,  0.1077, -0.0300,  0.1626,  0.1503,\n",
       "            0.0871,  0.0382,  0.0417,  0.1244,  0.0581,  0.1683, -0.1723,\n",
       "           -0.1397,  0.1199, -0.0215, -0.0308,  0.0728, -0.0834, -0.0195,\n",
       "            0.0328, -0.0859, -0.1331, -0.2134,  0.0172, -0.1180,  0.0077,\n",
       "            0.0585,  0.2400,  0.1397, -0.0475,  0.0160, -0.1617,  0.2574,\n",
       "           -0.0068,  0.1319, -0.1005, -0.0851,  0.0382, -0.1093, -0.0796,\n",
       "            0.0453, -0.1010,  0.0410, -0.2322,  0.0705, -0.2357, -0.0645,\n",
       "            0.2119,  0.0413, -0.1439,  0.0612, -0.1561, -0.0982, -0.0497,\n",
       "           -0.1272, -0.2335,  0.0068, -0.2308, -0.0961, -0.0474,  0.1586,\n",
       "           -0.1553],\n",
       "          [ 0.0646,  0.1694,  0.1419,  0.1077, -0.0300,  0.1626,  0.1503,\n",
       "            0.0871,  0.0382,  0.0417,  0.1244,  0.0581,  0.1683, -0.1723,\n",
       "           -0.1397,  0.1199, -0.0215, -0.0308,  0.0728, -0.0834, -0.0195,\n",
       "            0.0328, -0.0859, -0.1331, -0.2134,  0.0172, -0.1180,  0.0077,\n",
       "            0.0585,  0.2400,  0.1397, -0.0475,  0.0160, -0.1617,  0.2574,\n",
       "           -0.0068,  0.1319, -0.1005, -0.0851,  0.0382, -0.1093, -0.0796,\n",
       "            0.0453, -0.1010,  0.0410, -0.2322,  0.0705, -0.2357, -0.0645,\n",
       "            0.2119,  0.0413, -0.1439,  0.0612, -0.1561, -0.0982, -0.0497,\n",
       "           -0.1272, -0.2335,  0.0068, -0.2308, -0.0961, -0.0474,  0.1586,\n",
       "           -0.1553]],\n",
       "\n",
       "         [[-0.0913, -0.1972,  0.0606, -0.4475, -0.1878,  0.1448, -0.2653,\n",
       "            0.1148, -0.3407,  0.0034,  0.1909,  0.5640, -0.1523, -0.5594,\n",
       "           -0.6521, -0.0180,  0.0271,  0.1456, -0.0532,  0.2314, -0.1374,\n",
       "            0.0125,  0.3764, -0.2319,  0.0050, -0.0363, -0.2293, -0.1705,\n",
       "            0.2266,  0.0613, -0.1526,  0.0488,  0.3454,  0.1686,  0.1545,\n",
       "           -0.1692,  0.1626,  0.0902,  0.4540, -0.0846,  0.3384, -0.1262,\n",
       "           -0.3410,  0.1885, -0.2077, -0.0127, -0.1531,  0.1345, -0.4901,\n",
       "           -0.2151,  0.1438,  0.5619,  0.3646,  0.0469, -0.0583,  0.0260,\n",
       "            0.4030,  0.5684, -0.3526, -0.1925, -0.2699, -0.2827, -0.1343,\n",
       "           -0.0117],\n",
       "          [-0.0913, -0.1972,  0.0606, -0.4475, -0.1878,  0.1448, -0.2653,\n",
       "            0.1148, -0.3407,  0.0034,  0.1909,  0.5640, -0.1523, -0.5594,\n",
       "           -0.6521, -0.0180,  0.0271,  0.1456, -0.0532,  0.2314, -0.1374,\n",
       "            0.0125,  0.3764, -0.2319,  0.0050, -0.0363, -0.2293, -0.1705,\n",
       "            0.2266,  0.0613, -0.1526,  0.0488,  0.3454,  0.1686,  0.1545,\n",
       "           -0.1692,  0.1626,  0.0902,  0.4540, -0.0846,  0.3384, -0.1262,\n",
       "           -0.3410,  0.1885, -0.2077, -0.0127, -0.1531,  0.1345, -0.4901,\n",
       "           -0.2151,  0.1438,  0.5619,  0.3646,  0.0469, -0.0583,  0.0260,\n",
       "            0.4030,  0.5684, -0.3526, -0.1925, -0.2699, -0.2827, -0.1343,\n",
       "           -0.0117]],\n",
       "\n",
       "         [[-0.0920, -0.2912,  0.4153,  0.0288,  0.1807, -0.1790,  0.0669,\n",
       "           -0.1780, -0.1370, -0.1244,  0.0688,  0.0258,  0.1478, -0.1489,\n",
       "            0.2168,  0.0504, -0.1175,  0.0885,  0.1309,  0.0712, -0.2747,\n",
       "            0.0724,  0.1120, -0.3070, -0.0266,  0.0416,  0.0164, -0.2067,\n",
       "           -0.1749,  0.1006, -0.3767, -0.1936, -0.0038, -0.2701,  0.1344,\n",
       "           -0.0015,  0.0967, -0.2162,  0.1162,  0.2684,  0.1266,  0.0378,\n",
       "           -0.3258,  0.0855, -0.1119, -0.1309,  0.0260,  0.1181, -0.0929,\n",
       "           -0.3456,  0.2124,  0.3590,  0.0070, -0.0116,  0.1443, -0.1595,\n",
       "            0.0321,  0.0171, -0.1546,  0.2974,  0.0966,  0.1506, -0.1708,\n",
       "            0.1084],\n",
       "          [-0.0920, -0.2912,  0.4153,  0.0288,  0.1807, -0.1790,  0.0669,\n",
       "           -0.1780, -0.1370, -0.1244,  0.0688,  0.0258,  0.1478, -0.1489,\n",
       "            0.2168,  0.0504, -0.1175,  0.0885,  0.1309,  0.0712, -0.2747,\n",
       "            0.0724,  0.1120, -0.3070, -0.0266,  0.0416,  0.0164, -0.2067,\n",
       "           -0.1749,  0.1006, -0.3767, -0.1936, -0.0038, -0.2701,  0.1344,\n",
       "           -0.0015,  0.0967, -0.2162,  0.1162,  0.2684,  0.1266,  0.0378,\n",
       "           -0.3258,  0.0855, -0.1119, -0.1309,  0.0260,  0.1181, -0.0929,\n",
       "           -0.3456,  0.2124,  0.3590,  0.0070, -0.0116,  0.1443, -0.1595,\n",
       "            0.0321,  0.0171, -0.1546,  0.2974,  0.0966,  0.1506, -0.1708,\n",
       "            0.1084]],\n",
       "\n",
       "         [[-0.0210, -0.0954, -0.1296, -0.1239,  0.0024,  0.0173,  0.0276,\n",
       "            0.1042, -0.0627, -0.1078, -0.0007, -0.0621,  0.1063, -0.0165,\n",
       "            0.0193,  0.0547,  0.1678,  0.0219, -0.1012,  0.0600,  0.0206,\n",
       "           -0.3591,  0.1384,  0.1817, -0.0056,  0.0846,  0.0559, -0.0451,\n",
       "           -0.0438,  0.0129,  0.0151,  0.2521, -0.0207,  0.0679, -0.0413,\n",
       "            0.0040,  0.1490,  0.0799, -0.0163, -0.0381, -0.1239,  0.0227,\n",
       "            0.0859, -0.0424,  0.1427, -0.0024,  0.0966, -0.1487, -0.1186,\n",
       "           -0.0453,  0.1044, -0.0294, -0.0221, -0.0238, -0.0709, -0.2454,\n",
       "            0.0100, -0.0552, -0.0031,  0.0275, -0.0376,  0.1706,  0.1005,\n",
       "            0.0456],\n",
       "          [-0.0210, -0.0954, -0.1296, -0.1239,  0.0024,  0.0173,  0.0276,\n",
       "            0.1042, -0.0627, -0.1078, -0.0007, -0.0621,  0.1063, -0.0165,\n",
       "            0.0193,  0.0547,  0.1678,  0.0219, -0.1012,  0.0600,  0.0206,\n",
       "           -0.3591,  0.1384,  0.1817, -0.0056,  0.0846,  0.0559, -0.0451,\n",
       "           -0.0438,  0.0129,  0.0151,  0.2521, -0.0207,  0.0679, -0.0413,\n",
       "            0.0040,  0.1490,  0.0799, -0.0163, -0.0381, -0.1239,  0.0227,\n",
       "            0.0859, -0.0424,  0.1427, -0.0024,  0.0966, -0.1487, -0.1186,\n",
       "           -0.0453,  0.1044, -0.0294, -0.0221, -0.0238, -0.0709, -0.2454,\n",
       "            0.0100, -0.0552, -0.0031,  0.0275, -0.0376,  0.1706,  0.1005,\n",
       "            0.0456]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 1.5420e+00, -1.9306e+00, -1.0777e+00,  ..., -1.6708e+00,\n",
       "            4.7148e-01, -1.1328e+00],\n",
       "          [-1.0301e+00, -1.9157e+00, -4.1272e-02,  ...,  7.0912e-01,\n",
       "            2.1339e-01,  4.7577e-01],\n",
       "          [-9.6716e-01, -1.6273e+00,  1.7179e-01,  ..., -1.3844e+00,\n",
       "           -1.8915e+00, -2.2304e+00],\n",
       "          ...,\n",
       "          [-1.0555e+00,  1.4880e+00, -6.9797e-02,  ..., -2.4728e+00,\n",
       "            1.3986e+00, -1.6499e+00],\n",
       "          [-1.1604e+00,  1.2894e+00, -4.4969e-01,  ...,  3.2837e-01,\n",
       "           -2.1153e+00,  6.9265e-02],\n",
       "          [ 2.9105e-02,  3.0006e-01,  6.0483e-02,  ...,  3.6572e-01,\n",
       "           -1.5673e-02,  4.2263e-01]],\n",
       "\n",
       "         [[ 1.4145e+00, -1.7237e+00,  1.1503e+00,  ..., -1.7033e+00,\n",
       "            3.4582e+00, -3.7183e-01],\n",
       "          [ 2.8686e+00,  2.8389e+00, -1.4171e+00,  ..., -3.8280e+00,\n",
       "            1.9350e+00,  1.9843e+00],\n",
       "          [ 1.9813e+00,  2.1119e+00,  1.0070e-01,  ..., -8.3098e-01,\n",
       "            1.4449e+00, -6.9585e-01],\n",
       "          ...,\n",
       "          [ 5.9762e-01,  3.7003e+00,  1.9010e+00,  ..., -1.6821e+00,\n",
       "           -4.2095e-01, -2.5314e+00],\n",
       "          [ 6.4093e-01,  1.5660e+00, -8.1113e-01,  ..., -1.9762e+00,\n",
       "            4.1037e-01, -1.2253e+00],\n",
       "          [-3.6225e-02, -3.2309e-01, -9.1180e-02,  ...,  1.1298e-01,\n",
       "           -2.4937e-01,  1.4678e-01]],\n",
       "\n",
       "         [[ 8.9461e-01,  2.1731e+00,  9.4401e-01,  ..., -2.0253e+00,\n",
       "           -2.2521e+00,  2.4365e-01],\n",
       "          [ 1.2557e+00,  5.1939e+00, -3.4187e+00,  ..., -2.6934e+00,\n",
       "            3.1797e+00, -1.9386e+00],\n",
       "          [ 2.2246e+00,  3.3285e+00, -2.8071e+00,  ..., -7.5599e-01,\n",
       "            2.4397e+00,  1.1838e+00],\n",
       "          ...,\n",
       "          [ 4.9567e+00, -3.1438e+00, -5.0615e-01,  ..., -1.2952e+00,\n",
       "           -2.7091e+00, -1.2278e+00],\n",
       "          [ 6.7466e-01, -1.3340e+00,  1.8761e-01,  ..., -1.0884e+00,\n",
       "           -1.2674e+00, -2.6306e+00],\n",
       "          [-2.0538e-03, -3.5004e-01, -3.4239e-01,  ...,  3.5597e-01,\n",
       "           -3.8030e-02,  9.2606e-02]],\n",
       "\n",
       "         [[-4.1480e-01, -2.0203e+00,  1.4140e+00,  ..., -2.1328e+00,\n",
       "           -7.7692e-01, -2.3639e+00],\n",
       "          [-6.8941e-01, -2.8038e-03,  1.1092e+00,  ..., -1.2504e+00,\n",
       "           -1.0851e+00, -2.4328e+00],\n",
       "          [ 3.2373e-01,  5.5222e-01,  1.6364e+00,  ..., -2.8376e+00,\n",
       "           -8.7696e-01, -7.0212e-01],\n",
       "          ...,\n",
       "          [-9.8758e-01,  2.9858e-01, -2.9831e+00,  ..., -7.1427e-01,\n",
       "           -6.2474e-01, -9.0172e-01],\n",
       "          [-5.2676e-01,  1.4320e+00, -1.1336e+00,  ...,  5.6496e-01,\n",
       "           -5.5881e-01, -1.9504e+00],\n",
       "          [ 9.1572e-02, -2.1892e-01,  3.9652e-02,  ...,  1.5546e-01,\n",
       "            1.0499e-01,  3.6203e-01]],\n",
       "\n",
       "         [[-3.8209e-01, -1.3175e+00,  1.4589e+00,  ..., -3.9270e+00,\n",
       "            1.5653e+00,  2.5549e+00],\n",
       "          [ 1.4929e+00, -1.2468e+00, -3.3147e-01,  ..., -3.4139e+00,\n",
       "           -8.0459e-02, -1.5430e+00],\n",
       "          [ 1.7951e+00,  8.5806e-02,  8.5660e-01,  ..., -2.9440e+00,\n",
       "            7.7793e-01, -8.3467e-01],\n",
       "          ...,\n",
       "          [-1.1230e-01, -1.8325e+00,  4.7376e-02,  ...,  2.9122e+00,\n",
       "           -5.0464e-01, -2.7830e+00],\n",
       "          [ 1.0106e+00, -8.5420e-01, -2.2348e+00,  ...,  6.0041e-01,\n",
       "            3.1262e-01, -2.6901e+00],\n",
       "          [-2.2646e-01,  2.6740e-01, -2.4358e-01,  ...,  2.5528e-01,\n",
       "            1.0377e-01,  2.8257e-04]],\n",
       "\n",
       "         [[ 1.1401e+00,  2.3114e+00, -2.5020e+00,  ..., -7.4705e-01,\n",
       "            1.1659e-01,  1.4900e+00],\n",
       "          [ 1.8847e+00,  6.3243e-01, -1.1822e+00,  ..., -1.2246e+00,\n",
       "            7.5431e-01,  3.6920e-01],\n",
       "          [ 5.0287e-02, -1.1619e+00,  1.1998e+00,  ..., -4.9302e-01,\n",
       "            7.4175e-01,  5.2337e-01],\n",
       "          ...,\n",
       "          [ 1.5051e+00,  3.3114e-01, -6.4324e-01,  ..., -2.4966e-01,\n",
       "            1.2453e+00, -1.3072e+00],\n",
       "          [ 5.3213e-01, -1.4978e+00,  1.1083e+00,  ...,  5.6951e-01,\n",
       "            1.6048e-01,  3.1582e-01],\n",
       "          [-4.3230e-02,  1.2725e-01, -7.6122e-02,  ...,  1.2239e-01,\n",
       "           -1.1320e-03, -9.5001e-03]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 2.6203e+00, -2.0014e+00,  1.4305e+00,  ...,  2.5135e+00,\n",
       "           -3.2781e-01,  2.8752e+00],\n",
       "          [ 2.5159e+00,  3.1276e-01,  6.3839e-01,  ...,  1.5686e+00,\n",
       "            1.4912e+00,  7.1996e-01],\n",
       "          [ 2.3012e+00, -2.0879e+00,  1.3693e+00,  ...,  1.1653e+00,\n",
       "            1.2804e+00,  1.4393e+00],\n",
       "          ...,\n",
       "          [-6.2015e-01,  1.2911e-01, -1.3862e+00,  ...,  9.0751e-01,\n",
       "            4.6067e-01,  1.1175e+00],\n",
       "          [ 6.7648e-01,  1.5293e+00, -8.8316e-01,  ..., -6.8151e-01,\n",
       "           -6.2411e-02,  2.2186e+00],\n",
       "          [-3.0757e-01,  1.7216e-01, -8.4097e-02,  ..., -9.1227e-02,\n",
       "           -9.5159e-02, -1.4470e-01]],\n",
       "\n",
       "         [[-1.6858e+00, -4.5870e-01, -1.2833e+00,  ..., -2.3224e+00,\n",
       "            2.2030e-01,  2.7995e+00],\n",
       "          [ 8.9865e-01, -3.1990e+00, -8.3201e-01,  ..., -5.5381e-01,\n",
       "            4.0640e-01,  1.1408e+00],\n",
       "          [-5.2726e-01, -2.2595e-01, -1.8997e+00,  ..., -1.2268e+00,\n",
       "           -3.6591e-01,  2.5778e-01],\n",
       "          ...,\n",
       "          [ 6.9055e-01, -3.4438e-01, -5.8008e-01,  ...,  1.1066e+00,\n",
       "           -2.6555e-01, -1.2522e+00],\n",
       "          [ 2.4436e-01,  2.1260e+00, -1.2616e+00,  ..., -7.7108e-01,\n",
       "            4.4515e-01, -5.6166e-01],\n",
       "          [-3.2302e-02, -1.3321e-01, -2.1382e-02,  ..., -7.7094e-02,\n",
       "           -8.3637e-02,  9.3342e-02]],\n",
       "\n",
       "         [[-1.1791e+00,  2.4899e+00, -3.3725e+00,  ..., -1.0502e+00,\n",
       "           -2.1071e+00,  2.0040e+00],\n",
       "          [ 1.1459e+00, -1.1521e+00, -2.6876e+00,  ...,  3.6872e-01,\n",
       "           -1.6669e+00,  3.2663e+00],\n",
       "          [-4.4041e-01,  1.2440e+00, -3.8996e+00,  ..., -1.5092e+00,\n",
       "            5.3126e-01,  2.5234e+00],\n",
       "          ...,\n",
       "          [-9.1472e-01,  2.7425e+00, -2.0127e-01,  ...,  1.9736e+00,\n",
       "           -2.8460e+00,  3.8755e+00],\n",
       "          [ 9.5956e-02,  3.5286e+00, -1.2893e+00,  ...,  3.3714e-02,\n",
       "            1.0589e-01,  2.6372e+00],\n",
       "          [ 3.8882e-02,  4.6762e-02, -2.1498e-02,  ...,  8.1336e-02,\n",
       "            7.2179e-02, -2.0214e-01]],\n",
       "\n",
       "         [[-3.8295e-01, -1.8184e+00, -2.6710e+00,  ..., -4.6164e+00,\n",
       "           -2.7740e+00, -2.4033e+00],\n",
       "          [-1.8995e+00,  1.9869e-01, -2.9079e+00,  ..., -6.4480e-01,\n",
       "            9.5833e-02, -4.8868e+00],\n",
       "          [-5.0747e-01, -1.2838e+00, -3.2877e+00,  ..., -3.0945e+00,\n",
       "            1.2067e-01, -3.3673e+00],\n",
       "          ...,\n",
       "          [ 4.5305e-01,  3.7243e+00,  1.0472e-01,  ...,  4.4197e-01,\n",
       "            9.8457e-01, -4.2926e-01],\n",
       "          [-3.4028e-01,  1.1191e-01,  1.6348e+00,  ..., -4.2333e-01,\n",
       "            1.2310e+00,  4.1468e-01],\n",
       "          [ 4.3748e-02, -3.6383e-01, -9.8813e-02,  ..., -8.1149e-02,\n",
       "           -6.8406e-02,  1.9763e-01]],\n",
       "\n",
       "         [[ 1.8046e+00, -3.0744e+00,  2.2780e+00,  ...,  2.0512e-02,\n",
       "           -1.3845e+00, -1.6583e+00],\n",
       "          [ 2.0289e+00, -5.9276e-01,  1.4205e+00,  ...,  5.8305e-01,\n",
       "           -1.7680e+00, -7.8414e-01],\n",
       "          [ 4.5110e-01, -5.0897e-02,  1.0162e+00,  ...,  1.0149e+00,\n",
       "           -1.7564e+00,  3.5368e-01],\n",
       "          ...,\n",
       "          [-1.9014e-01, -9.5526e-01,  1.6341e+00,  ...,  8.4214e-01,\n",
       "            6.4888e-01,  7.5869e-01],\n",
       "          [ 2.9166e-01,  1.2196e+00, -2.5778e-01,  ...,  1.4456e+00,\n",
       "            9.7341e-01, -2.7755e-01],\n",
       "          [-3.9732e-04,  5.5346e-02,  2.6489e-02,  ..., -3.4515e-02,\n",
       "            9.3743e-02,  7.0934e-02]],\n",
       "\n",
       "         [[-2.7853e+00, -1.4011e+00,  6.4969e-02,  ..., -1.3373e-01,\n",
       "           -4.1019e+00,  7.1205e-01],\n",
       "          [-1.5615e+00, -2.8004e-01, -1.2288e+00,  ..., -6.0813e-01,\n",
       "            6.9172e-02,  2.0011e-01],\n",
       "          [-1.6378e+00, -1.2476e+00, -7.6295e-01,  ...,  7.0114e-01,\n",
       "           -4.1793e-01,  8.0367e-01],\n",
       "          ...,\n",
       "          [-8.5907e-01, -1.1698e+00, -3.5269e+00,  ...,  6.3207e-01,\n",
       "           -4.5342e-01,  1.6667e-01],\n",
       "          [-2.1540e+00, -2.3773e+00, -5.1813e-02,  ...,  5.6273e-01,\n",
       "           -9.6944e-01, -2.3803e+00],\n",
       "          [-4.5103e-02, -8.0996e-02, -1.0968e-01,  ...,  6.5754e-03,\n",
       "            7.1618e-02, -9.2199e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-7.8854e-01, -4.9403e-01,  2.0693e-01,  1.2364e+00,  1.2465e-02,\n",
       "           -3.7293e-01, -4.5669e-01, -4.7575e-01, -3.8076e-01,  2.9071e-01,\n",
       "           -6.3832e-02, -2.1423e-01, -5.3049e-01, -5.3477e-02,  2.2239e-01,\n",
       "           -2.0294e-01,  5.3935e-01, -8.7575e-01, -5.6044e-01, -3.2191e-01,\n",
       "           -6.2859e-01, -2.2494e-01, -1.2736e-01, -3.9863e-01,  3.0316e-01,\n",
       "           -3.1805e-03, -1.2683e+00, -2.6332e-01, -7.4225e-02,  3.9618e-01,\n",
       "           -3.5141e-01, -1.8959e-01,  3.0198e-03, -5.8169e-01,  9.9174e-02,\n",
       "            5.7881e-01,  6.1626e-01, -2.1449e-01, -8.0829e-01,  3.6456e-01,\n",
       "           -5.3696e-01, -7.9763e-02,  3.8506e-02,  4.0208e-01,  3.7709e-01,\n",
       "           -1.1458e-01,  4.9593e-02,  7.2784e-01, -7.6221e-01,  5.0734e-01,\n",
       "            1.7374e-01, -1.1924e-01, -1.3783e+00, -4.6829e-01, -1.3134e+00,\n",
       "           -4.0614e-02, -2.6350e-01, -9.0609e-01, -9.8597e-02,  7.5883e-01,\n",
       "           -2.1920e-01,  1.2199e+00, -1.6038e-01, -2.5176e-01],\n",
       "          [-7.8854e-01, -4.9403e-01,  2.0693e-01,  1.2364e+00,  1.2465e-02,\n",
       "           -3.7293e-01, -4.5669e-01, -4.7575e-01, -3.8076e-01,  2.9071e-01,\n",
       "           -6.3832e-02, -2.1423e-01, -5.3049e-01, -5.3477e-02,  2.2239e-01,\n",
       "           -2.0294e-01,  5.3935e-01, -8.7575e-01, -5.6044e-01, -3.2191e-01,\n",
       "           -6.2859e-01, -2.2494e-01, -1.2736e-01, -3.9863e-01,  3.0316e-01,\n",
       "           -3.1804e-03, -1.2683e+00, -2.6332e-01, -7.4225e-02,  3.9618e-01,\n",
       "           -3.5141e-01, -1.8959e-01,  3.0198e-03, -5.8169e-01,  9.9174e-02,\n",
       "            5.7881e-01,  6.1626e-01, -2.1449e-01, -8.0829e-01,  3.6456e-01,\n",
       "           -5.3696e-01, -7.9763e-02,  3.8506e-02,  4.0208e-01,  3.7709e-01,\n",
       "           -1.1458e-01,  4.9593e-02,  7.2784e-01, -7.6221e-01,  5.0734e-01,\n",
       "            1.7374e-01, -1.1924e-01, -1.3783e+00, -4.6829e-01, -1.3134e+00,\n",
       "           -4.0614e-02, -2.6350e-01, -9.0609e-01, -9.8597e-02,  7.5883e-01,\n",
       "           -2.1920e-01,  1.2199e+00, -1.6038e-01, -2.5176e-01]],\n",
       "\n",
       "         [[ 4.3444e-01,  2.5670e-01, -1.1387e-01, -3.8002e-01,  1.5025e+00,\n",
       "            9.4180e-01,  1.1710e-01,  4.3667e-01,  3.9954e-01,  2.3823e-01,\n",
       "            1.3504e-01,  1.1579e-01,  4.4806e-01, -5.7429e-01,  3.6376e-01,\n",
       "            5.6868e-02, -3.6646e-01,  3.0846e-01, -1.2112e-02,  2.8355e-01,\n",
       "           -9.3987e-01,  3.9058e-01,  1.8336e-01, -2.5752e-01,  3.4734e-03,\n",
       "           -3.2737e-01,  3.6276e-01, -2.5306e-01,  5.2357e-02,  3.1526e-01,\n",
       "            2.6213e-01, -1.9379e-01, -4.2836e-01,  7.5128e-01, -5.4574e-01,\n",
       "           -5.2459e-01, -1.7084e-01,  3.9708e-01,  2.0180e-01,  2.8613e+00,\n",
       "           -5.9270e-02,  5.8327e-01,  3.2942e-01, -1.6153e-01, -6.8572e-01,\n",
       "           -8.2440e-02, -3.2187e-01,  4.7902e-01, -3.2466e-01,  2.1140e-01,\n",
       "           -1.9867e-01, -1.4042e-01,  2.2021e-01, -1.6653e-01, -1.5546e-01,\n",
       "           -4.0282e-01, -1.6047e-01, -9.6509e-01, -1.1310e-01, -5.6866e-02,\n",
       "            5.4979e-02, -4.3013e-01, -3.0268e-02,  1.0597e-01],\n",
       "          [ 4.3444e-01,  2.5670e-01, -1.1387e-01, -3.8002e-01,  1.5025e+00,\n",
       "            9.4180e-01,  1.1710e-01,  4.3667e-01,  3.9954e-01,  2.3823e-01,\n",
       "            1.3504e-01,  1.1579e-01,  4.4806e-01, -5.7429e-01,  3.6376e-01,\n",
       "            5.6868e-02, -3.6646e-01,  3.0846e-01, -1.2112e-02,  2.8355e-01,\n",
       "           -9.3987e-01,  3.9058e-01,  1.8336e-01, -2.5752e-01,  3.4733e-03,\n",
       "           -3.2737e-01,  3.6276e-01, -2.5306e-01,  5.2357e-02,  3.1526e-01,\n",
       "            2.6213e-01, -1.9379e-01, -4.2836e-01,  7.5128e-01, -5.4574e-01,\n",
       "           -5.2459e-01, -1.7084e-01,  3.9708e-01,  2.0180e-01,  2.8613e+00,\n",
       "           -5.9270e-02,  5.8327e-01,  3.2942e-01, -1.6153e-01, -6.8572e-01,\n",
       "           -8.2440e-02, -3.2187e-01,  4.7902e-01, -3.2466e-01,  2.1140e-01,\n",
       "           -1.9867e-01, -1.4042e-01,  2.2021e-01, -1.6653e-01, -1.5546e-01,\n",
       "           -4.0282e-01, -1.6047e-01, -9.6509e-01, -1.1310e-01, -5.6866e-02,\n",
       "            5.4979e-02, -4.3013e-01, -3.0268e-02,  1.0597e-01]],\n",
       "\n",
       "         [[-1.9075e-01, -2.2968e-01, -9.2516e-02, -2.7443e-01, -4.8192e-01,\n",
       "           -1.0047e-01,  5.3943e-01, -4.0251e-01, -7.1932e-02,  6.2786e-02,\n",
       "            6.3472e-01, -6.2447e-02, -1.0079e+00,  5.8024e-01,  1.5248e-02,\n",
       "           -9.0987e-02, -3.3999e-01, -9.2838e-01,  1.1512e-01, -2.9602e-02,\n",
       "            9.3471e-01, -2.2674e-01,  1.3888e-01, -9.0211e-02, -1.6470e-01,\n",
       "            5.5152e-01,  4.2195e-02, -3.6087e-01,  1.6030e-01,  9.4582e-02,\n",
       "            1.6546e-01,  8.7914e-01,  1.7777e-01, -8.1097e-02,  9.6891e-02,\n",
       "            8.7104e-01, -1.0001e-01, -9.4437e-02,  5.8099e-01, -2.2494e-01,\n",
       "           -6.6984e-02,  4.0382e-01,  3.5475e-01,  7.3183e-02, -1.0631e-01,\n",
       "           -6.6491e-01, -6.3695e-01,  3.1760e-01, -2.2165e-01, -5.1518e-01,\n",
       "            4.8945e-01,  1.0716e+00,  2.6657e-01, -9.0346e-01,  6.2851e-01,\n",
       "           -2.0755e-01,  1.4379e-01, -4.5592e-01,  2.0868e-02, -4.9421e-01,\n",
       "            9.4870e-01, -4.9033e-01, -1.8247e-01, -5.5198e-01],\n",
       "          [-1.9075e-01, -2.2968e-01, -9.2516e-02, -2.7443e-01, -4.8192e-01,\n",
       "           -1.0047e-01,  5.3943e-01, -4.0251e-01, -7.1932e-02,  6.2786e-02,\n",
       "            6.3472e-01, -6.2447e-02, -1.0079e+00,  5.8024e-01,  1.5248e-02,\n",
       "           -9.0987e-02, -3.3999e-01, -9.2838e-01,  1.1512e-01, -2.9602e-02,\n",
       "            9.3471e-01, -2.2674e-01,  1.3888e-01, -9.0211e-02, -1.6470e-01,\n",
       "            5.5152e-01,  4.2195e-02, -3.6087e-01,  1.6030e-01,  9.4582e-02,\n",
       "            1.6546e-01,  8.7914e-01,  1.7777e-01, -8.1097e-02,  9.6891e-02,\n",
       "            8.7104e-01, -1.0001e-01, -9.4437e-02,  5.8099e-01, -2.2494e-01,\n",
       "           -6.6984e-02,  4.0382e-01,  3.5475e-01,  7.3183e-02, -1.0631e-01,\n",
       "           -6.6491e-01, -6.3695e-01,  3.1760e-01, -2.2165e-01, -5.1518e-01,\n",
       "            4.8945e-01,  1.0716e+00,  2.6657e-01, -9.0346e-01,  6.2851e-01,\n",
       "           -2.0755e-01,  1.4379e-01, -4.5592e-01,  2.0868e-02, -4.9421e-01,\n",
       "            9.4870e-01, -4.9033e-01, -1.8247e-01, -5.5198e-01]],\n",
       "\n",
       "         [[ 7.6742e-01, -3.0450e-01,  9.4901e-01,  2.6973e-01,  1.8847e-01,\n",
       "           -1.7972e+00, -7.5800e-02, -3.4309e-01, -7.8332e-02, -3.9362e-01,\n",
       "            4.3936e-01, -1.1763e-01, -4.2555e-01,  4.2109e-01, -1.2806e+00,\n",
       "           -2.0412e-01, -1.0946e+00,  2.0611e-01, -2.9690e-01, -1.6243e+00,\n",
       "            1.2942e-01,  2.6295e-03,  2.4230e-02,  7.2155e-02,  2.5318e-01,\n",
       "            5.5838e-01, -8.6098e-01, -1.6624e+00,  1.6746e+00,  1.4396e-01,\n",
       "            1.9997e-01, -2.3516e-01,  2.6921e-01, -5.2270e-01,  8.6518e-02,\n",
       "           -3.1662e-01, -1.3758e+00, -4.2203e-01,  1.4966e+00,  8.2155e-01,\n",
       "           -6.9331e-01,  6.1710e-01, -5.5519e-02, -2.3996e-01,  7.2118e-01,\n",
       "           -2.7785e-01, -2.3563e-01,  8.8926e-01, -3.3348e-01,  2.8727e-01,\n",
       "           -6.3673e-01,  8.2691e-01, -1.6621e+00, -3.2084e-01, -2.0145e-02,\n",
       "           -8.8767e-01, -3.7123e-01,  6.8100e-01, -1.3690e-01,  7.1207e-01,\n",
       "           -2.7170e-01, -2.0215e-01, -9.7109e-02,  5.7018e-01],\n",
       "          [ 7.6742e-01, -3.0450e-01,  9.4901e-01,  2.6973e-01,  1.8847e-01,\n",
       "           -1.7972e+00, -7.5800e-02, -3.4309e-01, -7.8332e-02, -3.9362e-01,\n",
       "            4.3936e-01, -1.1763e-01, -4.2555e-01,  4.2109e-01, -1.2806e+00,\n",
       "           -2.0412e-01, -1.0946e+00,  2.0611e-01, -2.9690e-01, -1.6243e+00,\n",
       "            1.2942e-01,  2.6295e-03,  2.4230e-02,  7.2155e-02,  2.5318e-01,\n",
       "            5.5838e-01, -8.6098e-01, -1.6624e+00,  1.6746e+00,  1.4396e-01,\n",
       "            1.9997e-01, -2.3516e-01,  2.6921e-01, -5.2270e-01,  8.6518e-02,\n",
       "           -3.1662e-01, -1.3758e+00, -4.2203e-01,  1.4966e+00,  8.2155e-01,\n",
       "           -6.9331e-01,  6.1710e-01, -5.5519e-02, -2.3996e-01,  7.2118e-01,\n",
       "           -2.7785e-01, -2.3563e-01,  8.8926e-01, -3.3348e-01,  2.8727e-01,\n",
       "           -6.3673e-01,  8.2691e-01, -1.6621e+00, -3.2084e-01, -2.0145e-02,\n",
       "           -8.8767e-01, -3.7123e-01,  6.8100e-01, -1.3690e-01,  7.1207e-01,\n",
       "           -2.7170e-01, -2.0215e-01, -9.7109e-02,  5.7018e-01]],\n",
       "\n",
       "         [[ 4.9434e-01, -3.4549e-02,  6.3306e-02, -2.8854e-01, -1.7321e-01,\n",
       "           -2.5499e-01,  3.4424e-01,  3.0133e-01,  2.7259e-01,  3.4456e-01,\n",
       "            2.0823e-01, -2.5420e-01, -9.4577e-02,  7.2528e-01,  2.0843e-02,\n",
       "           -2.7356e-01, -1.3139e-02,  3.7077e-01,  2.5331e-03, -7.5682e-01,\n",
       "            1.2111e-01, -7.7497e-02, -1.1074e-01,  3.1271e-01, -1.5566e-01,\n",
       "            2.6323e-01, -2.6630e-01,  1.3165e-02, -4.6446e-02, -4.1066e-01,\n",
       "           -6.4378e-03,  1.1878e+00, -1.3242e+00,  8.6117e-02, -2.4808e-02,\n",
       "           -5.0351e-01,  1.3405e+00, -6.7119e-02, -9.9666e-02,  2.2849e-01,\n",
       "           -2.8430e-02,  1.6621e-01, -2.0851e-01,  1.3726e-02,  2.6689e-01,\n",
       "           -4.9312e-01,  3.3073e-03,  1.9241e-01,  1.2340e-01,  2.9398e-01,\n",
       "           -1.0634e-01,  3.5617e-01, -2.8443e-01,  4.9490e-01, -8.0729e-01,\n",
       "            1.0548e-01, -2.7677e-01, -1.9072e-02, -3.0431e-01,  1.1274e-01,\n",
       "            1.6894e-01,  1.6551e-01,  2.6669e-01, -1.0891e-01],\n",
       "          [ 4.9434e-01, -3.4549e-02,  6.3306e-02, -2.8854e-01, -1.7321e-01,\n",
       "           -2.5499e-01,  3.4424e-01,  3.0133e-01,  2.7259e-01,  3.4456e-01,\n",
       "            2.0823e-01, -2.5420e-01, -9.4577e-02,  7.2528e-01,  2.0843e-02,\n",
       "           -2.7356e-01, -1.3139e-02,  3.7077e-01,  2.5330e-03, -7.5682e-01,\n",
       "            1.2111e-01, -7.7497e-02, -1.1074e-01,  3.1271e-01, -1.5566e-01,\n",
       "            2.6323e-01, -2.6630e-01,  1.3165e-02, -4.6446e-02, -4.1066e-01,\n",
       "           -6.4378e-03,  1.1878e+00, -1.3242e+00,  8.6117e-02, -2.4808e-02,\n",
       "           -5.0351e-01,  1.3405e+00, -6.7119e-02, -9.9666e-02,  2.2849e-01,\n",
       "           -2.8430e-02,  1.6621e-01, -2.0851e-01,  1.3726e-02,  2.6689e-01,\n",
       "           -4.9312e-01,  3.3073e-03,  1.9241e-01,  1.2340e-01,  2.9398e-01,\n",
       "           -1.0634e-01,  3.5617e-01, -2.8443e-01,  4.9490e-01, -8.0729e-01,\n",
       "            1.0548e-01, -2.7677e-01, -1.9072e-02, -3.0431e-01,  1.1274e-01,\n",
       "            1.6894e-01,  1.6551e-01,  2.6669e-01, -1.0891e-01]],\n",
       "\n",
       "         [[-1.1976e-01,  3.5150e-01, -3.2217e-01, -7.1784e-01,  1.0333e+00,\n",
       "            2.4680e-01, -4.1413e-01,  1.4511e-01,  6.0624e-02,  2.4439e-02,\n",
       "           -1.0923e+00,  2.6662e-01, -1.1385e-01,  4.0106e-01, -1.1146e-02,\n",
       "           -8.0138e-01, -5.7293e-01, -7.2153e-01, -2.4317e-01,  1.0966e+00,\n",
       "           -4.2946e-01, -1.6929e+00,  1.6991e-01, -6.6287e-02, -3.8496e-01,\n",
       "           -2.7565e-01, -1.0133e-01, -9.3830e-01,  7.2531e-02, -3.1703e-01,\n",
       "           -3.0543e-02,  1.0991e+00,  1.1883e-01,  1.3642e-01, -1.3371e+00,\n",
       "            6.1269e-02,  5.9512e-01,  7.6484e-02,  2.2789e-01,  1.3429e-01,\n",
       "            8.1054e-01, -4.8101e-01, -1.5002e+00, -2.0484e-01,  1.4308e+00,\n",
       "            2.3980e-01, -6.9345e-01,  8.6719e-02, -7.9177e-01, -9.4015e-01,\n",
       "            6.7197e-01, -1.8997e-01, -4.3253e-01, -2.3623e-01,  9.0750e-02,\n",
       "           -3.0087e-02, -3.0901e-01, -9.9215e-01,  1.3640e-01, -3.0519e-01,\n",
       "            6.1757e-01,  2.0487e-01,  6.7369e-01,  4.2194e-01],\n",
       "          [-1.1976e-01,  3.5150e-01, -3.2217e-01, -7.1784e-01,  1.0333e+00,\n",
       "            2.4680e-01, -4.1413e-01,  1.4511e-01,  6.0624e-02,  2.4439e-02,\n",
       "           -1.0923e+00,  2.6662e-01, -1.1385e-01,  4.0106e-01, -1.1146e-02,\n",
       "           -8.0138e-01, -5.7293e-01, -7.2153e-01, -2.4317e-01,  1.0966e+00,\n",
       "           -4.2946e-01, -1.6929e+00,  1.6991e-01, -6.6287e-02, -3.8496e-01,\n",
       "           -2.7565e-01, -1.0133e-01, -9.3830e-01,  7.2531e-02, -3.1703e-01,\n",
       "           -3.0543e-02,  1.0991e+00,  1.1883e-01,  1.3642e-01, -1.3371e+00,\n",
       "            6.1269e-02,  5.9512e-01,  7.6484e-02,  2.2789e-01,  1.3429e-01,\n",
       "            8.1054e-01, -4.8101e-01, -1.5002e+00, -2.0484e-01,  1.4308e+00,\n",
       "            2.3980e-01, -6.9345e-01,  8.6719e-02, -7.9177e-01, -9.4015e-01,\n",
       "            6.7197e-01, -1.8997e-01, -4.3253e-01, -2.3623e-01,  9.0750e-02,\n",
       "           -3.0087e-02, -3.0901e-01, -9.9215e-01,  1.3640e-01, -3.0519e-01,\n",
       "            6.1757e-01,  2.0487e-01,  6.7369e-01,  4.2194e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 7.3466e-02, -1.1258e-01, -1.1357e-01, -7.4745e-02, -1.8774e-02,\n",
       "           -7.5338e-02, -8.9667e-02,  1.6906e-01,  1.2230e-01, -1.8494e-02,\n",
       "            1.2089e-01, -3.9820e-02,  6.1240e-02, -3.7622e-02,  6.7902e-02,\n",
       "            5.8332e-02, -5.7147e-02,  9.7254e-02, -4.9840e-02,  1.0529e-01,\n",
       "           -4.1558e-02, -1.1419e-02, -2.3225e-02,  1.0590e-02, -6.5957e-03,\n",
       "           -4.5932e-02, -1.2990e-02,  7.9259e-02,  2.4261e-02,  4.7065e-02,\n",
       "           -8.6077e-02, -1.5279e-01, -6.1447e-02, -1.0855e-02,  2.7373e-02,\n",
       "           -1.0619e-01, -1.3377e-01,  9.6018e-02,  8.6586e-02,  1.3031e-01,\n",
       "           -1.6688e-02,  6.8110e-02,  3.3965e-02,  1.6283e-01, -1.6650e-01,\n",
       "           -1.4567e-01,  4.6753e-03, -5.2499e-02,  1.0220e-02,  3.3777e-02,\n",
       "            5.1231e-02, -2.6475e-03,  5.5270e-02,  1.6716e-01, -8.1678e-02,\n",
       "            1.5612e-01, -2.4854e-02,  9.5958e-02, -2.4683e-02, -1.3152e-02,\n",
       "           -8.6819e-02,  8.9434e-02,  6.6587e-02,  1.7277e-01],\n",
       "          [ 7.3466e-02, -1.1258e-01, -1.1357e-01, -7.4745e-02, -1.8774e-02,\n",
       "           -7.5338e-02, -8.9667e-02,  1.6906e-01,  1.2230e-01, -1.8494e-02,\n",
       "            1.2089e-01, -3.9820e-02,  6.1240e-02, -3.7622e-02,  6.7902e-02,\n",
       "            5.8332e-02, -5.7147e-02,  9.7254e-02, -4.9840e-02,  1.0529e-01,\n",
       "           -4.1558e-02, -1.1419e-02, -2.3225e-02,  1.0590e-02, -6.5957e-03,\n",
       "           -4.5932e-02, -1.2990e-02,  7.9259e-02,  2.4261e-02,  4.7065e-02,\n",
       "           -8.6077e-02, -1.5279e-01, -6.1447e-02, -1.0855e-02,  2.7373e-02,\n",
       "           -1.0619e-01, -1.3377e-01,  9.6018e-02,  8.6586e-02,  1.3031e-01,\n",
       "           -1.6688e-02,  6.8110e-02,  3.3965e-02,  1.6283e-01, -1.6650e-01,\n",
       "           -1.4567e-01,  4.6752e-03, -5.2499e-02,  1.0220e-02,  3.3777e-02,\n",
       "            5.1231e-02, -2.6475e-03,  5.5270e-02,  1.6716e-01, -8.1678e-02,\n",
       "            1.5612e-01, -2.4854e-02,  9.5958e-02, -2.4683e-02, -1.3152e-02,\n",
       "           -8.6819e-02,  8.9434e-02,  6.6587e-02,  1.7277e-01]],\n",
       "\n",
       "         [[ 6.4349e-02,  2.0247e-01,  5.2436e-02,  6.8347e-02,  7.6793e-02,\n",
       "            2.0838e-01,  2.0819e-01, -1.0147e-01,  1.3537e-01,  2.9118e-04,\n",
       "            8.8167e-02,  1.3259e-01,  5.5421e-02,  9.3705e-02,  8.7508e-02,\n",
       "           -1.1577e-01,  3.2657e-01, -5.4269e-02,  5.1181e-02, -3.7508e-02,\n",
       "           -3.5964e-02, -1.9077e-01, -2.6972e-01,  2.6537e-01,  1.7189e-01,\n",
       "            1.8577e-01,  7.9281e-03, -6.9247e-03, -1.3272e-01,  5.2503e-02,\n",
       "            6.0307e-02, -2.2165e-01, -4.2550e-02,  2.9286e-02,  6.8848e-02,\n",
       "            3.9285e-02,  2.2602e-01,  2.0975e-01, -2.1274e-02, -3.3617e-01,\n",
       "           -4.3465e-02, -7.0835e-02,  3.7512e-02, -8.4485e-02, -1.7809e-01,\n",
       "           -5.0487e-02, -1.2392e-02,  5.3696e-02, -8.8884e-02, -3.9755e-02,\n",
       "           -2.3327e-01,  1.2854e-01, -4.6540e-02, -2.3654e-02,  1.5209e-01,\n",
       "            1.4305e-02,  9.8003e-02, -1.4939e-02,  1.3325e-01, -1.3748e-01,\n",
       "           -9.9849e-02, -6.2584e-03, -1.2192e-01,  1.1585e-01],\n",
       "          [ 6.4349e-02,  2.0247e-01,  5.2436e-02,  6.8347e-02,  7.6793e-02,\n",
       "            2.0838e-01,  2.0819e-01, -1.0147e-01,  1.3537e-01,  2.9117e-04,\n",
       "            8.8167e-02,  1.3259e-01,  5.5421e-02,  9.3705e-02,  8.7508e-02,\n",
       "           -1.1577e-01,  3.2657e-01, -5.4269e-02,  5.1181e-02, -3.7508e-02,\n",
       "           -3.5964e-02, -1.9077e-01, -2.6972e-01,  2.6537e-01,  1.7189e-01,\n",
       "            1.8577e-01,  7.9281e-03, -6.9247e-03, -1.3272e-01,  5.2503e-02,\n",
       "            6.0307e-02, -2.2165e-01, -4.2550e-02,  2.9286e-02,  6.8848e-02,\n",
       "            3.9285e-02,  2.2602e-01,  2.0975e-01, -2.1274e-02, -3.3617e-01,\n",
       "           -4.3465e-02, -7.0835e-02,  3.7512e-02, -8.4485e-02, -1.7809e-01,\n",
       "           -5.0487e-02, -1.2392e-02,  5.3696e-02, -8.8884e-02, -3.9755e-02,\n",
       "           -2.3327e-01,  1.2854e-01, -4.6540e-02, -2.3654e-02,  1.5209e-01,\n",
       "            1.4305e-02,  9.8003e-02, -1.4939e-02,  1.3325e-01, -1.3748e-01,\n",
       "           -9.9849e-02, -6.2584e-03, -1.2192e-01,  1.1585e-01]],\n",
       "\n",
       "         [[ 3.5275e-01,  5.6663e-03,  1.5032e-01,  3.6963e-02, -6.6455e-01,\n",
       "            8.2245e-03, -1.6177e-01, -1.5903e-01, -2.1173e-01,  1.8611e-01,\n",
       "           -1.7880e-02,  1.7120e-01, -1.7990e-01,  8.9419e-02,  3.7616e-01,\n",
       "            1.3272e-02, -2.6414e-01, -1.2791e-01, -3.4738e-01, -2.3124e-01,\n",
       "            1.0478e-01,  6.2957e-02,  4.5373e-02,  9.7504e-02,  1.6483e-01,\n",
       "            6.0067e-01,  1.7422e-01, -1.4464e-01,  2.8060e-01, -3.4239e-01,\n",
       "            1.5389e-01,  1.2845e-01, -1.9265e-01, -1.1084e-01,  7.0732e-02,\n",
       "            6.9678e-02,  2.3889e-01,  3.2771e-02, -1.6484e-01,  3.3911e-01,\n",
       "           -2.2238e-02,  1.1521e-01, -9.4592e-02,  3.4489e-01,  2.9315e-01,\n",
       "            2.5644e-01,  1.4255e-01, -2.5077e-02,  1.5606e-01, -1.0775e-02,\n",
       "           -8.6749e-02, -2.8597e-01,  2.3911e-01, -1.2416e-01,  1.4724e-01,\n",
       "           -6.8820e-02, -1.2628e-01, -2.4263e-01,  7.9021e-02,  9.0877e-02,\n",
       "            4.3408e-01, -2.7259e-01,  5.4851e-02, -9.1904e-02],\n",
       "          [ 3.5275e-01,  5.6663e-03,  1.5032e-01,  3.6963e-02, -6.6455e-01,\n",
       "            8.2245e-03, -1.6177e-01, -1.5903e-01, -2.1173e-01,  1.8611e-01,\n",
       "           -1.7880e-02,  1.7120e-01, -1.7990e-01,  8.9419e-02,  3.7616e-01,\n",
       "            1.3272e-02, -2.6414e-01, -1.2791e-01, -3.4738e-01, -2.3124e-01,\n",
       "            1.0478e-01,  6.2957e-02,  4.5373e-02,  9.7504e-02,  1.6483e-01,\n",
       "            6.0067e-01,  1.7422e-01, -1.4464e-01,  2.8060e-01, -3.4239e-01,\n",
       "            1.5389e-01,  1.2845e-01, -1.9265e-01, -1.1084e-01,  7.0732e-02,\n",
       "            6.9678e-02,  2.3889e-01,  3.2771e-02, -1.6484e-01,  3.3911e-01,\n",
       "           -2.2238e-02,  1.1521e-01, -9.4592e-02,  3.4489e-01,  2.9315e-01,\n",
       "            2.5644e-01,  1.4255e-01, -2.5077e-02,  1.5606e-01, -1.0775e-02,\n",
       "           -8.6749e-02, -2.8597e-01,  2.3911e-01, -1.2416e-01,  1.4724e-01,\n",
       "           -6.8820e-02, -1.2628e-01, -2.4263e-01,  7.9021e-02,  9.0877e-02,\n",
       "            4.3408e-01, -2.7259e-01,  5.4851e-02, -9.1904e-02]],\n",
       "\n",
       "         [[-3.5971e-01,  2.2137e-01,  1.7767e-01,  2.3952e-01, -1.2061e-01,\n",
       "            3.2398e-02,  7.6972e-02, -1.8790e-01, -2.7517e-02, -3.5274e-01,\n",
       "            1.8269e-01, -3.2480e-01,  1.5513e-01,  4.4503e-02,  2.8103e-01,\n",
       "           -2.2422e-01, -6.3217e-02,  2.0625e-01, -4.1426e-02,  1.1131e-01,\n",
       "            2.9642e-02, -1.2011e-01,  1.8626e-01,  3.3266e-02,  1.7386e-01,\n",
       "           -5.8398e-02,  1.9422e-01,  1.0496e-01, -1.6284e-01,  2.3245e-01,\n",
       "            2.8684e-01,  2.7770e-01, -7.3228e-02,  3.2707e-01, -7.2439e-02,\n",
       "            1.3121e-02, -1.2084e-01, -1.0464e-01, -3.6873e-01, -5.9623e-02,\n",
       "            5.9329e-01, -9.8761e-02,  3.4214e-01, -4.3063e-01, -3.7533e-01,\n",
       "            8.7824e-02,  1.5908e-01,  7.2747e-02,  2.8431e-01, -3.0708e-01,\n",
       "           -1.8797e-01, -1.5810e-01, -6.6286e-02,  3.2247e-02, -4.1718e-01,\n",
       "           -3.5093e-01,  1.7068e-01, -2.4419e-01,  3.2101e-02,  2.1420e-01,\n",
       "            4.4551e-01,  3.4033e-01, -1.1913e-02, -6.3266e-01],\n",
       "          [-3.5971e-01,  2.2137e-01,  1.7767e-01,  2.3952e-01, -1.2061e-01,\n",
       "            3.2398e-02,  7.6972e-02, -1.8790e-01, -2.7517e-02, -3.5274e-01,\n",
       "            1.8269e-01, -3.2480e-01,  1.5513e-01,  4.4503e-02,  2.8103e-01,\n",
       "           -2.2422e-01, -6.3217e-02,  2.0625e-01, -4.1426e-02,  1.1131e-01,\n",
       "            2.9642e-02, -1.2011e-01,  1.8626e-01,  3.3266e-02,  1.7386e-01,\n",
       "           -5.8398e-02,  1.9422e-01,  1.0496e-01, -1.6284e-01,  2.3245e-01,\n",
       "            2.8684e-01,  2.7770e-01, -7.3228e-02,  3.2707e-01, -7.2439e-02,\n",
       "            1.3121e-02, -1.2084e-01, -1.0464e-01, -3.6873e-01, -5.9623e-02,\n",
       "            5.9329e-01, -9.8761e-02,  3.4214e-01, -4.3063e-01, -3.7533e-01,\n",
       "            8.7824e-02,  1.5908e-01,  7.2747e-02,  2.8431e-01, -3.0708e-01,\n",
       "           -1.8797e-01, -1.5810e-01, -6.6286e-02,  3.2247e-02, -4.1718e-01,\n",
       "           -3.5093e-01,  1.7068e-01, -2.4419e-01,  3.2101e-02,  2.1420e-01,\n",
       "            4.4551e-01,  3.4033e-01, -1.1913e-02, -6.3266e-01]],\n",
       "\n",
       "         [[-2.1068e-03,  1.4452e-02,  2.4039e-02, -3.2528e-02,  3.8879e-02,\n",
       "           -1.4703e-01,  1.5847e-01,  1.8037e-02,  6.3126e-02, -1.5730e-01,\n",
       "            1.9620e-01, -8.9748e-03,  2.5700e-02,  1.1569e-01, -1.5422e-01,\n",
       "           -4.9173e-02, -1.2920e-01,  8.1924e-02,  2.1766e-03,  1.8168e-01,\n",
       "           -1.3086e-01,  1.9358e-02,  1.3487e-01, -1.2462e-01, -2.1288e-02,\n",
       "            1.0661e-02,  3.1996e-01,  4.8812e-02,  1.4371e-01,  1.3115e-01,\n",
       "           -1.0517e-02, -7.0122e-02, -8.6512e-02,  1.1313e-02, -1.4976e-02,\n",
       "            1.1121e-01,  3.9644e-03, -1.5638e-01,  3.7311e-02, -1.6361e-01,\n",
       "           -6.2859e-02, -2.4016e-01,  6.4591e-02, -1.4812e-01,  2.2039e-03,\n",
       "            9.3601e-02,  2.5427e-02, -2.0589e-01,  7.9373e-02,  7.6686e-03,\n",
       "           -1.4801e-01, -8.8587e-02, -7.4635e-02,  7.0467e-02, -1.5385e-02,\n",
       "            1.8363e-01,  8.3192e-04,  3.3234e-02, -2.3011e-02,  7.6695e-02,\n",
       "           -1.4385e-01, -1.3588e-01,  4.9571e-02,  1.2633e-01],\n",
       "          [-2.1068e-03,  1.4452e-02,  2.4039e-02, -3.2528e-02,  3.8879e-02,\n",
       "           -1.4703e-01,  1.5847e-01,  1.8037e-02,  6.3126e-02, -1.5730e-01,\n",
       "            1.9620e-01, -8.9748e-03,  2.5700e-02,  1.1569e-01, -1.5422e-01,\n",
       "           -4.9173e-02, -1.2920e-01,  8.1924e-02,  2.1766e-03,  1.8168e-01,\n",
       "           -1.3086e-01,  1.9358e-02,  1.3487e-01, -1.2462e-01, -2.1288e-02,\n",
       "            1.0661e-02,  3.1996e-01,  4.8812e-02,  1.4371e-01,  1.3115e-01,\n",
       "           -1.0517e-02, -7.0122e-02, -8.6512e-02,  1.1313e-02, -1.4976e-02,\n",
       "            1.1121e-01,  3.9644e-03, -1.5638e-01,  3.7311e-02, -1.6361e-01,\n",
       "           -6.2859e-02, -2.4016e-01,  6.4591e-02, -1.4812e-01,  2.2039e-03,\n",
       "            9.3601e-02,  2.5427e-02, -2.0589e-01,  7.9373e-02,  7.6686e-03,\n",
       "           -1.4801e-01, -8.8587e-02, -7.4635e-02,  7.0467e-02, -1.5385e-02,\n",
       "            1.8363e-01,  8.3190e-04,  3.3234e-02, -2.3011e-02,  7.6695e-02,\n",
       "           -1.4385e-01, -1.3588e-01,  4.9571e-02,  1.2633e-01]],\n",
       "\n",
       "         [[ 4.6542e-01,  2.4324e-01,  4.2350e-01, -7.6786e-02,  3.3150e-02,\n",
       "            3.5201e-01,  2.1973e-01,  1.6456e-01, -9.6429e-02, -1.3180e-01,\n",
       "           -6.1409e-02, -1.1614e-01,  1.7391e-01, -1.3128e-01,  1.5464e-01,\n",
       "            5.1297e-02,  2.5305e-01, -8.6456e-02,  5.8230e-02, -7.5337e-01,\n",
       "            6.2129e-01,  4.1109e-02,  5.1390e-01,  1.6784e-01,  3.1084e-02,\n",
       "           -2.1649e-01,  9.8947e-02,  3.8320e-02, -5.4728e-02,  2.5943e-01,\n",
       "           -6.0372e-02,  4.2891e-01,  6.1813e-01,  1.7513e-01,  1.6649e-01,\n",
       "            8.4898e-01, -7.4504e-02,  7.3554e-02,  2.7244e-01,  1.7069e-01,\n",
       "            9.0558e-02,  2.8753e-01,  2.6989e-01,  5.6329e-02,  7.4772e-04,\n",
       "            2.5699e-01, -7.6206e-01, -5.1222e-01, -1.6589e-01,  3.1249e-01,\n",
       "           -1.3085e-01, -2.4592e-01,  1.0028e-01, -7.8936e-02,  2.3436e-01,\n",
       "           -7.7691e-01, -2.1395e-01, -3.8130e-01,  1.6769e-01,  5.0529e-02,\n",
       "            3.5915e-01,  1.3469e-01, -1.3617e-01, -2.7728e-01],\n",
       "          [ 4.6542e-01,  2.4324e-01,  4.2350e-01, -7.6786e-02,  3.3150e-02,\n",
       "            3.5201e-01,  2.1973e-01,  1.6456e-01, -9.6429e-02, -1.3180e-01,\n",
       "           -6.1409e-02, -1.1614e-01,  1.7391e-01, -1.3128e-01,  1.5464e-01,\n",
       "            5.1297e-02,  2.5305e-01, -8.6456e-02,  5.8230e-02, -7.5337e-01,\n",
       "            6.2129e-01,  4.1109e-02,  5.1390e-01,  1.6784e-01,  3.1084e-02,\n",
       "           -2.1649e-01,  9.8947e-02,  3.8320e-02, -5.4728e-02,  2.5943e-01,\n",
       "           -6.0372e-02,  4.2891e-01,  6.1813e-01,  1.7513e-01,  1.6649e-01,\n",
       "            8.4898e-01, -7.4504e-02,  7.3554e-02,  2.7244e-01,  1.7069e-01,\n",
       "            9.0558e-02,  2.8753e-01,  2.6989e-01,  5.6329e-02,  7.4773e-04,\n",
       "            2.5699e-01, -7.6206e-01, -5.1222e-01, -1.6589e-01,  3.1249e-01,\n",
       "           -1.3085e-01, -2.4592e-01,  1.0028e-01, -7.8936e-02,  2.3436e-01,\n",
       "           -7.7691e-01, -2.1395e-01, -3.8130e-01,  1.6769e-01,  5.0529e-02,\n",
       "            3.5915e-01,  1.3469e-01, -1.3617e-01, -2.7728e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-4.2251, -1.9105,  1.4189,  ...,  0.7780, -1.8154, -0.6046],\n",
       "          [-3.2166, -0.9024,  1.4027,  ..., -2.8168,  0.9198, -1.0778],\n",
       "          [-1.8708, -3.7763,  0.5263,  ..., -0.4445, -2.3319, -0.6484],\n",
       "          ...,\n",
       "          [ 0.1643, -0.0200, -1.0146,  ...,  0.4349,  0.3249, -1.4587],\n",
       "          [ 2.3498, -2.3927,  0.2800,  ...,  2.5887,  2.2019,  0.6381],\n",
       "          [-0.0195,  1.1832,  0.5131,  ...,  0.1182, -0.0792,  0.5819]],\n",
       "\n",
       "         [[ 0.1626,  0.4006,  0.8934,  ..., -1.8606, -0.9087, -1.3518],\n",
       "          [-1.0279, -0.3565,  0.5012,  ..., -0.6599, -1.8016,  1.0188],\n",
       "          [-0.3248,  0.2332,  0.9346,  ..., -2.0312, -1.8977,  0.8187],\n",
       "          ...,\n",
       "          [-0.2724, -1.7402, -2.6240,  ..., -2.0097, -0.4298,  0.8527],\n",
       "          [ 0.1255, -0.4852, -2.0100,  ..., -0.3563, -0.9967,  1.4266],\n",
       "          [-0.2621,  0.0447, -0.0256,  ...,  0.2299,  0.0738, -0.2141]],\n",
       "\n",
       "         [[-0.9846, -0.2372,  3.9682,  ...,  0.3614,  0.3322,  1.0859],\n",
       "          [ 1.1899,  1.7866,  2.3639,  ..., -1.2478,  2.0059, -0.5867],\n",
       "          [ 1.1862,  0.0066,  2.5790,  ..., -0.2277,  1.8493,  0.7166],\n",
       "          ...,\n",
       "          [ 0.4220,  0.6928, -0.2636,  ..., -1.3080,  0.1711, -1.6538],\n",
       "          [ 0.2964,  2.0934, -0.1942,  ..., -0.2411, -2.4697, -0.3663],\n",
       "          [ 0.0878, -0.0073, -0.4256,  ...,  0.0821,  0.0968, -0.5559]],\n",
       "\n",
       "         [[ 0.6779, -0.6786, -0.3837,  ..., -1.9476, -3.8403,  0.6364],\n",
       "          [-0.2889, -0.1458,  0.7506,  ..., -3.8011, -2.2298, -0.6114],\n",
       "          [ 2.4348,  0.8546,  1.2851,  ..., -2.5755,  1.3265,  2.9120],\n",
       "          ...,\n",
       "          [-1.6006,  1.2098,  1.0835,  ...,  0.9948, -0.0669, -0.6596],\n",
       "          [ 0.0684,  0.2398,  0.9548,  ..., -0.4955,  2.1270, -1.2116],\n",
       "          [ 0.1287, -0.3331, -0.0593,  ...,  0.3119,  0.0453,  0.1211]],\n",
       "\n",
       "         [[-2.7635, -1.1666,  2.8325,  ...,  0.3832,  1.3501,  2.1814],\n",
       "          [-2.7178, -1.6264,  1.8317,  ..., -1.1694,  0.5125,  3.5432],\n",
       "          [-2.8810, -0.6931,  3.0505,  ..., -3.5087, -1.9799,  1.7324],\n",
       "          ...,\n",
       "          [-0.0135,  0.2295, -1.1472,  ..., -2.6627, -3.0060,  2.1391],\n",
       "          [ 1.2850, -2.5289, -1.0265,  ..., -1.4971, -4.0747, -0.8460],\n",
       "          [ 0.0276,  0.3686, -0.2861,  ...,  0.1961, -0.1707, -0.0568]],\n",
       "\n",
       "         [[ 1.3065, -0.5786, -4.4335,  ...,  1.5501, -0.0224,  0.4595],\n",
       "          [ 3.8705, -0.3399, -3.2517,  ..., -0.0219,  0.7378,  2.5889],\n",
       "          [ 0.3189,  0.6794, -3.5189,  ..., -0.1658,  1.1557,  0.6054],\n",
       "          ...,\n",
       "          [ 0.1786,  1.7008, -3.1336,  ...,  2.2968,  1.9499, -3.4458],\n",
       "          [ 1.7529,  3.4605, -1.5777,  ...,  0.6257,  2.9563, -0.1608],\n",
       "          [-0.2578, -0.0587,  1.3200,  ...,  0.0049,  0.0407, -0.0369]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-1.4157e+00, -6.3637e-01,  8.2324e-01,  ...,  8.0446e-01,\n",
       "           -4.5252e-01, -8.4191e-02],\n",
       "          [-8.7536e-01, -7.1252e-01,  1.8452e-01,  ..., -5.9691e-01,\n",
       "            6.9235e-01, -1.1664e+00],\n",
       "          [-9.8348e-01, -1.9941e-01, -1.3059e+00,  ..., -1.6396e-01,\n",
       "           -1.2680e+00, -1.0658e+00],\n",
       "          ...,\n",
       "          [-2.2554e-01,  8.5997e-01, -9.6651e-01,  ..., -3.2095e+00,\n",
       "            2.1203e-01,  2.0536e+00],\n",
       "          [-2.3113e+00,  1.7618e+00,  2.7936e-01,  ..., -8.2771e-02,\n",
       "           -3.1797e-02,  1.5051e+00],\n",
       "          [ 4.1204e-02, -2.7080e-02,  2.4173e-01,  ...,  2.1191e-01,\n",
       "           -7.4280e-03,  1.2310e-01]],\n",
       "\n",
       "         [[ 1.4189e+00,  1.2563e+00, -7.7798e-02,  ...,  5.0931e+00,\n",
       "            1.3756e+00,  1.7997e+00],\n",
       "          [ 2.5186e+00,  3.0924e-01,  3.3996e+00,  ...,  2.8250e+00,\n",
       "            5.7138e-01,  1.8764e+00],\n",
       "          [ 4.1066e-01, -5.2971e-01, -2.7873e-01,  ...,  3.5950e+00,\n",
       "            1.6008e-01,  3.3203e+00],\n",
       "          ...,\n",
       "          [ 3.9773e+00,  1.5031e+00,  1.1002e+00,  ...,  1.2489e+00,\n",
       "           -2.7406e+00,  2.9745e-01],\n",
       "          [-2.1068e+00,  2.8271e+00, -1.5830e+00,  ...,  1.1157e+00,\n",
       "           -2.5440e+00, -1.9575e+00],\n",
       "          [-5.4643e-02,  1.1233e-03,  9.8239e-02,  ...,  2.5478e-03,\n",
       "            7.0655e-03, -1.5047e-02]],\n",
       "\n",
       "         [[-4.4961e-01,  1.7153e-01, -2.8249e+00,  ..., -1.5638e+00,\n",
       "            1.3940e+00,  5.5687e-01],\n",
       "          [ 1.9106e+00, -2.9569e+00, -3.3663e+00,  ...,  1.5543e-01,\n",
       "            4.2911e-01, -1.4980e+00],\n",
       "          [ 6.5525e-01, -1.5499e+00, -3.1436e+00,  ...,  2.0798e+00,\n",
       "            2.0478e+00, -8.6831e-01],\n",
       "          ...,\n",
       "          [ 9.5035e-01, -1.4052e+00, -4.4450e+00,  ..., -4.0910e-01,\n",
       "            3.0123e+00, -2.2604e+00],\n",
       "          [-2.2666e+00, -5.5491e-01, -3.1511e+00,  ...,  2.2267e+00,\n",
       "            3.3702e+00, -1.8220e-01],\n",
       "          [-1.5377e-01,  2.4534e-02, -8.2374e-02,  ..., -1.4547e-01,\n",
       "           -1.2294e-02, -1.8973e-01]],\n",
       "\n",
       "         [[-2.8697e+00,  7.3868e+00, -1.5284e+00,  ...,  1.6434e+00,\n",
       "           -5.9213e-01,  1.1424e+00],\n",
       "          [-7.0143e+00,  2.7697e+00, -3.0247e+00,  ...,  2.1928e+00,\n",
       "           -2.9533e+00, -1.4381e+00],\n",
       "          [-5.4050e+00,  4.5578e+00, -2.1808e+00,  ...,  1.5221e+00,\n",
       "           -2.3460e+00, -4.4839e+00],\n",
       "          ...,\n",
       "          [-1.3042e+00, -1.3276e+00,  3.6088e-01,  ..., -9.7663e-01,\n",
       "            1.8530e+00, -1.6781e+00],\n",
       "          [-2.5089e+00, -6.8416e-01,  1.5488e+00,  ..., -1.9247e+00,\n",
       "            1.7493e+00,  1.0485e+00],\n",
       "          [ 1.4329e-01,  6.9799e-03,  3.7217e-01,  ..., -5.9735e-02,\n",
       "            1.1066e-01, -4.7298e-01]],\n",
       "\n",
       "         [[-1.8694e-01,  1.0718e-01,  3.7091e-01,  ...,  5.0425e-01,\n",
       "            3.2801e+00, -8.7270e-02],\n",
       "          [-2.9385e+00,  4.1737e-01,  2.1933e+00,  ..., -1.4775e+00,\n",
       "            1.0264e+00,  8.0377e-01],\n",
       "          [-1.2103e+00,  1.3254e+00,  4.8593e-01,  ...,  2.2733e+00,\n",
       "            2.8600e+00, -8.6914e-01],\n",
       "          ...,\n",
       "          [-6.5059e-01, -2.6985e-01, -2.9000e+00,  ..., -1.5460e+00,\n",
       "            1.2552e+00,  4.2623e-01],\n",
       "          [-8.4334e-01, -1.1542e+00, -2.8879e+00,  ...,  6.1075e-02,\n",
       "            2.2631e+00, -1.0444e+00],\n",
       "          [ 1.4643e-02, -9.6950e-02, -4.4148e-02,  ...,  8.7768e-03,\n",
       "            2.0026e-02,  4.9316e-02]],\n",
       "\n",
       "         [[ 1.2004e+00, -5.7540e-01, -3.7276e+00,  ...,  2.3904e+00,\n",
       "            2.1238e+00,  2.4305e-01],\n",
       "          [-4.4159e+00, -3.6538e+00, -3.9155e+00,  ...,  2.0786e+00,\n",
       "            7.1457e-02,  2.4816e+00],\n",
       "          [-4.8076e-01,  2.7275e+00, -4.0197e+00,  ..., -8.0696e-01,\n",
       "           -1.9602e-01, -5.9344e-02],\n",
       "          ...,\n",
       "          [-1.7359e+00, -4.0276e+00, -1.4358e+00,  ...,  1.7302e+00,\n",
       "            2.6031e+00,  1.6099e+00],\n",
       "          [ 9.1617e-01, -3.3169e+00, -1.3500e+00,  ..., -3.2089e+00,\n",
       "           -2.9007e+00,  9.6733e-01],\n",
       "          [ 4.0856e-02,  9.2128e-02,  1.0561e-01,  ...,  1.2623e-01,\n",
       "            3.7905e-02,  1.7242e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.9670,  0.6117,  0.0439, -0.7283, -0.2128,  0.0382, -0.5976,\n",
       "           -0.4968,  0.5186,  0.6504, -1.3769, -0.1680, -0.2870,  0.3017,\n",
       "            0.3843,  1.2086, -0.0142, -0.5935, -0.3402,  0.1307,  0.1226,\n",
       "            0.1458,  0.1127,  0.8396, -0.2166,  0.0107,  0.1282, -0.2598,\n",
       "           -0.2248, -0.2949, -0.0634, -0.2813, -0.6510,  0.1753,  0.1071,\n",
       "            0.1719,  0.4922,  0.5473,  0.3336,  0.5331, -0.2205,  0.1936,\n",
       "           -0.4566,  0.6712, -0.2430,  0.4205, -0.5193,  0.0191, -0.5498,\n",
       "            0.9873,  0.5191,  0.5260, -0.7343, -0.0593, -0.2027,  0.9570,\n",
       "           -0.3827, -0.1536,  0.4748,  0.3277,  0.1942, -0.3827,  0.5718,\n",
       "           -0.0973],\n",
       "          [ 0.9670,  0.6117,  0.0439, -0.7283, -0.2128,  0.0382, -0.5976,\n",
       "           -0.4968,  0.5186,  0.6504, -1.3769, -0.1680, -0.2870,  0.3017,\n",
       "            0.3843,  1.2086, -0.0142, -0.5935, -0.3402,  0.1307,  0.1226,\n",
       "            0.1458,  0.1127,  0.8396, -0.2166,  0.0107,  0.1282, -0.2598,\n",
       "           -0.2248, -0.2949, -0.0634, -0.2813, -0.6510,  0.1753,  0.1071,\n",
       "            0.1719,  0.4922,  0.5473,  0.3336,  0.5331, -0.2205,  0.1936,\n",
       "           -0.4566,  0.6712, -0.2430,  0.4205, -0.5193,  0.0191, -0.5498,\n",
       "            0.9873,  0.5191,  0.5260, -0.7343, -0.0593, -0.2027,  0.9570,\n",
       "           -0.3827, -0.1536,  0.4748,  0.3277,  0.1942, -0.3827,  0.5718,\n",
       "           -0.0973]],\n",
       "\n",
       "         [[ 0.3678, -0.4594, -0.0973,  0.5955,  0.2477, -0.2536,  0.2278,\n",
       "            1.2338, -0.4315, -0.4941, -0.6753, -1.1523,  0.1322,  0.2674,\n",
       "           -0.0060, -0.3146,  0.0135, -0.0502,  0.4605, -0.3188,  0.7155,\n",
       "            0.0566,  0.3924, -0.4534,  0.3706, -0.3309, -0.6167,  1.0358,\n",
       "            0.1869,  0.1143,  0.4875,  0.0290, -0.0048,  0.4707,  0.5449,\n",
       "            0.3240,  0.0714,  0.2202, -0.0579, -0.0337,  0.6888,  1.1203,\n",
       "           -0.5995, -0.2480, -0.3345,  0.3782, -0.4584, -1.2064, -0.1071,\n",
       "           -1.3026,  0.0184,  0.1781, -0.0872, -0.1304, -0.3496, -0.5343,\n",
       "            0.2335, -0.2166,  0.0071, -0.1965,  0.0363, -0.3704, -0.0977,\n",
       "           -0.5737],\n",
       "          [ 0.3678, -0.4594, -0.0973,  0.5955,  0.2477, -0.2536,  0.2278,\n",
       "            1.2338, -0.4315, -0.4941, -0.6753, -1.1523,  0.1322,  0.2674,\n",
       "           -0.0060, -0.3146,  0.0135, -0.0502,  0.4605, -0.3188,  0.7155,\n",
       "            0.0566,  0.3924, -0.4534,  0.3706, -0.3309, -0.6167,  1.0358,\n",
       "            0.1869,  0.1143,  0.4875,  0.0290, -0.0048,  0.4707,  0.5449,\n",
       "            0.3240,  0.0714,  0.2202, -0.0579, -0.0337,  0.6888,  1.1203,\n",
       "           -0.5995, -0.2480, -0.3345,  0.3782, -0.4584, -1.2064, -0.1071,\n",
       "           -1.3026,  0.0184,  0.1781, -0.0872, -0.1304, -0.3496, -0.5343,\n",
       "            0.2335, -0.2166,  0.0071, -0.1965,  0.0363, -0.3704, -0.0977,\n",
       "           -0.5737]],\n",
       "\n",
       "         [[-0.4448, -0.0870, -0.0908, -0.1350, -0.9071,  0.5727,  0.7532,\n",
       "           -0.5260,  0.6072,  0.6970, -0.0417, -0.4736,  0.2991,  0.1352,\n",
       "           -0.1261,  0.3174, -0.5305,  0.2265, -0.8223, -0.8362,  0.5937,\n",
       "            0.6815,  0.3670,  1.2157, -0.0831, -0.7555,  0.3590,  0.3683,\n",
       "           -0.2369, -1.0469,  0.4853,  0.3019,  0.1217,  0.1069,  0.2429,\n",
       "            0.1088, -0.1512,  0.1665,  0.8807,  0.5933,  0.8063,  0.2144,\n",
       "            0.4567,  0.4435,  0.2541,  0.2289, -0.1693, -0.1881,  0.9961,\n",
       "            0.3246, -1.5995, -0.2617, -0.2369, -0.3243, -0.3834,  0.8568,\n",
       "            0.7245, -0.7659, -1.4968, -0.0301,  0.0327, -0.3941, -1.1527,\n",
       "           -0.4183],\n",
       "          [-0.4448, -0.0870, -0.0908, -0.1350, -0.9071,  0.5727,  0.7532,\n",
       "           -0.5260,  0.6072,  0.6970, -0.0417, -0.4736,  0.2991,  0.1352,\n",
       "           -0.1261,  0.3174, -0.5305,  0.2265, -0.8223, -0.8362,  0.5937,\n",
       "            0.6815,  0.3670,  1.2157, -0.0831, -0.7555,  0.3590,  0.3683,\n",
       "           -0.2369, -1.0469,  0.4853,  0.3019,  0.1217,  0.1069,  0.2429,\n",
       "            0.1088, -0.1512,  0.1665,  0.8807,  0.5933,  0.8063,  0.2144,\n",
       "            0.4567,  0.4435,  0.2541,  0.2289, -0.1693, -0.1881,  0.9961,\n",
       "            0.3246, -1.5995, -0.2617, -0.2369, -0.3243, -0.3834,  0.8568,\n",
       "            0.7245, -0.7659, -1.4968, -0.0301,  0.0327, -0.3941, -1.1527,\n",
       "           -0.4183]],\n",
       "\n",
       "         [[ 0.3482, -1.3202, -1.3677,  0.5557,  0.6780, -0.2533, -0.4545,\n",
       "            0.4562, -0.3339, -0.4809, -0.0202, -0.2481, -0.6748, -0.3485,\n",
       "           -0.4551,  0.7978, -0.3578, -0.6466, -0.7003,  1.5080, -0.2219,\n",
       "           -1.0558,  0.1201, -0.4216, -0.4844,  0.3083,  0.2039, -0.0897,\n",
       "            0.7622,  0.0057, -0.2069, -0.6312,  0.4551,  0.7536, -0.3726,\n",
       "            0.0926, -0.6968,  0.1689,  1.4308,  0.4423,  0.3373,  0.1964,\n",
       "           -0.1468, -1.0238, -0.7468,  0.3657, -0.7888,  0.7420, -1.0354,\n",
       "           -0.3486, -0.8238,  1.0301,  0.4936,  0.3562, -0.4343,  0.5103,\n",
       "            0.0636,  0.2590, -0.3785, -0.2455, -0.3924,  0.0463, -0.5061,\n",
       "           -0.5225],\n",
       "          [ 0.3482, -1.3202, -1.3677,  0.5557,  0.6780, -0.2533, -0.4545,\n",
       "            0.4562, -0.3339, -0.4809, -0.0202, -0.2481, -0.6748, -0.3485,\n",
       "           -0.4551,  0.7978, -0.3578, -0.6466, -0.7003,  1.5080, -0.2219,\n",
       "           -1.0558,  0.1201, -0.4216, -0.4844,  0.3083,  0.2039, -0.0897,\n",
       "            0.7622,  0.0057, -0.2069, -0.6312,  0.4551,  0.7536, -0.3726,\n",
       "            0.0926, -0.6968,  0.1689,  1.4308,  0.4423,  0.3373,  0.1964,\n",
       "           -0.1468, -1.0238, -0.7468,  0.3657, -0.7888,  0.7420, -1.0354,\n",
       "           -0.3486, -0.8238,  1.0301,  0.4936,  0.3562, -0.4343,  0.5103,\n",
       "            0.0636,  0.2590, -0.3785, -0.2455, -0.3924,  0.0463, -0.5061,\n",
       "           -0.5225]],\n",
       "\n",
       "         [[ 0.4944,  0.3165,  0.2139, -0.0279, -0.6750,  0.0634,  0.0125,\n",
       "            0.1439, -0.3770,  0.2017, -0.3140, -0.1778, -0.1042,  0.2675,\n",
       "           -0.0636, -0.0945, -0.0754,  0.2243,  0.0419, -0.1294, -0.6092,\n",
       "            0.0724,  0.1418,  0.0017, -0.2248,  0.1349,  0.3066, -1.0660,\n",
       "           -0.3001,  0.2580, -0.1263,  0.3966,  0.1730,  0.0827,  0.0720,\n",
       "            0.2464, -0.3264, -0.3986, -0.0984, -1.4048, -0.1437, -0.4381,\n",
       "           -0.1979,  0.0740,  0.0449, -0.1354,  0.1616,  0.0752,  0.0788,\n",
       "           -0.1523,  0.2636, -0.3551, -0.2002, -0.2978, -0.0154,  0.1107,\n",
       "            0.0541,  0.4894, -0.4288, -0.3204, -0.1635,  0.0773,  0.3201,\n",
       "            0.2799],\n",
       "          [ 0.4944,  0.3165,  0.2139, -0.0279, -0.6750,  0.0634,  0.0125,\n",
       "            0.1439, -0.3770,  0.2017, -0.3140, -0.1778, -0.1042,  0.2675,\n",
       "           -0.0636, -0.0945, -0.0754,  0.2243,  0.0419, -0.1294, -0.6092,\n",
       "            0.0724,  0.1418,  0.0017, -0.2248,  0.1349,  0.3066, -1.0660,\n",
       "           -0.3001,  0.2580, -0.1263,  0.3966,  0.1730,  0.0827,  0.0720,\n",
       "            0.2464, -0.3264, -0.3986, -0.0984, -1.4048, -0.1437, -0.4381,\n",
       "           -0.1979,  0.0740,  0.0449, -0.1354,  0.1616,  0.0752,  0.0788,\n",
       "           -0.1523,  0.2636, -0.3551, -0.2002, -0.2978, -0.0154,  0.1107,\n",
       "            0.0541,  0.4894, -0.4288, -0.3204, -0.1635,  0.0773,  0.3201,\n",
       "            0.2799]],\n",
       "\n",
       "         [[-1.0483,  0.0585,  0.6058, -0.6031,  0.4715, -0.6730, -0.5551,\n",
       "           -0.2259, -0.1508, -0.7274, -0.7028, -0.7385,  1.4349, -0.2853,\n",
       "            0.6723,  0.0857, -1.2734,  0.4305,  0.1290, -0.2154, -0.3418,\n",
       "            0.4427,  0.7529,  0.3630, -0.1258,  0.0690,  0.6184,  0.4704,\n",
       "            0.1631, -0.1201, -0.7190,  0.3868, -0.1930, -0.0181, -0.8653,\n",
       "           -0.1089, -0.4926, -1.2158,  0.3092,  1.2469, -0.3331,  0.8577,\n",
       "           -0.7900,  0.3385,  0.9512,  0.8000,  0.5823, -0.8574, -1.6570,\n",
       "           -0.3325,  0.5270,  0.7111,  0.1141,  0.5100,  1.6153, -0.3810,\n",
       "           -0.0076,  0.1513,  0.4695, -0.3810, -0.1138, -0.1237,  0.9571,\n",
       "            0.6275],\n",
       "          [-1.0483,  0.0585,  0.6058, -0.6031,  0.4715, -0.6730, -0.5551,\n",
       "           -0.2259, -0.1508, -0.7274, -0.7028, -0.7385,  1.4349, -0.2853,\n",
       "            0.6723,  0.0857, -1.2734,  0.4305,  0.1290, -0.2154, -0.3418,\n",
       "            0.4427,  0.7529,  0.3630, -0.1258,  0.0690,  0.6184,  0.4704,\n",
       "            0.1631, -0.1201, -0.7190,  0.3868, -0.1930, -0.0181, -0.8653,\n",
       "           -0.1089, -0.4926, -1.2158,  0.3092,  1.2469, -0.3331,  0.8577,\n",
       "           -0.7900,  0.3385,  0.9512,  0.8000,  0.5823, -0.8574, -1.6570,\n",
       "           -0.3325,  0.5270,  0.7111,  0.1141,  0.5100,  1.6153, -0.3810,\n",
       "           -0.0076,  0.1513,  0.4695, -0.3810, -0.1138, -0.1237,  0.9571,\n",
       "            0.6275]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 0.0109,  0.0202,  0.0991, -0.0812, -0.1284,  0.1000,  0.0337,\n",
       "           -0.0974,  0.0250, -0.0621,  0.0661, -0.0787,  0.0160, -0.0283,\n",
       "            0.0608, -0.1206, -0.1006, -0.1007, -0.0369,  0.0081, -0.0226,\n",
       "            0.1243,  0.0393,  0.0290,  0.1575,  0.0921,  0.0322,  0.0463,\n",
       "           -0.0383,  0.0121, -0.1000, -0.1353,  0.0407,  0.0517,  0.0576,\n",
       "           -0.0859,  0.0739, -0.0957,  0.0445, -0.0222, -0.0381,  0.0102,\n",
       "            0.1074, -0.0515,  0.1906,  0.0770,  0.1495,  0.2170,  0.0484,\n",
       "           -0.1198, -0.0036,  0.0405,  0.0097,  0.2191, -0.0673, -0.0290,\n",
       "            0.1087, -0.0980,  0.0720,  0.0014,  0.0394, -0.0730, -0.0107,\n",
       "            0.0096],\n",
       "          [ 0.0109,  0.0202,  0.0991, -0.0812, -0.1284,  0.1000,  0.0337,\n",
       "           -0.0974,  0.0250, -0.0621,  0.0661, -0.0787,  0.0160, -0.0283,\n",
       "            0.0608, -0.1206, -0.1006, -0.1007, -0.0369,  0.0081, -0.0226,\n",
       "            0.1243,  0.0393,  0.0290,  0.1575,  0.0921,  0.0322,  0.0463,\n",
       "           -0.0383,  0.0121, -0.1000, -0.1353,  0.0407,  0.0517,  0.0576,\n",
       "           -0.0859,  0.0739, -0.0957,  0.0445, -0.0222, -0.0381,  0.0102,\n",
       "            0.1074, -0.0515,  0.1906,  0.0770,  0.1495,  0.2170,  0.0484,\n",
       "           -0.1198, -0.0036,  0.0405,  0.0097,  0.2191, -0.0673, -0.0290,\n",
       "            0.1087, -0.0980,  0.0720,  0.0014,  0.0394, -0.0730, -0.0107,\n",
       "            0.0096]],\n",
       "\n",
       "         [[ 0.0280,  0.0264,  0.0255, -0.0053, -0.0649,  0.0338,  0.0330,\n",
       "           -0.2969, -0.1608,  0.0617,  0.0329,  0.3141, -0.0814, -0.0669,\n",
       "            0.1467,  0.0358, -0.0870, -0.0644,  0.0462,  0.0415,  0.0202,\n",
       "            0.0968,  0.2452, -0.1684,  0.0822,  0.0211, -0.0293,  0.0360,\n",
       "           -0.0320,  0.2985,  0.0451, -0.0488, -0.0650, -0.3041, -0.0386,\n",
       "           -0.0530, -0.0274,  0.0642, -0.0970,  0.1166,  0.1777,  0.0099,\n",
       "           -0.1881,  0.0582, -0.0602,  0.2185,  0.0118, -0.0402,  0.2380,\n",
       "           -0.0295, -0.1021,  0.1199,  0.0659, -0.2069, -0.0304,  0.0492,\n",
       "            0.1840, -0.0189, -0.0833,  0.1428, -0.1030, -0.0555, -0.0371,\n",
       "            0.0157],\n",
       "          [ 0.0280,  0.0264,  0.0255, -0.0053, -0.0649,  0.0338,  0.0330,\n",
       "           -0.2969, -0.1608,  0.0617,  0.0329,  0.3141, -0.0814, -0.0669,\n",
       "            0.1467,  0.0358, -0.0870, -0.0644,  0.0462,  0.0415,  0.0202,\n",
       "            0.0968,  0.2452, -0.1684,  0.0822,  0.0211, -0.0293,  0.0360,\n",
       "           -0.0320,  0.2985,  0.0451, -0.0488, -0.0650, -0.3041, -0.0386,\n",
       "           -0.0530, -0.0274,  0.0642, -0.0970,  0.1166,  0.1777,  0.0099,\n",
       "           -0.1881,  0.0582, -0.0602,  0.2185,  0.0118, -0.0402,  0.2380,\n",
       "           -0.0295, -0.1021,  0.1199,  0.0659, -0.2069, -0.0304,  0.0492,\n",
       "            0.1840, -0.0189, -0.0833,  0.1428, -0.1030, -0.0555, -0.0371,\n",
       "            0.0157]],\n",
       "\n",
       "         [[-0.2778,  0.0485, -0.3064, -0.0787, -0.2949, -0.2167,  0.0646,\n",
       "            0.4337, -0.0440,  0.4030, -0.1847,  0.0150, -0.9833, -0.1727,\n",
       "            0.3413, -0.1376,  0.1631,  0.3768, -0.7635,  0.3974,  0.2322,\n",
       "           -0.4129, -0.2009, -0.1309, -0.3078,  0.2788,  0.0426,  0.5305,\n",
       "           -0.3247,  0.0103,  0.4073,  0.0654,  0.0503,  0.2559, -0.1293,\n",
       "           -0.2779, -0.0956,  0.3608,  0.2087,  0.0673,  0.1341,  0.3594,\n",
       "           -0.1181,  0.1747, -0.0777, -0.2852, -0.0714,  0.0679, -0.1151,\n",
       "           -0.0165,  0.1923,  0.1812,  0.1185, -0.3166,  0.2704,  0.0721,\n",
       "            0.4909, -0.1514, -0.2817, -0.0109,  0.1078, -0.2228,  0.2900,\n",
       "           -0.4438],\n",
       "          [-0.2778,  0.0485, -0.3064, -0.0787, -0.2949, -0.2167,  0.0646,\n",
       "            0.4337, -0.0440,  0.4030, -0.1847,  0.0150, -0.9833, -0.1727,\n",
       "            0.3413, -0.1376,  0.1631,  0.3768, -0.7635,  0.3974,  0.2322,\n",
       "           -0.4129, -0.2009, -0.1309, -0.3078,  0.2788,  0.0426,  0.5305,\n",
       "           -0.3247,  0.0103,  0.4073,  0.0654,  0.0503,  0.2559, -0.1293,\n",
       "           -0.2779, -0.0956,  0.3608,  0.2087,  0.0673,  0.1341,  0.3594,\n",
       "           -0.1181,  0.1747, -0.0777, -0.2852, -0.0714,  0.0679, -0.1151,\n",
       "           -0.0165,  0.1923,  0.1812,  0.1185, -0.3166,  0.2704,  0.0721,\n",
       "            0.4909, -0.1514, -0.2817, -0.0109,  0.1078, -0.2228,  0.2900,\n",
       "           -0.4438]],\n",
       "\n",
       "         [[ 0.0035,  0.0360, -0.2114, -0.1300, -0.3498,  0.0897, -0.1338,\n",
       "           -0.0103,  0.0031, -0.0658,  0.0248, -0.1584, -0.1077, -0.1302,\n",
       "           -0.1396,  0.0651,  0.0783, -0.2157,  0.1918, -0.0365, -0.0637,\n",
       "            0.1890,  0.1415,  0.0842, -0.0683,  0.0745, -0.0970,  0.2365,\n",
       "            0.0626, -0.1017,  0.1615,  0.0960,  0.0978, -0.0613,  0.1305,\n",
       "            0.0764, -0.0925,  0.0187,  0.0835,  0.0471, -0.1494, -0.2965,\n",
       "            0.1158, -0.0398, -0.0344,  0.0785,  0.1142,  0.0821, -0.0864,\n",
       "           -0.0566, -0.2235, -0.1240,  0.0604, -0.1546,  0.0627, -0.0080,\n",
       "           -0.1326, -0.0672, -0.1821, -0.2288, -0.0766,  0.0600, -0.0295,\n",
       "            0.0612],\n",
       "          [ 0.0035,  0.0360, -0.2114, -0.1300, -0.3498,  0.0897, -0.1338,\n",
       "           -0.0103,  0.0031, -0.0658,  0.0248, -0.1584, -0.1077, -0.1302,\n",
       "           -0.1396,  0.0651,  0.0783, -0.2157,  0.1918, -0.0365, -0.0637,\n",
       "            0.1890,  0.1415,  0.0842, -0.0683,  0.0745, -0.0970,  0.2365,\n",
       "            0.0626, -0.1017,  0.1615,  0.0960,  0.0978, -0.0613,  0.1305,\n",
       "            0.0764, -0.0925,  0.0187,  0.0835,  0.0471, -0.1494, -0.2965,\n",
       "            0.1158, -0.0398, -0.0344,  0.0785,  0.1142,  0.0821, -0.0864,\n",
       "           -0.0566, -0.2235, -0.1240,  0.0604, -0.1546,  0.0627, -0.0080,\n",
       "           -0.1326, -0.0672, -0.1821, -0.2288, -0.0766,  0.0600, -0.0295,\n",
       "            0.0612]],\n",
       "\n",
       "         [[-0.0795,  0.0031,  0.0566,  0.0845,  0.1235, -0.0120, -0.1661,\n",
       "            0.1269,  0.0205,  0.0716,  0.0249, -0.4068,  0.4051,  0.0433,\n",
       "            0.1392,  0.0282, -0.0477, -0.1894, -0.2474,  0.3603,  0.0898,\n",
       "           -0.1820, -0.1293,  0.1103,  0.2778,  0.0561,  0.0361, -0.0226,\n",
       "            0.1088, -0.1036, -0.1131, -0.0166,  0.0600,  0.1149, -0.1272,\n",
       "            0.2685,  0.4629,  0.3167, -0.0307,  0.0957, -0.0125,  0.0884,\n",
       "           -0.1734,  0.0429,  0.0052,  0.0056, -0.0031, -0.1425, -0.2663,\n",
       "            0.0483, -0.0545,  0.0531, -0.1609,  0.0191,  0.0577, -0.1131,\n",
       "            0.1973,  0.0370, -0.0255,  0.0603,  0.3520, -0.2718,  0.0624,\n",
       "           -0.1141],\n",
       "          [-0.0795,  0.0031,  0.0566,  0.0845,  0.1235, -0.0120, -0.1661,\n",
       "            0.1269,  0.0205,  0.0716,  0.0249, -0.4068,  0.4051,  0.0433,\n",
       "            0.1392,  0.0282, -0.0477, -0.1894, -0.2474,  0.3603,  0.0898,\n",
       "           -0.1820, -0.1293,  0.1103,  0.2778,  0.0561,  0.0361, -0.0226,\n",
       "            0.1088, -0.1036, -0.1131, -0.0166,  0.0600,  0.1149, -0.1272,\n",
       "            0.2685,  0.4629,  0.3167, -0.0307,  0.0957, -0.0125,  0.0884,\n",
       "           -0.1734,  0.0429,  0.0052,  0.0056, -0.0031, -0.1425, -0.2663,\n",
       "            0.0483, -0.0545,  0.0531, -0.1609,  0.0191,  0.0577, -0.1131,\n",
       "            0.1973,  0.0370, -0.0255,  0.0603,  0.3520, -0.2718,  0.0623,\n",
       "           -0.1141]],\n",
       "\n",
       "         [[-0.0556,  0.2762, -0.0171,  0.0378,  0.0049,  0.0809,  0.0943,\n",
       "           -0.0561, -0.1326, -0.1146,  0.0747, -0.1908,  0.0541, -0.1695,\n",
       "            0.2179, -0.0622, -0.0293, -0.0574, -0.0471, -0.0125,  0.0517,\n",
       "            0.3146,  0.1763,  0.0818, -0.0604, -0.0856, -0.0895,  0.0293,\n",
       "            0.2358,  0.2633, -0.0362,  0.0900,  0.0618, -0.0544,  0.3004,\n",
       "            0.1661, -0.0657,  0.0578,  0.0653, -0.1063, -0.3763, -0.3128,\n",
       "            0.1618, -0.0941,  0.2663, -0.0282, -0.0260, -0.0625, -0.1404,\n",
       "           -0.1168, -0.2622, -0.0615,  0.0141, -0.2657, -0.2286, -0.0849,\n",
       "            0.0393, -0.2784, -0.2838,  0.1246, -0.4495, -0.1070,  0.0471,\n",
       "            0.0166],\n",
       "          [-0.0556,  0.2762, -0.0171,  0.0378,  0.0049,  0.0809,  0.0943,\n",
       "           -0.0561, -0.1326, -0.1146,  0.0747, -0.1908,  0.0541, -0.1695,\n",
       "            0.2179, -0.0622, -0.0293, -0.0574, -0.0471, -0.0125,  0.0517,\n",
       "            0.3146,  0.1763,  0.0818, -0.0604, -0.0856, -0.0895,  0.0293,\n",
       "            0.2358,  0.2633, -0.0362,  0.0900,  0.0618, -0.0544,  0.3004,\n",
       "            0.1661, -0.0657,  0.0578,  0.0653, -0.1063, -0.3763, -0.3128,\n",
       "            0.1618, -0.0941,  0.2663, -0.0282, -0.0260, -0.0625, -0.1404,\n",
       "           -0.1168, -0.2622, -0.0615,  0.0141, -0.2657, -0.2286, -0.0849,\n",
       "            0.0393, -0.2784, -0.2838,  0.1246, -0.4495, -0.1070,  0.0471,\n",
       "            0.0166]]]], grad_fn=<TransposeBackward0>), tensor([[[[-2.9630e+00, -1.6420e+00,  1.2023e+00,  ..., -1.3340e+00,\n",
       "            3.4827e+00,  1.2192e+00],\n",
       "          [-6.1611e-01, -7.1704e-01,  2.7875e+00,  ...,  2.0141e+00,\n",
       "            2.4501e+00,  9.2164e-01],\n",
       "          [-1.2954e+00, -1.0329e+00,  3.6381e+00,  ..., -2.8248e+00,\n",
       "            1.9805e+00,  1.5062e-02],\n",
       "          ...,\n",
       "          [ 8.6962e-02,  5.6692e-01,  4.3675e+00,  ...,  3.6058e-01,\n",
       "           -4.2890e-01, -2.1273e+00],\n",
       "          [ 6.4688e-01,  1.0897e+00,  1.6553e+00,  ..., -6.6395e-01,\n",
       "           -7.3670e-02, -1.9509e+00],\n",
       "          [-4.2050e-02,  1.8281e-01, -1.6647e-01,  ..., -1.5249e-01,\n",
       "           -7.5354e-01, -4.1050e-01]],\n",
       "\n",
       "         [[-6.5374e-01, -2.2035e-02,  2.1042e+00,  ..., -1.8312e+00,\n",
       "           -9.5874e-01,  5.5328e-01],\n",
       "          [ 1.2671e-02,  3.2306e+00,  3.2568e+00,  ...,  8.6006e-02,\n",
       "            6.3760e-01,  2.6720e+00],\n",
       "          [-2.3693e-01,  1.9609e+00,  2.5732e+00,  ..., -1.4470e+00,\n",
       "            5.7377e-02,  3.6011e+00],\n",
       "          ...,\n",
       "          [ 1.2101e+00,  1.7298e+00,  7.3292e-02,  ...,  2.5130e+00,\n",
       "           -2.6723e-01, -1.9050e+00],\n",
       "          [-9.5042e-01,  1.6468e+00, -1.3768e+00,  ...,  3.0476e+00,\n",
       "            1.1092e+00, -5.7736e-01],\n",
       "          [ 1.3877e-01, -6.8804e-01,  1.4271e-01,  ...,  5.0496e-02,\n",
       "           -8.9177e-02,  6.2863e-02]],\n",
       "\n",
       "         [[ 5.4169e-01,  1.8054e+00, -6.8325e-01,  ...,  1.8417e+00,\n",
       "           -1.4405e+00,  2.4790e-01],\n",
       "          [-1.1329e-01, -7.0705e-01,  1.8761e+00,  ...,  1.4134e+00,\n",
       "           -8.9336e-01, -2.3109e+00],\n",
       "          [-4.4861e-01, -1.7436e+00,  1.5071e+00,  ...,  1.3477e+00,\n",
       "           -1.8953e+00, -3.2013e-02],\n",
       "          ...,\n",
       "          [-2.2383e-01, -1.0569e+00,  2.4207e+00,  ..., -1.1909e+00,\n",
       "           -1.1252e+00, -4.3964e-01],\n",
       "          [ 1.2288e+00,  9.1203e-01,  5.9076e+00,  ..., -1.9973e-01,\n",
       "           -1.3401e+00, -2.0607e+00],\n",
       "          [-1.7001e-01, -3.3093e-01, -2.6360e-01,  ..., -7.0725e-01,\n",
       "            1.8804e-01,  1.4082e-01]],\n",
       "\n",
       "         [[ 1.4259e+00, -1.5842e-01,  3.3079e-01,  ...,  1.1671e+00,\n",
       "            1.2827e+00, -8.7733e-01],\n",
       "          [ 2.9459e+00,  8.2736e-01, -3.1173e+00,  ...,  1.0838e+00,\n",
       "           -3.0330e+00,  1.1948e+00],\n",
       "          [ 2.8437e+00,  1.5151e+00, -5.7080e-01,  ...,  1.5385e+00,\n",
       "            5.1209e-01, -1.1843e+00],\n",
       "          ...,\n",
       "          [ 2.6134e+00,  4.2346e+00, -3.4746e+00,  ...,  1.1854e+00,\n",
       "           -2.5667e+00, -2.1731e+00],\n",
       "          [ 1.1132e+00,  2.3109e+00, -1.6537e+00,  ..., -1.6268e+00,\n",
       "           -2.3315e+00, -8.0235e-01],\n",
       "          [-1.4209e-01,  2.0800e-01,  4.4089e-01,  ..., -5.6145e-01,\n",
       "           -2.9272e-01,  5.0813e-02]],\n",
       "\n",
       "         [[ 1.7854e+00, -5.2197e-01, -1.4238e+00,  ..., -6.9795e-01,\n",
       "           -1.6989e+00, -3.1157e+00],\n",
       "          [ 2.1681e+00,  1.4813e+00,  2.0589e+00,  ..., -1.6316e+00,\n",
       "            1.4853e+00, -2.4080e+00],\n",
       "          [ 3.9783e+00,  2.6944e+00, -1.4973e+00,  ..., -6.0097e-01,\n",
       "            1.2141e+00, -2.6614e+00],\n",
       "          ...,\n",
       "          [ 3.2368e+00,  2.5047e-01,  5.2860e-02,  ...,  1.9735e-02,\n",
       "            3.1789e+00, -2.3842e+00],\n",
       "          [ 1.8242e+00,  1.5621e+00,  1.9231e-01,  ..., -1.4135e+00,\n",
       "            2.8945e+00, -3.6885e+00],\n",
       "          [-3.4971e-01, -1.5500e-01,  8.0885e-02,  ...,  3.6384e-01,\n",
       "           -2.9946e-01,  7.0148e-01]],\n",
       "\n",
       "         [[-1.0551e+00,  1.4055e+00,  1.1718e+00,  ...,  2.6826e+00,\n",
       "            1.1181e+00, -1.1022e-01],\n",
       "          [-2.3820e+00, -3.1456e-01, -1.1209e+00,  ..., -2.2261e-01,\n",
       "            1.1500e+00,  1.2323e+00],\n",
       "          [-1.6687e+00, -1.6162e+00, -2.4753e+00,  ...,  1.0414e+00,\n",
       "            1.3622e+00, -1.7438e-01],\n",
       "          ...,\n",
       "          [-3.0578e+00, -1.9638e+00, -4.2132e+00,  ..., -3.8889e+00,\n",
       "           -1.3333e+00,  3.7507e+00],\n",
       "          [-1.6463e-01,  4.7705e-03, -2.5334e+00,  ..., -3.8675e+00,\n",
       "           -2.2467e+00,  6.8121e-01],\n",
       "          [ 3.3504e-01, -3.9617e-01,  2.0245e-01,  ...,  1.4438e-02,\n",
       "           -3.1555e-01,  5.0809e-02]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 2.8804e-01,  1.2230e+00,  1.6195e+00,  ..., -1.0715e+00,\n",
       "            4.0115e+00,  1.1671e+00],\n",
       "          [-4.2121e-01, -2.7832e-01, -1.3552e+00,  ..., -2.5163e+00,\n",
       "            2.4593e+00,  4.0436e+00],\n",
       "          [-3.7715e+00,  5.4960e-01,  3.5192e+00,  ..., -1.9903e+00,\n",
       "            2.1531e+00,  2.6589e+00],\n",
       "          ...,\n",
       "          [-2.0692e+00, -7.2160e-02, -8.8768e-01,  ..., -4.0620e+00,\n",
       "            1.0084e+00, -2.6540e+00],\n",
       "          [ 4.8632e-01, -3.2074e+00, -1.6402e+00,  ..., -3.4255e+00,\n",
       "            5.3062e+00, -1.7440e+00],\n",
       "          [-8.7446e-03, -2.4230e-02, -3.1643e-03,  ..., -1.1797e-01,\n",
       "           -2.2617e-02,  6.2024e-02]],\n",
       "\n",
       "         [[-3.0806e+00, -5.1228e-02, -3.9162e-01,  ...,  7.4633e-01,\n",
       "           -3.2391e+00,  1.8459e+00],\n",
       "          [-5.7610e-01,  7.8281e-01, -1.9535e+00,  ...,  8.2036e-01,\n",
       "           -4.1073e+00,  2.5461e+00],\n",
       "          [-1.7644e+00,  6.5564e-01, -2.3332e-01,  ...,  3.7433e-01,\n",
       "           -1.3535e+00,  1.7929e+00],\n",
       "          ...,\n",
       "          [-7.1506e-01,  4.0366e-01,  1.0783e+00,  ..., -7.2727e-02,\n",
       "           -1.6431e+00, -6.5354e-01],\n",
       "          [-1.9245e-01,  6.7370e-01,  4.8331e-01,  ...,  1.4147e-01,\n",
       "           -4.1355e-01, -1.2080e+00],\n",
       "          [ 2.9506e-01, -2.9555e-01, -1.1905e-01,  ..., -3.4230e-01,\n",
       "            8.0669e-01, -4.9742e-01]],\n",
       "\n",
       "         [[-1.2468e+00, -6.5075e-01,  7.5045e-01,  ...,  4.7343e-01,\n",
       "           -1.2202e+00, -3.5947e+00],\n",
       "          [ 6.3565e-01,  1.2278e+00,  1.8915e+00,  ...,  4.7096e-01,\n",
       "           -2.9548e+00, -1.8484e+00],\n",
       "          [-3.6890e-01,  3.5825e+00,  6.3052e-01,  ...,  9.1499e-01,\n",
       "            3.6612e-01, -1.5946e+00],\n",
       "          ...,\n",
       "          [-1.4769e+00,  1.4708e+00,  1.3473e+00,  ...,  1.2065e-01,\n",
       "            9.9722e-01,  1.2979e+00],\n",
       "          [ 6.2433e-01,  1.1490e-01,  1.6028e+00,  ...,  4.9638e-01,\n",
       "            1.4461e+00, -1.6838e-01],\n",
       "          [-2.1857e-03, -1.0564e-02, -1.1830e-01,  ..., -1.1467e-01,\n",
       "            1.8201e-01,  1.5052e-02]],\n",
       "\n",
       "         [[-1.3479e+00, -4.5054e-01,  2.5508e+00,  ...,  2.7251e+00,\n",
       "            8.2332e-01,  5.2231e+00],\n",
       "          [-8.4508e-01, -2.2509e-01,  1.2371e+00,  ..., -2.1431e+00,\n",
       "            4.0032e+00,  4.1576e+00],\n",
       "          [ 1.1384e+00,  9.7391e-01, -3.6040e-01,  ..., -8.1199e-01,\n",
       "            1.6313e+00,  2.8571e+00],\n",
       "          ...,\n",
       "          [ 3.4595e-01,  1.2561e+00,  2.7240e+00,  ...,  5.6789e-01,\n",
       "            1.5816e+00,  1.4591e-01],\n",
       "          [ 2.9979e+00, -8.7727e-02,  2.5866e+00,  ...,  1.6029e-03,\n",
       "            2.8939e+00, -2.0120e-01],\n",
       "          [-1.9826e-01, -1.7650e-01,  4.4764e-02,  ..., -5.4240e-02,\n",
       "           -2.3102e-01,  4.9019e-02]],\n",
       "\n",
       "         [[ 2.2990e+00,  8.7716e-01, -2.6213e+00,  ..., -1.9812e+00,\n",
       "           -1.5638e+00,  9.2362e-01],\n",
       "          [-1.0630e+00,  6.0633e+00, -2.0421e+00,  ..., -3.5992e+00,\n",
       "            1.2382e+00, -9.9852e-01],\n",
       "          [ 2.9031e+00,  1.5199e+00, -1.9333e+00,  ..., -3.4310e-01,\n",
       "            8.3471e-01,  2.5892e-01],\n",
       "          ...,\n",
       "          [-2.7227e+00,  1.1901e+00, -2.3012e+00,  ...,  1.2695e+00,\n",
       "            6.0177e-01,  1.3961e+00],\n",
       "          [-7.2883e-01,  6.1262e+00,  2.2226e+00,  ...,  1.9293e+00,\n",
       "           -1.0562e+00, -4.6124e-01],\n",
       "          [ 1.3384e-03,  4.5445e-02,  6.5043e-02,  ...,  5.4510e-02,\n",
       "            5.2914e-03, -1.4317e-02]],\n",
       "\n",
       "         [[-7.6752e-01,  1.1776e+00,  1.8393e+00,  ...,  4.0836e-01,\n",
       "           -1.2504e+00, -1.8967e+00],\n",
       "          [-1.4679e-01,  1.1064e-01, -3.7565e-01,  ..., -2.8769e+00,\n",
       "           -4.6496e-01, -2.7130e+00],\n",
       "          [ 6.6314e-02,  6.2205e-01,  5.3129e-01,  ..., -8.2708e-01,\n",
       "           -1.1655e+00, -1.2307e+00],\n",
       "          ...,\n",
       "          [-1.3594e+00, -1.2329e+00, -2.4299e+00,  ..., -1.9880e+00,\n",
       "            6.3140e-01,  1.6482e+00],\n",
       "          [-8.2531e-01, -1.5284e+00, -3.7450e+00,  ..., -1.5186e+00,\n",
       "           -2.6004e-01,  3.0539e+00],\n",
       "          [-4.1451e-02,  3.4802e-02,  1.2888e-01,  ..., -1.7567e-01,\n",
       "            9.6211e-02,  2.2015e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.2833e-01, -2.9516e-03,  6.1659e-01, -7.2574e-01,  6.0244e-01,\n",
       "            5.9918e-01, -1.1912e-01, -6.6525e-01,  9.8909e-02,  3.1039e-01,\n",
       "            6.3781e-02,  8.1084e-01,  2.5755e-01,  6.0255e-01,  5.8610e-01,\n",
       "           -1.3486e-01,  5.6888e-02, -2.1124e-01,  6.6571e-02, -9.6965e-01,\n",
       "            2.5064e-01, -3.4589e-01, -3.4330e-01,  7.9237e-02, -2.8709e-01,\n",
       "            1.9039e-01,  2.8393e-01, -3.8627e-01, -1.3395e-01,  8.6965e-02,\n",
       "           -1.1646e+00, -2.9831e-01, -4.2907e-01,  9.2728e-01,  2.6178e-02,\n",
       "            6.0482e-01, -2.9231e-01, -6.9373e-01,  3.2936e-01,  1.1188e+00,\n",
       "           -1.0831e-01,  4.3150e-01,  5.3018e-02, -2.0693e-01, -2.5665e-01,\n",
       "           -2.3035e-01,  7.4510e-02, -5.0839e-01,  5.8077e-01,  4.6618e-01,\n",
       "            2.1035e-01,  1.2608e-01, -8.5342e-02, -7.0574e-02, -1.2468e-01,\n",
       "           -3.9370e-01, -1.0144e-01, -1.0024e+00, -5.9260e-02, -4.6781e-01,\n",
       "           -4.8863e-01,  4.3636e-01,  1.8053e-01, -5.3674e-01],\n",
       "          [-2.2833e-01, -2.9516e-03,  6.1659e-01, -7.2574e-01,  6.0244e-01,\n",
       "            5.9918e-01, -1.1912e-01, -6.6525e-01,  9.8909e-02,  3.1039e-01,\n",
       "            6.3781e-02,  8.1084e-01,  2.5755e-01,  6.0255e-01,  5.8610e-01,\n",
       "           -1.3486e-01,  5.6888e-02, -2.1124e-01,  6.6571e-02, -9.6965e-01,\n",
       "            2.5064e-01, -3.4589e-01, -3.4330e-01,  7.9237e-02, -2.8709e-01,\n",
       "            1.9039e-01,  2.8393e-01, -3.8627e-01, -1.3395e-01,  8.6964e-02,\n",
       "           -1.1646e+00, -2.9831e-01, -4.2907e-01,  9.2727e-01,  2.6178e-02,\n",
       "            6.0482e-01, -2.9231e-01, -6.9373e-01,  3.2936e-01,  1.1188e+00,\n",
       "           -1.0831e-01,  4.3150e-01,  5.3018e-02, -2.0693e-01, -2.5665e-01,\n",
       "           -2.3035e-01,  7.4510e-02, -5.0839e-01,  5.8077e-01,  4.6618e-01,\n",
       "            2.1035e-01,  1.2608e-01, -8.5343e-02, -7.0574e-02, -1.2468e-01,\n",
       "           -3.9370e-01, -1.0144e-01, -1.0024e+00, -5.9260e-02, -4.6781e-01,\n",
       "           -4.8863e-01,  4.3636e-01,  1.8053e-01, -5.3674e-01]],\n",
       "\n",
       "         [[-1.2699e-01, -4.0600e-02, -1.2246e-01,  2.2288e-01, -1.3483e+00,\n",
       "           -5.9666e-01,  2.8800e-01,  4.2597e-01,  1.2241e-01,  9.3398e-01,\n",
       "            3.5712e-01,  2.4168e-01,  3.4947e-01, -4.3207e-01,  3.2506e-01,\n",
       "            3.7716e-01,  1.1999e+00, -1.7008e+00, -2.0098e-01,  2.4000e-01,\n",
       "            4.9386e-02,  4.6085e-02, -1.3436e-01,  1.5887e-01,  5.8869e-01,\n",
       "           -1.3162e-01,  5.4090e-01, -2.4379e-01, -1.1442e-01,  5.1627e-01,\n",
       "           -2.1484e-01,  6.1018e-01,  9.5711e-02, -4.6226e-02, -4.9639e-01,\n",
       "            7.4383e-02,  2.3056e-01,  2.0032e-01, -1.7645e+00,  3.8654e-01,\n",
       "            2.7256e-01, -5.2764e-02,  2.6352e-01, -3.0339e-03, -3.2746e-01,\n",
       "            1.4690e-01, -1.6824e+00, -6.4431e-01,  1.2966e-01,  3.0041e-01,\n",
       "           -2.4201e-01, -5.6701e-01, -1.3212e-01,  1.8310e-01, -2.1332e-01,\n",
       "            2.1278e-01,  3.9501e-01,  5.3657e-01,  1.4996e-01, -1.2046e-01,\n",
       "            6.9178e-01, -7.7320e-01,  5.8906e-02, -1.2171e-02],\n",
       "          [-1.2699e-01, -4.0600e-02, -1.2246e-01,  2.2288e-01, -1.3483e+00,\n",
       "           -5.9666e-01,  2.8800e-01,  4.2597e-01,  1.2241e-01,  9.3398e-01,\n",
       "            3.5712e-01,  2.4168e-01,  3.4947e-01, -4.3207e-01,  3.2506e-01,\n",
       "            3.7716e-01,  1.1999e+00, -1.7008e+00, -2.0098e-01,  2.4000e-01,\n",
       "            4.9386e-02,  4.6085e-02, -1.3436e-01,  1.5887e-01,  5.8869e-01,\n",
       "           -1.3162e-01,  5.4090e-01, -2.4379e-01, -1.1442e-01,  5.1627e-01,\n",
       "           -2.1484e-01,  6.1018e-01,  9.5711e-02, -4.6226e-02, -4.9639e-01,\n",
       "            7.4383e-02,  2.3056e-01,  2.0032e-01, -1.7645e+00,  3.8654e-01,\n",
       "            2.7256e-01, -5.2764e-02,  2.6352e-01, -3.0339e-03, -3.2746e-01,\n",
       "            1.4690e-01, -1.6824e+00, -6.4431e-01,  1.2966e-01,  3.0041e-01,\n",
       "           -2.4201e-01, -5.6701e-01, -1.3212e-01,  1.8310e-01, -2.1332e-01,\n",
       "            2.1278e-01,  3.9501e-01,  5.3657e-01,  1.4996e-01, -1.2046e-01,\n",
       "            6.9178e-01, -7.7320e-01,  5.8906e-02, -1.2171e-02]],\n",
       "\n",
       "         [[ 6.3775e-01,  3.8993e-01, -1.9378e-01, -1.0239e+00, -6.3472e-01,\n",
       "           -7.9092e-02, -4.2214e-02, -5.2826e-01,  1.3028e-01,  7.7260e-01,\n",
       "           -2.8761e-01, -7.3997e-01, -3.2593e-01,  6.7652e-01,  9.2110e-01,\n",
       "           -6.1733e-01,  1.5083e-01, -4.5710e-01, -6.5507e-02,  5.7841e-01,\n",
       "            3.9976e-01,  4.3715e-01,  1.0165e+00, -1.7586e+00, -5.6809e-01,\n",
       "           -5.6786e-02,  7.8102e-01,  2.4543e-01,  8.9339e-02,  9.4285e-01,\n",
       "            2.2527e-01,  3.0053e-01,  2.1355e-01, -3.0639e-01,  1.2592e-01,\n",
       "           -2.9311e-01, -3.2405e-01, -5.9263e-01,  6.1945e-01,  1.9587e-01,\n",
       "            2.0638e-01, -1.2424e-02, -1.3756e+00,  6.7710e-01, -1.7098e-01,\n",
       "           -1.0765e+00, -1.3479e-01, -2.6574e-01,  2.1990e-02,  6.4066e-01,\n",
       "           -2.6238e-02,  4.9589e-01, -2.6478e-01, -1.4235e+00, -4.5004e-01,\n",
       "           -1.4260e-01, -1.4947e+00,  4.0282e-02, -4.8557e-02,  1.8426e-02,\n",
       "           -8.4736e-02,  3.6998e-01, -2.7523e-01,  1.1129e-01],\n",
       "          [ 6.3775e-01,  3.8993e-01, -1.9378e-01, -1.0239e+00, -6.3472e-01,\n",
       "           -7.9092e-02, -4.2214e-02, -5.2826e-01,  1.3028e-01,  7.7260e-01,\n",
       "           -2.8761e-01, -7.3997e-01, -3.2593e-01,  6.7652e-01,  9.2110e-01,\n",
       "           -6.1733e-01,  1.5083e-01, -4.5710e-01, -6.5507e-02,  5.7841e-01,\n",
       "            3.9976e-01,  4.3715e-01,  1.0165e+00, -1.7586e+00, -5.6809e-01,\n",
       "           -5.6786e-02,  7.8102e-01,  2.4543e-01,  8.9339e-02,  9.4285e-01,\n",
       "            2.2527e-01,  3.0053e-01,  2.1355e-01, -3.0639e-01,  1.2592e-01,\n",
       "           -2.9311e-01, -3.2405e-01, -5.9263e-01,  6.1945e-01,  1.9587e-01,\n",
       "            2.0638e-01, -1.2424e-02, -1.3756e+00,  6.7710e-01, -1.7098e-01,\n",
       "           -1.0765e+00, -1.3479e-01, -2.6574e-01,  2.1990e-02,  6.4066e-01,\n",
       "           -2.6238e-02,  4.9589e-01, -2.6478e-01, -1.4235e+00, -4.5004e-01,\n",
       "           -1.4260e-01, -1.4947e+00,  4.0282e-02, -4.8558e-02,  1.8425e-02,\n",
       "           -8.4736e-02,  3.6998e-01, -2.7523e-01,  1.1129e-01]],\n",
       "\n",
       "         [[ 1.9520e-01,  7.8961e-01,  1.0118e-01, -2.0378e-01, -1.2520e-01,\n",
       "            1.1945e-01, -7.8835e-01,  1.2257e+00,  1.2550e+00,  3.0724e-02,\n",
       "           -8.9741e-01, -8.6446e-01, -1.0121e+00,  1.9235e-01,  6.0714e-01,\n",
       "           -2.3095e-02, -8.4182e-01, -4.2828e-01, -1.4256e+00,  4.2135e-01,\n",
       "            3.2112e-01, -1.2430e-01,  6.7258e-01, -5.8768e-01, -1.2913e-01,\n",
       "           -2.9076e-01, -2.2749e-01, -1.2348e-02, -4.6334e-01, -3.8970e-01,\n",
       "           -9.4171e-02, -7.8802e-01,  5.0394e-01,  8.0264e-01, -2.3982e-01,\n",
       "            5.7420e-01,  9.4525e-01,  1.4314e-01, -6.4272e-01, -2.6164e-01,\n",
       "           -7.5415e-01,  2.2865e-01,  9.7272e-01, -7.2435e-01, -6.5148e-01,\n",
       "           -1.1006e-01, -9.2643e-01, -1.0255e+00,  1.4442e-01, -5.5567e-01,\n",
       "            4.1305e-01, -1.1491e-01, -9.9974e-02,  4.7338e-01, -3.6765e-03,\n",
       "           -2.1333e-01, -7.6449e-01, -8.9462e-01, -3.1269e-01, -4.9228e-01,\n",
       "            1.4732e-01,  5.1407e-01, -4.2159e-01, -1.0038e-01],\n",
       "          [ 1.9520e-01,  7.8961e-01,  1.0118e-01, -2.0378e-01, -1.2520e-01,\n",
       "            1.1945e-01, -7.8835e-01,  1.2257e+00,  1.2550e+00,  3.0724e-02,\n",
       "           -8.9741e-01, -8.6446e-01, -1.0121e+00,  1.9235e-01,  6.0714e-01,\n",
       "           -2.3095e-02, -8.4182e-01, -4.2828e-01, -1.4256e+00,  4.2135e-01,\n",
       "            3.2112e-01, -1.2430e-01,  6.7258e-01, -5.8768e-01, -1.2913e-01,\n",
       "           -2.9076e-01, -2.2749e-01, -1.2348e-02, -4.6334e-01, -3.8970e-01,\n",
       "           -9.4171e-02, -7.8802e-01,  5.0394e-01,  8.0264e-01, -2.3982e-01,\n",
       "            5.7420e-01,  9.4525e-01,  1.4314e-01, -6.4272e-01, -2.6164e-01,\n",
       "           -7.5415e-01,  2.2865e-01,  9.7272e-01, -7.2435e-01, -6.5148e-01,\n",
       "           -1.1006e-01, -9.2643e-01, -1.0255e+00,  1.4442e-01, -5.5567e-01,\n",
       "            4.1305e-01, -1.1491e-01, -9.9974e-02,  4.7338e-01, -3.6765e-03,\n",
       "           -2.1333e-01, -7.6449e-01, -8.9462e-01, -3.1269e-01, -4.9228e-01,\n",
       "            1.4732e-01,  5.1407e-01, -4.2159e-01, -1.0038e-01]],\n",
       "\n",
       "         [[ 9.2580e-02,  9.8918e-02, -3.8382e-01,  3.6511e-01,  9.5438e-01,\n",
       "           -3.5180e-01,  1.2777e-01, -1.0051e-01,  3.6400e-01, -3.8073e-01,\n",
       "            4.5952e-01,  2.2727e-01, -3.1794e-01,  4.0317e-02,  7.3912e-02,\n",
       "            1.4195e-01, -7.7694e-01, -9.7982e-02, -2.3364e-01, -8.1745e-02,\n",
       "            4.0169e-01,  5.0335e-01,  2.2478e-01,  1.0215e-01,  5.7177e-01,\n",
       "            6.9545e-01,  4.1180e-01, -3.0864e-01, -1.1053e-01,  2.6010e-02,\n",
       "            2.9746e-04, -2.4057e-01,  3.6668e-01, -1.4521e-01,  1.3897e-01,\n",
       "           -2.0457e-01, -3.4699e-01, -2.1715e-01, -2.9574e-01,  1.3462e-01,\n",
       "            2.8617e-01,  4.9122e-01, -4.3157e-01, -1.5772e-01,  1.2967e-01,\n",
       "            3.0246e-01, -3.7928e-01,  1.8320e-01, -1.6468e-01, -2.2633e-01,\n",
       "            3.5497e-01,  9.6864e-02, -3.2288e-02,  1.6035e-01,  3.6191e-01,\n",
       "            5.5950e-01,  4.5069e-01, -1.3104e-02,  1.0776e-01,  2.1371e-01,\n",
       "            1.0497e-01, -7.2409e-01, -1.9514e-01,  2.1783e-01],\n",
       "          [ 9.2580e-02,  9.8918e-02, -3.8382e-01,  3.6511e-01,  9.5438e-01,\n",
       "           -3.5180e-01,  1.2777e-01, -1.0051e-01,  3.6400e-01, -3.8073e-01,\n",
       "            4.5952e-01,  2.2727e-01, -3.1794e-01,  4.0317e-02,  7.3912e-02,\n",
       "            1.4195e-01, -7.7694e-01, -9.7982e-02, -2.3364e-01, -8.1745e-02,\n",
       "            4.0169e-01,  5.0335e-01,  2.2478e-01,  1.0215e-01,  5.7177e-01,\n",
       "            6.9545e-01,  4.1180e-01, -3.0864e-01, -1.1053e-01,  2.6010e-02,\n",
       "            2.9748e-04, -2.4057e-01,  3.6668e-01, -1.4521e-01,  1.3897e-01,\n",
       "           -2.0457e-01, -3.4699e-01, -2.1715e-01, -2.9574e-01,  1.3462e-01,\n",
       "            2.8617e-01,  4.9122e-01, -4.3157e-01, -1.5772e-01,  1.2967e-01,\n",
       "            3.0246e-01, -3.7928e-01,  1.8320e-01, -1.6468e-01, -2.2633e-01,\n",
       "            3.5497e-01,  9.6864e-02, -3.2288e-02,  1.6035e-01,  3.6191e-01,\n",
       "            5.5950e-01,  4.5069e-01, -1.3104e-02,  1.0776e-01,  2.1371e-01,\n",
       "            1.0497e-01, -7.2409e-01, -1.9514e-01,  2.1783e-01]],\n",
       "\n",
       "         [[ 1.4610e-01, -9.4023e-01,  9.9648e-01, -7.6675e-01, -3.1732e-01,\n",
       "            3.4369e-01,  5.3178e-01,  8.3877e-01,  1.9938e-02, -1.1730e+00,\n",
       "           -5.4600e-01,  4.0148e-01,  9.4529e-01,  1.9920e-01, -3.9598e-01,\n",
       "            2.7059e-01, -1.3215e-01, -7.9768e-01,  6.7297e-01,  1.2468e+00,\n",
       "            2.9962e-02,  8.2925e-01, -6.7699e-01,  3.6469e-01,  4.0734e-01,\n",
       "            1.4100e+00,  7.5518e-02, -5.6882e-01,  1.1046e-01,  3.0078e-02,\n",
       "            5.6641e-01,  2.8528e-02, -9.4958e-01,  2.1341e-01,  7.7981e-01,\n",
       "           -7.3869e-01, -5.5164e-01, -3.7550e-01,  6.1630e-01,  9.9807e-01,\n",
       "           -9.4515e-01, -5.4430e-01,  5.0428e-01, -1.0059e+00,  3.3374e-01,\n",
       "            1.1138e-01, -5.3508e-01, -9.6249e-02,  2.4282e-01, -4.0324e-01,\n",
       "            9.8617e-01,  3.4887e-01,  8.5894e-01, -5.3172e-01, -2.3484e-01,\n",
       "            3.7431e-02, -3.2567e-01,  1.8267e+00,  2.6774e-01,  5.0429e-01,\n",
       "            4.4480e-01, -4.7741e-01,  4.1562e-01, -5.9130e-01],\n",
       "          [ 1.4610e-01, -9.4023e-01,  9.9647e-01, -7.6675e-01, -3.1732e-01,\n",
       "            3.4369e-01,  5.3178e-01,  8.3877e-01,  1.9938e-02, -1.1730e+00,\n",
       "           -5.4600e-01,  4.0148e-01,  9.4529e-01,  1.9920e-01, -3.9598e-01,\n",
       "            2.7059e-01, -1.3215e-01, -7.9768e-01,  6.7297e-01,  1.2468e+00,\n",
       "            2.9962e-02,  8.2925e-01, -6.7699e-01,  3.6469e-01,  4.0734e-01,\n",
       "            1.4100e+00,  7.5518e-02, -5.6882e-01,  1.1046e-01,  3.0078e-02,\n",
       "            5.6641e-01,  2.8528e-02, -9.4958e-01,  2.1341e-01,  7.7981e-01,\n",
       "           -7.3869e-01, -5.5164e-01, -3.7550e-01,  6.1630e-01,  9.9807e-01,\n",
       "           -9.4515e-01, -5.4430e-01,  5.0428e-01, -1.0059e+00,  3.3374e-01,\n",
       "            1.1138e-01, -5.3508e-01, -9.6249e-02,  2.4282e-01, -4.0324e-01,\n",
       "            9.8617e-01,  3.4887e-01,  8.5894e-01, -5.3172e-01, -2.3484e-01,\n",
       "            3.7431e-02, -3.2567e-01,  1.8267e+00,  2.6774e-01,  5.0429e-01,\n",
       "            4.4480e-01, -4.7741e-01,  4.1562e-01, -5.9130e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-2.7114e-01,  1.3682e-01,  4.5226e-02, -3.3139e-01,  5.3942e-01,\n",
       "            1.4533e-01, -8.9488e-02, -4.6934e-01, -2.8014e-01,  1.1301e-01,\n",
       "           -3.4284e-01, -3.2154e-01,  8.2101e-02, -5.1974e-01,  1.1354e-01,\n",
       "           -1.8965e-01,  2.2941e-03,  1.2750e-02,  9.8155e-02,  5.5290e-02,\n",
       "            1.3088e-01, -4.8854e-02, -3.8927e-01,  5.0914e-01,  7.1322e-02,\n",
       "           -3.4695e-01,  8.9011e-02,  4.9628e-01, -1.1545e-02, -2.1352e-01,\n",
       "           -3.6160e-01,  2.5289e-01, -8.9002e-02, -1.2292e-01, -5.3131e-01,\n",
       "            2.9911e-01, -5.1049e-01, -7.3997e-02,  2.0980e-02,  8.9957e-02,\n",
       "           -3.6804e-02, -2.4514e-01,  1.8889e-01,  1.5759e-01,  1.1001e-01,\n",
       "            2.2710e-01,  3.5605e-01,  6.6319e-02, -1.5051e-01,  6.6124e-02,\n",
       "           -1.9264e-02, -2.4821e-01,  6.0066e-02,  3.2286e-01, -3.9900e-02,\n",
       "           -1.7891e-01,  4.1213e-01, -5.1334e-02, -1.8069e-01, -6.3176e-04,\n",
       "           -4.7876e-01,  2.0719e-01,  2.6333e-01,  1.6144e-01],\n",
       "          [-2.7114e-01,  1.3682e-01,  4.5226e-02, -3.3139e-01,  5.3942e-01,\n",
       "            1.4533e-01, -8.9488e-02, -4.6934e-01, -2.8014e-01,  1.1301e-01,\n",
       "           -3.4284e-01, -3.2154e-01,  8.2101e-02, -5.1974e-01,  1.1354e-01,\n",
       "           -1.8965e-01,  2.2942e-03,  1.2750e-02,  9.8155e-02,  5.5290e-02,\n",
       "            1.3088e-01, -4.8854e-02, -3.8927e-01,  5.0914e-01,  7.1322e-02,\n",
       "           -3.4695e-01,  8.9011e-02,  4.9628e-01, -1.1545e-02, -2.1352e-01,\n",
       "           -3.6160e-01,  2.5289e-01, -8.9002e-02, -1.2292e-01, -5.3131e-01,\n",
       "            2.9911e-01, -5.1049e-01, -7.3997e-02,  2.0980e-02,  8.9957e-02,\n",
       "           -3.6804e-02, -2.4514e-01,  1.8889e-01,  1.5759e-01,  1.1001e-01,\n",
       "            2.2710e-01,  3.5605e-01,  6.6319e-02, -1.5051e-01,  6.6124e-02,\n",
       "           -1.9264e-02, -2.4821e-01,  6.0066e-02,  3.2286e-01, -3.9900e-02,\n",
       "           -1.7891e-01,  4.1213e-01, -5.1334e-02, -1.8069e-01, -6.3184e-04,\n",
       "           -4.7876e-01,  2.0719e-01,  2.6333e-01,  1.6144e-01]],\n",
       "\n",
       "         [[-3.7076e-03, -2.4655e-01, -1.7910e-01, -2.7971e-01,  3.2543e-02,\n",
       "            1.3179e-01, -1.5306e-01, -6.6779e-02,  1.6669e-01, -1.1340e-01,\n",
       "           -7.2232e-01,  2.0307e-01,  1.7502e-01, -1.4036e-01,  6.6148e-02,\n",
       "            1.9561e-03,  7.4335e-02,  6.9942e-02, -1.7589e-01, -3.0465e-02,\n",
       "           -2.8254e-02,  2.6609e-02, -1.6292e-02,  2.4314e-01,  1.7866e-01,\n",
       "            2.2180e-01, -1.3691e-01,  6.9819e-02, -4.4781e-01,  3.2788e-02,\n",
       "           -3.3811e-01,  3.2119e-01,  5.0728e-02, -2.0047e-02,  2.9787e-01,\n",
       "            4.8591e-02, -1.6783e-01, -2.3600e-02, -4.2041e-02,  1.8560e-01,\n",
       "           -2.0337e-01, -1.0759e-02, -1.0755e-01,  3.1214e-02,  2.1930e-01,\n",
       "            1.3841e-01,  9.1532e-02,  7.5527e-02, -6.4424e-02, -1.1147e-01,\n",
       "            1.1176e-01,  9.1384e-02,  7.3954e-02,  1.3766e-01,  2.2199e-01,\n",
       "            3.2613e-02, -1.7131e-01,  3.1362e-01,  5.1670e-04, -7.4430e-02,\n",
       "            1.9540e-01, -1.8462e-01, -3.2342e-01,  2.9067e-01],\n",
       "          [-3.7076e-03, -2.4655e-01, -1.7910e-01, -2.7971e-01,  3.2542e-02,\n",
       "            1.3179e-01, -1.5306e-01, -6.6779e-02,  1.6669e-01, -1.1340e-01,\n",
       "           -7.2232e-01,  2.0307e-01,  1.7502e-01, -1.4036e-01,  6.6148e-02,\n",
       "            1.9561e-03,  7.4335e-02,  6.9942e-02, -1.7589e-01, -3.0465e-02,\n",
       "           -2.8254e-02,  2.6609e-02, -1.6292e-02,  2.4314e-01,  1.7866e-01,\n",
       "            2.2180e-01, -1.3691e-01,  6.9819e-02, -4.4781e-01,  3.2788e-02,\n",
       "           -3.3811e-01,  3.2119e-01,  5.0728e-02, -2.0047e-02,  2.9787e-01,\n",
       "            4.8591e-02, -1.6783e-01, -2.3600e-02, -4.2041e-02,  1.8560e-01,\n",
       "           -2.0337e-01, -1.0759e-02, -1.0755e-01,  3.1214e-02,  2.1930e-01,\n",
       "            1.3841e-01,  9.1532e-02,  7.5527e-02, -6.4424e-02, -1.1147e-01,\n",
       "            1.1176e-01,  9.1384e-02,  7.3954e-02,  1.3766e-01,  2.2199e-01,\n",
       "            3.2613e-02, -1.7131e-01,  3.1362e-01,  5.1669e-04, -7.4430e-02,\n",
       "            1.9540e-01, -1.8462e-01, -3.2342e-01,  2.9067e-01]],\n",
       "\n",
       "         [[ 2.0059e-01, -1.5324e-01, -2.2906e-01,  4.2998e-01, -3.9706e-01,\n",
       "           -1.9119e-01, -2.3683e-01,  5.8309e-02, -3.4880e-01, -2.8529e-01,\n",
       "           -3.8139e-01,  3.4463e-02,  3.6472e-02,  8.0666e-02,  3.1655e-01,\n",
       "           -2.1488e-02,  1.1680e-02,  2.4401e-01,  8.3748e-02,  8.8695e-02,\n",
       "           -1.7024e-01,  4.3148e-01,  3.1543e-02,  1.2067e-01, -7.8012e-04,\n",
       "            1.3281e-01,  7.6684e-02, -3.2045e-01,  4.1596e-01,  2.2547e-01,\n",
       "           -3.4598e-01, -2.5215e-02, -3.6180e-01,  9.9245e-02,  4.8697e-02,\n",
       "            5.0698e-01,  2.7714e-02,  9.7766e-02,  4.7516e-01, -3.5529e-02,\n",
       "           -2.9153e-02, -2.7567e-01, -4.4968e-01,  5.6776e-02, -5.3536e-02,\n",
       "            1.2111e-01,  2.7973e-01, -2.4677e-01,  5.6048e-03,  2.0449e-03,\n",
       "            2.8559e-01, -6.5411e-02,  1.0626e-01, -1.7415e-02,  2.2973e-01,\n",
       "           -2.5867e-01,  9.4571e-02, -4.9075e-02,  1.7637e-01,  2.1353e-01,\n",
       "            3.6581e-01,  9.8793e-02,  1.6791e-01,  2.1470e-01],\n",
       "          [ 2.0059e-01, -1.5324e-01, -2.2906e-01,  4.2998e-01, -3.9706e-01,\n",
       "           -1.9119e-01, -2.3683e-01,  5.8309e-02, -3.4880e-01, -2.8529e-01,\n",
       "           -3.8139e-01,  3.4463e-02,  3.6472e-02,  8.0666e-02,  3.1655e-01,\n",
       "           -2.1488e-02,  1.1680e-02,  2.4401e-01,  8.3749e-02,  8.8695e-02,\n",
       "           -1.7024e-01,  4.3148e-01,  3.1543e-02,  1.2067e-01, -7.8013e-04,\n",
       "            1.3281e-01,  7.6685e-02, -3.2045e-01,  4.1596e-01,  2.2547e-01,\n",
       "           -3.4598e-01, -2.5215e-02, -3.6180e-01,  9.9245e-02,  4.8697e-02,\n",
       "            5.0698e-01,  2.7714e-02,  9.7766e-02,  4.7516e-01, -3.5529e-02,\n",
       "           -2.9153e-02, -2.7567e-01, -4.4968e-01,  5.6776e-02, -5.3536e-02,\n",
       "            1.2111e-01,  2.7973e-01, -2.4677e-01,  5.6048e-03,  2.0449e-03,\n",
       "            2.8559e-01, -6.5411e-02,  1.0626e-01, -1.7415e-02,  2.2973e-01,\n",
       "           -2.5867e-01,  9.4571e-02, -4.9075e-02,  1.7637e-01,  2.1353e-01,\n",
       "            3.6581e-01,  9.8793e-02,  1.6791e-01,  2.1470e-01]],\n",
       "\n",
       "         [[-2.5544e-02, -9.5390e-02, -3.1845e-01,  8.7304e-03,  2.5873e-01,\n",
       "           -2.6202e-01, -7.0130e-02, -6.3457e-02, -5.5170e-01, -1.2846e-02,\n",
       "           -7.4814e-01,  4.8504e-01,  3.4868e-01, -5.3905e-01,  3.7780e-01,\n",
       "           -8.7536e-02,  3.6370e-02, -1.3785e-01,  3.6221e-01, -2.8667e-01,\n",
       "           -3.5907e-01,  1.0538e-01, -6.9235e-01, -9.5442e-02, -1.8231e-01,\n",
       "            1.7192e-01, -1.2524e-01, -6.9173e-01, -1.1642e-01, -6.9372e-02,\n",
       "           -3.4073e-01, -7.0555e-01,  1.0747e+00, -4.8206e-02, -3.8526e-01,\n",
       "           -1.4435e-01,  2.7733e-01,  4.8236e-01,  1.6022e-01,  4.6600e-01,\n",
       "           -1.7864e-02, -1.0016e-01, -1.2556e-01,  2.2100e-01,  8.8796e-02,\n",
       "            3.3879e-02, -2.0287e-01, -2.5503e-01, -9.2222e-02,  2.3342e-01,\n",
       "           -2.2296e-02, -3.3437e-02,  1.8001e-01,  9.5357e-02,  4.5755e-01,\n",
       "            1.4466e-01,  2.3465e-01, -5.4965e-01,  2.2337e-01,  9.3972e-02,\n",
       "           -5.1277e-01, -9.3881e-02,  3.1022e-01,  1.5582e-01],\n",
       "          [-2.5544e-02, -9.5390e-02, -3.1845e-01,  8.7304e-03,  2.5873e-01,\n",
       "           -2.6202e-01, -7.0129e-02, -6.3456e-02, -5.5170e-01, -1.2846e-02,\n",
       "           -7.4814e-01,  4.8504e-01,  3.4868e-01, -5.3905e-01,  3.7780e-01,\n",
       "           -8.7536e-02,  3.6370e-02, -1.3785e-01,  3.6221e-01, -2.8667e-01,\n",
       "           -3.5907e-01,  1.0538e-01, -6.9235e-01, -9.5442e-02, -1.8231e-01,\n",
       "            1.7192e-01, -1.2524e-01, -6.9173e-01, -1.1642e-01, -6.9372e-02,\n",
       "           -3.4073e-01, -7.0555e-01,  1.0747e+00, -4.8206e-02, -3.8526e-01,\n",
       "           -1.4435e-01,  2.7733e-01,  4.8236e-01,  1.6022e-01,  4.6600e-01,\n",
       "           -1.7864e-02, -1.0016e-01, -1.2556e-01,  2.2100e-01,  8.8796e-02,\n",
       "            3.3879e-02, -2.0287e-01, -2.5503e-01, -9.2222e-02,  2.3342e-01,\n",
       "           -2.2296e-02, -3.3437e-02,  1.8001e-01,  9.5357e-02,  4.5755e-01,\n",
       "            1.4466e-01,  2.3465e-01, -5.4965e-01,  2.2337e-01,  9.3973e-02,\n",
       "           -5.1277e-01, -9.3881e-02,  3.1022e-01,  1.5582e-01]],\n",
       "\n",
       "         [[-1.3241e-01, -3.4005e-01, -2.4608e-01, -4.0495e-02,  1.0863e-01,\n",
       "            4.7587e-01, -1.3457e-01,  1.8354e-02, -3.0594e-02, -4.5381e-01,\n",
       "            3.2852e-01, -3.0821e-01, -9.9837e-02, -3.7343e-01,  3.6902e-01,\n",
       "            3.3367e-01,  1.8270e-01, -7.0021e-01, -3.9054e-02, -3.6942e-01,\n",
       "           -4.4606e-01, -3.3738e-01, -1.4831e-01, -3.0767e-01,  4.1004e-02,\n",
       "           -3.1717e-01,  1.5773e-02,  3.0833e-01,  3.2224e-01, -7.3557e-02,\n",
       "           -1.1762e-01, -2.0139e-01, -1.1980e-01,  5.7711e-01, -8.0302e-03,\n",
       "           -5.1079e-01, -5.9408e-02,  3.8109e-02, -1.1928e-01, -2.9071e-01,\n",
       "            1.7609e-01, -5.0532e-01,  9.3064e-02,  8.6632e-02,  2.4006e-02,\n",
       "           -4.6583e-01,  1.0725e-01,  4.2606e-01,  1.1800e-01, -2.9251e-01,\n",
       "            4.5453e-01, -8.2833e-02, -1.8852e-01,  1.3783e-01,  9.3083e-01,\n",
       "           -5.4826e-01, -3.5226e-01, -4.8940e-01,  2.9792e-01,  1.8982e-01,\n",
       "           -3.1130e-01, -5.1315e-01, -3.8179e-01,  8.6556e-02],\n",
       "          [-1.3241e-01, -3.4005e-01, -2.4608e-01, -4.0495e-02,  1.0863e-01,\n",
       "            4.7587e-01, -1.3457e-01,  1.8354e-02, -3.0594e-02, -4.5381e-01,\n",
       "            3.2852e-01, -3.0821e-01, -9.9838e-02, -3.7343e-01,  3.6902e-01,\n",
       "            3.3367e-01,  1.8270e-01, -7.0021e-01, -3.9054e-02, -3.6942e-01,\n",
       "           -4.4606e-01, -3.3738e-01, -1.4831e-01, -3.0767e-01,  4.1004e-02,\n",
       "           -3.1717e-01,  1.5773e-02,  3.0833e-01,  3.2224e-01, -7.3557e-02,\n",
       "           -1.1762e-01, -2.0139e-01, -1.1980e-01,  5.7711e-01, -8.0302e-03,\n",
       "           -5.1079e-01, -5.9408e-02,  3.8109e-02, -1.1928e-01, -2.9071e-01,\n",
       "            1.7609e-01, -5.0532e-01,  9.3064e-02,  8.6632e-02,  2.4006e-02,\n",
       "           -4.6583e-01,  1.0725e-01,  4.2606e-01,  1.1800e-01, -2.9251e-01,\n",
       "            4.5453e-01, -8.2833e-02, -1.8852e-01,  1.3783e-01,  9.3083e-01,\n",
       "           -5.4826e-01, -3.5226e-01, -4.8940e-01,  2.9792e-01,  1.8982e-01,\n",
       "           -3.1130e-01, -5.1315e-01, -3.8179e-01,  8.6556e-02]],\n",
       "\n",
       "         [[ 3.9916e-01, -3.3009e-01, -2.5856e-01,  4.8113e-01, -3.3260e-01,\n",
       "            4.6252e-01,  5.7386e-01, -2.4813e-01,  1.3812e-01, -9.8552e-02,\n",
       "            2.4668e-01, -1.2660e-01,  5.9894e-02,  5.5026e-01, -1.6123e-01,\n",
       "            2.9959e-01, -1.9638e-01,  3.6336e-01, -1.3167e-01,  2.8514e-01,\n",
       "            3.5786e-01, -3.7799e-01, -7.0368e-02, -3.9437e-01, -1.0291e-01,\n",
       "            8.6382e-02, -9.7848e-02,  1.7359e-01, -4.9828e-01,  1.1562e-01,\n",
       "           -2.9424e-01,  2.6999e-01, -6.2216e-02,  4.9238e-01,  1.8625e-01,\n",
       "            1.5333e-01,  1.2565e-01,  1.8509e-01, -1.7046e-02,  4.9183e-01,\n",
       "            6.5596e-02, -2.2180e-01, -1.5647e-01,  1.3060e-01, -1.6813e-01,\n",
       "            1.1934e-01, -4.4596e-01,  4.7126e-01, -5.7927e-01, -3.0255e-01,\n",
       "            1.6752e-01, -5.7549e-01,  5.3949e-01,  8.3263e-02, -1.2037e-02,\n",
       "            1.8757e-01, -1.5921e-02, -3.7586e-02, -3.0908e-01,  3.7895e-01,\n",
       "            1.7230e-01, -2.4876e-01, -2.1814e-01,  3.9212e-01],\n",
       "          [ 3.9916e-01, -3.3009e-01, -2.5856e-01,  4.8113e-01, -3.3260e-01,\n",
       "            4.6252e-01,  5.7386e-01, -2.4813e-01,  1.3812e-01, -9.8552e-02,\n",
       "            2.4668e-01, -1.2660e-01,  5.9894e-02,  5.5026e-01, -1.6123e-01,\n",
       "            2.9959e-01, -1.9638e-01,  3.6336e-01, -1.3167e-01,  2.8514e-01,\n",
       "            3.5786e-01, -3.7799e-01, -7.0368e-02, -3.9437e-01, -1.0291e-01,\n",
       "            8.6382e-02, -9.7848e-02,  1.7359e-01, -4.9828e-01,  1.1562e-01,\n",
       "           -2.9425e-01,  2.6999e-01, -6.2217e-02,  4.9238e-01,  1.8625e-01,\n",
       "            1.5333e-01,  1.2565e-01,  1.8509e-01, -1.7046e-02,  4.9183e-01,\n",
       "            6.5596e-02, -2.2180e-01, -1.5647e-01,  1.3060e-01, -1.6813e-01,\n",
       "            1.1934e-01, -4.4596e-01,  4.7126e-01, -5.7927e-01, -3.0255e-01,\n",
       "            1.6752e-01, -5.7549e-01,  5.3949e-01,  8.3263e-02, -1.2037e-02,\n",
       "            1.8757e-01, -1.5922e-02, -3.7586e-02, -3.0908e-01,  3.7895e-01,\n",
       "            1.7230e-01, -2.4876e-01, -2.1814e-01,  3.9212e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.8908, -1.5089,  2.8993,  ...,  1.3479, -1.3185, -3.7326],\n",
       "          [ 2.0615, -3.2853,  1.8580,  ..., -2.5257, -3.8191, -0.7285],\n",
       "          [ 4.0658, -2.3991,  0.2688,  ..., -0.7453, -0.9974, -2.9379],\n",
       "          ...,\n",
       "          [ 3.5326,  0.6997,  1.9755,  ...,  2.9321, -4.3390, -2.0140],\n",
       "          [ 2.2958,  0.3498,  1.2437,  ...,  2.4116, -1.3379,  0.7800],\n",
       "          [-0.3440,  0.3448, -0.1884,  ...,  0.4593,  0.9843,  0.0814]],\n",
       "\n",
       "         [[ 0.9082,  3.0825,  1.6080,  ...,  5.3102, -0.6786, -3.2410],\n",
       "          [-2.7949,  3.0606,  1.3924,  ...,  1.7782,  1.5552, -2.8715],\n",
       "          [-2.1654,  1.5232,  0.5267,  ...,  4.3637, -0.2920, -2.4949],\n",
       "          ...,\n",
       "          [-3.9172, -0.1677,  1.8481,  ...,  0.6902,  1.4593, -1.3349],\n",
       "          [-0.3628,  1.1793,  1.0618,  ...,  2.2465,  2.2298, -2.0570],\n",
       "          [ 0.4188, -0.7298, -0.0860,  ..., -0.5938,  0.3028,  0.0144]],\n",
       "\n",
       "         [[-1.6233, -1.2315, -1.9075,  ..., -0.3607,  0.1129,  0.6297],\n",
       "          [-1.2323, -3.7037, -0.1736,  ...,  0.9045,  3.5339,  0.7319],\n",
       "          [-0.7050, -0.2071, -2.0422,  ..., -0.7459,  1.5166,  1.1316],\n",
       "          ...,\n",
       "          [-0.8595,  1.7383,  2.4046,  ...,  0.7167,  0.2478,  4.1907],\n",
       "          [-1.8388,  2.9440, -0.4132,  ..., -3.3102, -0.5272,  3.6019],\n",
       "          [-0.1414, -0.0796,  0.8465,  ...,  0.0181,  0.1639, -0.3622]],\n",
       "\n",
       "         [[-0.3552,  0.8001,  1.3065,  ...,  0.9879, -2.0915,  0.2039],\n",
       "          [-0.1218, -1.3319,  1.0938,  ...,  1.0052, -1.7088,  1.2206],\n",
       "          [-1.1850, -1.7129,  2.9645,  ..., -0.0824, -2.9263,  0.4142],\n",
       "          ...,\n",
       "          [ 0.9832, -0.4868, -0.4616,  ...,  0.7124, -2.2757,  0.1761],\n",
       "          [ 1.0877, -0.7946, -1.7451,  ...,  0.6816, -0.8911,  0.9023],\n",
       "          [-0.2799,  0.1694, -0.2152,  ..., -0.2341,  0.1977, -0.1976]],\n",
       "\n",
       "         [[ 0.5147,  0.8745,  0.3410,  ..., -0.3390, -2.0559, -1.0883],\n",
       "          [-0.0362, -0.1685,  2.3257,  ...,  0.1559, -1.3787, -2.2511],\n",
       "          [ 1.4063, -1.6544,  1.0070,  ...,  0.1767, -2.6166,  0.8063],\n",
       "          ...,\n",
       "          [-3.3326, -3.7714,  3.8153,  ...,  1.0593,  0.7535,  1.9819],\n",
       "          [-7.4501, -0.2250,  1.4935,  ...,  4.0050, -2.2445,  3.5449],\n",
       "          [ 0.0856,  0.5232, -0.7800,  ...,  0.0217, -0.7523, -0.1154]],\n",
       "\n",
       "         [[ 4.5540, -1.1817,  2.3787,  ...,  0.6116, -0.5674, -0.3475],\n",
       "          [ 2.6444, -1.7896,  1.9507,  ..., -2.9955, -2.4781,  0.6346],\n",
       "          [ 2.8109, -1.7731, -0.8407,  ..., -4.1630, -1.5013,  0.7085],\n",
       "          ...,\n",
       "          [-0.3530, -2.1024,  2.1636,  ..., -4.7570,  0.8360, -0.6978],\n",
       "          [-0.2216,  1.1728, -1.0911,  ..., -0.3063,  1.0750, -0.0482],\n",
       "          [-0.4121,  0.1856, -0.0939,  ...,  0.5092, -0.5512, -0.4428]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[-4.3830e+00,  3.6116e+00,  4.8143e+00,  ..., -3.2748e+00,\n",
       "            2.6760e+00,  3.2016e+00],\n",
       "          [ 4.3699e-01,  3.3995e+00, -1.2409e+00,  ...,  2.8377e+00,\n",
       "           -4.3251e-01, -1.0726e-01],\n",
       "          [-3.5803e+00,  6.4686e-01, -5.8138e-01,  ..., -5.4366e+00,\n",
       "           -2.5293e+00,  1.3248e-01],\n",
       "          ...,\n",
       "          [ 9.4542e-01,  4.1754e+00, -4.0710e+00,  ...,  8.0599e-01,\n",
       "           -4.3935e+00,  2.9223e+00],\n",
       "          [-3.5280e+00, -2.1326e+00, -2.7778e+00,  ..., -4.2501e+00,\n",
       "            2.3132e-01,  6.2731e+00],\n",
       "          [ 4.3043e-03, -1.1550e-01, -1.2999e-01,  ...,  5.4983e-02,\n",
       "            2.7868e-02, -6.2680e-02]],\n",
       "\n",
       "         [[ 4.5096e-01,  4.8888e-01,  2.1439e+00,  ...,  4.1715e-01,\n",
       "            7.6833e-01,  1.0243e+00],\n",
       "          [ 1.1343e+00, -2.4474e+00,  3.3384e+00,  ..., -3.5386e+00,\n",
       "            4.3431e+00,  1.9926e+00],\n",
       "          [ 7.1293e-01,  2.3867e+00,  1.7668e+00,  ..., -2.0587e+00,\n",
       "            3.1878e+00, -4.9797e-03],\n",
       "          ...,\n",
       "          [-4.8857e+00,  2.4674e+00,  3.3291e+00,  ..., -1.1083e+00,\n",
       "            1.7434e+00, -8.4526e-01],\n",
       "          [-6.6044e+00, -2.8694e-01,  3.3827e+00,  ..., -3.1507e+00,\n",
       "            1.3759e+00,  1.5375e+00],\n",
       "          [-1.4897e-01, -2.1008e-01, -4.0116e-01,  ...,  9.7998e-02,\n",
       "           -5.9319e-02,  1.1104e-02]],\n",
       "\n",
       "         [[-2.1403e+00,  3.2403e+00,  4.3206e-01,  ..., -7.2623e-01,\n",
       "           -3.7382e+00, -7.0025e-01],\n",
       "          [-1.6391e+00,  2.0911e+00,  9.9284e-01,  ..., -5.1299e-01,\n",
       "           -4.6837e-01,  5.6460e-01],\n",
       "          [-1.7236e+00,  2.4153e+00,  2.9380e+00,  ...,  2.1176e-01,\n",
       "           -1.9287e+00, -8.6105e-01],\n",
       "          ...,\n",
       "          [-2.6895e+00,  1.2895e+00, -8.4127e-02,  ..., -1.1003e+00,\n",
       "            3.1664e+00, -6.5450e-01],\n",
       "          [-3.4402e-01,  3.8979e+00, -1.8144e+00,  ..., -1.6821e+00,\n",
       "            4.2965e+00,  2.5274e+00],\n",
       "          [ 5.7525e-01,  7.7593e-02, -1.3286e-01,  ...,  8.4599e-02,\n",
       "           -1.0036e-01, -9.4929e-02]],\n",
       "\n",
       "         [[-5.6323e+00,  2.8887e+00, -1.1398e+00,  ...,  9.4577e-01,\n",
       "           -2.5332e+00,  4.5568e+00],\n",
       "          [-4.5333e+00,  1.5870e+00,  3.6077e-01,  ..., -1.1993e+00,\n",
       "           -3.4037e+00, -5.1127e+00],\n",
       "          [-3.2749e+00,  1.6921e+00,  1.8097e+00,  ..., -2.1098e+00,\n",
       "           -2.8040e+00, -1.8001e+00],\n",
       "          ...,\n",
       "          [-2.4365e+00,  4.6332e+00, -6.4091e-01,  ...,  2.8179e+00,\n",
       "           -3.3426e+00,  5.2060e+00],\n",
       "          [-3.6690e+00, -3.6484e-01,  1.6438e+00,  ...,  3.9384e+00,\n",
       "           -2.6196e+00,  2.7499e+00],\n",
       "          [-1.1812e-01, -1.8130e-01,  4.4963e-02,  ..., -1.0707e-03,\n",
       "            5.5665e-02,  2.4290e-02]],\n",
       "\n",
       "         [[ 4.4149e+00,  4.4833e-01, -6.5147e+00,  ..., -7.4518e+00,\n",
       "           -1.1174e+00, -4.8247e+00],\n",
       "          [ 2.9280e+00,  9.0951e-02, -6.7410e-01,  ..., -7.8048e+00,\n",
       "           -6.0520e-01,  2.4661e-01],\n",
       "          [ 2.4018e+00, -2.3702e+00, -1.4875e+00,  ..., -7.8122e+00,\n",
       "           -5.2278e-02, -7.3592e-01],\n",
       "          ...,\n",
       "          [-6.9992e-01,  6.0493e-01, -7.9360e-01,  ...,  3.0238e+00,\n",
       "           -2.4898e+00,  9.8655e-01],\n",
       "          [-3.8182e+00,  1.9362e+00, -3.8807e-01,  ...,  4.1625e+00,\n",
       "           -3.3478e+00,  2.1330e+00],\n",
       "          [ 4.7943e-02,  1.3113e-01,  5.5646e-02,  ...,  1.8243e-02,\n",
       "            1.5671e-01, -7.6101e-02]],\n",
       "\n",
       "         [[ 4.5033e+00, -3.3368e+00, -6.5400e+00,  ..., -6.9929e-01,\n",
       "           -7.7139e-01, -5.1046e-01],\n",
       "          [ 5.2610e+00,  3.1384e+00, -6.9041e-01,  ...,  1.8774e+00,\n",
       "            3.7236e+00, -3.9459e+00],\n",
       "          [ 1.6780e+00, -3.8997e-01, -7.2951e+00,  ..., -4.2028e-01,\n",
       "           -1.2564e+00, -2.5159e+00],\n",
       "          ...,\n",
       "          [-1.7933e+00,  2.6305e+00,  4.8428e+00,  ..., -1.2653e+00,\n",
       "            2.6395e+00,  1.3266e+00],\n",
       "          [-1.6105e+00, -1.4055e+00,  1.2572e+00,  ..., -2.9016e+00,\n",
       "            5.5362e-01,  3.9169e+00],\n",
       "          [ 9.1398e-02, -1.9029e-02,  5.9066e-02,  ...,  1.5016e-02,\n",
       "           -8.8335e-02,  5.0688e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.2145, -0.3523,  0.2711, -0.1938, -0.9874, -0.3351, -0.1558,\n",
       "            0.2761, -0.4779,  0.2913, -0.2346,  0.0918, -0.3703,  0.1211,\n",
       "           -0.7928, -0.3656,  0.3426,  0.9027, -0.2933,  0.3652,  0.7706,\n",
       "            0.6391,  0.0441,  0.1440, -0.8150,  0.4756,  0.4977,  0.2007,\n",
       "           -0.1977, -0.1395, -0.4428,  0.7557, -0.1410,  0.7714,  0.1013,\n",
       "           -0.2457, -0.0447, -0.9441,  0.4838, -0.0936, -0.0896, -0.2821,\n",
       "           -0.2819, -0.1470, -1.0677, -0.3748, -0.0338,  0.1703, -0.9582,\n",
       "           -0.1503,  0.2040, -0.2946, -0.9878, -0.2671,  0.2430,  0.6826,\n",
       "           -0.4919,  0.3926, -0.8465, -0.5473,  0.3003, -0.3027,  1.0323,\n",
       "           -0.0080],\n",
       "          [-0.2145, -0.3523,  0.2711, -0.1938, -0.9874, -0.3351, -0.1558,\n",
       "            0.2761, -0.4779,  0.2913, -0.2346,  0.0918, -0.3703,  0.1211,\n",
       "           -0.7928, -0.3656,  0.3426,  0.9027, -0.2933,  0.3652,  0.7706,\n",
       "            0.6391,  0.0441,  0.1440, -0.8150,  0.4756,  0.4977,  0.2007,\n",
       "           -0.1977, -0.1395, -0.4428,  0.7557, -0.1410,  0.7714,  0.1013,\n",
       "           -0.2457, -0.0447, -0.9441,  0.4838, -0.0936, -0.0896, -0.2821,\n",
       "           -0.2819, -0.1470, -1.0677, -0.3748, -0.0338,  0.1703, -0.9582,\n",
       "           -0.1503,  0.2040, -0.2946, -0.9878, -0.2671,  0.2430,  0.6826,\n",
       "           -0.4919,  0.3926, -0.8465, -0.5473,  0.3003, -0.3027,  1.0323,\n",
       "           -0.0080]],\n",
       "\n",
       "         [[-0.2794, -0.6070, -0.4776,  0.2115,  0.0871, -0.0369, -0.1212,\n",
       "            0.2019, -0.2534, -0.0941,  0.4874, -0.7070,  0.0232,  0.2573,\n",
       "            1.0592,  0.0321,  0.5390,  0.2732, -0.0919, -1.0351, -0.9483,\n",
       "           -0.0178,  0.0916,  0.1492, -0.2739, -0.2934,  1.1872, -0.6144,\n",
       "           -0.5771, -0.0452,  0.1566, -0.0711,  0.2745,  0.8043, -0.0080,\n",
       "            0.0459,  0.0110, -0.0344,  1.0688, -0.1827, -0.1386, -0.5390,\n",
       "            0.2622,  0.3561,  0.9614,  0.5353,  0.2464,  0.5504,  0.3640,\n",
       "           -0.0517, -0.5155,  0.1522,  0.3963, -0.2151, -0.7620,  0.3767,\n",
       "           -0.4678,  0.7787, -0.0807,  0.5135,  0.3337,  0.8275,  0.0429,\n",
       "            0.1150],\n",
       "          [-0.2794, -0.6070, -0.4776,  0.2115,  0.0871, -0.0369, -0.1212,\n",
       "            0.2019, -0.2534, -0.0941,  0.4874, -0.7070,  0.0232,  0.2573,\n",
       "            1.0592,  0.0321,  0.5390,  0.2732, -0.0919, -1.0351, -0.9483,\n",
       "           -0.0178,  0.0916,  0.1492, -0.2739, -0.2934,  1.1872, -0.6144,\n",
       "           -0.5771, -0.0452,  0.1566, -0.0711,  0.2745,  0.8043, -0.0080,\n",
       "            0.0459,  0.0110, -0.0344,  1.0688, -0.1827, -0.1386, -0.5390,\n",
       "            0.2622,  0.3561,  0.9614,  0.5353,  0.2464,  0.5504,  0.3640,\n",
       "           -0.0517, -0.5155,  0.1522,  0.3963, -0.2151, -0.7620,  0.3767,\n",
       "           -0.4678,  0.7787, -0.0807,  0.5135,  0.3337,  0.8275,  0.0429,\n",
       "            0.1150]],\n",
       "\n",
       "         [[-0.2787,  0.1500, -0.2936, -0.0067, -0.0238, -0.6508,  0.1504,\n",
       "            0.1866, -0.8410, -0.5027, -0.7850, -0.1463, -0.0394, -0.0421,\n",
       "            0.8353, -1.2943,  0.4524,  0.2445,  0.1519,  0.5612, -0.4839,\n",
       "            0.5777,  0.5012,  0.2168, -1.1515, -1.2300,  0.4205,  0.2376,\n",
       "            0.8554,  0.3419,  1.6007, -0.2874, -1.0556,  0.2272,  0.4903,\n",
       "           -0.8371,  0.1573,  0.0193,  1.1494, -0.2022,  0.2310, -0.1071,\n",
       "            0.5800,  0.0408,  0.4879,  0.7332,  0.4052,  0.1232,  0.3406,\n",
       "            0.7728, -0.2939,  0.4567, -0.1526,  0.6700,  0.2135,  0.5980,\n",
       "            0.2779, -0.6797,  0.1343, -0.2993, -0.4350,  0.6565,  0.1346,\n",
       "            0.2503],\n",
       "          [-0.2787,  0.1500, -0.2936, -0.0067, -0.0238, -0.6508,  0.1504,\n",
       "            0.1866, -0.8410, -0.5027, -0.7850, -0.1463, -0.0394, -0.0421,\n",
       "            0.8353, -1.2943,  0.4524,  0.2445,  0.1519,  0.5612, -0.4839,\n",
       "            0.5777,  0.5012,  0.2168, -1.1515, -1.2300,  0.4205,  0.2376,\n",
       "            0.8554,  0.3419,  1.6007, -0.2874, -1.0556,  0.2272,  0.4903,\n",
       "           -0.8371,  0.1573,  0.0193,  1.1494, -0.2022,  0.2310, -0.1071,\n",
       "            0.5800,  0.0408,  0.4879,  0.7332,  0.4052,  0.1232,  0.3406,\n",
       "            0.7728, -0.2939,  0.4567, -0.1526,  0.6700,  0.2135,  0.5980,\n",
       "            0.2779, -0.6797,  0.1343, -0.2993, -0.4350,  0.6565,  0.1346,\n",
       "            0.2503]],\n",
       "\n",
       "         [[-0.1123, -0.2355,  0.5074, -0.3583,  0.1946,  0.6238,  0.7838,\n",
       "           -0.3288,  0.2760, -0.8142, -0.0514, -0.6835, -0.5872, -0.8090,\n",
       "           -0.5084,  0.2305, -0.6391, -1.0664,  0.8438, -0.1438, -0.2941,\n",
       "           -0.7048,  0.9136,  0.1463, -0.2659, -0.7714, -0.1719, -0.4389,\n",
       "           -0.7487, -0.6025, -0.7452, -0.4797,  0.4728, -0.0606, -0.0230,\n",
       "           -0.7169, -0.0657,  0.1191,  0.7631,  0.1335, -0.8380, -0.3168,\n",
       "            0.1812, -0.1536, -0.3137,  0.1091,  0.3388,  0.8152,  0.6501,\n",
       "            0.3982,  0.4297,  0.0890,  0.7267,  0.3939, -1.0735, -0.0265,\n",
       "            0.4366, -0.8866, -0.9071,  1.0316, -0.2181,  0.1777, -0.8018,\n",
       "            0.4177],\n",
       "          [-0.1123, -0.2355,  0.5074, -0.3583,  0.1946,  0.6238,  0.7838,\n",
       "           -0.3288,  0.2760, -0.8142, -0.0514, -0.6835, -0.5872, -0.8090,\n",
       "           -0.5084,  0.2305, -0.6391, -1.0664,  0.8438, -0.1438, -0.2941,\n",
       "           -0.7048,  0.9136,  0.1463, -0.2659, -0.7714, -0.1719, -0.4389,\n",
       "           -0.7487, -0.6025, -0.7452, -0.4797,  0.4728, -0.0606, -0.0230,\n",
       "           -0.7169, -0.0657,  0.1191,  0.7631,  0.1335, -0.8380, -0.3168,\n",
       "            0.1812, -0.1536, -0.3137,  0.1091,  0.3388,  0.8152,  0.6501,\n",
       "            0.3982,  0.4297,  0.0890,  0.7267,  0.3939, -1.0735, -0.0265,\n",
       "            0.4366, -0.8866, -0.9071,  1.0316, -0.2181,  0.1777, -0.8018,\n",
       "            0.4177]],\n",
       "\n",
       "         [[-0.1889,  0.6440, -0.0438, -0.0249, -0.2429,  0.5179, -0.1628,\n",
       "           -0.3263,  0.5577, -0.3173, -0.1668,  0.0066,  0.1299, -0.3002,\n",
       "           -0.0754,  0.2792,  0.1679, -0.0172,  0.2931, -0.7027, -0.0771,\n",
       "           -0.3694, -0.0946,  0.1536, -0.4287,  0.2827,  0.1043, -0.1270,\n",
       "           -0.0927,  0.0534,  0.2213,  0.3255, -0.4038, -0.2330,  0.1343,\n",
       "            0.3258,  0.4732,  0.1620,  0.2568, -0.3111,  0.5400, -0.1995,\n",
       "           -0.0195, -0.6358, -0.3332,  0.0747,  0.4107,  0.5231, -0.0646,\n",
       "            0.5214,  0.4601,  0.0907,  0.1418, -0.3242, -0.0755, -0.3662,\n",
       "           -0.0787, -0.4787, -0.1252,  0.2656, -0.1032, -0.0978,  0.0574,\n",
       "            0.3039],\n",
       "          [-0.1889,  0.6440, -0.0438, -0.0249, -0.2429,  0.5179, -0.1628,\n",
       "           -0.3263,  0.5577, -0.3173, -0.1668,  0.0066,  0.1299, -0.3002,\n",
       "           -0.0754,  0.2792,  0.1679, -0.0172,  0.2931, -0.7027, -0.0771,\n",
       "           -0.3694, -0.0946,  0.1536, -0.4287,  0.2827,  0.1043, -0.1270,\n",
       "           -0.0927,  0.0534,  0.2213,  0.3255, -0.4038, -0.2330,  0.1343,\n",
       "            0.3258,  0.4732,  0.1620,  0.2568, -0.3111,  0.5400, -0.1995,\n",
       "           -0.0195, -0.6358, -0.3332,  0.0747,  0.4107,  0.5231, -0.0646,\n",
       "            0.5214,  0.4601,  0.0907,  0.1418, -0.3242, -0.0755, -0.3662,\n",
       "           -0.0787, -0.4787, -0.1252,  0.2656, -0.1032, -0.0978,  0.0574,\n",
       "            0.3039]],\n",
       "\n",
       "         [[-0.0485, -0.5525,  0.0087, -0.0427, -0.3861,  0.0379,  0.1819,\n",
       "            0.7149,  0.1577,  0.4409, -0.5163,  0.7491, -1.2562,  0.8255,\n",
       "            0.5962, -0.2692,  0.4742, -0.0629,  0.7728,  1.0196, -0.0044,\n",
       "            0.3210, -0.3065,  0.7592,  0.6432, -0.3579,  0.4946, -0.6142,\n",
       "            0.1213, -0.2313, -0.4706,  0.1960, -0.5918, -0.2417, -0.7111,\n",
       "           -0.2101, -0.4240,  0.3155,  0.1563,  0.6146,  0.4764, -0.6181,\n",
       "           -0.6254, -0.3275,  0.6129, -0.0239,  0.3575, -0.2154,  0.3733,\n",
       "            0.6701, -0.1656,  0.1072,  0.6765, -0.7004, -0.8512,  1.0908,\n",
       "           -0.3267,  0.5571, -0.1926,  0.4115,  0.1562, -0.0657, -0.6278,\n",
       "           -0.4895],\n",
       "          [-0.0485, -0.5525,  0.0087, -0.0427, -0.3861,  0.0379,  0.1819,\n",
       "            0.7149,  0.1577,  0.4409, -0.5163,  0.7491, -1.2562,  0.8255,\n",
       "            0.5962, -0.2692,  0.4742, -0.0629,  0.7728,  1.0196, -0.0044,\n",
       "            0.3210, -0.3065,  0.7592,  0.6432, -0.3579,  0.4946, -0.6142,\n",
       "            0.1213, -0.2313, -0.4706,  0.1960, -0.5918, -0.2417, -0.7111,\n",
       "           -0.2101, -0.4240,  0.3155,  0.1563,  0.6146,  0.4764, -0.6181,\n",
       "           -0.6254, -0.3275,  0.6129, -0.0239,  0.3575, -0.2154,  0.3733,\n",
       "            0.6701, -0.1656,  0.1072,  0.6765, -0.7004, -0.8512,  1.0908,\n",
       "           -0.3267,  0.5571, -0.1926,  0.4115,  0.1562, -0.0657, -0.6278,\n",
       "           -0.4895]]]], grad_fn=<TransposeBackward0>), tensor([[[[-0.0277,  0.3226,  0.1978, -0.4947, -0.1433,  0.3347,  0.0898,\n",
       "           -0.1661,  0.5141, -0.6486, -0.1623, -0.6022,  0.3406,  0.2487,\n",
       "           -0.2709,  0.4857,  0.1862,  0.3185,  0.1556,  0.1195,  0.4743,\n",
       "            0.1314,  0.1554, -0.4982,  0.8033, -0.2041, -0.3252, -0.4781,\n",
       "           -0.0394,  0.5411,  0.4569,  0.3180, -0.2491,  0.2798, -0.2026,\n",
       "            0.4002, -0.7039,  0.2695, -0.8969, -0.6420,  0.2573,  0.3224,\n",
       "           -0.2764,  0.3949, -0.2196, -0.2147,  0.2416, -0.1328,  0.1959,\n",
       "            0.1588,  0.1675, -0.0779, -0.3597,  0.1122, -0.1310,  0.0832,\n",
       "           -0.2471,  0.2971, -0.1004,  0.2987,  0.0166, -0.0974, -0.2433,\n",
       "            0.1380],\n",
       "          [-0.0277,  0.3226,  0.1978, -0.4947, -0.1433,  0.3347,  0.0898,\n",
       "           -0.1661,  0.5141, -0.6486, -0.1623, -0.6022,  0.3406,  0.2487,\n",
       "           -0.2709,  0.4857,  0.1862,  0.3185,  0.1556,  0.1195,  0.4743,\n",
       "            0.1314,  0.1554, -0.4982,  0.8033, -0.2041, -0.3252, -0.4781,\n",
       "           -0.0394,  0.5411,  0.4569,  0.3180, -0.2491,  0.2798, -0.2026,\n",
       "            0.4002, -0.7039,  0.2695, -0.8969, -0.6420,  0.2573,  0.3224,\n",
       "           -0.2764,  0.3949, -0.2196, -0.2147,  0.2416, -0.1328,  0.1959,\n",
       "            0.1588,  0.1675, -0.0779, -0.3597,  0.1122, -0.1310,  0.0832,\n",
       "           -0.2471,  0.2971, -0.1004,  0.2987,  0.0166, -0.0974, -0.2433,\n",
       "            0.1380]],\n",
       "\n",
       "         [[ 0.0979, -0.0036, -0.1926,  0.2324, -0.0734, -0.2597, -0.2391,\n",
       "           -0.0132,  0.1049, -0.1597,  0.2575,  0.0143, -0.2715, -0.0204,\n",
       "            0.0104,  0.1593, -0.2034, -0.1735,  0.0230, -0.0122,  0.2744,\n",
       "           -0.2630,  0.0938, -0.2833, -0.2849, -0.1313,  0.1217, -0.1147,\n",
       "            0.0868,  0.0035,  0.1847, -0.0905, -0.0286,  0.0037,  0.0077,\n",
       "            0.0083, -0.3556, -0.0730,  0.0501,  0.1135,  0.1023, -0.0804,\n",
       "            0.1158, -0.3074, -0.1008,  0.2683,  0.0183, -0.2846, -0.0601,\n",
       "            0.0857, -0.1952,  0.1813,  0.1656, -0.2509,  0.1664, -0.2361,\n",
       "            0.2137, -0.1128,  0.0634, -0.2450,  0.0291, -0.0135,  0.0794,\n",
       "            0.0977],\n",
       "          [ 0.0979, -0.0036, -0.1926,  0.2324, -0.0734, -0.2597, -0.2391,\n",
       "           -0.0132,  0.1049, -0.1597,  0.2575,  0.0143, -0.2715, -0.0204,\n",
       "            0.0104,  0.1593, -0.2034, -0.1735,  0.0230, -0.0122,  0.2744,\n",
       "           -0.2630,  0.0938, -0.2833, -0.2849, -0.1313,  0.1217, -0.1147,\n",
       "            0.0868,  0.0035,  0.1847, -0.0905, -0.0286,  0.0037,  0.0077,\n",
       "            0.0083, -0.3556, -0.0730,  0.0501,  0.1135,  0.1023, -0.0804,\n",
       "            0.1158, -0.3074, -0.1008,  0.2683,  0.0183, -0.2846, -0.0601,\n",
       "            0.0857, -0.1952,  0.1813,  0.1656, -0.2509,  0.1664, -0.2361,\n",
       "            0.2137, -0.1128,  0.0634, -0.2450,  0.0291, -0.0135,  0.0794,\n",
       "            0.0977]],\n",
       "\n",
       "         [[ 0.6007, -0.4041, -0.1625, -0.5339,  1.0117,  0.6942, -0.4999,\n",
       "            0.4010,  0.5564,  0.2404,  0.3062,  0.5731,  0.2751, -0.1968,\n",
       "            0.2034,  0.4140, -0.3760, -0.0601,  0.0141, -0.3843,  0.4133,\n",
       "           -0.4617, -0.5332,  0.4112,  0.4285,  0.4723,  0.7130,  0.5331,\n",
       "            0.3588, -0.5840,  0.2803,  0.6371,  1.0148,  0.3423, -0.1498,\n",
       "           -0.1371, -0.1444,  0.3475,  0.1068,  0.7433,  0.1604, -0.1213,\n",
       "            0.3486,  0.3526,  0.1675, -0.0514, -0.4356, -0.8305, -0.0469,\n",
       "           -0.4050,  0.2620, -0.2374,  0.2582, -0.2433, -0.4115,  0.2377,\n",
       "           -0.4181, -1.0158,  0.0051, -0.2310, -0.5908,  0.1928,  0.1195,\n",
       "           -0.4238],\n",
       "          [ 0.6007, -0.4041, -0.1625, -0.5339,  1.0117,  0.6942, -0.4999,\n",
       "            0.4010,  0.5564,  0.2404,  0.3062,  0.5731,  0.2751, -0.1968,\n",
       "            0.2034,  0.4140, -0.3760, -0.0601,  0.0141, -0.3843,  0.4133,\n",
       "           -0.4617, -0.5332,  0.4112,  0.4285,  0.4723,  0.7130,  0.5331,\n",
       "            0.3588, -0.5840,  0.2803,  0.6371,  1.0148,  0.3423, -0.1498,\n",
       "           -0.1371, -0.1444,  0.3475,  0.1068,  0.7433,  0.1604, -0.1213,\n",
       "            0.3486,  0.3526,  0.1675, -0.0514, -0.4356, -0.8305, -0.0469,\n",
       "           -0.4050,  0.2620, -0.2374,  0.2582, -0.2433, -0.4115,  0.2377,\n",
       "           -0.4181, -1.0158,  0.0051, -0.2310, -0.5908,  0.1928,  0.1195,\n",
       "           -0.4238]],\n",
       "\n",
       "         [[ 0.0232,  0.0350, -0.2361,  0.2018, -0.0696, -0.2366,  0.2655,\n",
       "           -0.0716, -0.1802, -0.2454, -0.4446, -0.0359,  0.7009, -0.1411,\n",
       "           -0.2845, -0.2920, -0.0387,  0.0627,  0.1501, -0.0935, -0.3396,\n",
       "           -0.3596,  0.5487,  0.6393,  0.0112, -0.1784,  0.0117,  0.1994,\n",
       "           -0.1883,  0.4467, -0.4608, -0.4512, -0.0864,  0.0364, -0.1051,\n",
       "            0.0842,  0.2076, -0.4678, -0.2392, -0.2254,  0.2158, -0.0829,\n",
       "           -0.4151,  0.1011,  0.0912,  0.5308, -0.4259,  0.5391,  0.1825,\n",
       "            0.1665,  0.0896,  0.3204,  0.5385, -0.2460,  0.4848,  0.2154,\n",
       "            0.0404, -0.0032,  0.2006, -0.7475,  0.3019, -0.0686, -0.3127,\n",
       "           -0.2283],\n",
       "          [ 0.0232,  0.0350, -0.2361,  0.2018, -0.0696, -0.2366,  0.2655,\n",
       "           -0.0716, -0.1802, -0.2454, -0.4446, -0.0359,  0.7009, -0.1411,\n",
       "           -0.2845, -0.2920, -0.0387,  0.0627,  0.1501, -0.0935, -0.3396,\n",
       "           -0.3596,  0.5487,  0.6393,  0.0112, -0.1784,  0.0117,  0.1994,\n",
       "           -0.1883,  0.4467, -0.4608, -0.4512, -0.0864,  0.0364, -0.1051,\n",
       "            0.0842,  0.2076, -0.4678, -0.2392, -0.2254,  0.2158, -0.0829,\n",
       "           -0.4151,  0.1011,  0.0912,  0.5308, -0.4259,  0.5391,  0.1825,\n",
       "            0.1665,  0.0896,  0.3204,  0.5385, -0.2460,  0.4848,  0.2154,\n",
       "            0.0404, -0.0032,  0.2006, -0.7475,  0.3019, -0.0686, -0.3127,\n",
       "           -0.2283]],\n",
       "\n",
       "         [[-0.0172,  0.1098, -0.3023, -0.1756,  0.0209,  0.4869, -0.7408,\n",
       "           -0.1698, -0.0250, -0.0257,  0.0523,  0.2341,  0.2597,  0.4216,\n",
       "           -0.3681, -0.3859, -0.1166,  0.6640, -0.0624, -0.3060, -0.2843,\n",
       "            0.4978, -0.2106, -0.1673, -0.7802, -0.6983, -0.0858,  0.0636,\n",
       "           -0.7362, -0.3418,  0.3194,  0.3488, -0.1026, -0.4021, -0.1010,\n",
       "           -0.4340,  0.5029,  0.4687, -0.7561, -0.9410, -0.1008,  0.3329,\n",
       "            0.3984, -0.2984,  0.1673,  0.2832,  0.3296, -0.1587,  0.2785,\n",
       "           -0.1092, -0.2272,  0.1231,  0.3746,  0.3463,  0.5534,  0.0738,\n",
       "            0.3895,  0.5571,  0.0914, -0.0935, -0.2762,  0.2656,  0.0123,\n",
       "            0.0111],\n",
       "          [-0.0172,  0.1098, -0.3023, -0.1756,  0.0209,  0.4869, -0.7408,\n",
       "           -0.1698, -0.0250, -0.0257,  0.0523,  0.2341,  0.2597,  0.4216,\n",
       "           -0.3681, -0.3859, -0.1166,  0.6640, -0.0624, -0.3060, -0.2843,\n",
       "            0.4978, -0.2106, -0.1673, -0.7802, -0.6983, -0.0858,  0.0636,\n",
       "           -0.7362, -0.3418,  0.3194,  0.3488, -0.1026, -0.4021, -0.1010,\n",
       "           -0.4340,  0.5029,  0.4687, -0.7561, -0.9410, -0.1008,  0.3329,\n",
       "            0.3984, -0.2984,  0.1673,  0.2832,  0.3296, -0.1587,  0.2785,\n",
       "           -0.1092, -0.2272,  0.1231,  0.3746,  0.3463,  0.5534,  0.0738,\n",
       "            0.3895,  0.5571,  0.0914, -0.0935, -0.2762,  0.2656,  0.0123,\n",
       "            0.0111]],\n",
       "\n",
       "         [[-0.3182,  0.0111, -0.4592,  0.6089,  0.0364, -0.1314,  0.1467,\n",
       "            0.0112, -0.1593,  0.2172,  0.0346,  0.2146, -0.1592,  0.3880,\n",
       "           -0.5085, -0.5341, -0.3064, -0.0430, -0.3294,  0.4998, -0.7398,\n",
       "            0.0450,  1.1481,  0.5169, -0.1228, -0.7640, -0.1330, -0.0314,\n",
       "            0.5531, -0.2337, -0.0533,  0.3273, -0.2077,  0.7700, -0.1033,\n",
       "           -0.0292, -0.3813, -0.7254,  0.4022,  0.0701, -0.0411, -0.1405,\n",
       "           -0.6469, -0.4257, -0.5535, -0.2176,  0.0407,  0.3667,  0.0584,\n",
       "            0.1383,  0.3617, -0.2261,  0.3179, -0.1209,  0.5424,  0.4428,\n",
       "           -0.5390, -0.3499, -0.2863,  0.3037, -0.2396, -0.0169,  0.2015,\n",
       "           -0.5222],\n",
       "          [-0.3182,  0.0111, -0.4592,  0.6089,  0.0364, -0.1314,  0.1467,\n",
       "            0.0112, -0.1593,  0.2172,  0.0346,  0.2146, -0.1592,  0.3880,\n",
       "           -0.5085, -0.5341, -0.3064, -0.0430, -0.3294,  0.4998, -0.7398,\n",
       "            0.0450,  1.1481,  0.5169, -0.1228, -0.7640, -0.1330, -0.0314,\n",
       "            0.5531, -0.2337, -0.0533,  0.3273, -0.2077,  0.7700, -0.1033,\n",
       "           -0.0292, -0.3813, -0.7254,  0.4022,  0.0701, -0.0411, -0.1405,\n",
       "           -0.6469, -0.4257, -0.5535, -0.2176,  0.0407,  0.3667,  0.0584,\n",
       "            0.1383,  0.3617, -0.2261,  0.3179, -0.1209,  0.5424,  0.4428,\n",
       "           -0.5390, -0.3499, -0.2863,  0.3037, -0.2396, -0.0169,  0.2015,\n",
       "           -0.5222]]]], grad_fn=<TransposeBackward0>), tensor([[[[-2.8878e+00, -1.4092e+00,  2.6278e+00,  ..., -1.4439e+00,\n",
       "            5.0584e-01,  3.2217e+00],\n",
       "          [-8.1362e-02, -1.9570e+00,  1.8507e+00,  ..., -1.0692e+00,\n",
       "           -2.6491e+00, -5.6491e-02],\n",
       "          [-1.8201e+00, -1.3866e+00,  4.1582e+00,  ...,  3.8494e-01,\n",
       "           -2.4614e+00,  2.5368e+00],\n",
       "          ...,\n",
       "          [-3.5404e-01, -1.5674e+00, -2.1657e-01,  ...,  1.6739e+00,\n",
       "           -2.5383e+00,  5.1532e-01],\n",
       "          [-1.9277e-01,  3.4872e-01,  1.7313e+00,  ...,  4.5714e-01,\n",
       "           -1.6079e+00,  1.8707e+00],\n",
       "          [ 1.8431e-01,  2.5035e-01, -6.3298e-01,  ...,  1.2848e-01,\n",
       "            2.1294e-01,  3.5162e-01]],\n",
       "\n",
       "         [[-3.5921e+00, -1.8593e+00, -1.6303e+00,  ..., -1.8298e+00,\n",
       "           -1.6850e+00,  4.6459e+00],\n",
       "          [-2.9543e+00, -2.1115e+00, -2.2971e+00,  ..., -1.6241e+00,\n",
       "           -1.2039e+00,  2.4549e+00],\n",
       "          [-2.3061e-02, -2.7472e+00, -4.0037e+00,  ...,  2.3993e+00,\n",
       "            3.3156e-01,  2.4453e+00],\n",
       "          ...,\n",
       "          [ 1.0429e+00,  1.1436e+00, -2.8566e+00,  ..., -1.6336e+00,\n",
       "           -1.6185e+00,  5.8915e-01],\n",
       "          [ 2.1542e-01, -4.1494e-01, -3.0224e+00,  ..., -6.8907e-01,\n",
       "            3.3436e-01, -1.0444e+00],\n",
       "          [ 5.4150e-01,  2.0054e-01,  1.4026e+00,  ...,  9.3324e-02,\n",
       "            2.2755e-02, -1.2392e+00]],\n",
       "\n",
       "         [[-1.7073e+00,  2.1329e+00, -1.0489e+00,  ...,  7.6126e-01,\n",
       "            8.7985e-01,  5.5777e-01],\n",
       "          [-5.7746e-01,  1.3866e+00,  1.8250e+00,  ...,  1.1751e-01,\n",
       "            6.2516e-01, -3.2915e+00],\n",
       "          [-1.5045e+00, -1.1992e-01,  2.8446e-01,  ...,  1.1902e+00,\n",
       "            2.8584e+00, -8.8143e-01],\n",
       "          ...,\n",
       "          [-2.3656e+00, -7.0300e-01,  2.1584e+00,  ...,  1.3806e-01,\n",
       "            8.2051e-02, -2.5019e+00],\n",
       "          [-3.2109e+00, -2.1840e-01,  6.3567e-01,  ...,  4.4346e-01,\n",
       "            2.8066e-01, -1.1787e+00],\n",
       "          [ 4.7383e-01, -2.5167e-01, -4.9897e-03,  ...,  2.9184e-01,\n",
       "           -6.5778e-01,  3.2735e-01]],\n",
       "\n",
       "         [[ 1.7027e+00,  1.8671e+00, -3.6074e-01,  ..., -1.0912e-01,\n",
       "           -1.6264e+00,  1.2501e+00],\n",
       "          [-7.4712e-01,  1.8185e+00, -1.8016e-02,  ...,  7.0761e+00,\n",
       "           -3.3889e-02, -1.1590e+00],\n",
       "          [ 4.2150e-01,  1.6844e+00, -4.0225e-01,  ...,  4.0177e+00,\n",
       "           -1.1541e+00, -2.3736e-01],\n",
       "          ...,\n",
       "          [-4.3695e+00,  7.2788e-01,  2.1250e+00,  ...,  9.7592e-01,\n",
       "           -1.0099e+00,  2.3568e+00],\n",
       "          [-2.2493e+00,  9.3108e-01, -1.5006e+00,  ..., -4.2212e-01,\n",
       "           -1.3050e+00, -7.3675e-01],\n",
       "          [ 5.0108e-01, -5.1115e-01,  4.2955e-02,  ...,  2.5678e-03,\n",
       "            4.2361e-01, -6.0713e-01]],\n",
       "\n",
       "         [[ 6.6409e-01, -9.1712e-01,  2.3867e+00,  ...,  3.1376e-01,\n",
       "           -8.8142e-01,  3.3367e-01],\n",
       "          [ 2.5601e-01, -6.7366e-01,  1.4919e+00,  ...,  6.2022e-01,\n",
       "           -9.7944e-01, -1.7649e+00],\n",
       "          [-2.4767e+00,  7.3377e-01, -6.3627e-01,  ...,  9.3956e-01,\n",
       "           -7.3747e-02, -1.8454e-01],\n",
       "          ...,\n",
       "          [-6.9043e-01, -1.3883e+00, -3.4392e-01,  ...,  1.0469e+00,\n",
       "            1.1206e+00, -2.8069e+00],\n",
       "          [-4.0219e-01,  4.9928e-01, -1.7030e+00,  ...,  1.3272e+00,\n",
       "            1.5181e+00,  5.2738e-01],\n",
       "          [ 3.4193e-01,  2.0882e-01,  1.3184e-02,  ..., -4.1356e-01,\n",
       "           -1.7424e-01,  6.5031e-03]],\n",
       "\n",
       "         [[-1.0623e-01, -9.9311e-01,  1.7862e+00,  ..., -5.7082e-01,\n",
       "           -9.4409e-01, -1.6132e+00],\n",
       "          [ 5.0000e-01, -1.4144e+00, -5.6452e-01,  ...,  6.9027e-01,\n",
       "            9.2675e-01,  2.3122e+00],\n",
       "          [ 9.3875e-01, -7.0016e-03,  6.4196e-01,  ...,  8.8649e-01,\n",
       "            8.9359e-01, -7.3013e-01],\n",
       "          ...,\n",
       "          [-2.4622e-01,  3.1338e+00, -3.4835e+00,  ...,  3.4808e-01,\n",
       "            1.4426e+00, -7.1702e-01],\n",
       "          [-1.2729e+00,  1.7179e-01, -4.7079e+00,  ..., -6.1813e-01,\n",
       "            7.0764e-01,  3.5588e-01],\n",
       "          [-1.7730e-01,  2.5950e-01,  1.3053e-01,  ...,  2.8462e-01,\n",
       "            2.9461e-03,  4.5798e-01]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 4.9352e-01,  2.0587e+00,  1.6616e+00,  ...,  5.3020e+00,\n",
       "           -7.1276e+00,  7.1032e+00],\n",
       "          [-6.2724e-01, -4.1747e-01,  3.5569e+00,  ...,  1.3315e+00,\n",
       "           -3.3286e+00,  4.3520e+00],\n",
       "          [-5.0418e-01, -6.1903e-01,  3.6259e-01,  ...,  3.1436e-01,\n",
       "           -4.9426e+00,  2.4318e+00],\n",
       "          ...,\n",
       "          [-3.1586e-01,  2.7660e+00,  1.4572e+00,  ..., -3.3511e+00,\n",
       "            2.8101e+00,  1.6009e-01],\n",
       "          [ 2.7484e-02,  2.6147e+00,  4.7341e+00,  ..., -2.5899e+00,\n",
       "            5.4599e+00,  9.7889e-01],\n",
       "          [-3.0822e-01,  2.2260e-01, -2.0963e-02,  ..., -1.0740e-01,\n",
       "            1.1253e-01, -4.6473e-02]],\n",
       "\n",
       "         [[ 2.0578e+00, -3.2179e+00, -3.1530e-01,  ..., -7.7820e-01,\n",
       "           -1.1561e-01, -4.2334e-01],\n",
       "          [-1.5141e+00, -7.1618e+00, -3.6859e+00,  ..., -1.5351e+00,\n",
       "           -6.3910e-01, -1.3563e+00],\n",
       "          [ 3.9110e+00, -1.4850e+00, -7.9682e+00,  ..., -2.4541e+00,\n",
       "            4.0342e+00, -2.6689e-01],\n",
       "          ...,\n",
       "          [-3.3108e-01,  6.7663e-02, -4.5960e+00,  ..., -1.6793e+00,\n",
       "            1.0797e+00,  2.1889e+00],\n",
       "          [ 1.2980e+00,  6.0367e-01, -1.9370e+00,  ..., -4.7263e+00,\n",
       "            2.7639e+00,  2.9860e+00],\n",
       "          [-1.2999e-02, -7.8713e-02, -1.9245e-01,  ...,  1.8736e-01,\n",
       "            1.9011e-01, -4.0716e-02]],\n",
       "\n",
       "         [[-4.2907e-01,  2.4874e+00, -2.7197e-01,  ..., -1.7263e-02,\n",
       "            1.1678e+00,  7.0642e-01],\n",
       "          [ 1.7326e+00,  2.6794e+00,  3.1791e+00,  ..., -2.2589e+00,\n",
       "            2.3529e+00, -4.4263e+00],\n",
       "          [-5.1986e-01, -2.4125e+00, -2.8054e+00,  ...,  1.3245e-01,\n",
       "            2.3188e+00, -1.1414e+00],\n",
       "          ...,\n",
       "          [ 3.8280e+00, -5.2900e+00,  1.7166e-02,  ..., -1.0197e+00,\n",
       "            2.4547e+00, -3.6088e+00],\n",
       "          [ 1.5333e-01,  2.6702e+00, -3.8614e+00,  ..., -7.9792e-01,\n",
       "            3.5669e+00,  6.7452e-01],\n",
       "          [ 1.5089e-01,  6.1110e-02, -1.4058e-01,  ...,  1.8912e-01,\n",
       "            2.6451e-01,  6.6746e-02]],\n",
       "\n",
       "         [[-1.5275e-01,  1.7810e+00, -5.7367e+00,  ..., -3.2269e+00,\n",
       "           -2.5499e+00, -3.5800e+00],\n",
       "          [-6.3368e+00,  2.4734e+00,  3.3442e-01,  ..., -7.7767e-01,\n",
       "           -2.6199e+00,  4.0217e-01],\n",
       "          [ 5.1940e-01,  2.6464e-01, -5.3139e-01,  ..., -1.0452e+00,\n",
       "            2.8585e-01, -2.3510e+00],\n",
       "          ...,\n",
       "          [-4.5476e+00, -3.8213e+00, -2.0033e+00,  ..., -2.2876e-01,\n",
       "           -1.0884e+00, -4.6975e+00],\n",
       "          [ 1.8513e+00,  8.8439e-02,  2.9927e+00,  ..., -3.1598e+00,\n",
       "            3.8689e+00, -2.4070e+00],\n",
       "          [ 3.3911e-02,  5.7071e-02, -4.7287e-02,  ...,  3.8998e-02,\n",
       "           -7.0945e-02,  9.6557e-02]],\n",
       "\n",
       "         [[ 2.6973e+00,  4.2144e+00, -4.5772e+00,  ..., -1.8186e+00,\n",
       "            5.7712e+00, -4.7231e+00],\n",
       "          [ 1.2792e+00, -1.5406e+00,  2.9089e+00,  ...,  3.5177e+00,\n",
       "            1.5372e+00,  6.3412e-01],\n",
       "          [ 4.7602e+00,  2.4056e+00, -3.3956e+00,  ...,  1.1871e+01,\n",
       "            2.0213e+00,  5.0895e+00],\n",
       "          ...,\n",
       "          [-3.1679e+00,  5.3015e+00,  1.1574e+00,  ...,  4.0195e+00,\n",
       "           -3.9889e-01, -2.0697e+00],\n",
       "          [ 3.8379e+00, -4.6350e+00,  2.6696e+00,  ..., -6.7029e-01,\n",
       "            7.2004e-01, -4.7416e-01],\n",
       "          [-2.6882e-01,  8.4170e-02, -1.4034e-03,  ..., -2.4118e-01,\n",
       "            1.3221e-01, -8.8158e-04]],\n",
       "\n",
       "         [[ 2.9010e+00, -1.7743e-01,  1.5830e+00,  ..., -1.2664e+00,\n",
       "            4.8462e-01, -3.4128e+00],\n",
       "          [ 2.4120e+00,  9.2626e-01, -1.5251e+00,  ..., -2.0961e+00,\n",
       "           -3.7786e+00,  2.9748e+00],\n",
       "          [ 3.0572e+00,  9.1674e-01, -1.4485e+00,  ..., -6.3234e-01,\n",
       "           -6.3037e-01,  1.5100e+00],\n",
       "          ...,\n",
       "          [-1.7742e+00,  1.5785e+00,  2.1434e+00,  ...,  1.5657e+00,\n",
       "           -3.5612e+00,  4.5007e+00],\n",
       "          [-3.3389e-01, -6.8149e-01, -4.2648e-01,  ..., -3.1313e+00,\n",
       "            1.5406e+00,  1.7994e+00],\n",
       "          [ 1.7233e-01,  2.3199e-01, -2.2902e-01,  ..., -5.0274e-02,\n",
       "            1.1729e-01,  4.6737e-03]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 6.9381e-02, -1.4549e-01,  4.9405e-01, -4.4339e-01,  2.4676e-02,\n",
       "            4.2513e-01,  5.5794e-01, -1.8035e-01,  3.8550e-01, -4.4362e-01,\n",
       "           -1.4022e-01,  2.0258e-01, -1.5706e-01, -5.6682e-01, -2.1266e-01,\n",
       "            5.6127e-01, -3.2382e-01,  3.0692e-01, -3.6418e-01,  1.0415e-01,\n",
       "            4.8077e-01,  2.8189e-03,  1.0265e-01,  3.8358e-01,  1.6433e-01,\n",
       "            3.5233e-01,  4.5923e-02,  1.1674e-01,  4.8509e-01, -6.3776e-01,\n",
       "           -5.5900e-01,  9.0554e-01, -1.2211e-01, -8.2202e-01, -1.0044e-02,\n",
       "            5.5859e-02,  6.3433e-01, -2.8613e-01,  4.1547e-02, -2.2468e-03,\n",
       "            3.5056e-01,  8.2785e-01, -2.5422e-01,  3.4569e-01,  4.0111e-02,\n",
       "           -4.3724e-01, -4.4890e-01,  5.6705e-01,  5.9927e-02, -5.7079e-01,\n",
       "           -5.8361e-01,  6.1542e-01, -6.2187e-01,  1.1707e-01, -3.2399e-01,\n",
       "           -3.2268e-01,  4.9810e-01, -5.2620e-01, -1.1792e-01,  2.0705e-01,\n",
       "            7.5483e-01, -4.0198e-01, -3.8928e-01, -3.6280e-02],\n",
       "          [ 6.9381e-02, -1.4549e-01,  4.9405e-01, -4.4339e-01,  2.4676e-02,\n",
       "            4.2513e-01,  5.5794e-01, -1.8035e-01,  3.8550e-01, -4.4362e-01,\n",
       "           -1.4022e-01,  2.0258e-01, -1.5706e-01, -5.6682e-01, -2.1266e-01,\n",
       "            5.6127e-01, -3.2382e-01,  3.0692e-01, -3.6417e-01,  1.0415e-01,\n",
       "            4.8077e-01,  2.8189e-03,  1.0265e-01,  3.8358e-01,  1.6433e-01,\n",
       "            3.5233e-01,  4.5923e-02,  1.1674e-01,  4.8509e-01, -6.3776e-01,\n",
       "           -5.5900e-01,  9.0554e-01, -1.2211e-01, -8.2202e-01, -1.0044e-02,\n",
       "            5.5859e-02,  6.3433e-01, -2.8613e-01,  4.1547e-02, -2.2468e-03,\n",
       "            3.5056e-01,  8.2785e-01, -2.5422e-01,  3.4569e-01,  4.0111e-02,\n",
       "           -4.3724e-01, -4.4890e-01,  5.6705e-01,  5.9927e-02, -5.7079e-01,\n",
       "           -5.8361e-01,  6.1542e-01, -6.2187e-01,  1.1707e-01, -3.2399e-01,\n",
       "           -3.2268e-01,  4.9810e-01, -5.2620e-01, -1.1792e-01,  2.0705e-01,\n",
       "            7.5483e-01, -4.0198e-01, -3.8928e-01, -3.6280e-02]],\n",
       "\n",
       "         [[ 2.8497e-01, -4.8970e-01, -2.7715e-01, -6.0292e-02, -1.9455e-01,\n",
       "           -6.9252e-02,  1.4084e-01,  5.2790e-01, -1.7421e-01,  4.1347e-01,\n",
       "           -1.8980e-02, -4.5403e-02, -2.3165e-01, -4.9696e-02, -5.4623e-02,\n",
       "           -4.8552e-01,  6.4621e-01, -8.5241e-01, -3.5686e-01,  2.2615e-04,\n",
       "            2.9975e-01, -3.6354e-01,  2.2492e-01, -4.6588e-01, -6.6774e-01,\n",
       "            6.9653e-02, -8.7969e-02, -6.8707e-02,  4.8922e-01,  7.5063e-02,\n",
       "           -3.6535e-01,  2.0555e-01, -7.3909e-01, -3.0754e-01,  3.4051e-01,\n",
       "            1.4912e-01,  8.0127e-01, -5.4595e-01, -8.7659e-01,  1.5304e-01,\n",
       "           -1.9631e-01,  3.5925e-02,  7.1978e-01,  6.1192e-01,  1.1845e+00,\n",
       "           -4.6925e-01, -7.5022e-01, -1.0849e-01,  7.2876e-01, -4.2462e-01,\n",
       "           -1.6614e-02, -3.9244e-01, -4.4673e-01,  5.6227e-01,  3.7630e-01,\n",
       "            2.6157e-01,  2.1103e-01,  4.4734e-01, -6.2323e-01, -8.6361e-01,\n",
       "           -4.5842e-01,  6.2051e-01, -6.5310e-01,  2.4395e-01],\n",
       "          [ 2.8497e-01, -4.8970e-01, -2.7715e-01, -6.0292e-02, -1.9455e-01,\n",
       "           -6.9252e-02,  1.4084e-01,  5.2790e-01, -1.7421e-01,  4.1347e-01,\n",
       "           -1.8980e-02, -4.5403e-02, -2.3165e-01, -4.9696e-02, -5.4623e-02,\n",
       "           -4.8552e-01,  6.4621e-01, -8.5241e-01, -3.5686e-01,  2.2614e-04,\n",
       "            2.9975e-01, -3.6354e-01,  2.2492e-01, -4.6588e-01, -6.6774e-01,\n",
       "            6.9653e-02, -8.7969e-02, -6.8707e-02,  4.8922e-01,  7.5063e-02,\n",
       "           -3.6535e-01,  2.0555e-01, -7.3909e-01, -3.0754e-01,  3.4051e-01,\n",
       "            1.4912e-01,  8.0127e-01, -5.4595e-01, -8.7659e-01,  1.5304e-01,\n",
       "           -1.9631e-01,  3.5925e-02,  7.1978e-01,  6.1192e-01,  1.1845e+00,\n",
       "           -4.6925e-01, -7.5022e-01, -1.0849e-01,  7.2876e-01, -4.2462e-01,\n",
       "           -1.6614e-02, -3.9244e-01, -4.4673e-01,  5.6227e-01,  3.7630e-01,\n",
       "            2.6157e-01,  2.1103e-01,  4.4734e-01, -6.2323e-01, -8.6361e-01,\n",
       "           -4.5842e-01,  6.2051e-01, -6.5310e-01,  2.4395e-01]],\n",
       "\n",
       "         [[ 5.0465e-01, -3.1787e-01, -5.8153e-01,  6.6909e-01,  4.5987e-01,\n",
       "            3.0770e-01, -3.4484e-02, -2.8152e-01,  4.9926e-01, -4.5077e-01,\n",
       "           -8.4727e-01, -8.6578e-01, -4.8067e-01, -6.4998e-01,  1.8966e-01,\n",
       "           -3.3052e-01,  2.3703e-01,  3.1142e-01,  2.9904e-01, -6.0814e-01,\n",
       "            3.5393e-01,  6.5547e-01, -2.5115e-03, -4.1846e-01, -8.5009e-01,\n",
       "            6.0229e-01, -1.4068e-01, -1.1183e-01, -8.5864e-02, -7.3239e-01,\n",
       "           -8.2382e-01,  7.3088e-01,  8.0173e-01,  4.0708e-01, -7.1165e-01,\n",
       "           -6.2687e-01, -3.8406e-01,  3.5792e-01,  6.0925e-01,  5.7504e-01,\n",
       "            3.8765e-01,  5.4251e-01, -4.5222e-01, -2.1472e-01,  4.2766e-01,\n",
       "            1.9423e-01,  1.4290e-01, -3.1659e-01, -1.1269e+00,  3.1271e-01,\n",
       "            1.8358e-01, -7.8918e-01,  7.3247e-01, -7.5832e-01, -2.6377e-01,\n",
       "            2.6509e-01, -9.0026e-01,  5.0333e-01,  2.3387e-01, -6.2211e-01,\n",
       "            3.0987e-01, -7.3514e-03, -1.6400e-01,  3.7791e-01],\n",
       "          [ 5.0465e-01, -3.1787e-01, -5.8153e-01,  6.6909e-01,  4.5987e-01,\n",
       "            3.0770e-01, -3.4484e-02, -2.8152e-01,  4.9927e-01, -4.5077e-01,\n",
       "           -8.4727e-01, -8.6578e-01, -4.8067e-01, -6.4998e-01,  1.8966e-01,\n",
       "           -3.3052e-01,  2.3703e-01,  3.1142e-01,  2.9904e-01, -6.0814e-01,\n",
       "            3.5393e-01,  6.5547e-01, -2.5115e-03, -4.1846e-01, -8.5009e-01,\n",
       "            6.0229e-01, -1.4068e-01, -1.1183e-01, -8.5864e-02, -7.3239e-01,\n",
       "           -8.2382e-01,  7.3088e-01,  8.0173e-01,  4.0708e-01, -7.1165e-01,\n",
       "           -6.2687e-01, -3.8406e-01,  3.5792e-01,  6.0925e-01,  5.7504e-01,\n",
       "            3.8765e-01,  5.4251e-01, -4.5222e-01, -2.1472e-01,  4.2766e-01,\n",
       "            1.9423e-01,  1.4290e-01, -3.1659e-01, -1.1269e+00,  3.1271e-01,\n",
       "            1.8358e-01, -7.8918e-01,  7.3247e-01, -7.5832e-01, -2.6377e-01,\n",
       "            2.6509e-01, -9.0026e-01,  5.0333e-01,  2.3387e-01, -6.2211e-01,\n",
       "            3.0987e-01, -7.3514e-03, -1.6400e-01,  3.7791e-01]],\n",
       "\n",
       "         [[ 2.3938e-01, -1.9720e-03, -5.3003e-01,  4.9172e-01, -1.8720e-02,\n",
       "            3.7021e-01, -2.7423e-01,  1.9137e-01,  8.9431e-01,  4.7676e-01,\n",
       "            1.0648e-01,  1.1823e-01,  5.0962e-01,  9.0258e-02, -6.4203e-01,\n",
       "           -2.7346e-01,  2.5105e-01, -6.3623e-01,  7.2388e-01, -5.1708e-01,\n",
       "           -1.4693e-01,  1.2161e+00, -4.6663e-01,  3.7742e-01,  3.6296e-01,\n",
       "           -6.6657e-01,  2.8659e-01,  4.5645e-01, -6.0530e-01,  1.0295e+00,\n",
       "           -1.2138e+00,  7.4406e-01,  7.1727e-02,  1.1466e-01, -7.4360e-01,\n",
       "           -5.2329e-01,  6.3594e-01, -7.7115e-02,  1.0830e-01,  5.5476e-01,\n",
       "           -6.6378e-01, -5.1149e-01, -4.5956e-01, -5.4532e-01,  1.7890e-01,\n",
       "           -5.1254e-01, -3.0380e-01,  6.2724e-01,  9.4765e-01, -4.3601e-01,\n",
       "           -7.3700e-01, -1.2259e-01,  5.1679e-01,  5.6781e-01, -3.6912e-01,\n",
       "           -3.6343e-02,  7.9001e-01, -9.6671e-01,  6.7687e-01,  3.3707e-01,\n",
       "            6.3940e-01,  4.3161e-01,  6.8515e-01, -1.2668e+00],\n",
       "          [ 2.3938e-01, -1.9719e-03, -5.3003e-01,  4.9172e-01, -1.8720e-02,\n",
       "            3.7021e-01, -2.7423e-01,  1.9137e-01,  8.9431e-01,  4.7676e-01,\n",
       "            1.0648e-01,  1.1823e-01,  5.0962e-01,  9.0258e-02, -6.4203e-01,\n",
       "           -2.7346e-01,  2.5105e-01, -6.3623e-01,  7.2388e-01, -5.1708e-01,\n",
       "           -1.4693e-01,  1.2161e+00, -4.6663e-01,  3.7742e-01,  3.6296e-01,\n",
       "           -6.6656e-01,  2.8659e-01,  4.5645e-01, -6.0530e-01,  1.0295e+00,\n",
       "           -1.2138e+00,  7.4406e-01,  7.1727e-02,  1.1466e-01, -7.4360e-01,\n",
       "           -5.2329e-01,  6.3594e-01, -7.7115e-02,  1.0830e-01,  5.5476e-01,\n",
       "           -6.6378e-01, -5.1149e-01, -4.5956e-01, -5.4532e-01,  1.7890e-01,\n",
       "           -5.1254e-01, -3.0380e-01,  6.2724e-01,  9.4765e-01, -4.3601e-01,\n",
       "           -7.3700e-01, -1.2259e-01,  5.1679e-01,  5.6781e-01, -3.6912e-01,\n",
       "           -3.6343e-02,  7.9001e-01, -9.6671e-01,  6.7687e-01,  3.3707e-01,\n",
       "            6.3940e-01,  4.3161e-01,  6.8515e-01, -1.2668e+00]],\n",
       "\n",
       "         [[-1.5216e-02,  1.5825e-01,  1.3389e-01,  9.5692e-02,  5.1967e-01,\n",
       "            4.9032e-01, -6.8489e-01, -4.3660e-01,  1.5929e-01,  2.5091e-01,\n",
       "           -3.4126e-02, -2.9602e-01, -3.1434e-01, -4.7798e-01,  6.2461e-01,\n",
       "            6.2754e-01, -1.9484e-01,  5.7412e-02,  1.6090e-01,  2.6850e-01,\n",
       "            3.8484e-01, -2.8803e-01, -6.4913e-01, -2.4570e-02, -5.0367e-01,\n",
       "            1.4529e-01, -6.1190e-01, -8.2880e-02, -2.6584e-01,  7.7683e-01,\n",
       "           -9.0408e-01,  1.0736e-01, -6.4018e-01, -1.7172e-02, -4.6376e-01,\n",
       "            3.7844e-01,  1.7394e-01,  8.3312e-01, -8.9943e-01,  4.0108e-01,\n",
       "            2.7304e-01,  3.4567e-01, -2.3543e-01, -3.7528e-01, -5.0268e-01,\n",
       "            3.3975e-01, -8.6519e-02, -3.2062e-01,  2.9217e-01, -1.8579e-01,\n",
       "            2.0885e-01,  9.2326e-02,  2.1810e-01,  1.8558e-02,  1.8066e-01,\n",
       "           -8.0757e-04,  1.0935e+00, -5.3392e-01,  4.4794e-01,  8.9880e-02,\n",
       "            1.6996e-01, -2.6244e-01,  6.3237e-01, -4.6840e-01],\n",
       "          [-1.5216e-02,  1.5825e-01,  1.3389e-01,  9.5692e-02,  5.1967e-01,\n",
       "            4.9032e-01, -6.8489e-01, -4.3660e-01,  1.5929e-01,  2.5091e-01,\n",
       "           -3.4126e-02, -2.9602e-01, -3.1434e-01, -4.7798e-01,  6.2461e-01,\n",
       "            6.2754e-01, -1.9484e-01,  5.7412e-02,  1.6090e-01,  2.6850e-01,\n",
       "            3.8484e-01, -2.8803e-01, -6.4913e-01, -2.4570e-02, -5.0367e-01,\n",
       "            1.4529e-01, -6.1190e-01, -8.2880e-02, -2.6584e-01,  7.7683e-01,\n",
       "           -9.0408e-01,  1.0736e-01, -6.4018e-01, -1.7172e-02, -4.6376e-01,\n",
       "            3.7844e-01,  1.7394e-01,  8.3312e-01, -8.9943e-01,  4.0108e-01,\n",
       "            2.7304e-01,  3.4567e-01, -2.3543e-01, -3.7528e-01, -5.0268e-01,\n",
       "            3.3975e-01, -8.6519e-02, -3.2062e-01,  2.9217e-01, -1.8579e-01,\n",
       "            2.0885e-01,  9.2326e-02,  2.1810e-01,  1.8558e-02,  1.8066e-01,\n",
       "           -8.0755e-04,  1.0935e+00, -5.3392e-01,  4.4794e-01,  8.9880e-02,\n",
       "            1.6996e-01, -2.6244e-01,  6.3237e-01, -4.6840e-01]],\n",
       "\n",
       "         [[ 8.2765e-01, -8.5783e-01, -1.4857e-02,  2.8619e-01,  6.1426e-01,\n",
       "            3.9414e-01,  2.2814e-01,  1.3522e-01,  1.9907e-01, -1.6345e-01,\n",
       "           -5.2363e-01,  5.6879e-01, -2.1391e-01, -1.9789e-01, -2.2054e-02,\n",
       "           -6.4758e-02,  6.6862e-01,  4.8904e-01, -6.0723e-02,  6.3399e-01,\n",
       "            4.7669e-01, -7.4289e-02, -6.3348e-01,  5.9624e-01,  9.2964e-03,\n",
       "            7.9870e-02, -6.0736e-01, -1.3358e+00, -4.6497e-01, -2.2231e-01,\n",
       "            4.1878e-02,  3.8245e-01, -1.3870e+00,  4.9138e-01,  1.9591e-01,\n",
       "           -1.0227e+00,  2.7135e-01,  3.4018e-01,  1.1163e+00, -1.9123e-01,\n",
       "           -6.3066e-01, -7.4266e-01,  2.9226e-03, -7.2740e-01,  6.6856e-02,\n",
       "           -1.0799e+00,  4.8122e-01, -1.3432e-01,  5.9406e-01, -2.4281e-02,\n",
       "           -3.0243e-01, -5.1630e-01,  1.0952e-01, -1.7146e-01, -2.5340e-01,\n",
       "           -6.1144e-02,  7.4223e-01, -3.9683e-02, -3.7471e-01, -3.8431e-01,\n",
       "           -2.7885e-01, -5.7280e-01, -5.9195e-01,  4.0487e-01],\n",
       "          [ 8.2765e-01, -8.5783e-01, -1.4857e-02,  2.8619e-01,  6.1426e-01,\n",
       "            3.9414e-01,  2.2814e-01,  1.3522e-01,  1.9907e-01, -1.6345e-01,\n",
       "           -5.2363e-01,  5.6879e-01, -2.1391e-01, -1.9789e-01, -2.2054e-02,\n",
       "           -6.4758e-02,  6.6862e-01,  4.8904e-01, -6.0723e-02,  6.3399e-01,\n",
       "            4.7669e-01, -7.4289e-02, -6.3348e-01,  5.9624e-01,  9.2964e-03,\n",
       "            7.9870e-02, -6.0736e-01, -1.3358e+00, -4.6497e-01, -2.2231e-01,\n",
       "            4.1878e-02,  3.8245e-01, -1.3870e+00,  4.9138e-01,  1.9591e-01,\n",
       "           -1.0227e+00,  2.7135e-01,  3.4018e-01,  1.1163e+00, -1.9123e-01,\n",
       "           -6.3066e-01, -7.4266e-01,  2.9226e-03, -7.2740e-01,  6.6856e-02,\n",
       "           -1.0799e+00,  4.8122e-01, -1.3432e-01,  5.9406e-01, -2.4281e-02,\n",
       "           -3.0243e-01, -5.1630e-01,  1.0952e-01, -1.7146e-01, -2.5340e-01,\n",
       "           -6.1144e-02,  7.4223e-01, -3.9683e-02, -3.7471e-01, -3.8431e-01,\n",
       "           -2.7885e-01, -5.7280e-01, -5.9195e-01,  4.0487e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.6623e+00,  1.6551e-01, -6.0316e-02,  5.7015e-01, -1.3684e-01,\n",
       "            1.3859e-01, -1.8144e-01, -3.0138e-01, -1.3813e-02,  5.4601e-01,\n",
       "            6.1139e-02,  2.4445e-01,  5.5266e-01, -2.4576e-01, -3.1328e-01,\n",
       "            3.2860e-01,  6.2635e-01, -3.0594e-01, -4.8759e-01,  3.9018e-01,\n",
       "           -3.8838e-01, -5.5916e-01,  5.7777e-01, -1.6247e+00, -4.8809e-01,\n",
       "            4.6043e-01,  1.0971e+00, -1.2359e+00, -1.5989e+00, -3.4115e-01,\n",
       "            1.3695e-01, -3.0999e-01, -5.9995e-01,  1.8016e-01, -1.1431e+00,\n",
       "           -2.9963e-01,  4.2535e-01,  2.0304e+00, -6.5702e-01, -5.9728e-01,\n",
       "           -3.5286e-01, -8.6667e-01, -1.4148e-01,  3.2194e-01, -1.7546e-01,\n",
       "            9.0656e-01, -3.1602e-01, -1.0934e+00, -3.2227e-01,  2.2999e-01,\n",
       "            1.1630e+00,  4.1298e-02,  2.9607e-01, -1.5138e-01,  2.5918e-01,\n",
       "           -7.6401e-01, -3.7969e-02, -6.1767e-01,  1.5619e-01,  9.1986e-01,\n",
       "           -6.9516e-01, -3.5806e-01, -3.7644e-02,  3.5207e-01],\n",
       "          [ 1.6623e+00,  1.6551e-01, -6.0316e-02,  5.7015e-01, -1.3684e-01,\n",
       "            1.3859e-01, -1.8144e-01, -3.0138e-01, -1.3813e-02,  5.4601e-01,\n",
       "            6.1139e-02,  2.4445e-01,  5.5266e-01, -2.4576e-01, -3.1328e-01,\n",
       "            3.2860e-01,  6.2635e-01, -3.0594e-01, -4.8759e-01,  3.9018e-01,\n",
       "           -3.8838e-01, -5.5916e-01,  5.7777e-01, -1.6247e+00, -4.8809e-01,\n",
       "            4.6043e-01,  1.0971e+00, -1.2359e+00, -1.5989e+00, -3.4115e-01,\n",
       "            1.3695e-01, -3.0999e-01, -5.9995e-01,  1.8016e-01, -1.1431e+00,\n",
       "           -2.9963e-01,  4.2535e-01,  2.0304e+00, -6.5702e-01, -5.9728e-01,\n",
       "           -3.5286e-01, -8.6667e-01, -1.4148e-01,  3.2194e-01, -1.7546e-01,\n",
       "            9.0656e-01, -3.1602e-01, -1.0934e+00, -3.2227e-01,  2.2999e-01,\n",
       "            1.1630e+00,  4.1299e-02,  2.9607e-01, -1.5138e-01,  2.5918e-01,\n",
       "           -7.6401e-01, -3.7969e-02, -6.1767e-01,  1.5619e-01,  9.1986e-01,\n",
       "           -6.9516e-01, -3.5806e-01, -3.7644e-02,  3.5207e-01]],\n",
       "\n",
       "         [[-4.2934e-01,  9.3980e-02, -5.5753e-02, -2.4523e-01,  4.3089e-01,\n",
       "            5.5249e-01, -1.1742e-01, -3.9175e-01,  1.1465e-01,  8.3798e-01,\n",
       "           -2.8558e-01,  3.0466e-01,  1.9854e-01,  1.0589e-01,  3.5609e-01,\n",
       "            9.8970e-03,  5.4817e-01,  2.5495e-01, -1.4285e-01, -8.7641e-01,\n",
       "            6.9271e-02,  1.2073e-01,  5.6405e-01, -1.4380e-01,  3.9311e-01,\n",
       "           -1.3767e+00,  9.0450e-04, -4.6542e-01,  5.7563e-01,  4.1038e-02,\n",
       "           -2.2213e-01, -4.6432e-01, -5.3102e-02,  1.5911e-02,  3.3253e-02,\n",
       "            3.9672e-01, -4.7513e-01, -2.3399e-01, -3.4010e-01,  5.9412e-02,\n",
       "           -8.4058e-01,  2.6305e-02,  9.6100e-02, -3.2768e-01,  6.8842e-01,\n",
       "           -5.1917e-01, -3.6966e-01,  5.4427e-02,  1.0207e+00,  4.4007e-01,\n",
       "           -4.1806e-01,  1.8340e-01,  1.9296e-01,  3.5339e-02,  4.5659e-01,\n",
       "           -1.9765e-01, -2.4482e-01,  4.8957e-01, -7.3010e-02, -3.8195e-02,\n",
       "            2.2387e-01,  9.4573e-02, -4.4304e-02, -5.1004e-02],\n",
       "          [-4.2934e-01,  9.3980e-02, -5.5752e-02, -2.4523e-01,  4.3089e-01,\n",
       "            5.5249e-01, -1.1742e-01, -3.9175e-01,  1.1465e-01,  8.3798e-01,\n",
       "           -2.8558e-01,  3.0466e-01,  1.9854e-01,  1.0589e-01,  3.5609e-01,\n",
       "            9.8969e-03,  5.4817e-01,  2.5495e-01, -1.4285e-01, -8.7641e-01,\n",
       "            6.9271e-02,  1.2073e-01,  5.6405e-01, -1.4380e-01,  3.9311e-01,\n",
       "           -1.3767e+00,  9.0465e-04, -4.6542e-01,  5.7563e-01,  4.1038e-02,\n",
       "           -2.2213e-01, -4.6432e-01, -5.3102e-02,  1.5911e-02,  3.3253e-02,\n",
       "            3.9672e-01, -4.7513e-01, -2.3399e-01, -3.4010e-01,  5.9412e-02,\n",
       "           -8.4058e-01,  2.6306e-02,  9.6100e-02, -3.2768e-01,  6.8842e-01,\n",
       "           -5.1917e-01, -3.6966e-01,  5.4427e-02,  1.0207e+00,  4.4007e-01,\n",
       "           -4.1806e-01,  1.8340e-01,  1.9296e-01,  3.5339e-02,  4.5659e-01,\n",
       "           -1.9765e-01, -2.4482e-01,  4.8957e-01, -7.3010e-02, -3.8195e-02,\n",
       "            2.2387e-01,  9.4573e-02, -4.4304e-02, -5.1004e-02]],\n",
       "\n",
       "         [[ 1.0838e+00, -3.2461e-02, -2.7988e-01, -4.8974e-01,  2.7203e-01,\n",
       "           -9.8594e-02, -8.7503e-01,  1.9826e-01,  2.9905e-02, -7.7956e-01,\n",
       "            3.4838e-01, -3.2412e-01,  3.1270e-01,  6.3249e-01,  1.0758e-01,\n",
       "            9.0514e-01, -1.0961e+00,  9.1043e-02, -4.1534e-01, -1.1940e+00,\n",
       "           -7.3946e-02,  4.8163e-01,  1.8730e-01, -9.5164e-01,  5.0521e-01,\n",
       "           -3.2029e-02,  1.6841e+00,  7.2960e-02,  2.4935e-02, -3.1128e-01,\n",
       "           -6.0785e-02,  3.5296e-01, -4.1657e-01, -4.4933e-02, -3.5834e-01,\n",
       "           -9.5359e-01, -4.0348e-01, -5.7013e-01,  1.6311e+00, -7.2666e-01,\n",
       "           -5.4866e-01, -1.0136e-01, -3.7753e-01,  2.6620e-01,  3.6616e-01,\n",
       "           -1.6992e-01,  1.1176e-01,  1.7296e-01,  7.7597e-01,  7.7345e-01,\n",
       "           -3.9700e-01,  2.0900e-01, -9.6667e-01, -1.0394e+00,  4.0872e-01,\n",
       "           -8.1275e-01,  4.6748e-01,  2.1477e-01,  1.1688e+00,  3.6269e-01,\n",
       "            6.8963e-01, -7.1293e-01,  1.0637e+00, -2.6915e-01],\n",
       "          [ 1.0838e+00, -3.2461e-02, -2.7988e-01, -4.8974e-01,  2.7203e-01,\n",
       "           -9.8594e-02, -8.7503e-01,  1.9826e-01,  2.9905e-02, -7.7956e-01,\n",
       "            3.4838e-01, -3.2412e-01,  3.1270e-01,  6.3249e-01,  1.0758e-01,\n",
       "            9.0514e-01, -1.0961e+00,  9.1043e-02, -4.1534e-01, -1.1940e+00,\n",
       "           -7.3946e-02,  4.8163e-01,  1.8730e-01, -9.5164e-01,  5.0521e-01,\n",
       "           -3.2029e-02,  1.6841e+00,  7.2959e-02,  2.4935e-02, -3.1128e-01,\n",
       "           -6.0785e-02,  3.5296e-01, -4.1657e-01, -4.4933e-02, -3.5834e-01,\n",
       "           -9.5359e-01, -4.0348e-01, -5.7013e-01,  1.6311e+00, -7.2666e-01,\n",
       "           -5.4866e-01, -1.0136e-01, -3.7753e-01,  2.6620e-01,  3.6616e-01,\n",
       "           -1.6992e-01,  1.1176e-01,  1.7296e-01,  7.7597e-01,  7.7345e-01,\n",
       "           -3.9700e-01,  2.0900e-01, -9.6667e-01, -1.0394e+00,  4.0872e-01,\n",
       "           -8.1275e-01,  4.6748e-01,  2.1477e-01,  1.1688e+00,  3.6269e-01,\n",
       "            6.8963e-01, -7.1293e-01,  1.0637e+00, -2.6915e-01]],\n",
       "\n",
       "         [[ 1.7138e+00,  1.6006e+00,  1.1987e-01, -8.0254e-02, -8.0006e-02,\n",
       "           -9.8460e-02,  4.2601e-01, -4.9508e-02, -3.4767e-02, -1.1221e+00,\n",
       "           -4.5201e-01,  7.8220e-01, -1.2477e+00, -4.0687e-01, -1.1874e+00,\n",
       "           -1.6081e-01, -3.3584e-01,  7.3563e-01, -9.0546e-02, -2.4625e-01,\n",
       "           -2.1845e+00,  8.4094e-01,  6.9325e-01,  7.6516e-01, -3.8355e-01,\n",
       "           -1.0254e-02, -3.3348e-01,  5.2099e-01, -1.2749e-01,  5.6894e-01,\n",
       "           -3.5402e-01,  7.6250e-01,  4.0187e-01,  9.5503e-01, -5.5642e-01,\n",
       "            3.6345e-01, -2.2395e-01, -1.3808e+00, -1.4866e-01, -6.4118e-01,\n",
       "           -8.8085e-01, -1.2988e+00, -1.0558e+00,  3.4942e-01,  9.9175e-02,\n",
       "           -2.5259e-01,  2.2007e-01, -6.9015e-02,  7.9705e-04,  3.6076e-01,\n",
       "           -9.0904e-01, -1.1997e+00,  1.1958e+00,  3.8560e-02,  3.5657e-01,\n",
       "           -5.0804e-01, -1.0604e+00,  1.1234e+00,  7.2190e-01, -5.4070e-01,\n",
       "            1.2388e-01,  8.7525e-01,  5.8562e-01,  4.6005e-02],\n",
       "          [ 1.7138e+00,  1.6006e+00,  1.1987e-01, -8.0254e-02, -8.0006e-02,\n",
       "           -9.8460e-02,  4.2601e-01, -4.9508e-02, -3.4767e-02, -1.1221e+00,\n",
       "           -4.5201e-01,  7.8220e-01, -1.2477e+00, -4.0687e-01, -1.1874e+00,\n",
       "           -1.6081e-01, -3.3584e-01,  7.3563e-01, -9.0546e-02, -2.4625e-01,\n",
       "           -2.1845e+00,  8.4094e-01,  6.9325e-01,  7.6516e-01, -3.8355e-01,\n",
       "           -1.0253e-02, -3.3348e-01,  5.2099e-01, -1.2749e-01,  5.6894e-01,\n",
       "           -3.5402e-01,  7.6250e-01,  4.0187e-01,  9.5503e-01, -5.5642e-01,\n",
       "            3.6345e-01, -2.2395e-01, -1.3808e+00, -1.4866e-01, -6.4118e-01,\n",
       "           -8.8085e-01, -1.2988e+00, -1.0558e+00,  3.4942e-01,  9.9176e-02,\n",
       "           -2.5259e-01,  2.2007e-01, -6.9015e-02,  7.9715e-04,  3.6076e-01,\n",
       "           -9.0904e-01, -1.1997e+00,  1.1958e+00,  3.8560e-02,  3.5657e-01,\n",
       "           -5.0804e-01, -1.0604e+00,  1.1234e+00,  7.2190e-01, -5.4071e-01,\n",
       "            1.2388e-01,  8.7525e-01,  5.8562e-01,  4.6005e-02]],\n",
       "\n",
       "         [[-1.2474e-01,  4.7932e-01,  3.5976e-01, -8.9282e-02,  3.5087e-01,\n",
       "            1.2262e+00,  2.0717e-01, -2.4910e-01, -8.4299e-01,  2.4736e-01,\n",
       "           -2.6684e-01, -1.7745e-01, -1.1015e-01,  2.3957e-01,  1.7766e-01,\n",
       "           -9.3271e-03,  4.0192e-01,  2.2469e-01, -5.8571e-01,  8.0875e-01,\n",
       "           -4.8735e-01, -7.8355e-01,  1.6337e-02, -2.6681e-01, -4.9544e-01,\n",
       "           -5.0509e-02, -5.3002e-01, -3.6628e-01,  1.4160e-01, -4.5734e-01,\n",
       "            3.1946e-01,  7.7903e-01,  9.9592e-01, -1.6182e-02, -1.8068e-01,\n",
       "            2.5032e-01,  1.3100e-01,  3.9560e-01,  1.4710e-01, -3.5760e-01,\n",
       "           -9.7906e-02,  7.2191e-01,  6.1801e-01, -5.4994e-01,  4.8897e-01,\n",
       "           -8.3905e-01, -2.2236e-01,  4.1501e-01, -9.0684e-01, -2.0856e-01,\n",
       "           -4.2028e-02,  6.5618e-01,  2.3637e-01,  4.0155e-01,  3.8532e-01,\n",
       "            3.4272e-01,  5.4832e-01, -2.5728e-01, -3.5090e-01, -5.8152e-02,\n",
       "           -8.5591e-01,  2.4841e-02,  3.6587e-01, -5.4656e-01],\n",
       "          [-1.2474e-01,  4.7932e-01,  3.5976e-01, -8.9282e-02,  3.5087e-01,\n",
       "            1.2262e+00,  2.0716e-01, -2.4910e-01, -8.4299e-01,  2.4736e-01,\n",
       "           -2.6684e-01, -1.7745e-01, -1.1015e-01,  2.3957e-01,  1.7766e-01,\n",
       "           -9.3268e-03,  4.0192e-01,  2.2469e-01, -5.8571e-01,  8.0875e-01,\n",
       "           -4.8735e-01, -7.8356e-01,  1.6337e-02, -2.6681e-01, -4.9544e-01,\n",
       "           -5.0509e-02, -5.3002e-01, -3.6628e-01,  1.4160e-01, -4.5734e-01,\n",
       "            3.1946e-01,  7.7903e-01,  9.9592e-01, -1.6183e-02, -1.8068e-01,\n",
       "            2.5032e-01,  1.3100e-01,  3.9560e-01,  1.4710e-01, -3.5760e-01,\n",
       "           -9.7905e-02,  7.2191e-01,  6.1801e-01, -5.4994e-01,  4.8897e-01,\n",
       "           -8.3905e-01, -2.2236e-01,  4.1501e-01, -9.0684e-01, -2.0856e-01,\n",
       "           -4.2028e-02,  6.5618e-01,  2.3637e-01,  4.0155e-01,  3.8532e-01,\n",
       "            3.4272e-01,  5.4831e-01, -2.5728e-01, -3.5090e-01, -5.8152e-02,\n",
       "           -8.5591e-01,  2.4841e-02,  3.6587e-01, -5.4656e-01]],\n",
       "\n",
       "         [[ 7.1011e-02,  9.0093e-01,  1.1544e+00,  7.7756e-01, -2.8012e+00,\n",
       "           -6.0646e-02, -7.5621e-01, -2.9081e-01,  1.5925e+00, -7.6446e-01,\n",
       "            8.2084e-01, -5.8797e-01,  5.4766e-01,  2.3568e+00,  2.3030e+00,\n",
       "           -3.0958e-02, -2.8292e-01, -1.3418e+00, -4.5040e-02, -7.0141e-01,\n",
       "           -1.0857e+00, -6.1864e-01,  7.8137e-01, -1.1031e+00,  4.4914e-01,\n",
       "           -1.9345e+00, -1.2060e-01, -6.0181e-01,  8.7516e-01,  1.4231e+00,\n",
       "            3.7562e-01,  1.1515e-01,  5.5787e-01, -7.2062e-02, -1.0238e+00,\n",
       "           -1.0784e+00,  5.5491e-01, -9.7844e-01,  9.1136e-01, -3.1671e-01,\n",
       "            9.0851e-01, -5.1611e-01,  1.1507e+00, -3.6915e-01,  1.0989e-01,\n",
       "            3.0879e-01, -4.8810e-01, -4.9414e-01,  3.0132e-01,  6.3896e-01,\n",
       "            1.6675e+00, -4.6570e-01, -2.0553e-02, -9.7408e-01,  5.7398e-02,\n",
       "            2.6750e-01,  4.0654e-01,  3.4523e-01,  4.4720e-01,  2.5188e+00,\n",
       "           -2.7987e-01, -4.2983e-01, -5.7387e-02,  3.1771e-01],\n",
       "          [ 7.1011e-02,  9.0093e-01,  1.1544e+00,  7.7756e-01, -2.8012e+00,\n",
       "           -6.0646e-02, -7.5621e-01, -2.9081e-01,  1.5925e+00, -7.6446e-01,\n",
       "            8.2084e-01, -5.8797e-01,  5.4766e-01,  2.3568e+00,  2.3030e+00,\n",
       "           -3.0958e-02, -2.8292e-01, -1.3418e+00, -4.5040e-02, -7.0141e-01,\n",
       "           -1.0857e+00, -6.1864e-01,  7.8137e-01, -1.1031e+00,  4.4914e-01,\n",
       "           -1.9345e+00, -1.2060e-01, -6.0181e-01,  8.7516e-01,  1.4231e+00,\n",
       "            3.7562e-01,  1.1515e-01,  5.5787e-01, -7.2062e-02, -1.0238e+00,\n",
       "           -1.0784e+00,  5.5491e-01, -9.7844e-01,  9.1136e-01, -3.1671e-01,\n",
       "            9.0851e-01, -5.1611e-01,  1.1507e+00, -3.6915e-01,  1.0989e-01,\n",
       "            3.0879e-01, -4.8810e-01, -4.9414e-01,  3.0132e-01,  6.3896e-01,\n",
       "            1.6675e+00, -4.6570e-01, -2.0553e-02, -9.7408e-01,  5.7398e-02,\n",
       "            2.6750e-01,  4.0654e-01,  3.4523e-01,  4.4720e-01,  2.5188e+00,\n",
       "           -2.7987e-01, -4.2983e-01, -5.7387e-02,  3.1771e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.3345e+00, -4.8331e+00,  9.7182e-01,  ..., -3.7579e+00,\n",
       "           -4.3025e-01, -8.8822e-01],\n",
       "          [-1.1194e+00, -2.5945e+00,  7.4098e-01,  ..., -1.0496e+00,\n",
       "           -8.3188e-01, -1.8740e-01],\n",
       "          [-7.8713e-01, -3.4360e+00,  1.0057e+00,  ..., -6.5331e-01,\n",
       "           -1.0131e+00, -7.0517e-01],\n",
       "          ...,\n",
       "          [-7.5773e-01, -2.3528e+00,  1.1811e+00,  ...,  9.8360e-01,\n",
       "           -5.0703e-01, -1.2026e+00],\n",
       "          [-2.1346e+00,  1.1973e+00, -2.3136e-01,  ...,  8.1450e-01,\n",
       "            1.9052e+00, -2.6870e-01],\n",
       "          [-1.5717e-02,  3.6424e-01, -1.4058e-02,  ...,  1.4148e-01,\n",
       "           -2.7401e-01, -1.9368e-01]],\n",
       "\n",
       "         [[-3.1530e+00,  3.5690e+00,  3.1824e-01,  ...,  1.4397e+00,\n",
       "            1.8173e-01, -5.9080e-01],\n",
       "          [-2.8472e+00, -8.5362e-01, -3.1841e+00,  ..., -3.5952e+00,\n",
       "           -2.1438e+00, -2.4377e+00],\n",
       "          [-1.9251e+00,  1.5137e+00, -1.7617e+00,  ..., -7.7397e-01,\n",
       "           -5.8958e-02, -3.7254e-01],\n",
       "          ...,\n",
       "          [-2.9778e-01,  1.2674e+00, -7.2922e-01,  ...,  1.8518e+00,\n",
       "           -3.0523e+00, -7.4879e-01],\n",
       "          [-2.9578e-01,  6.7052e-01, -1.5235e+00,  ...,  2.1524e+00,\n",
       "           -3.3704e+00,  1.4595e+00],\n",
       "          [ 3.7561e-01,  1.3795e-03,  3.5332e-02,  ..., -2.0788e-01,\n",
       "            7.9263e-02,  4.5341e-01]],\n",
       "\n",
       "         [[ 3.2271e+00,  5.5045e-01, -1.1189e+00,  ...,  1.1497e+00,\n",
       "            9.3490e-03, -1.2374e+00],\n",
       "          [-9.8801e-01, -3.4110e+00,  1.2923e+00,  ..., -2.9468e-01,\n",
       "           -6.3667e-01,  1.8947e+00],\n",
       "          [ 2.5423e+00, -1.6073e+00,  9.9601e-01,  ..., -1.0897e+00,\n",
       "           -1.3698e+00, -1.3356e+00],\n",
       "          ...,\n",
       "          [ 2.9914e+00,  3.4541e+00,  6.7186e-01,  ..., -3.0811e+00,\n",
       "            2.4223e+00, -5.3312e-01],\n",
       "          [-4.5014e-01,  5.3150e-01,  2.0323e+00,  ..., -2.5107e+00,\n",
       "            2.8039e+00,  1.3638e+00],\n",
       "          [-3.5847e-01, -2.4946e-01, -6.0540e-01,  ..., -1.9948e-01,\n",
       "           -4.9684e-01,  2.0310e-01]],\n",
       "\n",
       "         [[ 2.5734e+00, -1.6779e+00,  1.1523e-01,  ..., -1.4398e+00,\n",
       "           -2.8201e-01,  1.5006e+00],\n",
       "          [ 3.4296e+00, -8.2649e-01, -4.5205e-01,  ..., -1.2083e+00,\n",
       "            5.4362e-01, -1.1920e+00],\n",
       "          [ 4.0882e+00, -1.5514e+00, -4.2314e-01,  ..., -8.6249e-01,\n",
       "            5.9341e-01, -8.0554e-01],\n",
       "          ...,\n",
       "          [ 3.1570e+00,  1.8979e+00, -4.0323e+00,  ..., -1.1145e+00,\n",
       "            1.5611e+00, -2.8339e+00],\n",
       "          [ 1.2283e+00,  1.2802e+00, -3.0978e-01,  ..., -2.4340e-02,\n",
       "            1.8632e+00, -1.4734e+00],\n",
       "          [-6.2816e-01, -3.0176e-01,  3.2895e-01,  ..., -8.5712e-02,\n",
       "            3.5869e-01, -1.9373e-01]],\n",
       "\n",
       "         [[-1.4682e+00, -1.1678e+00, -7.8437e-01,  ...,  1.6869e+00,\n",
       "            1.7290e+00, -5.0081e-02],\n",
       "          [ 3.0204e+00,  1.3669e+00,  3.0047e+00,  ...,  2.6775e+00,\n",
       "           -1.6072e+00, -4.5363e+00],\n",
       "          [-2.3690e+00, -1.6089e+00, -9.9044e-01,  ...,  1.7882e+00,\n",
       "           -1.6522e+00, -1.0306e+00],\n",
       "          ...,\n",
       "          [-3.0084e+00, -2.7416e+00,  8.9473e-01,  ...,  2.6920e+00,\n",
       "            6.7068e-01, -1.9029e+00],\n",
       "          [ 4.5098e-01, -4.9577e-01,  6.9618e-01,  ..., -4.6722e-01,\n",
       "            1.5478e+00, -6.5871e-01],\n",
       "          [ 2.7916e-01, -1.4941e-01,  1.2088e-01,  ..., -2.0165e-01,\n",
       "           -6.1511e-01,  5.8365e-01]],\n",
       "\n",
       "         [[ 2.6757e+00, -3.7397e+00, -4.4235e-01,  ..., -9.7761e-01,\n",
       "           -5.0528e-01, -1.9963e+00],\n",
       "          [ 2.8628e+00, -9.1842e-01, -4.6423e-01,  ...,  3.5943e-01,\n",
       "           -1.5303e-01, -1.2509e+00],\n",
       "          [-1.0318e+00, -1.2676e+00,  1.0297e+00,  ..., -1.3858e+00,\n",
       "           -9.9496e-01,  2.4411e-01],\n",
       "          ...,\n",
       "          [ 1.1174e+00, -1.2997e+00,  6.6005e-01,  ...,  4.9035e-01,\n",
       "            2.6512e-02,  1.1748e+00],\n",
       "          [ 1.0610e+00, -1.2621e+00,  1.8968e-01,  ...,  1.2765e-01,\n",
       "           -3.8174e+00,  1.4018e+00],\n",
       "          [-9.5650e-01,  4.3708e-01,  7.0382e-02,  ...,  3.4484e-01,\n",
       "           -1.1547e-01, -4.5696e-01]]]], grad_fn=<TransposeBackward0>), tensor([[[[ -6.9137,   1.1892,   3.9392,  ...,   2.8873,   1.3163,   3.6480],\n",
       "          [ -2.0188,  -0.9774,   1.2174,  ...,  -5.2293,  -3.4435,  -3.9239],\n",
       "          [ -7.7974,  -5.8420,  -0.3480,  ...,  -3.1903,  -5.4483,  -6.9544],\n",
       "          ...,\n",
       "          [ -3.1013,   0.2096,  -3.8771,  ...,   5.8210,  -4.5308,  -5.5302],\n",
       "          [  0.2513,   1.6957,   4.8955,  ...,  -1.9539,   1.6749,  -5.2975],\n",
       "          [  0.1769,  -0.1503,  -0.0253,  ...,   0.0381,  -0.0189,   0.0902]],\n",
       "\n",
       "         [[  0.5702,  -1.9639,  -2.2712,  ...,   2.7908,  -0.6476,   0.9543],\n",
       "          [ -3.4321,   1.8086,  -4.2707,  ...,   2.6889,   0.3070,   0.9068],\n",
       "          [  0.6542,  -3.0087,  -5.1508,  ...,   3.8919,  -5.3171,   1.5386],\n",
       "          ...,\n",
       "          [ -0.4728,   3.3392,  -4.8950,  ...,  -2.9960,   0.7297,   4.1018],\n",
       "          [ -3.2765,  -0.0718,  -0.7163,  ...,  -3.9928,   1.1609,  -2.9695],\n",
       "          [  0.2053,   0.0825,  -0.0824,  ...,  -0.0236,  -0.1242,  -0.2326]],\n",
       "\n",
       "         [[  4.8912,   1.9393,   2.9691,  ...,   5.3297,  -2.8731,   2.5706],\n",
       "          [  3.0780,  -3.3507,  -1.4588,  ...,  -1.3084,   3.8958,  -0.7236],\n",
       "          [  4.7175,   5.7795,  -4.8951,  ...,  -0.5121,   0.8116,  -8.3401],\n",
       "          ...,\n",
       "          [  2.9596,  -3.2287,   5.5080,  ...,  -0.6275,  -6.4598,   2.6591],\n",
       "          [  4.2413,   0.9058,   3.0465,  ...,  -2.5144,   2.4016,   3.4094],\n",
       "          [  0.1636,  -0.1698,   0.0890,  ...,   0.0470,   0.1197,  -0.1100]],\n",
       "\n",
       "         [[ -2.7808,   3.1355,  -4.5955,  ...,  -5.3281,  -2.1473,  -0.6895],\n",
       "          [ -1.7136,  -2.1148,   2.9636,  ...,  -8.5961,  -0.8519,  -2.5321],\n",
       "          [ -2.6036,   0.6811,   0.1678,  ...,  -5.2892,  -0.2943,   0.0409],\n",
       "          ...,\n",
       "          [ -2.5631,   0.0983,   5.0525,  ...,  -9.5177,   0.4939,   0.3116],\n",
       "          [  0.6037,  -0.5975,   6.0308,  ...,  -5.1191,   0.5866,   0.9063],\n",
       "          [  0.3324,  -0.2798,   0.3062,  ...,   1.0964,   0.2135,  -0.1786]],\n",
       "\n",
       "         [[ -5.7189,   5.4837,  -8.4045,  ..., -12.0302,   0.3998,   2.6161],\n",
       "          [  0.2144,   1.2908,   3.7067,  ...,   2.6661,  -2.8643,   5.1026],\n",
       "          [  1.6327,  -4.0698,  -1.3361,  ...,   1.3413,  -1.2182,  -2.4166],\n",
       "          ...,\n",
       "          [ -5.4231,   5.9057,  -2.9380,  ...,  -4.0887,  -3.1711,  -0.8847],\n",
       "          [ -5.4310,  -4.0774,  -2.4560,  ...,   0.2347,  -1.4777,   1.7685],\n",
       "          [  0.0474,   0.0765,   0.1162,  ...,   0.0248,  -0.0396,   0.0445]],\n",
       "\n",
       "         [[ -3.4150,   1.9591,  -1.1069,  ...,  -0.7963,   0.6611,  -2.5273],\n",
       "          [  3.0636,   0.7467,  -1.3461,  ...,  -0.2640,  -3.6556,  -4.0913],\n",
       "          [  5.4386,  -2.3441,   3.3697,  ...,  -0.2931,   4.1380,  -4.8704],\n",
       "          ...,\n",
       "          [  2.8260,  -0.8341,   0.4304,  ...,   1.2044,  -1.3115,   1.2568],\n",
       "          [  1.8277,  -0.4957,  -1.5487,  ...,  -2.2910,   1.9807,   2.7168],\n",
       "          [  0.3214,  -0.1279,  -0.0951,  ...,   0.0147,  -0.6375,   0.2599]]]],\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.2074,  0.9024, -0.0154, -0.9205, -0.8335, -0.7702,  0.0249,\n",
       "            0.6210,  0.3799,  1.2085,  0.9560,  0.1242, -0.5272, -0.1711,\n",
       "            0.6356,  1.4963,  0.3311,  0.4090, -0.7425, -0.0117, -1.6705,\n",
       "            0.0188,  0.0434,  0.9547,  1.0580, -0.2652,  0.1425,  0.7851,\n",
       "            0.1522, -0.0539,  0.1151,  0.0969,  0.3227,  0.7336,  0.3060,\n",
       "           -0.5292,  0.5512, -1.2146, -0.5156,  0.6561, -1.7364, -0.1863,\n",
       "            0.5460, -0.4018,  0.4718,  0.1479, -0.3995, -0.3479, -0.8996,\n",
       "           -0.5201,  1.4871,  0.0700,  1.9322,  0.6795, -1.2617, -0.0252,\n",
       "           -0.6621, -1.1106,  0.3567, -0.1654, -0.3723,  0.2964, -0.5035,\n",
       "           -0.1875],\n",
       "          [-0.2074,  0.9024, -0.0154, -0.9205, -0.8335, -0.7702,  0.0249,\n",
       "            0.6210,  0.3799,  1.2085,  0.9560,  0.1242, -0.5272, -0.1711,\n",
       "            0.6356,  1.4963,  0.3311,  0.4090, -0.7425, -0.0117, -1.6705,\n",
       "            0.0188,  0.0434,  0.9547,  1.0580, -0.2652,  0.1425,  0.7851,\n",
       "            0.1522, -0.0539,  0.1151,  0.0969,  0.3227,  0.7336,  0.3060,\n",
       "           -0.5292,  0.5512, -1.2146, -0.5156,  0.6561, -1.7364, -0.1863,\n",
       "            0.5460, -0.4018,  0.4718,  0.1479, -0.3995, -0.3479, -0.8996,\n",
       "           -0.5201,  1.4871,  0.0700,  1.9322,  0.6795, -1.2617, -0.0252,\n",
       "           -0.6621, -1.1106,  0.3567, -0.1654, -0.3723,  0.2964, -0.5035,\n",
       "           -0.1875]],\n",
       "\n",
       "         [[-1.0713, -0.4383, -0.1200,  0.1459,  0.4925,  0.5630,  0.3973,\n",
       "           -0.0693, -0.2817, -0.6139, -0.5566,  0.5550,  0.3599,  0.0130,\n",
       "            0.7184,  0.7186,  0.0499, -0.6339, -1.6483,  0.8209,  0.1434,\n",
       "            0.4263, -0.5296,  0.5113, -1.4210,  0.6931, -0.1513, -0.0241,\n",
       "            0.4545,  0.2686, -0.0149,  0.1478, -0.1242, -0.6168, -0.3343,\n",
       "           -0.3072,  0.3914, -0.1740,  0.3979, -0.1303, -0.2316, -2.0543,\n",
       "            0.1042, -0.8161, -0.1735, -0.7943,  0.6916, -0.2453,  0.8793,\n",
       "            0.3731, -0.2784,  0.1545, -0.2218,  0.7223, -0.0914, -0.0355,\n",
       "           -0.3274, -0.8498,  0.5890, -1.4212, -0.0110, -0.4404,  0.8576,\n",
       "            0.6665],\n",
       "          [-1.0713, -0.4383, -0.1200,  0.1459,  0.4925,  0.5630,  0.3973,\n",
       "           -0.0693, -0.2817, -0.6139, -0.5566,  0.5550,  0.3599,  0.0130,\n",
       "            0.7184,  0.7186,  0.0499, -0.6339, -1.6483,  0.8209,  0.1434,\n",
       "            0.4263, -0.5296,  0.5113, -1.4210,  0.6931, -0.1513, -0.0241,\n",
       "            0.4545,  0.2686, -0.0149,  0.1478, -0.1242, -0.6168, -0.3343,\n",
       "           -0.3072,  0.3914, -0.1740,  0.3979, -0.1303, -0.2316, -2.0543,\n",
       "            0.1042, -0.8161, -0.1735, -0.7943,  0.6916, -0.2453,  0.8793,\n",
       "            0.3731, -0.2784,  0.1545, -0.2218,  0.7223, -0.0914, -0.0355,\n",
       "           -0.3274, -0.8498,  0.5890, -1.4212, -0.0110, -0.4404,  0.8576,\n",
       "            0.6665]],\n",
       "\n",
       "         [[ 0.7794, -1.1034, -0.2010, -0.2598,  0.6298, -1.0828,  0.3301,\n",
       "           -1.3759,  0.1772, -1.8621,  1.0404,  1.6840, -0.3561, -0.5739,\n",
       "            0.0303, -0.3725, -0.3766,  0.2896, -0.1854, -1.1957,  0.7152,\n",
       "           -2.0232,  0.3970,  1.5214, -0.6601,  0.8723,  0.7544,  0.0231,\n",
       "            0.5136,  0.4999,  0.3480,  0.3309, -0.6593,  1.8917, -0.7496,\n",
       "           -0.5154, -1.1382, -0.3353, -0.1090, -0.4630, -0.1435, -0.7466,\n",
       "            0.4676,  0.7305, -0.3665,  0.9700, -0.1676,  1.5932,  0.6499,\n",
       "            0.8765,  1.0744,  0.1674,  0.1861, -1.1076, -0.6345, -1.3426,\n",
       "            0.3406,  0.5406,  0.2666, -0.2625, -0.0106,  0.9429,  0.1233,\n",
       "            0.1552],\n",
       "          [ 0.7794, -1.1034, -0.2010, -0.2598,  0.6298, -1.0828,  0.3301,\n",
       "           -1.3759,  0.1772, -1.8621,  1.0404,  1.6840, -0.3561, -0.5739,\n",
       "            0.0303, -0.3725, -0.3766,  0.2896, -0.1854, -1.1957,  0.7152,\n",
       "           -2.0232,  0.3970,  1.5214, -0.6601,  0.8723,  0.7544,  0.0231,\n",
       "            0.5136,  0.4999,  0.3480,  0.3309, -0.6593,  1.8917, -0.7496,\n",
       "           -0.5154, -1.1382, -0.3353, -0.1090, -0.4630, -0.1435, -0.7466,\n",
       "            0.4676,  0.7305, -0.3665,  0.9700, -0.1676,  1.5932,  0.6499,\n",
       "            0.8765,  1.0744,  0.1674,  0.1861, -1.1076, -0.6345, -1.3426,\n",
       "            0.3406,  0.5406,  0.2666, -0.2625, -0.0106,  0.9429,  0.1233,\n",
       "            0.1552]],\n",
       "\n",
       "         [[ 0.4711, -0.5045, -1.6025,  0.3222, -0.2956,  0.3148, -0.4820,\n",
       "            0.8797,  0.5661, -1.0180, -0.5563, -0.6730, -0.1430,  0.0426,\n",
       "           -1.7557, -0.4659,  0.5259, -0.7101,  0.8984, -0.2837, -0.3847,\n",
       "            1.5163, -0.2444, -0.7661,  0.3145, -0.7560,  0.3455, -1.5368,\n",
       "            0.2168,  0.8375,  0.5520,  0.5849,  0.3576,  1.8091, -0.5661,\n",
       "            1.7335, -0.1264, -0.8930, -1.0123, -0.0800,  1.0485,  0.5375,\n",
       "            0.1433,  0.6077,  0.3674, -0.5524,  0.0592,  0.9700,  1.1401,\n",
       "            0.0900, -1.0364, -0.9019, -0.6600,  0.0108,  0.8143,  1.2503,\n",
       "            0.3677,  0.7349,  0.8164, -0.0568, -0.1633, -0.2731,  0.6031,\n",
       "           -0.0381],\n",
       "          [ 0.4711, -0.5045, -1.6025,  0.3222, -0.2956,  0.3148, -0.4820,\n",
       "            0.8797,  0.5661, -1.0180, -0.5563, -0.6730, -0.1430,  0.0426,\n",
       "           -1.7557, -0.4659,  0.5259, -0.7101,  0.8984, -0.2837, -0.3847,\n",
       "            1.5163, -0.2444, -0.7661,  0.3145, -0.7560,  0.3455, -1.5368,\n",
       "            0.2168,  0.8375,  0.5520,  0.5849,  0.3576,  1.8091, -0.5661,\n",
       "            1.7335, -0.1264, -0.8930, -1.0123, -0.0800,  1.0485,  0.5375,\n",
       "            0.1433,  0.6077,  0.3674, -0.5524,  0.0592,  0.9700,  1.1401,\n",
       "            0.0900, -1.0364, -0.9019, -0.6600,  0.0108,  0.8143,  1.2503,\n",
       "            0.3677,  0.7349,  0.8164, -0.0568, -0.1633, -0.2731,  0.6031,\n",
       "           -0.0381]],\n",
       "\n",
       "         [[ 0.3546, -1.5188,  1.0198,  0.1569,  0.1934, -0.9944,  0.1094,\n",
       "           -0.8101, -1.1565, -1.5668, -0.7664, -1.1403,  0.3011,  0.7716,\n",
       "            0.7087,  1.4862,  0.7638,  0.5106, -0.6492, -2.0701, -0.3181,\n",
       "           -1.0033,  0.5162, -0.5144,  0.0453, -1.6730, -0.6592,  1.0997,\n",
       "           -1.3076, -1.8235, -1.2509, -0.6619, -1.7817, -0.0362, -0.4376,\n",
       "           -1.1558, -1.2021, -0.7156,  1.0047, -0.0287, -0.5917,  0.2421,\n",
       "           -0.5762,  0.0403,  0.7444, -0.9663,  0.1605,  0.5010, -0.7317,\n",
       "            0.2058,  1.0655,  0.9828,  1.6515, -0.3598, -1.0612, -0.2523,\n",
       "            0.0886, -0.7582,  0.0333,  0.5343,  1.5033,  0.0754, -1.2746,\n",
       "           -1.2200],\n",
       "          [ 0.3546, -1.5188,  1.0198,  0.1569,  0.1934, -0.9944,  0.1094,\n",
       "           -0.8101, -1.1565, -1.5668, -0.7664, -1.1403,  0.3011,  0.7716,\n",
       "            0.7087,  1.4862,  0.7638,  0.5106, -0.6492, -2.0701, -0.3181,\n",
       "           -1.0033,  0.5162, -0.5144,  0.0453, -1.6730, -0.6592,  1.0997,\n",
       "           -1.3076, -1.8235, -1.2509, -0.6619, -1.7817, -0.0362, -0.4376,\n",
       "           -1.1558, -1.2021, -0.7156,  1.0047, -0.0287, -0.5917,  0.2421,\n",
       "           -0.5762,  0.0403,  0.7444, -0.9663,  0.1605,  0.5010, -0.7317,\n",
       "            0.2058,  1.0655,  0.9828,  1.6515, -0.3598, -1.0612, -0.2523,\n",
       "            0.0886, -0.7582,  0.0333,  0.5343,  1.5033,  0.0754, -1.2746,\n",
       "           -1.2200]],\n",
       "\n",
       "         [[-0.5928,  1.0448,  0.1234, -0.6219, -0.7665,  0.1780, -0.9318,\n",
       "           -0.4184,  0.4170,  0.2100, -0.5766, -0.9472, -0.6742, -0.4534,\n",
       "           -0.7878, -0.5546,  0.6491, -0.6436, -0.7044, -0.4338, -0.1825,\n",
       "            0.6080,  0.6537, -1.3826, -0.1804,  0.5069,  0.0724, -0.2381,\n",
       "           -0.0272, -0.3559,  0.1947, -1.1554, -0.8214,  0.6876,  1.1638,\n",
       "           -1.4827, -0.7000,  0.5435, -0.6384,  1.4732,  0.8729, -0.9470,\n",
       "           -0.2131, -0.0958, -0.3284, -0.0222, -0.6455, -0.6865, -0.4655,\n",
       "            1.8861, -0.6691,  0.4481,  0.5025, -0.4452, -0.4412, -1.4340,\n",
       "           -0.0694,  0.1890,  0.2184, -0.2369, -0.1558, -0.1349,  0.0534,\n",
       "            0.0896],\n",
       "          [-0.5928,  1.0448,  0.1234, -0.6219, -0.7665,  0.1780, -0.9318,\n",
       "           -0.4184,  0.4170,  0.2100, -0.5766, -0.9472, -0.6742, -0.4534,\n",
       "           -0.7878, -0.5546,  0.6491, -0.6436, -0.7044, -0.4338, -0.1825,\n",
       "            0.6080,  0.6537, -1.3826, -0.1804,  0.5069,  0.0724, -0.2381,\n",
       "           -0.0272, -0.3559,  0.1947, -1.1554, -0.8214,  0.6876,  1.1638,\n",
       "           -1.4827, -0.7000,  0.5435, -0.6384,  1.4732,  0.8729, -0.9470,\n",
       "           -0.2131, -0.0958, -0.3284, -0.0222, -0.6455, -0.6865, -0.4655,\n",
       "            1.8861, -0.6691,  0.4481,  0.5025, -0.4452, -0.4412, -1.4340,\n",
       "           -0.0694,  0.1890,  0.2184, -0.2369, -0.1558, -0.1349,  0.0534,\n",
       "            0.0896]]]], grad_fn=<TransposeBackward0>), tensor([[[[-1.4947e+00, -3.9695e-03,  1.9914e+00, -3.6183e-01, -8.4557e-01,\n",
       "            1.3387e+00,  1.2214e-01,  4.0157e+00,  1.2595e+00,  1.6554e-01,\n",
       "           -6.2706e-01, -1.0805e+00,  4.7306e-01,  1.9820e-01, -2.1012e-01,\n",
       "            1.8440e+00,  1.5705e+00,  1.1908e+00,  5.9900e-01, -5.4384e-01,\n",
       "           -1.8455e+00, -1.3152e+00,  5.5404e-01,  1.1997e+00, -2.3196e+00,\n",
       "            1.9922e-01,  2.2627e+00,  1.8017e+00,  4.5096e-02,  2.3901e-02,\n",
       "            5.2188e-01, -1.6061e-02,  7.0288e-02,  4.8913e-01, -5.9956e-02,\n",
       "            2.0514e+00, -4.1485e+00,  1.7975e+00,  2.3467e+00, -5.8082e-01,\n",
       "           -1.2956e+00,  1.4312e+00, -1.1809e+00,  2.2275e+00, -2.6154e+00,\n",
       "           -2.4552e-02, -1.1390e+00, -2.0193e+00, -3.0751e+00, -6.5774e-01,\n",
       "           -6.3400e-01, -3.1570e+00, -6.5643e-01, -5.8933e-01,  2.5408e+00,\n",
       "           -1.3355e-02,  1.9665e-01,  2.3758e+00, -2.2017e+00,  1.6026e+00,\n",
       "           -2.6962e+00, -4.8513e-01,  1.0155e+00,  3.3780e-01],\n",
       "          [-1.4947e+00, -3.9691e-03,  1.9914e+00, -3.6183e-01, -8.4558e-01,\n",
       "            1.3387e+00,  1.2214e-01,  4.0157e+00,  1.2595e+00,  1.6554e-01,\n",
       "           -6.2706e-01, -1.0805e+00,  4.7306e-01,  1.9820e-01, -2.1012e-01,\n",
       "            1.8440e+00,  1.5705e+00,  1.1908e+00,  5.9900e-01, -5.4384e-01,\n",
       "           -1.8455e+00, -1.3152e+00,  5.5404e-01,  1.1997e+00, -2.3196e+00,\n",
       "            1.9922e-01,  2.2627e+00,  1.8017e+00,  4.5096e-02,  2.3900e-02,\n",
       "            5.2188e-01, -1.6061e-02,  7.0288e-02,  4.8913e-01, -5.9956e-02,\n",
       "            2.0514e+00, -4.1485e+00,  1.7975e+00,  2.3467e+00, -5.8082e-01,\n",
       "           -1.2956e+00,  1.4312e+00, -1.1809e+00,  2.2275e+00, -2.6154e+00,\n",
       "           -2.4552e-02, -1.1390e+00, -2.0193e+00, -3.0751e+00, -6.5775e-01,\n",
       "           -6.3400e-01, -3.1570e+00, -6.5643e-01, -5.8933e-01,  2.5408e+00,\n",
       "           -1.3355e-02,  1.9665e-01,  2.3758e+00, -2.2017e+00,  1.6026e+00,\n",
       "           -2.6962e+00, -4.8513e-01,  1.0155e+00,  3.3780e-01]],\n",
       "\n",
       "         [[ 1.8550e+00,  1.1704e-01, -6.6136e-01, -7.8287e-01,  1.4273e+00,\n",
       "           -5.3251e-01,  2.0551e+00, -2.1627e+00, -9.6849e-01,  1.5745e+00,\n",
       "            1.0760e+00,  1.9811e+00,  1.5820e+00,  8.7453e-01, -1.1000e-01,\n",
       "           -5.2078e-01,  5.4821e-02, -1.4319e+00,  1.0267e+00,  1.0699e+00,\n",
       "           -6.9321e-01, -3.7250e-01, -1.2747e+00,  1.6030e+00,  2.2566e+00,\n",
       "           -1.1820e+00, -1.0957e+00,  1.2205e-02,  2.2045e+00,  4.6135e-01,\n",
       "            2.6843e+00,  8.0677e-01,  1.6947e+00,  5.5831e-01, -1.1658e+00,\n",
       "           -1.1087e+00, -1.5586e+00, -1.8586e+00,  4.5598e-01, -2.9007e+00,\n",
       "            1.0009e-01, -1.8924e+00,  5.2766e-01, -4.7674e-01, -1.8977e+00,\n",
       "            9.3262e-01, -2.3345e+00,  5.2388e-01,  2.0504e-02,  1.0437e+00,\n",
       "            6.6083e-01, -2.9076e+00, -6.4700e-01, -6.6863e-01,  2.8538e-01,\n",
       "           -6.4329e-01, -2.2673e+00,  1.6238e-01,  5.1233e-01,  2.4761e+00,\n",
       "           -3.3503e-01, -2.1737e-01,  1.9762e+00, -1.6228e+00],\n",
       "          [ 1.8550e+00,  1.1704e-01, -6.6136e-01, -7.8287e-01,  1.4273e+00,\n",
       "           -5.3251e-01,  2.0551e+00, -2.1627e+00, -9.6849e-01,  1.5745e+00,\n",
       "            1.0760e+00,  1.9811e+00,  1.5820e+00,  8.7453e-01, -1.1000e-01,\n",
       "           -5.2078e-01,  5.4821e-02, -1.4319e+00,  1.0267e+00,  1.0699e+00,\n",
       "           -6.9321e-01, -3.7250e-01, -1.2747e+00,  1.6030e+00,  2.2566e+00,\n",
       "           -1.1820e+00, -1.0957e+00,  1.2205e-02,  2.2045e+00,  4.6135e-01,\n",
       "            2.6843e+00,  8.0677e-01,  1.6947e+00,  5.5831e-01, -1.1658e+00,\n",
       "           -1.1087e+00, -1.5586e+00, -1.8586e+00,  4.5598e-01, -2.9007e+00,\n",
       "            1.0009e-01, -1.8924e+00,  5.2766e-01, -4.7674e-01, -1.8977e+00,\n",
       "            9.3262e-01, -2.3345e+00,  5.2388e-01,  2.0503e-02,  1.0437e+00,\n",
       "            6.6082e-01, -2.9076e+00, -6.4700e-01, -6.6863e-01,  2.8538e-01,\n",
       "           -6.4329e-01, -2.2673e+00,  1.6238e-01,  5.1233e-01,  2.4761e+00,\n",
       "           -3.3503e-01, -2.1737e-01,  1.9762e+00, -1.6228e+00]],\n",
       "\n",
       "         [[-1.5654e+00, -2.1604e+00,  2.2922e-01,  1.1212e+00, -4.0685e+00,\n",
       "            1.1520e+00,  2.0970e+00, -3.5980e-01, -1.9498e+00, -1.1315e-01,\n",
       "            1.6221e-01, -5.7453e-01, -1.2881e+00,  1.1886e-01,  1.0423e+00,\n",
       "           -2.2281e+00,  6.2132e-01,  5.8498e-02, -1.0997e+00, -8.4757e-01,\n",
       "            2.5679e+00, -1.0977e+00, -5.8334e-01, -3.7003e-02,  3.1853e+00,\n",
       "            1.1638e-01,  1.7905e+00,  3.5455e-02,  1.9826e-01, -7.3859e-01,\n",
       "           -2.3467e-01,  1.2819e+00, -4.2664e+00, -2.1729e+00, -2.3518e+00,\n",
       "            2.1880e+00,  1.5043e+00, -4.9947e-01,  5.8687e-01, -2.9600e-01,\n",
       "            8.0100e-01, -1.4027e-01, -3.7574e-01, -1.0263e+00,  2.3772e+00,\n",
       "           -1.2040e+00,  1.5243e+00,  1.2178e+00, -2.5514e+00,  1.8904e+00,\n",
       "            2.2734e+00, -1.1705e+00, -4.5384e-02, -4.9021e-01, -7.6177e-01,\n",
       "           -1.1305e+00, -1.8352e+00,  1.0102e+00,  1.6361e+00, -1.9830e+00,\n",
       "            2.4241e+00,  8.5068e-01, -4.9124e+00,  3.0450e-01],\n",
       "          [-1.5654e+00, -2.1604e+00,  2.2923e-01,  1.1212e+00, -4.0685e+00,\n",
       "            1.1520e+00,  2.0970e+00, -3.5980e-01, -1.9498e+00, -1.1315e-01,\n",
       "            1.6221e-01, -5.7453e-01, -1.2881e+00,  1.1886e-01,  1.0423e+00,\n",
       "           -2.2281e+00,  6.2132e-01,  5.8498e-02, -1.0997e+00, -8.4757e-01,\n",
       "            2.5679e+00, -1.0977e+00, -5.8334e-01, -3.7003e-02,  3.1853e+00,\n",
       "            1.1638e-01,  1.7905e+00,  3.5455e-02,  1.9826e-01, -7.3859e-01,\n",
       "           -2.3467e-01,  1.2819e+00, -4.2664e+00, -2.1729e+00, -2.3518e+00,\n",
       "            2.1880e+00,  1.5043e+00, -4.9947e-01,  5.8687e-01, -2.9600e-01,\n",
       "            8.0100e-01, -1.4027e-01, -3.7574e-01, -1.0263e+00,  2.3772e+00,\n",
       "           -1.2040e+00,  1.5243e+00,  1.2178e+00, -2.5514e+00,  1.8904e+00,\n",
       "            2.2734e+00, -1.1705e+00, -4.5384e-02, -4.9021e-01, -7.6177e-01,\n",
       "           -1.1305e+00, -1.8352e+00,  1.0102e+00,  1.6361e+00, -1.9830e+00,\n",
       "            2.4241e+00,  8.5068e-01, -4.9124e+00,  3.0450e-01]],\n",
       "\n",
       "         [[ 1.2220e+00, -2.0140e+00,  1.1684e+00, -3.8044e-01,  3.3782e-01,\n",
       "            7.5586e-01, -1.3219e+00,  2.8420e+00, -1.1847e+00, -1.2501e+00,\n",
       "           -3.0328e+00,  1.4703e+00,  1.7598e+00,  1.5549e-01,  2.4010e-01,\n",
       "            2.6130e+00, -1.3591e+00,  1.3263e+00, -7.1550e-01,  1.9180e+00,\n",
       "            1.5950e+00,  4.8260e-01,  1.7576e+00, -3.2007e+00,  1.5831e+00,\n",
       "           -1.8019e-01,  7.0095e-01, -1.1994e+00,  1.4629e+00,  1.0024e+00,\n",
       "           -6.4724e-01, -7.1075e-01,  3.3671e+00, -1.4369e+00, -6.9677e-01,\n",
       "            9.0304e-02, -1.6256e+00,  1.3846e+00, -1.3046e+00, -1.8636e+00,\n",
       "            9.7387e-01, -2.1871e+00, -3.4198e-01, -1.2510e+00, -4.0352e-01,\n",
       "           -1.5090e+00, -1.3885e+00, -1.6299e+00, -3.2257e+00,  4.9507e-01,\n",
       "            3.0088e-01,  4.7974e-01,  2.0754e+00,  7.4785e-01, -8.5476e-01,\n",
       "            8.6610e-01, -2.6934e-01,  6.4916e-01,  1.9236e+00,  2.5857e+00,\n",
       "           -1.4060e+00, -2.0253e+00, -1.7856e+00,  2.0339e+00],\n",
       "          [ 1.2220e+00, -2.0140e+00,  1.1684e+00, -3.8044e-01,  3.3782e-01,\n",
       "            7.5586e-01, -1.3219e+00,  2.8420e+00, -1.1847e+00, -1.2501e+00,\n",
       "           -3.0328e+00,  1.4703e+00,  1.7598e+00,  1.5549e-01,  2.4010e-01,\n",
       "            2.6130e+00, -1.3591e+00,  1.3263e+00, -7.1550e-01,  1.9180e+00,\n",
       "            1.5950e+00,  4.8260e-01,  1.7576e+00, -3.2007e+00,  1.5831e+00,\n",
       "           -1.8019e-01,  7.0095e-01, -1.1994e+00,  1.4629e+00,  1.0024e+00,\n",
       "           -6.4724e-01, -7.1075e-01,  3.3671e+00, -1.4369e+00, -6.9677e-01,\n",
       "            9.0304e-02, -1.6256e+00,  1.3846e+00, -1.3046e+00, -1.8636e+00,\n",
       "            9.7387e-01, -2.1871e+00, -3.4198e-01, -1.2510e+00, -4.0352e-01,\n",
       "           -1.5090e+00, -1.3885e+00, -1.6299e+00, -3.2257e+00,  4.9507e-01,\n",
       "            3.0088e-01,  4.7974e-01,  2.0754e+00,  7.4785e-01, -8.5476e-01,\n",
       "            8.6610e-01, -2.6934e-01,  6.4916e-01,  1.9236e+00,  2.5857e+00,\n",
       "           -1.4060e+00, -2.0253e+00, -1.7856e+00,  2.0339e+00]],\n",
       "\n",
       "         [[ 2.2071e+00,  3.0521e+00, -1.5075e+00,  3.4852e+00,  1.3854e+00,\n",
       "           -1.9845e+00, -5.1755e-01,  1.9952e+00, -1.0817e+00, -1.0434e+00,\n",
       "            1.2366e+00,  1.7313e+00,  1.4110e+00, -2.5688e+00,  2.9180e+00,\n",
       "            1.6238e+00,  1.2731e-01,  4.4672e-01, -9.0271e-01, -2.7223e+00,\n",
       "            4.0121e+00, -1.2062e+00,  3.1282e+00, -1.8609e-01,  3.1466e+00,\n",
       "            1.4694e+00, -1.4012e+00, -2.7122e+00, -2.5891e-01,  1.8903e+00,\n",
       "            3.0056e-01, -3.6220e+00,  1.1172e+00,  6.8563e-02,  3.4724e+00,\n",
       "            9.5552e-01,  1.7472e+00, -3.8456e+00, -1.1511e+00, -1.0879e+00,\n",
       "            1.3088e+00, -5.8826e-01,  1.2687e+00, -1.3308e+00, -8.2489e-01,\n",
       "           -2.0066e+00,  4.4723e+00, -2.9077e+00,  2.7821e+00, -1.3611e+00,\n",
       "           -2.2243e+00, -6.6654e-01, -1.6370e+00, -3.8487e-02,  2.1911e-01,\n",
       "            1.0953e+00,  1.6765e+00, -1.1809e+00, -1.2047e+00, -2.5187e+00,\n",
       "           -4.7394e-01, -1.7825e+00,  3.3675e-01, -6.0123e-02],\n",
       "          [ 2.2071e+00,  3.0521e+00, -1.5075e+00,  3.4852e+00,  1.3854e+00,\n",
       "           -1.9845e+00, -5.1755e-01,  1.9952e+00, -1.0817e+00, -1.0434e+00,\n",
       "            1.2366e+00,  1.7313e+00,  1.4110e+00, -2.5688e+00,  2.9180e+00,\n",
       "            1.6238e+00,  1.2731e-01,  4.4672e-01, -9.0271e-01, -2.7223e+00,\n",
       "            4.0121e+00, -1.2062e+00,  3.1282e+00, -1.8609e-01,  3.1466e+00,\n",
       "            1.4694e+00, -1.4012e+00, -2.7122e+00, -2.5891e-01,  1.8903e+00,\n",
       "            3.0055e-01, -3.6220e+00,  1.1172e+00,  6.8563e-02,  3.4724e+00,\n",
       "            9.5552e-01,  1.7472e+00, -3.8456e+00, -1.1511e+00, -1.0879e+00,\n",
       "            1.3088e+00, -5.8826e-01,  1.2687e+00, -1.3308e+00, -8.2489e-01,\n",
       "           -2.0066e+00,  4.4723e+00, -2.9077e+00,  2.7821e+00, -1.3611e+00,\n",
       "           -2.2243e+00, -6.6654e-01, -1.6370e+00, -3.8487e-02,  2.1911e-01,\n",
       "            1.0953e+00,  1.6765e+00, -1.1809e+00, -1.2047e+00, -2.5187e+00,\n",
       "           -4.7394e-01, -1.7825e+00,  3.3675e-01, -6.0124e-02]],\n",
       "\n",
       "         [[ 5.7815e-01,  5.1662e-01, -2.3713e+00, -2.9049e+00,  6.4864e-01,\n",
       "           -6.8450e-01,  2.1145e+00,  1.3391e-01,  1.5948e+00,  1.6112e+00,\n",
       "           -7.2161e-01,  1.3445e+00,  1.5666e+00, -2.9322e+00, -3.7139e-01,\n",
       "           -4.2850e-01,  1.0182e+00, -8.9494e-01, -1.5044e+00, -9.9895e-01,\n",
       "            2.7059e-01, -4.2073e-01,  5.9859e-01, -1.5344e+00, -1.5175e+00,\n",
       "            1.9269e+00,  4.3967e-01, -6.6634e-01, -3.2417e-01,  3.5323e-01,\n",
       "           -1.4723e+00,  2.3687e+00,  1.2325e+00, -5.1373e-01, -2.8492e-01,\n",
       "           -1.1310e+00,  1.7044e+00,  6.8507e-01, -3.4002e-01, -1.7396e+00,\n",
       "            1.0703e+00, -1.1472e-01, -8.3963e-01,  6.3704e-01, -4.6763e-01,\n",
       "           -5.7682e-01,  1.7390e+00,  1.9133e+00, -6.4106e-01, -1.7988e+00,\n",
       "            1.8148e+00,  2.0578e+00, -1.2772e+00,  5.1697e-02, -2.1664e+00,\n",
       "           -1.2454e-02, -2.1861e-01, -1.1282e+00, -3.1365e-01,  3.6213e-02,\n",
       "            7.0285e-01,  1.6922e+00,  1.2395e+00, -9.8834e-01],\n",
       "          [ 5.7815e-01,  5.1662e-01, -2.3713e+00, -2.9049e+00,  6.4864e-01,\n",
       "           -6.8450e-01,  2.1145e+00,  1.3391e-01,  1.5948e+00,  1.6112e+00,\n",
       "           -7.2161e-01,  1.3445e+00,  1.5666e+00, -2.9322e+00, -3.7139e-01,\n",
       "           -4.2850e-01,  1.0182e+00, -8.9494e-01, -1.5044e+00, -9.9895e-01,\n",
       "            2.7059e-01, -4.2073e-01,  5.9859e-01, -1.5344e+00, -1.5175e+00,\n",
       "            1.9269e+00,  4.3967e-01, -6.6634e-01, -3.2417e-01,  3.5323e-01,\n",
       "           -1.4723e+00,  2.3687e+00,  1.2325e+00, -5.1373e-01, -2.8492e-01,\n",
       "           -1.1310e+00,  1.7044e+00,  6.8507e-01, -3.4002e-01, -1.7396e+00,\n",
       "            1.0703e+00, -1.1472e-01, -8.3963e-01,  6.3704e-01, -4.6763e-01,\n",
       "           -5.7682e-01,  1.7390e+00,  1.9133e+00, -6.4106e-01, -1.7988e+00,\n",
       "            1.8148e+00,  2.0578e+00, -1.2772e+00,  5.1698e-02, -2.1664e+00,\n",
       "           -1.2454e-02, -2.1861e-01, -1.1282e+00, -3.1365e-01,  3.6212e-02,\n",
       "            7.0285e-01,  1.6922e+00,  1.2395e+00, -9.8834e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.1803e+00, -2.5025e+00, -1.5702e+00,  ...,  3.1666e+00,\n",
       "           -3.2564e+00,  2.0230e+00],\n",
       "          [ 1.4897e+00, -1.7941e+00,  7.3312e-01,  ...,  5.9665e+00,\n",
       "           -2.2809e+00,  5.8752e+00],\n",
       "          [-4.6471e-01, -1.7324e+00,  7.2099e-01,  ...,  4.5536e+00,\n",
       "           -6.3382e+00,  2.8629e+00],\n",
       "          ...,\n",
       "          [ 2.2365e-01,  2.4758e+00,  1.1191e+00,  ...,  7.7604e-01,\n",
       "           -7.0178e-01, -9.8186e-01],\n",
       "          [ 2.4743e+00, -1.5282e+00,  3.8787e+00,  ...,  2.8262e-01,\n",
       "           -3.1524e+00,  8.1348e-01],\n",
       "          [ 3.1340e-01,  4.4890e-01, -9.4275e-01,  ..., -1.1188e+00,\n",
       "            8.3239e-01,  2.0775e-01]],\n",
       "\n",
       "         [[-1.0807e+00,  3.2227e+00,  3.3166e+00,  ...,  4.3995e-01,\n",
       "           -4.6383e+00, -1.1063e+00],\n",
       "          [-4.2378e+00,  1.7337e+00,  4.2542e+00,  ..., -6.9216e-01,\n",
       "            1.0200e-01, -4.2186e+00],\n",
       "          [-4.1546e+00,  1.7019e+00,  2.6236e+00,  ..., -4.3347e-01,\n",
       "           -2.9590e+00, -2.4852e+00],\n",
       "          ...,\n",
       "          [-3.3372e-02, -9.2306e-01, -1.8828e+00,  ..., -2.8205e+00,\n",
       "            2.2279e+00, -2.1904e+00],\n",
       "          [-2.0371e-01,  2.0597e+00,  6.6469e-01,  ..., -1.2566e+00,\n",
       "            3.4321e+00, -9.1490e-01],\n",
       "          [ 1.0971e+00, -5.8875e-01, -3.8434e-01,  ...,  2.4158e-01,\n",
       "           -6.7949e-01,  1.2693e+00]],\n",
       "\n",
       "         [[ 2.9307e-02, -7.0587e-01,  3.9011e+00,  ...,  7.9706e-01,\n",
       "           -3.1127e+00, -4.0535e+00],\n",
       "          [-5.2623e+00,  3.4086e-03, -4.0425e-01,  ...,  8.1739e-01,\n",
       "           -1.8025e+00, -1.7490e+00],\n",
       "          [-1.3630e+00,  2.8833e-01, -3.9200e-01,  ...,  8.8128e-01,\n",
       "           -3.7319e+00, -4.3563e+00],\n",
       "          ...,\n",
       "          [ 4.7542e-01,  1.1294e+00, -2.4107e-01,  ..., -3.6867e+00,\n",
       "           -1.5777e+00, -3.4724e+00],\n",
       "          [-1.3464e+00, -2.0361e+00, -2.6526e+00,  ...,  6.7685e-01,\n",
       "           -4.5311e+00, -3.4774e+00],\n",
       "          [ 4.4576e-01, -5.6120e-01,  7.5420e-02,  ..., -4.0286e-01,\n",
       "            4.5040e-01,  5.6526e-01]],\n",
       "\n",
       "         [[-8.5678e-01,  4.1318e+00,  7.3264e-01,  ...,  4.2543e+00,\n",
       "           -7.1652e-01, -2.3072e+00],\n",
       "          [-2.4065e+00,  4.3667e+00, -3.9518e-01,  ...,  2.3359e+00,\n",
       "            1.6133e+00, -2.8800e+00],\n",
       "          [-1.8620e+00,  2.8699e+00, -1.9736e+00,  ...,  3.5305e+00,\n",
       "           -9.4842e-01, -1.1558e+00],\n",
       "          ...,\n",
       "          [-3.1227e+00,  2.8774e+00, -3.9082e-01,  ..., -2.9302e-01,\n",
       "            3.9374e-01, -2.2632e+00],\n",
       "          [-1.6596e+00,  1.5930e+00,  1.0709e+00,  ..., -1.8362e+00,\n",
       "           -1.5129e-01,  3.2548e-01],\n",
       "          [ 1.6318e-01, -2.6593e-01, -9.3142e-02,  ..., -2.7496e-01,\n",
       "            2.2442e-01,  1.0369e+00]],\n",
       "\n",
       "         [[ 5.4800e-03,  3.0113e+00,  8.8864e-01,  ..., -5.3347e+00,\n",
       "           -1.1806e+00, -4.1716e+00],\n",
       "          [-1.0971e+00,  3.5406e+00,  1.8368e+00,  ..., -4.1601e+00,\n",
       "           -1.6629e+00,  1.5043e+00],\n",
       "          [-2.8949e+00,  1.0683e+00,  1.7713e+00,  ..., -5.3078e+00,\n",
       "           -2.1082e+00, -2.3081e+00],\n",
       "          ...,\n",
       "          [ 5.2658e+00, -2.6755e+00,  1.6340e-01,  ..., -2.8532e+00,\n",
       "           -3.6081e-01, -4.3553e+00],\n",
       "          [ 1.0933e+00, -1.8753e+00, -1.4589e+00,  ...,  3.9503e-01,\n",
       "           -6.9060e-02, -4.5843e+00],\n",
       "          [-2.3700e-01,  1.4507e-01, -1.8266e-01,  ...,  1.1204e+00,\n",
       "           -1.0549e-01, -2.6549e-02]],\n",
       "\n",
       "         [[ 1.8887e+00, -3.2748e+00,  3.9862e+00,  ..., -2.9483e+00,\n",
       "            1.6739e+00,  1.8205e+00],\n",
       "          [ 3.0179e+00, -1.5439e+00,  2.4207e+00,  ..., -2.2741e+00,\n",
       "            3.6518e+00,  4.7319e+00],\n",
       "          [ 4.8249e+00, -1.9287e+00,  2.1154e+00,  ..., -2.5321e-01,\n",
       "            3.2593e+00,  3.8225e+00],\n",
       "          ...,\n",
       "          [ 4.1747e+00, -4.3796e+00,  5.4047e+00,  ...,  1.5550e+00,\n",
       "           -1.1966e+00,  1.0061e+00],\n",
       "          [ 2.2907e+00, -3.1829e+00,  3.1817e+00,  ...,  1.3093e+00,\n",
       "            2.3362e+00,  2.1481e+00],\n",
       "          [ 4.9100e-01,  1.0115e+00, -3.2882e-01,  ..., -7.1943e-02,\n",
       "           -5.9106e-01, -1.3259e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 2.0248e+00, -1.2488e+00, -5.5882e+00,  ...,  2.5922e+00,\n",
       "           -1.4392e+01, -2.1020e-01],\n",
       "          [ 5.9458e+00, -4.8566e+00, -3.0084e+00,  ...,  3.4321e+00,\n",
       "           -3.2522e+00,  6.4958e+00],\n",
       "          [ 1.5797e-02, -1.1906e+00, -6.1457e+00,  ..., -5.7872e+00,\n",
       "            2.8230e+00,  1.3798e+00],\n",
       "          ...,\n",
       "          [-5.2073e+00, -4.5975e+00,  7.1056e+00,  ..., -4.4363e-01,\n",
       "           -4.8257e+00,  2.9253e+00],\n",
       "          [-3.1836e+00,  8.2563e-01, -4.0976e+00,  ..., -4.0300e+00,\n",
       "            6.8933e+00,  3.2491e+00],\n",
       "          [ 6.5558e-03,  9.6963e-02,  5.0926e-02,  ..., -1.3800e-02,\n",
       "           -8.8653e-02,  4.0268e-02]],\n",
       "\n",
       "         [[-6.5312e-02,  3.0434e+00,  5.2348e+00,  ...,  7.1818e+00,\n",
       "            4.2408e+00,  1.1272e+01],\n",
       "          [ 1.6248e+00,  2.8602e+00,  1.3747e+00,  ...,  5.0312e+00,\n",
       "            3.2365e+00, -9.7057e-01],\n",
       "          [ 3.7922e+00, -6.0975e+00,  8.4382e+00,  ..., -5.4862e+00,\n",
       "           -2.3226e+00, -4.6966e-01],\n",
       "          ...,\n",
       "          [-2.4874e+00, -6.6938e-01,  1.1359e+00,  ..., -4.3300e+00,\n",
       "            1.2985e+00, -5.8160e+00],\n",
       "          [-5.4367e+00, -1.8277e+00,  2.3973e+00,  ..., -2.3391e+00,\n",
       "            1.2121e-01, -5.1198e+00],\n",
       "          [-1.0939e-01,  4.7455e-02,  1.4284e-01,  ..., -1.3915e-01,\n",
       "           -4.7992e-02, -1.4546e-01]],\n",
       "\n",
       "         [[-7.7649e+00, -5.3236e+00,  8.3176e+00,  ...,  3.7093e+00,\n",
       "            7.1658e+00, -1.0642e+01],\n",
       "          [-1.3012e+01,  2.1555e+00,  7.0987e+00,  ...,  1.6530e+00,\n",
       "           -3.1832e+00, -7.1848e+00],\n",
       "          [-5.2583e+00, -1.1061e+01,  3.4665e+00,  ...,  1.6962e+00,\n",
       "           -2.3711e+00, -8.3073e+00],\n",
       "          ...,\n",
       "          [ 1.8558e+00, -2.2150e+00,  1.2285e+01,  ...,  5.4933e+00,\n",
       "           -2.6085e+00, -8.8603e+00],\n",
       "          [-1.3688e+01,  3.3522e-01,  1.0227e+01,  ...,  1.3288e+01,\n",
       "           -1.1830e+01, -1.2387e+01],\n",
       "          [ 7.9540e-02, -3.0992e-02, -8.4832e-02,  ..., -3.0607e-02,\n",
       "           -2.9566e-02, -1.2445e-01]],\n",
       "\n",
       "         [[-3.7367e+00, -1.0683e+00, -7.0098e+00,  ..., -5.5336e+00,\n",
       "           -2.4524e+00,  6.8731e+00],\n",
       "          [-4.0170e+00, -3.3048e+00, -3.3747e+00,  ..., -5.5272e+00,\n",
       "            9.7666e-01, -6.7754e-01],\n",
       "          [-1.6266e+00, -1.4554e+00, -5.8966e+00,  ..., -3.1763e+00,\n",
       "           -8.5018e-01,  1.2442e+00],\n",
       "          ...,\n",
       "          [-3.3178e+00, -1.7650e+00, -4.2060e+00,  ...,  1.2774e+00,\n",
       "            3.9225e+00,  4.1330e+00],\n",
       "          [-1.9110e+00, -1.5253e+00,  5.1934e+00,  ...,  2.2189e-01,\n",
       "           -9.3792e-01,  3.8181e+00],\n",
       "          [ 2.2889e-01, -2.8889e-01, -2.4340e-01,  ...,  3.2055e-01,\n",
       "           -1.1440e-01,  2.3465e-01]],\n",
       "\n",
       "         [[ 1.9964e-01,  2.5399e+00,  9.7125e+00,  ...,  2.0539e+00,\n",
       "            9.4097e+00, -1.0505e+00],\n",
       "          [ 1.2207e+00,  3.9617e+00, -5.9528e+00,  ...,  5.7497e+00,\n",
       "            2.5062e+00, -4.9250e+00],\n",
       "          [-7.3204e+00,  6.8726e+00, -1.9601e+00,  ..., -1.8723e+00,\n",
       "            9.6822e+00, -1.8402e+00],\n",
       "          ...,\n",
       "          [ 1.7793e-01, -6.8092e-01, -1.0327e+01,  ..., -1.1756e-01,\n",
       "            1.6706e+00, -9.3853e+00],\n",
       "          [ 3.6829e+00,  1.6394e+00, -2.0209e+00,  ...,  2.7724e+00,\n",
       "           -7.7770e+00,  1.8678e+00],\n",
       "          [ 3.7030e-02, -1.5560e-01,  1.1723e-01,  ...,  3.5167e-02,\n",
       "           -3.7438e-03, -1.2227e-01]],\n",
       "\n",
       "         [[ 2.4777e+00,  4.1410e-01,  2.6571e+00,  ...,  3.3367e+00,\n",
       "           -3.5762e+00, -8.1786e+00],\n",
       "          [ 2.4539e+00, -9.9235e+00, -1.0758e+00,  ...,  3.1705e+00,\n",
       "            2.1170e+00,  5.7866e+00],\n",
       "          [-2.8427e+00,  9.8566e-01,  5.5300e+00,  ..., -8.4576e+00,\n",
       "           -4.4604e+00, -1.3612e+00],\n",
       "          ...,\n",
       "          [-3.7381e+00,  1.7472e-01, -2.2176e+00,  ...,  3.7345e+00,\n",
       "           -1.0935e+01,  5.6117e+00],\n",
       "          [-3.3324e+00,  4.6360e+00,  1.3904e+00,  ...,  1.9728e+00,\n",
       "           -1.1885e+01,  3.7266e+00],\n",
       "          [ 3.8370e-02, -7.5277e-02,  1.1194e-01,  ..., -2.4150e-01,\n",
       "           -1.2738e-01,  9.6236e-02]]]], grad_fn=<TransposeBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-1.8794e-01,  1.1876e-02, -3.8639e-01,  ..., -1.4963e-01,\n",
       "          -1.6208e-01, -4.1358e-02],\n",
       "         [-2.1442e-01,  3.2057e-02,  1.3780e-01,  ..., -8.2550e-02,\n",
       "          -3.5010e-02, -5.5262e-02],\n",
       "         [-2.3465e-01, -1.2388e-01,  1.7670e-01,  ..., -1.4079e-01,\n",
       "           8.7494e-02, -6.6120e-02],\n",
       "         ...,\n",
       "         [-8.2280e-04, -1.6746e-01,  1.5473e-01,  ..., -6.2441e-02,\n",
       "           5.7512e-02, -5.1751e-02],\n",
       "         [-5.9194e-02,  5.2731e-02,  5.0366e-03,  ..., -4.2328e-02,\n",
       "           2.4288e-02, -1.0130e-02],\n",
       "         [-1.6981e-03,  2.7640e-04, -2.4676e-03,  ..., -1.5716e-03,\n",
       "          -9.0317e-04,  1.7436e-01]]], grad_fn=<MulBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "561896f6-716e-4b4a-9127-364573eb1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 32128])\n"
     ]
    }
   ],
   "source": [
    "print(preds_tokenized.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d608c66-598c-4ad5-99c6-cd8a11b98a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 32128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_softmax = torch.nn.functional.softmax(preds_tokenized.logits, dim=-1)\n",
    "preds_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2ef2ac1-b6d2-49ae-bed2-0c8bc059442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 7142, 19, 10747, 7, 15, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['This sentence is False'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "052f96dc-a903-4f67-870f-cdf2b4055fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[100, 7142, 19, 10998, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['This sentence is True'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7eb776fd-6122-4466-a530-94115777cfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([10747, 7, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a80c6a40-7fe5-42d8-bbda-f56f703cb95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6215, 0.6215]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_softmax[...,10747]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5182fce-381a-43b0-96b1-7a34571aab03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3768, 0.3768]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_softmax[...,10998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c5b0a3-626f-416e-9d29-8f2067d5c0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5fc8978-85ca-4999-8590-3082be47ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_generate_tokenized = model.generate(inputs, max_new_tokens=1024, do_sample=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95707503-5fe8-447b-a675-0104361340cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 10747,     7,    15,     1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_generate_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef27372a-f678-467f-84a3-8122b7c12a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> False</s>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = tokenizer.batch_decode(preds_generate_tokenized)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d554c20b-d89e-4890-97b8-28eca69dcda4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m ref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(data_points[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, max_length\u001b[38;5;241m=\u001b[39mmax_input_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m---> 13\u001b[0m preds_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     14\u001b[0m preds \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(preds_tokenized)\n\u001b[1;32m     16\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(pattern, preds[\u001b[38;5;241m0\u001b[39m]) \n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/transformers/generation/utils.py:2454\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2462\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1112\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     )\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:725\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 725\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:636\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    625\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    634\u001b[0m ):\n\u001b[1;32m    635\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 636\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    648\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratches/dialfs/alta/hln35/miniconda/envs/distillation/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:532\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    527\u001b[0m value_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    528\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, key_value_states, past_key_value[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_relative_attention_bias:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for i in range(len(data_points)):\n",
    "correct = 0\n",
    "empty = 0\n",
    "pattern = r'<pad>\\s+(.*?)</s>'  \n",
    "\n",
    "for i in range(len(data_points)):\n",
    "    question = data_points[i][\"question\"]\n",
    "    passage = data_points[i][\"passage\"]\n",
    "    ans = \"\"\n",
    "    text = prefix + \"context: \" + passage + \". question: \" + question\n",
    "    ref = str(data_points[i][\"answer\"])\n",
    "    inputs = tokenizer(text, max_length=max_input_length, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    preds_tokenized = model.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "    preds = tokenizer.batch_decode(preds_tokenized)\n",
    "\n",
    "    match = re.search(pattern, preds[0]) \n",
    "\n",
    "    if match:\n",
    "        ans = match.group(1)\n",
    "    \n",
    "    if ans == ref:\n",
    "        correct += 1\n",
    "    elif ans == \"\":\n",
    "        empty += 1\n",
    "    # print(text)\n",
    "    # print(preds)\n",
    "    # print(ans)\n",
    "    # print(ref)\n",
    "# import json\n",
    "# with open(\"translate_bleu_small.txt\", \"w\") as fp:\n",
    "#     json.dump(scores, fp)\n",
    "# print(sum(scores)/len(data_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913179d-8319-492c-8e60-c24072bc1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total points are {len(data_points)}. Number of correct answers is {correct}. Number of empty answers is {empty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc337a-e765-42b7-aeca-a0f71fbe710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_large = \"google/flan-t5-large\"\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(model_large)\n",
    "model_large = AutoModelForSeq2SeqLM.from_pretrained(model_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e48f2f-5e3f-4c93-91f1-d206c1e76e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "empty = 0\n",
    "pattern = r'<pad>\\s+(.*?)</s>'  \n",
    "\n",
    "for i in range(len(data_points)):\n",
    "    question = data_points[i][\"question\"]\n",
    "    passage = data_points[i][\"passage\"]\n",
    "    ans = \"\"\n",
    "    text = prefix + \"context: \" + passage + \". question: \" + question\n",
    "    ref = str(data_points[i][\"answer\"])\n",
    "    inputs = tokenizer_large(text,max_length=max_input_length, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    preds_tokenized = model_large.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "    preds = tokenizer_large.batch_decode(preds_tokenized)\n",
    "\n",
    "    match = re.search(pattern, preds[0]) \n",
    "\n",
    "    if match:\n",
    "        ans = match.group(1)\n",
    "    \n",
    "        \n",
    "    if ans == ref:\n",
    "        correct += 1\n",
    "    elif ans == \"\":\n",
    "        empty += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17f68a-db97-41dc-85fd-9a12600a7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total points are {len(data_points)}. Number of correct answers is {correct}. Number of empty answers is {empty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cab367-8302-4b73-bf82-91784e24b9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ff592-75ec-4d3a-acd8-71f7ff1ed6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549da196-33d2-4d46-a320-6c6656072c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec93f36-c877-4238-a49a-2e45517c60c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
