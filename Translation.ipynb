{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b000096c-df2b-4e57-a2c3-aea36de1f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = \"/scratches/dialfs/alta/hln35/.cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratches/dialfs/alta/hln35/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b72a828-17e0-4250-975a-54818ea1b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_small = \"google/flan-t5-small\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855dc116-1d93-490c-86ab-e77f3368abd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae8529b-f7cc-4257-ac87-63306f38e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"wmt14\", \"fr-en\", split='test', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e774c2c-e10f-45fe-9a46-646d70ddb6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 3003\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67a9537-626d-413a-8dbf-6edb9c2240c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Spectacular Wingsuit Jump Over Bogota',\n",
       "  'fr': 'Spectaculaire saut en \"wingsuit\" au-dessus de Bogota'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643fd143-bed0-4dc2-8e0e-4dcbdb7203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    # model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bccc6f-6a68-425e-9a8d-8946eda42e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3550aadd-fef1-4ba0-aa3c-387d4535966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_small)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_small).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f0e44f-4187-4437-9425-d5d5c641fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d554c20b-d89e-4890-97b8-28eca69dcda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.442637790527135\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(books)):\n",
    "scores = []\n",
    "for i in range(len(books)):\n",
    "    text = prefix + books[i][\"translation\"][source_lang]\n",
    "    ref = books[i][\"translation\"][target_lang]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    preds_tokenized = model.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "    preds = tokenizer.batch_decode(preds_tokenized)\n",
    "    \n",
    "    bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "    scores.append(bleu_score[\"score\"])\n",
    "import json\n",
    "with open(\"translate_bleu_small.txt\", \"w\") as fp:\n",
    "    json.dump(scores, fp)\n",
    "print(sum(scores)/len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cebc337a-e765-42b7-aeca-a0f71fbe710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_large = \"google/flan-t5-large\"\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(model_large)\n",
    "model_large = AutoModelForSeq2SeqLM.from_pretrained(model_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e48f2f-5e3f-4c93-91f1-d206c1e76e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.373898826506785\n"
     ]
    }
   ],
   "source": [
    "scores_large = []\n",
    "for i in range(len(books)):\n",
    "    text = prefix + books[i][\"translation\"][source_lang]\n",
    "    ref = books[i][\"translation\"][target_lang]\n",
    "    inputs = tokenizer_large(text, return_tensors=\"pt\").input_ids\n",
    "    preds_tokenized = model_large.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "    preds = tokenizer_large.batch_decode(preds_tokenized)\n",
    "    \n",
    "    bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "    scores_large.append(bleu_score[\"score\"])\n",
    "import json\n",
    "with open(\"translate_bleu_large.txt\", \"w\") as fp:\n",
    "    json.dump(scores_large, fp)\n",
    "print(sum(scores_large)/len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c17f68a-db97-41dc-85fd-9a12600a7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small_fintuned = \"model/flant5_small_lr_10-4_qa_finetuning\"\n",
    "model_small_distill_qa = \"model/flant5_small_lr_10-5_qa_distill_match_large_output_abcd\"\n",
    "model_small_fintuned = AutoModelForSeq2SeqLM.from_pretrained(model_small_fintuned, local_files_only=True).to(device)\n",
    "model_small_distill_qa = AutoModelForSeq2SeqLM.from_pretrained(model_small_distill_qa, local_files_only=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a13a5d27-2a7b-4cc2-9439-1b5c74508145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75cab367-8302-4b73-bf82-91784e24b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.464688062977212\n"
     ]
    }
   ],
   "source": [
    "scores_distill = []\n",
    "for i in range(len(books)):\n",
    "    text = prefix + books[i][\"translation\"][source_lang]\n",
    "    ref = books[i][\"translation\"][target_lang]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device).input_ids\n",
    "    preds_tokenized = model_small_distill_qa.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "    preds = tokenizer.batch_decode(preds_tokenized)\n",
    "    \n",
    "    bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "    scores_distill.append(bleu_score[\"score\"])\n",
    "\n",
    "print(sum(scores_distill)/len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e78ff592-75ec-4d3a-acd8-71f7ff1ed6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa36b2d9f9f4a6abb8f36c55921cd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.798674983992763\n"
     ]
    }
   ],
   "source": [
    "scores_fintuned = []\n",
    "progress_bar = tqdm(range(len(books)))\n",
    "for i in range(len(books)):\n",
    "    text = prefix + books[i][\"translation\"][source_lang]\n",
    "    ref = books[i][\"translation\"][target_lang]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device).input_ids\n",
    "    preds_tokenized = model_small_fintuned.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "    preds = tokenizer.batch_decode(preds_tokenized)\n",
    "    \n",
    "    bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "    scores_fintuned.append(bleu_score[\"score\"])\n",
    "    progress_bar.update(1)\n",
    "print(sum(scores_fintuned)/len(books))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "549da196-33d2-4d46-a320-6c6656072c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small_fintuned = \"model/flant5_small_lr_10-4_race_finetuning_epoch11\"\n",
    "model_small_fintuned = AutoModelForSeq2SeqLM.from_pretrained(model_small_fintuned, local_files_only=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aec93f36-c877-4238-a49a-2e45517c60c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7243cde5266c4c49912edfe74921e98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00694616067271619\n"
     ]
    }
   ],
   "source": [
    "scores_fintuned = []\n",
    "progress_bar = tqdm(range(len(books)))\n",
    "for i in range(len(books)):\n",
    "    text = prefix + books[i][\"translation\"][source_lang]\n",
    "    ref = books[i][\"translation\"][target_lang]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device).input_ids\n",
    "    preds_tokenized = model_small_fintuned.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "    preds = tokenizer.batch_decode(preds_tokenized)\n",
    "    \n",
    "    bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "    scores_fintuned.append(bleu_score[\"score\"])\n",
    "    progress_bar.update(1)\n",
    "print(sum(scores_fintuned)/len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0085e8-a87d-444d-bbc5-4b6a343732b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb2c5b097b04b1782ef45d5bbdd6d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 3 the average score on the test set is 0.08576216846026037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff4f37d8284498db02b7c326115db75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 6 the average score on the test set is 0.07576191818667471\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d393459e534637bcc81d59571b510d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 9 the average score on the test set is 0.0016246515293337417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a625040f154a99a140cb1d8e8cc21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 12 the average score on the test set is 0.00694616067271619\n"
     ]
    }
   ],
   "source": [
    "for t in range(2,12,3):\n",
    "    model_small_fintuned = f\"model/flant5_small_lr_10-4_race_finetuning_epoch{t}\"\n",
    "    model_small_fintuned = AutoModelForSeq2SeqLM.from_pretrained(model_small_fintuned, local_files_only=True).to(device)\n",
    "    scores_fintuned = []\n",
    "    progress_bar = tqdm(range(len(books)))\n",
    "    for i in range(len(books)):\n",
    "        text = prefix + books[i][\"translation\"][source_lang]\n",
    "        ref = books[i][\"translation\"][target_lang]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device).input_ids\n",
    "        preds_tokenized = model_small_fintuned.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "        preds = tokenizer.batch_decode(preds_tokenized)\n",
    "        \n",
    "        bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "        scores_fintuned.append(bleu_score[\"score\"])\n",
    "        progress_bar.update(1)\n",
    "    print(f\"After epoch {t+1} the average score on the test set is {sum(scores_fintuned)/len(books)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7ead70c-6f55-41db-98c6-4e81a468ddb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7522fee2c846eebf40340383974bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 3 the average score on the test set is 0.32699462441924343\n"
     ]
    }
   ],
   "source": [
    "for t in range(2,3,1):\n",
    "    model_small_distill = f\"model/flant5_small_lr_10-4_race_distill_epoch{t}\"\n",
    "    model_small_distill = AutoModelForSeq2SeqLM.from_pretrained(model_small_distill, local_files_only=True).to(device)\n",
    "    scores_distill = []\n",
    "    progress_bar = tqdm(range(len(books)))\n",
    "    for i in range(len(books)):\n",
    "        text = prefix + books[i][\"translation\"][source_lang]\n",
    "        ref = books[i][\"translation\"][target_lang]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device).input_ids\n",
    "        preds_tokenized = model_small_distill.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "        preds = tokenizer.batch_decode(preds_tokenized)\n",
    "        \n",
    "        bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "        scores_distill.append(bleu_score[\"score\"])\n",
    "        progress_bar.update(1)\n",
    "    print(f\"After epoch {t+1} the average score on the test set is {sum(scores_distill)/len(books)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd217e-3354-48ce-919b-6f9285896cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cd4bc-dcf0-4674-a4c3-936040dd3799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886b684a-a6d5-4c77-baf0-02aaaabea77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11fb3fc9a624fc6ac1600a33aa637a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For importance 0.0001 the average score on the test set is 0.35055862012379657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5203006aa74970805bd22c1312c692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For importance 0.01 the average score on the test set is 0.5082259002421322\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f68e33fffb4b18855cfe05145a23b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For importance 1.0 the average score on the test set is 0.2778737692090612\n"
     ]
    }
   ],
   "source": [
    "for importance in [1e-4, 1e-2, 1e-0]:\n",
    "    model_small_ewc = f\"model/flant5_small_lr_10-4_race_ewc_importance_{'{:.0e}'.format(importance)}_epoch2\"\n",
    "    model_small_ewc = AutoModelForSeq2SeqLM.from_pretrained(model_small_ewc, local_files_only=True).to(device)\n",
    "    scores_ewc = []\n",
    "    progress_bar = tqdm(range(len(books)))\n",
    "    for i in range(len(books)):\n",
    "        text = prefix + books[i][\"translation\"][source_lang]\n",
    "        ref = books[i][\"translation\"][target_lang]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device).input_ids\n",
    "        preds_tokenized = model_small_ewc.generate(inputs, max_new_tokens=128, do_sample=False) \n",
    "        preds = tokenizer.batch_decode(preds_tokenized)\n",
    "        \n",
    "        bleu_score = metric.compute(predictions=preds, references=[ref])\n",
    "        scores_ewc.append(bleu_score[\"score\"])\n",
    "        progress_bar.update(1)\n",
    "    print(f\"For importance {importance} the average score on the test set is {sum(scores_ewc)/len(books)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
